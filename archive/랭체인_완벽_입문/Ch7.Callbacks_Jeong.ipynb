{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "43e82aca-347a-45e5-9346-02ec4911c840",
   "metadata": {},
   "source": [
    "# Ch7.Callbacks - 다양한 이벤트 발생 시 처리하기\n",
    "- 1. Callbacks 모듈로 할 수 있는 일 알아보기\n",
    "- 2. Callbacks 모듈을 사용하여 외부 라이브러리와 연동하기\n",
    "- 3. 로그를 터미널에 표시할 수 있는 Callbacks 만들기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca6ea66a-d1c2-494c-810a-b6cc7e469f25",
   "metadata": {},
   "source": [
    "## 1. Callbacks 모듈로 할 수 있는 일 알아보기\n",
    "> 로그 수집 및 모니터링, 다른 애플리케이션과 연동 가능\n",
    "\n",
    "<img src=\"./img/lang_7_1.png\" width=\"70%\" height=\"70%\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a29d1f7-b60e-481c-8290-d440dbc37c6c",
   "metadata": {},
   "source": [
    "## 2. Callbacks 모듈을 사용하여 외부 라이브러리와 연동하기\n",
    ">  chainlit에서 제공하는 랭체인과 연동을 위한 기능을 통해 callbacks모듈의 작동을 알아본다.\n",
    "```\n",
    "chainlit.exe run chainlit_callback.py --port 8001\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9659fba9-16d6-4979-96fb-b2b35d772f71",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "chainlit_callback.py 파일에서 검색agent를 어치면 에러가 발생합니다.\n",
    "'''\n",
    "import chainlit as cl\n",
    "from langchain.agents import AgentType, initialize_agent, load_tools\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from dotenv import load_dotenv\n",
    "import warnings\n",
    "import os\n",
    "warnings.filterwarnings('ignore')\n",
    "load_dotenv()\n",
    "\n",
    "chat = ChatOpenAI(\n",
    "    temperature=0,  \n",
    "    model=\"gpt-4o\"\n",
    ")\n",
    "\n",
    "tools = load_tools( \n",
    "    [\n",
    "        \"serpapi\",\n",
    "    ],\n",
    "    serpapi_api_key=os.getenv(\"SERP_API_KEY\")\n",
    ")\n",
    "\n",
    "agent = initialize_agent(tools=tools, llm=chat, agent=AgentType.OPENAI_FUNCTIONS, verbose=True)\n",
    "\n",
    "@cl.on_chat_start\n",
    "async def on_chat_start():\n",
    "    await cl.Message(content=\"Agent 초기화 완료\").send() \n",
    "\n",
    "@cl.on_message\n",
    "async def on_message(input_message):\n",
    "    result = agent.run( #← Agent를 실행\n",
    "        input_message, #← 입력 메시지\n",
    "        callbacks=[ #← 콜백을 지정\n",
    "            cl.LangchainCallbackHandler() #← chainlit에 준비된 Callbacks를 지정\n",
    "        ]\n",
    "    )\n",
    "    await cl.Message(content=result).send()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab391321-607b-405a-a4c2-3cb0b74c1b12",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "callback함수를 직접 만들어서 실행도 가능 \n",
    "'''\n",
    "\n",
    "import chainlit as cl\n",
    "from langchain.agents import AgentType, initialize_agent, load_tools\n",
    "from langchain.callbacks.base import BaseCallbackHandler\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from dotenv import load_dotenv\n",
    "import warnings\n",
    "import os\n",
    "warnings.filterwarnings('ignore')\n",
    "load_dotenv()\n",
    "\n",
    "chat = ChatOpenAI(\n",
    "    temperature=0,  \n",
    "    model=\"gpt-3.5-turbo\"\n",
    ")\n",
    "\n",
    "tools = load_tools( \n",
    "    [\n",
    "        \"serpapi\",\n",
    "    ],\n",
    "    serpapi_api_key=os.getenv(\"SERP_API_KEY\")\n",
    ")\n",
    "\n",
    "class MyCustomHandlerOne(BaseCallbackHandler):\n",
    "    def on_llm_start(\n",
    "        self, serialized: Dict[str, Any], prompts: List[str], **kwargs: Any\n",
    "    ) -> Any:\n",
    "        print(f\"on_llm_start {serialized['name']}\")\n",
    "\n",
    "    def on_llm_new_token(self, token: str, **kwargs: Any) -> Any:\n",
    "        print(f\"on_new_token {token}\")\n",
    "\n",
    "    def on_llm_error(\n",
    "        self, error: Union[Exception, KeyboardInterrupt], **kwargs: Any\n",
    "    ) -> Any:\"\"\"Run when LLM errors.\"\"\"\n",
    "\n",
    "    def on_chain_start(\n",
    "        self, serialized: Dict[str, Any], inputs: Dict[str, Any], **kwargs: Any\n",
    "    ) -> Any:\n",
    "        print(f\"on_chain_start {serialized['name']}\")\n",
    "\n",
    "    def on_tool_start(\n",
    "        self, serialized: Dict[str, Any], input_str: str, **kwargs: Any\n",
    "    ) -> Any:\n",
    "        print(f\"on_tool_start {serialized['name']}\")\n",
    "\n",
    "    def on_agent_action(self, action: AgentAction, **kwargs: Any) -> Any:\n",
    "        print(f\"on_agent_action {action}\")\n",
    "\n",
    "handler = MyCustomHandlerOne()\n",
    "agent = initialize_agent(\n",
    "    tools=tools,\n",
    "    llm =chat,\n",
    "    agent = AgentType.CHAT_ZERO_SHOT_REACT_DESCRIPTION,\n",
    "    verbose=True)\n",
    "\n",
    "@cl.on_chat_start\n",
    "async def on_chat_start():\n",
    "    await cl.Message(content=\"Agent 초기화 완료\").send() \n",
    "\n",
    "@cl.on_message\n",
    "async def on_message(input_message):\n",
    "    result = agent.run( #← Agent를 실행\n",
    "        input_message, #← 입력 메시지\n",
    "        callbacks=[ #← 콜백을 지정\n",
    "            handler #← chainlit에 준비된 Callbacks를 지정\n",
    "        ]\n",
    "    )\n",
    "    await cl.Message(content=result).send()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77d527f7-b711-4b8c-a78e-b5dc5dd3018f",
   "metadata": {},
   "source": [
    "## 3. 로그를 터미널에 표시할 수 있는 Callbacks 만들기\n",
    "> 이벤트 발생시 로그를 터미널에 표시할 수 있는 기능을 만들어 Callbacks모듈의 사용법을 알아보자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bb5c3b9c-8404-43be-bdbe-3136a5e7db8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\miniconda\\envs\\lang310\\lib\\site-packages\\langchain_core\\_api\\deprecation.py:119: LangChainDeprecationWarning: The method `BaseChatModel.__call__` was deprecated in langchain-core 0.1.7 and will be removed in 0.3.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chat models 실행 시작....\n",
      "입력: [[HumanMessage(content='안녕하세요!')]]\n",
      "안녕하세요! 무엇을 도와드릴까요? 😊\n"
     ]
    }
   ],
   "source": [
    "from langchain.callbacks.base import BaseCallbackHandler #← BaseCallbackHandler 가져오기\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.schema import HumanMessage\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "load_dotenv()\n",
    "\n",
    "class LogCallbackHandler(BaseCallbackHandler): #← Callback을 생성\n",
    "\n",
    "    def on_chat_model_start(self, serialized, messages, **kwargs): #← Chat models 실행 시작 시 호출되는 처리를 정의\n",
    "        print(\"Chat models 실행 시작....\")\n",
    "        print(f\"입력: {messages}\")\n",
    "\n",
    "    def on_chain_start(self, serialized, inputs, **kwargs): #← Chain 실행 시작 시 호출되는 처리를 정의\n",
    "        print(\"Chain 실행 시작....\")\n",
    "        print(f\"입력: {inputs}\")\n",
    "\n",
    "chat = ChatOpenAI(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    api_key=os.getenv('OPENAI_API_KEY'),\n",
    "    callbacks=[ #← Chat models 초기화 시 Callback을 지정\n",
    "        LogCallbackHandler() #← 생성한 LogCallbackHandler를 지정\n",
    "    ]\n",
    ")\n",
    "\n",
    "result = chat([\n",
    "    HumanMessage(content=\"안녕하세요!\"),\n",
    "])\n",
    "\n",
    "print(result.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e85bcd09-ab57-4ba7-9803-47fab535e344",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lang310",
   "language": "python",
   "name": "lang310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
