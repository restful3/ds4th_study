{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oa3jiAqTVFx8"
   },
   "source": [
    "**19장 – 대규모 텐서플로 모델 훈련과 배포**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d0mmskPJVFx_"
   },
   "source": [
    "# 개요\n",
    "\n",
    "1. **모델의 실제 제품화**  \n",
    "   - 모델을 실제 제품에 장착하려면 시스템의 실시간 데이터에 적용해야 하며, 이를 위해 모델을 웹 서비스화해야 함.\n",
    "   - REST API를 사용해 시스템에서 언제든지 모델에 쿼리를 보낼 수 있음.\n",
    "   - 새로운 데이터를 바탕으로 모델을 정기적으로 재훈련하고 업데이트된 버전을 반영해야 함.\n",
    "   - 모델 버전 관리 및 A/B 테스트를 통해 여러 모델을 동시에 실행하거나 문제 발생 시 롤백이 가능해야 함.\n",
    "\n",
    "2. **서비스 확장과 안정성 확보**  \n",
    "   - 많은 QPS(초당 쿼리 수)를 처리하기 위해 서비스 규모를 확장해야 함.\n",
    "   - 구글 버텍스 AI 플랫폼이나 TF Serving을 활용해 효율적이고 안정적으로 모델 서비스 가능.\n",
    "   - 클라우드 플랫폼을 통해 모니터링 도구 등 부가 기능 활용 가능.\n",
    "\n",
    "3. **훈련 속도와 실험의 중요성**  \n",
    "   - 훈련 데이터가 많거나 복잡한 모델은 훈련 시간이 길어질 수 있음.\n",
    "   - 빠른 훈련과 실험을 위해 GPU/TPU와 같은 하드웨어 가속기와 분산 전략 API 사용.\n",
    "   - 새로운 아이디어를 실험하기 위해 훈련 속도는 매우 중요.\n",
    "\n",
    "4. **구체적인 학습과 배포 전략**  \n",
    "   - TF 서빙 및 구글 버텍스 AI 플랫폼을 활용해 모델 배포 방법 학습.\n",
    "   - GPU 및 분산 전략을 사용해 훈련 속도 향상.\n",
    "   - 버텍스 AI로 대규모 모델 훈련 및 하이퍼파라미터 튜닝 방법 학습 예정."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Sh9j-moeVFx_"
   },
   "source": [
    "<table align=\"left\">\n",
    "  <td>\n",
    "    <a href=\"https://colab.research.google.com/github/rickiepark/handson-ml3/blob/main/19_training_and_deploying_at_scale.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
    "  </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dFXIv9qNpKzt",
    "tags": []
   },
   "source": [
    "# 설정"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8IPbJEmZpKzu"
   },
   "source": [
    "이 프로젝트에는 Python 3.7 이상이 필요합니다:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "TFSU3FCOpKzu"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "assert sys.version_info >= (3, 7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GJtVEqxfpKzw"
   },
   "source": [
    "그리고 TensorFlow ≥ 2.8:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "0Piq5se2pKzx"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-30 09:14:36.494242: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-11-30 09:14:36.494269: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-11-30 09:14:36.494293: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-11-30 09:14:36.499042: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-11-30 09:14:37.033368: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "from packaging import version\n",
    "import tensorflow as tf\n",
    "\n",
    "assert version.parse(tf.__version__) >= version.parse(\"2.8.0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ODwjUF3aVFyC"
   },
   "source": [
    "코랩에서 실행하는 경우, 이 노트북의 뒷부분에서 사용하게 될 구글 AI 플랫폼 클라이언트 라이브러리를 설치해야 합니다. 버전 비호환성에 대한 경고는 무시해도 됩니다.\n",
    "\n",
    "* **경고**: 코랩에서는 설치 후 런타임을 다시 시작하고 다음 셀을 계속 진행해야 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DDl364CIVFyC",
    "outputId": "9b3786b1-c97d-4a40-8408-16dec577070a"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "if \"google.colab\" in sys.modules or \"kaggle_secrets\" in sys.modules:\n",
    "    %pip install -q -U google-cloud-aiplatform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YTsawKlapKzy"
   },
   "source": [
    "이 장에서는 하나 이상의 GPU에서 모델을 실행하거나 훈련하는 방법에 대해 설명하므로 적어도 하나 이상의 GPU가 있는지 확인하거나 그렇지 않으면 경고를 발행합니다:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "Ekxzo6pOpKzy"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-30 09:14:39.521756: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-11-30 09:14:39.542195: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-11-30 09:14:39.542374: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n"
     ]
    }
   ],
   "source": [
    "if not tf.config.list_physical_devices('GPU'):\n",
    "    print(\"GPU가 감지되지 않았습니다. 신경망은 GPU가 없으면 매우 느릴 수 있습니다.\")\n",
    "    if \"google.colab\" in sys.modules:\n",
    "        print(\"런타임 > 런타임 유형 변경으로 이동하여 하드웨어 가속기로 GPU를 선택합니다.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L81AL0m7VFyD"
   },
   "source": [
    "# 텐서플로 모델 서빙하기\n",
    "\n",
    "1. **모델 서빙의 필요성**  \n",
    "   - 텐서플로로 학습된 모델은 일반적으로 `predict()` 메서드 호출만으로 사용할 수 있습니다.  \n",
    "   - 그러나 시스템 규모가 커지면서 모델을 별도의 서비스로 감싸는 것이 바람직한 경우가 생깁니다.  \n",
    "   - 이러한 서비스는 예측 요청(REST/gRPC API 등)을 처리하며, 모델과 나머지 시스템을 분리하는 역할을 합니다.\n",
    "\n",
    "2. **모델 서빙의 장점**  \n",
    "   - **독립성**: 모델을 나머지 시스템과 분리하여 운영.  \n",
    "   - **유연성**: 모델 버전 변경이 용이하며, 필요에 따라 서비스 규모를 확장 가능.  \n",
    "   - **테스트와 개발 단순화**: A/B 테스트 실행과 동일한 모델 버전 유지가 수월.  \n",
    "\n",
    "3. **기술 구성**  \n",
    "   - 모델 서빙에는 다양한 기술이 활용됩니다(예: Flask와 같은 웹 프레임워크).  \n",
    "   - 텐서플로 자체 서빙 도구(TensorFlow Serving)를 이용하면 별도로 구현할 필요가 없습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UeTybvxdVFyD"
   },
   "source": [
    "먼저 TF 서빙을 사용하여 모델을 배포한 다음 Google Vertex AI에 배포해 보겠습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LZaFLkHCVFyD"
   },
   "source": [
    "## 텐서플로 서빙 사용하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TF 서빙의 기술적 특징**\n",
    "- C++로 작성되어 높은 성능과 효율성을 제공.\n",
    "- **주요 기능**:\n",
    "  - 높은 부하 처리 능력.\n",
    "  - 여러 모델 서비스 가능.\n",
    "  - 모델 저장소에서 최신 버전을 자동 배포.\n",
    "- TensorFlow 모델을 `SavedModel` 포맷으로 변환하여 서빙에 적합하게 준비해야 함.\n",
    "\n",
    "<img src=\"./images/fig_19_01.png\" width=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "goSzUGwCVFyD"
   },
   "source": [
    "### SavedModel 내보내기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cRFHavXnVFyD"
   },
   "source": [
    "- 모델 저장은 `model.save()` 메서드로 간단히 수행 가능."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "G5Nnv92XVFyD",
    "outputId": "0997a3c3-35d7-450d-c4a2-9761f8bccece"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-30 09:19:05.926501: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-11-30 09:19:05.926688: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-11-30 09:19:05.926819: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-11-30 09:19:05.989708: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-11-30 09:19:05.989866: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-11-30 09:19:05.990002: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-11-30 09:19:05.990107: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1886] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 22301 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:0b:00.0, compute capability: 8.6\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import tensorflow as tf\n",
    "\n",
    "# MNIST 데이터셋을 로드하고 학습/검증 데이터로 분할\n",
    "# load_data()를 통해 학습용과 테스트용 데이터를 받아옴\n",
    "mnist = tf.keras.datasets.mnist.load_data()\n",
    "(X_train_full, y_train_full), (X_test, y_test) = mnist\n",
    "\n",
    "# 전체 학습 데이터에서 앞의 5000개를 검증 데이터로 분리\n",
    "X_valid, X_train = X_train_full[:5000], X_train_full[5000:]\n",
    "y_valid, y_train = y_train_full[:5000], y_train_full[5000:]\n",
    "\n",
    "# 재현성을 위해 랜덤 시드 설정 및 이전 모델 초기화\n",
    "tf.random.set_seed(42)\n",
    "tf.keras.backend.clear_session()\n",
    "\n",
    "# Sequential API를 사용하여 간단한 MNIST 분류 모델 구축\n",
    "model = tf.keras.Sequential([\n",
    "    # 28x28 이미지를 1차원으로 펼침, uint8 입력을 받음\n",
    "    tf.keras.layers.Flatten(input_shape=[28, 28], dtype=tf.uint8),\n",
    "    # 픽셀값을 0-1 범위로 정규화\n",
    "    tf.keras.layers.Rescaling(scale=1 / 255),\n",
    "    # 100개 뉴런을 가진 은닉층, ReLU 활성화 함수 사용\n",
    "    tf.keras.layers.Dense(100, activation=\"relu\"),\n",
    "    # 10개 뉴런을 가진 출력층(0-9 숫자), softmax 활성화 함수 사용\n",
    "    tf.keras.layers.Dense(10, activation=\"softmax\")\n",
    "])\n",
    "\n",
    "# 모델 컴파일: 손실 함수, 옵티마이저, 평가 지표 설정\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "              optimizer=tf.keras.optimizers.SGD(learning_rate=1e-2),\n",
    "              metrics=[\"accuracy\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 학습: 10 에포크 동안 학습하며 검증 데이터로 성능 모니터링\n",
    "model.fit(X_train, y_train, epochs=10, validation_data=(X_valid, y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습된 모델을 SavedModel 형식으로 저장\n",
    "model_name = \"my_mnist_model\"\n",
    "model_version = \"0001\"\n",
    "model_path = Path(model_name) / model_version\n",
    "# model.save(model_path, save_format=\"tf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 일반적으로 내보낸 최종 모델에 모든 전처리 층을 포함하는 것이 좋습니다. \n",
    "- 이렇게 하면 제품으로 배포했을 때 원래 형태 그대로 데이터를 주입할 수 있습니다. \n",
    "- 또 모델을 사용하는 애플리케이션 내에서 전처리를 별도로 관리할 필요가 없습니다. \n",
    "- 모델 안에서 전처리 단계를 처리하면 나중에 모델을 업데이트하기가 훨씬 수월하고 모델과 필요한 전처리 단계가 맞지 않는 문제를 피할 수 있습니다.\n",
    "\n",
    ">**주의사항** : SavedModel이 계산 그래프를 저장하므로 임의의 파이썬 코드를 갖는 tf.py_function() 연산을 제외한 텐서플로 연산만을 사용한 모델에 적용 가능"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uIK2q8hoVFyE"
   },
   "source": [
    "파일 트리를 살펴보겠습니다(각 파일의 용도에 대해서는 10장에서 설명했습니다):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FPiU7aVqVFyE",
    "outputId": "127720df-f443-40b5-b387-bf2697a6d017"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['my_mnist_model/0001',\n",
       " 'my_mnist_model/0001/assets',\n",
       " 'my_mnist_model/0001/fingerprint.pb',\n",
       " 'my_mnist_model/0001/keras_metadata.pb',\n",
       " 'my_mnist_model/0001/saved_model.pb',\n",
       " 'my_mnist_model/0001/variables',\n",
       " 'my_mnist_model/0001/variables/variables.data-00000-of-00001',\n",
       " 'my_mnist_model/0001/variables/variables.index']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted([str(path) for path in model_path.parent.glob(\"**/*\")])  # 추가 코드"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7Cx61kc5VFyE"
   },
   "source": [
    "SavedModel을 검사해 보겠습니다:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CQfjlLGzVFyE",
    "outputId": "bbdb9a4b-4c8b-4746-f20c-af746da30dc6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-11-30 09:16:11.443117: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-11-30 09:16:11.443156: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-11-30 09:16:11.443180: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-11-30 09:16:11.447674: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-11-30 09:16:11.976206: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "2024-11-30 09:16:12.570178: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-11-30 09:16:12.590163: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-11-30 09:16:12.590327: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "The given SavedModel contains the following tag-sets:\n",
      "'serve'\n"
     ]
    }
   ],
   "source": [
    "# saved_model_cli를 사용하여 저장된 모델의 기본 정보를 표시합니다\n",
    "!saved_model_cli show --dir '{model_path}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "출력 내용: SavedModel에 포함된 **태그 세트(tag-sets)** 확인 가능 (예: `'serve'`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **메타그래프(MetaGraph)**:\n",
    "  - 계산 그래프와 **입력/출력 정보, 타입, 크기**를 포함.\n",
    "  - 각 메타그래프는 하나 이상의 태그(tag)로 구분.\n",
    "  - 예시 태그:\n",
    "    - `'train'`: 훈련 연산 포함.\n",
    "    - `'serve'`: 예측 연산 포함 (서빙에 사용).\n",
    "    - `'gpu'`: GPU 관련 연산 포함.\n",
    "- **태그의 역할**:\n",
    "  - 특정 태그를 지정해 해당 메타그래프만 선택적으로 사용할 수 있음.\n",
    "  - 포함된 SignatureDef 키 목록 확인 (`serving_default`, `__saved_model_init_op` 등).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RqGsNMdfVFyF",
    "outputId": "55095f17-e3b6-4590-d826-fb73abe10e7a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-11-30 09:16:21.274769: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-11-30 09:16:21.274800: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-11-30 09:16:21.274823: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-11-30 09:16:21.279306: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-11-30 09:16:21.809311: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "2024-11-30 09:16:22.395017: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-11-30 09:16:22.414662: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-11-30 09:16:22.414820: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "The given SavedModel MetaGraphDef contains SignatureDefs with the following keys:\n",
      "SignatureDef key: \"__saved_model_init_op\"\n",
      "SignatureDef key: \"serving_default\"\n"
     ]
    }
   ],
   "source": [
    "!saved_model_cli show --dir '{model_path}' --tag_set serve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " **SignatureDef 정의**\n",
    "- **SignatureDef**: 모델의 입력과 출력 서명을 정의하는 텐서플로 구성 요소.\n",
    "- 주요 서명:\n",
    "  - **`__saved_model_init_op`**: 초기화 함수 (대부분 신경 쓰지 않아도 됨).\n",
    "  - **`serving_default`**: 기본 서명으로, 모델 저장 시 자동 생성.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ue8ArznoVFyF",
    "outputId": "7dc2a352-1a46-4c42-d3f2-ab4d9e8f6d7d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-11-30 09:16:26.354583: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-11-30 09:16:26.354612: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-11-30 09:16:26.354635: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-11-30 09:16:26.359087: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-11-30 09:16:26.881159: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "2024-11-30 09:16:27.491130: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-11-30 09:16:27.511600: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-11-30 09:16:27.511762: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "The given SavedModel SignatureDef contains the following input(s):\n",
      "  inputs['flatten_input'] tensor_info:\n",
      "      dtype: DT_UINT8\n",
      "      shape: (-1, 28, 28)\n",
      "      name: serving_default_flatten_input:0\n",
      "The given SavedModel SignatureDef contains the following output(s):\n",
      "  outputs['dense_1'] tensor_info:\n",
      "      dtype: DT_FLOAT\n",
      "      shape: (-1, 10)\n",
      "      name: StatefulPartitionedCall:0\n",
      "Method name is: tensorflow/serving/predict\n"
     ]
    }
   ],
   "source": [
    "!saved_model_cli show --dir '{model_path}' --tag_set serve \\\n",
    "                      --signature_def serving_default"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 출력:\n",
    "  - **입력 (Input)**:\n",
    "    - 이름: `flatten_input`\n",
    "    - 데이터 타입: `DT_UINT8`\n",
    "    - 크기: `(-1, 28, 28)` (28x28 크기의 유연한 배치 크기)\n",
    "    - 텐서 이름: `serving_default_flatten_input:0`\n",
    "  - **출력 (Output)**:\n",
    "    - 이름: `dense_1`\n",
    "    - 데이터 타입: `DT_FLOAT`\n",
    "    - 크기: `(-1, 10)` (10개의 클래스 출력)\n",
    "    - 텐서 이름: `StatefulPartitionedCall:0`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F--NjCq2VFyF"
   },
   "source": [
    "더 자세한 내용을 보려면 다음 명령을 실행하세요:\n",
    "\n",
    "```ipython\n",
    "!saved_model_cli show --dir '{model_path}' --all\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-11-30 09:16:30.777324: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-11-30 09:16:30.777358: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-11-30 09:16:30.777386: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-11-30 09:16:30.781937: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-11-30 09:16:31.316273: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "2024-11-30 09:16:31.911653: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-11-30 09:16:31.931608: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-11-30 09:16:31.931812: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "\n",
      "MetaGraphDef with tag-set: 'serve' contains the following SignatureDefs:\n",
      "\n",
      "signature_def['__saved_model_init_op']:\n",
      "  The given SavedModel SignatureDef contains the following input(s):\n",
      "  The given SavedModel SignatureDef contains the following output(s):\n",
      "    outputs['__saved_model_init_op'] tensor_info:\n",
      "        dtype: DT_INVALID\n",
      "        shape: unknown_rank\n",
      "        name: NoOp\n",
      "  Method name is: \n",
      "\n",
      "signature_def['serving_default']:\n",
      "  The given SavedModel SignatureDef contains the following input(s):\n",
      "    inputs['flatten_input'] tensor_info:\n",
      "        dtype: DT_UINT8\n",
      "        shape: (-1, 28, 28)\n",
      "        name: serving_default_flatten_input:0\n",
      "  The given SavedModel SignatureDef contains the following output(s):\n",
      "    outputs['dense_1'] tensor_info:\n",
      "        dtype: DT_FLOAT\n",
      "        shape: (-1, 10)\n",
      "        name: StatefulPartitionedCall:0\n",
      "  Method name is: tensorflow/serving/predict\n",
      "The MetaGraph with tag set ['serve'] contains the following ops: {'Softmax', 'MergeV2Checkpoints', 'ShardedFilename', 'Cast', 'StatefulPartitionedCall', 'Placeholder', 'VarHandleOp', 'Reshape', 'MatMul', 'ReadVariableOp', 'Pack', 'NoOp', 'RestoreV2', 'Identity', 'Relu', 'AssignVariableOp', 'AddV2', 'StaticRegexFullMatch', 'StringJoin', 'Const', 'Select', 'SaveV2', 'DisableCopyOnRead', 'Mul', 'BiasAdd'}\n",
      "2024-11-30 09:16:31.947206: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-11-30 09:16:31.947366: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-11-30 09:16:31.947508: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-11-30 09:16:32.008981: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-11-30 09:16:32.009152: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-11-30 09:16:32.009294: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-11-30 09:16:32.009402: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1886] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 22301 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:0b:00.0, compute capability: 8.6\n",
      "\n",
      "Concrete Functions:\n",
      "  Function Name: '__call__'\n",
      "    Option #1\n",
      "      Callable with:\n",
      "        Argument #1\n",
      "          flatten_input: TensorSpec(shape=(None, 28, 28), dtype=tf.uint8, name='flatten_input')\n",
      "        Argument #2\n",
      "          DType: bool\n",
      "          Value: False\n",
      "        Argument #3\n",
      "          DType: NoneType\n",
      "          Value: None\n",
      "    Option #2\n",
      "      Callable with:\n",
      "        Argument #1\n",
      "          flatten_input: TensorSpec(shape=(None, 28, 28), dtype=tf.uint8, name='flatten_input')\n",
      "        Argument #2\n",
      "          DType: bool\n",
      "          Value: True\n",
      "        Argument #3\n",
      "          DType: NoneType\n",
      "          Value: None\n",
      "\n",
      "  Function Name: '_default_save_signature'\n",
      "    Option #1\n",
      "      Callable with:\n",
      "        Argument #1\n",
      "          flatten_input: TensorSpec(shape=(None, 28, 28), dtype=tf.uint8, name='flatten_input')\n",
      "\n",
      "  Function Name: 'call_and_return_all_conditional_losses'\n",
      "    Option #1\n",
      "      Callable with:\n",
      "        Argument #1\n",
      "          flatten_input: TensorSpec(shape=(None, 28, 28), dtype=tf.uint8, name='flatten_input')\n",
      "        Argument #2\n",
      "          DType: bool\n",
      "          Value: False\n",
      "        Argument #3\n",
      "          DType: NoneType\n",
      "          Value: None\n",
      "    Option #2\n",
      "      Callable with:\n",
      "        Argument #1\n",
      "          flatten_input: TensorSpec(shape=(None, 28, 28), dtype=tf.uint8, name='flatten_input')\n",
      "        Argument #2\n",
      "          DType: bool\n",
      "          Value: True\n",
      "        Argument #3\n",
      "          DType: NoneType\n",
      "          Value: None\n"
     ]
    }
   ],
   "source": [
    "!saved_model_cli show --dir '{model_path}' --all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**사용 목적**\n",
    "- **입출력 명세 확인**:\n",
    "  - 모델의 예상 입력/출력 형태와 데이터 타입 확인.\n",
    "- **TF Serving 연결**:\n",
    "  - 이 정보를 활용하여 다음 단계에서 텐서플로 서빙을 설정."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OvMBNdCDVFyF"
   },
   "source": [
    "### 텐서플로 서빙 설치하고 시작하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xcX97TewVFyF"
   },
   "source": [
    "이 노트북을 코랩에서 실행하는 경우, 텐서플로 서버를 설치해야 합니다:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pKXcSIgjVFyF",
    "outputId": "52b3a991-e5cc-466c-aa33-a9348f3287d6"
   },
   "outputs": [],
   "source": [
    "if \"google.colab\" in sys.modules:\n",
    "    url = \"https://storage.googleapis.com/tensorflow-serving-apt\"\n",
    "    src = \"stable tensorflow-model-server tensorflow-model-server-universal\"\n",
    "    !echo 'deb {url} {src}' > /etc/apt/sources.list.d/tensorflow-serving.list\n",
    "    !curl '{url}/tensorflow-serving.release.pub.gpg' | apt-key add -\n",
    "    !apt update -q && apt-get install -y tensorflow-model-server\n",
    "    %pip install -q -U tensorflow-serving-api==2.11.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n9gGFmALVFyF"
   },
   "source": [
    "`tensorflow_model_server`가 설치된 경우(예: Colab에서 이 노트북을 실행하는 경우) 다음 2개의 셀을 실행하여 서버를 시작하세요. 사용 중인 OS가 Windows인 경우, 터미널에서 `tensorflow_model_server` 명령을 실행하고 `${MODEL_DIR}`을 `my_mnist_model` 디렉터리의 전체 경로로 바꿔야 할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/mnt/mydrive/workspaces/study/ds4th_study/source/핸즈온_머신러닝/my_mnist_model')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_path.parent.absolute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "BT-t-vBeVFyG"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"MODEL_DIR\"] = str(model_path.parent.absolute())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**서버 실행**\n",
    "- 모델 디렉토리 경로 설정 (`MODEL_DIR`).\n",
    "  - 디렉토리 예시: `my_mnist_model`\n",
    "- 서버 실행 명령어:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "jD-tXMjiVFyG"
   },
   "outputs": [],
   "source": [
    "%%bash --bg\n",
    "tensorflow_model_server \\\n",
    "    --port=8500 \\\n",
    "    --rest_api_port=8501 \\\n",
    "    --model_name=my_mnist_model \\\n",
    "    --model_base_path=\"${MODEL_DIR}\" >my_server.log 2>&1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **`--rest_api_port`**: REST API 요청 수신 포트.\n",
    "- **`--port`**: gRPC 요청 수신 포트.\n",
    "- 실행 로그: `my_server.log`에 저장.\n",
    "- 텐서플로 서빙은 **백그라운드 프로세스**로 실행되며, 지정된 경로의 모델을 로드.\n",
    "- **REST 및 gRPC 요청**을 통해 모델을 배포하고 사용할 준비가 됨.\n",
    "- 설치와 실행 과정은 우분투 환경에서 진행 가능하며, Docker와 같은 대안도 존재."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 도커 컨테이너에서 TF 서빙 실행하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ueDMUZy7VFyG"
   },
   "source": [
    "개인 컴퓨터에서 이 노트북을 실행하는 경우, 도커를 사용해 TF 서빙을 설치하려면 먼저 [Docker](https://docs.docker.com/install/)가 설치되어 있는지 확인한 후 터미널에서 다음 명령을 실행하세요. `path/to/my_mnist_model`을 `my_mnist_model` 디렉토리의 적절한 절대 경로로 대체해야 하지만, 컨테이너 경로 `/models/my_mnist_model`은 수정하지 마세요.\n",
    "\n",
    "```bash\n",
    "docker pull tensorflow/serving  # 최신 TF 서빙 이미지 다운로드\n",
    "\n",
    "docker run -it --rm -v \"/path/to/my_mnist_model:/models/my_mnist_model\" \\\n",
    "    -p 8500:8500 -p 8501:8501 -e MODEL_NAME=my_mnist_model tensorflow/serving\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0AK-0xoYVFyG"
   },
   "source": [
    "### REST API로 TF 서빙에 쿼리하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8xaftl-rVFyG"
   },
   "source": [
    "- REST API를 사용해 요청 데이터를 전달하기 위해 JSON 포맷 사용.\n",
    "- 예시 `request_json`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "q0NbOueUVFyG"
   },
   "outputs": [],
   "source": [
    "# json 모듈을 임포트하여 JSON 데이터를 다루기 위해 사용\n",
    "import json\n",
    "\n",
    "# 테스트 데이터셋에서 처음 3개의 이미지를 선택\n",
    "X_new = X_test[:3]  # 분류할 새로운 숫자 이미지가 3개 있다고 가정합니다.\n",
    "\n",
    "# REST API 요청을 위한 JSON 형식 데이터 생성\n",
    "# signature_name: 모델의 서명 이름 지정 \n",
    "# instances: 예측할 데이터를 리스트 형태로 변환\n",
    "request_json = json.dumps({\n",
    "    \"signature_name\": \"serving_default\",\n",
    "    \"instances\": X_new.tolist(),\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "id": "l0hVGuAfVFyG",
    "outputId": "7f0f0d0f-3c9e-4d41-d403-86853dcc8e66"
   },
   "outputs": [],
   "source": [
    "request_json[:100] + \"...\" + request_json[-10:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_xdNZs2rVFyG"
   },
   "source": [
    "- Python의 `requests` 라이브러리를 사용해 HTTP POST 방식으로 전송.\n",
    "- 이제 텐서플로 서빙의 REST API를 사용하여 예측을 해보겠습니다:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "j5yu0zFBVFyG"
   },
   "outputs": [],
   "source": [
    "# HTTP 요청을 위한 requests 라이브러리 임포트\n",
    "import requests\n",
    "\n",
    "# TensorFlow Serving REST API 엔드포인트 URL 설정\n",
    "# localhost:8501은 도커 컨테이너의 포트와 매핑됨\n",
    "server_url = \"http://localhost:8501/v1/models/my_mnist_model:predict\"\n",
    "\n",
    "# POST 요청으로 데이터 전송\n",
    "# request_json: 예측할 이미지 데이터가 담긴 JSON 문자열\n",
    "response = requests.post(server_url, data=request_json)\n",
    "\n",
    "# HTTP 요청이 실패하면 예외 발생\n",
    "response.raise_for_status()  \n",
    "\n",
    "# JSON 응답을 파이썬 객체로 변환\n",
    "response = response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'predictions': [[4.68634571e-05,\n",
       "   1.73043745e-07,\n",
       "   0.000472726271,\n",
       "   0.00249943556,\n",
       "   7.34300272e-07,\n",
       "   9.44667481e-05,\n",
       "   8.48132853e-09,\n",
       "   0.996759713,\n",
       "   1.58201346e-05,\n",
       "   0.000110146204],\n",
       "  [0.000311168784,\n",
       "   9.07744616e-05,\n",
       "   0.980463922,\n",
       "   0.00837160461,\n",
       "   1.03811928e-08,\n",
       "   0.000230177204,\n",
       "   0.0100099817,\n",
       "   6.15892268e-11,\n",
       "   0.000522418355,\n",
       "   3.16814464e-09],\n",
       "  [2.73339283e-05,\n",
       "   0.979730487,\n",
       "   0.00756764784,\n",
       "   0.00127741753,\n",
       "   0.000332536845,\n",
       "   0.000896065088,\n",
       "   0.00156705291,\n",
       "   0.00592685817,\n",
       "   0.00247175898,\n",
       "   0.000202946205]]}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZlJmlvAIVFyK",
    "outputId": "dc6bf9e1-c474-43d2-bb27-7793b0bdeaa1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 1.  , 0.  , 0.  ],\n",
       "       [0.  , 0.  , 0.98, 0.01, 0.  , 0.  , 0.01, 0.  , 0.  , 0.  ],\n",
       "       [0.  , 0.98, 0.01, 0.  , 0.  , 0.  , 0.  , 0.01, 0.  , 0.  ]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# numpy 배열 처리를 위한 라이브러리 임포트\n",
    "import numpy as np\n",
    "\n",
    "# response의 predictions를 numpy 배열로 변환\n",
    "y_proba = np.array(response[\"predictions\"])\n",
    "\n",
    "# 소수점 2자리까지 반올림하여 예측 확률 출력\n",
    "y_proba.round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**REST API의 장단점**\n",
    "- **장점**:\n",
    "  - JSON 기반으로 간단하며 다양한 클라이언트에서 사용 가능.\n",
    "  - 텍스트 포맷으로 표현되어 유연성 높음.\n",
    "- **단점**:\n",
    "  - 대규모 데이터 전송 시 효율성 낮음.\n",
    "  - 숫자 데이터를 텍스트로 변환하면서 네트워크 대역폭 소모가 큼.\n",
    "\n",
    "\n",
    " **gRPC로 전환 필요성**\n",
    "- 대규모 데이터 전송 및 응답 속도가 중요한 경우, **gRPC** 추천.\n",
    "  - HTTP/2 기반으로 효율적이고 컴팩트한 데이터 전송 가능."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eEiYgEi_VFyK"
   },
   "source": [
    "### gRPC API로 TF 서빙에 쿼리하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **PredictRequest** 객체를 사용하여 gRPC 요청 생성."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "QUToMOrzVFyL"
   },
   "outputs": [],
   "source": [
    "# tensorflow serving의 predict_pb2 모듈에서 PredictRequest 클래스 임포트\n",
    "from tensorflow_serving.apis.predict_pb2 import PredictRequest\n",
    "\n",
    "# 예측 요청 객체 생성\n",
    "request = PredictRequest()\n",
    "\n",
    "# 모델 스펙 설정\n",
    "request.model_spec.name = model_name  # 사용할 모델 이름 지정\n",
    "request.model_spec.signature_name = \"serving_default\"  # 기본 시그니처 이름 사용\n",
    "\n",
    "# 입력 텐서 이름 가져오기 (첫 번째 입력 레이어의 이름)\n",
    "input_name = model.input_names[0]  # flatten_input 레이어\n",
    "\n",
    "# 입력 데이터를 텐서 프로토콜 버퍼로 변환하여 요청에 추가\n",
    "request.inputs[input_name].CopyFrom(tf.make_tensor_proto(X_new))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- gRPC 채널을 사용해 요청 전송.\n",
    "\n",
    ">스텁(Stub)은 분산 시스템에서 클라이언트와 서버 간의 통신을 단순화하는 프록시 객체입니다. gRPC 컨텍스트에서 특히 중요한 역할을 합니다. 통신 추상화, 직렬화/역직렬화 처리 등을 담당합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "JHBmIeE7VFyL"
   },
   "outputs": [],
   "source": [
    "# gRPC 관련 모듈 임포트\n",
    "import grpc\n",
    "from tensorflow_serving.apis import prediction_service_pb2_grpc\n",
    "\n",
    "# localhost:8500으로 gRPC 채널 생성\n",
    "channel = grpc.insecure_channel('localhost:8500')\n",
    "\n",
    "# 예측 서비스 스텁 생성\n",
    "predict_service = prediction_service_pb2_grpc.PredictionServiceStub(channel)\n",
    "\n",
    "# 예측 요청을 보내고 10초 타임아웃으로 응답 받기\n",
    "response = predict_service.Predict(request, timeout=10.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_a-7lekeVFyL"
   },
   "source": [
    "- 응답 데이터를 TensorFlow 배열로 변환."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "outputs {\n",
       "  key: \"dense_1\"\n",
       "  value {\n",
       "    dtype: DT_FLOAT\n",
       "    tensor_shape {\n",
       "      dim {\n",
       "        size: 3\n",
       "      }\n",
       "      dim {\n",
       "        size: 10\n",
       "      }\n",
       "    }\n",
       "    float_val: 4.6863457100698724e-05\n",
       "    float_val: 1.7304374466675654e-07\n",
       "    float_val: 0.0004727262712549418\n",
       "    float_val: 0.0024994355626404285\n",
       "    float_val: 7.343002721427183e-07\n",
       "    float_val: 9.446674812352285e-05\n",
       "    float_val: 8.481328528375798e-09\n",
       "    float_val: 0.9967597126960754\n",
       "    float_val: 1.5820134649402462e-05\n",
       "    float_val: 0.00011014620395144448\n",
       "    float_val: 0.0003111687838099897\n",
       "    float_val: 9.077446156879887e-05\n",
       "    float_val: 0.9804639220237732\n",
       "    float_val: 0.008371604606509209\n",
       "    float_val: 1.0381192794284289e-08\n",
       "    float_val: 0.00023017720377538353\n",
       "    float_val: 0.01000998169183731\n",
       "    float_val: 6.158922677412804e-11\n",
       "    float_val: 0.0005224183551035821\n",
       "    float_val: 3.1681446355236176e-09\n",
       "    float_val: 2.7333928301231936e-05\n",
       "    float_val: 0.979730486869812\n",
       "    float_val: 0.007567647844552994\n",
       "    float_val: 0.0012774175265803933\n",
       "    float_val: 0.00033253684523515403\n",
       "    float_val: 0.000896065088454634\n",
       "    float_val: 0.001567052910104394\n",
       "    float_val: 0.00592685816809535\n",
       "    float_val: 0.002471758984029293\n",
       "    float_val: 0.00020294620480854064\n",
       "  }\n",
       "}\n",
       "model_spec {\n",
       "  name: \"my_mnist_model\"\n",
       "  version {\n",
       "    value: 1\n",
       "  }\n",
       "  signature_name: \"serving_default\"\n",
       "}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "WQHAgtqRVFyL",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 모델의 출력 레이어 이름 가져오기\n",
    "output_name = model.output_names[0]\n",
    "# 응답에서 출력 텐서 프로토콜 버퍼 가져오기\n",
    "outputs_proto = response.outputs[output_name]\n",
    "# 텐서 프로토콜 버퍼를 텐서플로우 배열로 변환\n",
    "y_proba = tf.make_ndarray(outputs_proto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "5LBQEosaVFyL",
    "outputId": "94dda864-4c1a-4418-ab34-399a561885db"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 1.  , 0.  , 0.  ],\n",
       "       [0.  , 0.  , 0.98, 0.01, 0.  , 0.  , 0.01, 0.  , 0.  , 0.  ],\n",
       "       [0.  , 0.98, 0.01, 0.  , 0.  , 0.  , 0.  , 0.01, 0.  , 0.  ]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_proba.round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hoybdjXXVFyL"
   },
   "source": [
    "클라이언트에 텐서플로 라이브러리가 포함되어 있지 않은 경우, 다음과 같이 응답을 넘파이 배열로 변환할 수 있습니다:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4Ky9iJVvVFyL",
    "outputId": "36604828-56cd-48f5-851e-7c6486ba63b4"
   },
   "outputs": [],
   "source": [
    "# tf.make_ndarray() 대신 넘파이로 직접 변환하는 방법\n",
    "# 1. 모델의 출력 레이어 이름을 가져옵니다\n",
    "output_name = model.output_names[0]\n",
    "\n",
    "# 2. 응답에서 출력 텐서 프로토콜 버퍼를 가져옵니다\n",
    "outputs_proto = response.outputs[output_name]\n",
    "\n",
    "# 3. 텐서의 차원 정보를 추출하여 shape 리스트를 만듭니다\n",
    "shape = [dim.size for dim in outputs_proto.tensor_shape.dim]\n",
    "\n",
    "# 4. float_val을 넘파이 배열로 변환하고 원래 shape으로 재구성합니다\n",
    "y_proba = np.array(outputs_proto.float_val).reshape(shape)\n",
    "\n",
    "# 5. 소수점 2자리까지 반올림하여 출력합니다\n",
    "y_proba.round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **장점**:\n",
    "  - REST API보다 효율적이고 빠른 데이터 전송.\n",
    "  - 이진 데이터로 처리되어 네트워크 대역폭 절약.\n",
    "- **보안**:\n",
    "  - gRPC는 기본적으로 SSL/TLS 보안 채널 지원 가능.\n",
    "  - 위 예시는 보안을 설정하지 않음 (테스트용)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MhgWRCnFVFyM"
   },
   "source": [
    "### 새 모델 버전 배포하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GJVZGSMvVFyM",
    "outputId": "e61bcadd-a716-4750-d6df-854e758a3dcc",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 추가 코드 - 새로운 MNIST 모델 버전 빌드 및 훈련\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=[28, 28], dtype=tf.uint8),\n",
    "    tf.keras.layers.Rescaling(scale=1 / 255),\n",
    "    tf.keras.layers.Dense(50, activation=\"relu\"),\n",
    "    tf.keras.layers.Dense(50, activation=\"relu\"),\n",
    "    tf.keras.layers.Dense(10, activation=\"softmax\")\n",
    "])\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "              optimizer=tf.keras.optimizers.SGD(learning_rate=1e-2),\n",
    "              metrics=[\"accuracy\"])\n",
    "history = model.fit(X_train, y_train, epochs=10,\n",
    "                    validation_data=(X_valid, y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **SavedModel 포맷**으로 새로운 모델 버전을 저장."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FHACx7frVFyM",
    "outputId": "7524fe33-7012-463b-8824-f0d2ff46774a"
   },
   "outputs": [],
   "source": [
    "# 새로운 모델 버전 번호 지정\n",
    "model_version = \"0002\"\n",
    "\n",
    "# 모델을 저장할 경로 생성 (model_name/버전)\n",
    "model_path = Path(model_name) / model_version\n",
    "\n",
    "# 모델을 SavedModel 형식으로 저장\n",
    "model.save(model_path, save_format=\"tf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n6XXX9L7VFyM"
   },
   "source": [
    "파일 트리를 다시 한 번 살펴봅시다:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8ltGecbxVFyM",
    "outputId": "064b7489-5de3-4e79-c303-cac0b5161e5e"
   },
   "outputs": [],
   "source": [
    "sorted([str(path) for path in model_path.parent.glob(\"**/*\")])  # 추가 코드"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S31FFXYGVFyM"
   },
   "source": [
    "**텐서플로 서빙에서 모델 버전 관리**\n",
    "- 새로운 모델 버전을 디렉토리에 추가하면, 텐서플로 서빙은 **자동으로 버전을 탐지하고 교체**.\n",
    "- 기본 동작:\n",
    "  - 요청 중인 경우, 이전 버전의 모델이 응답을 마칠 때까지 유지.\n",
    "  - 새 버전 준비 완료 후 요청 처리 시작.\n",
    "  - 이전 버전은 안전하게 언로드(unload).\n",
    "\n",
    ">**경고**: 텐서플로 서빙이 새 모델을 로드하기까지 잠시 기다려야 할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "kFFZeS6RVFyM"
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "server_url = \"http://localhost:8501/v1/models/my_mnist_model:predict\"\n",
    "\n",
    "response = requests.post(server_url, data=request_json)\n",
    "response.raise_for_status()\n",
    "response = response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "R52p4WotVFyN",
    "outputId": "7f34637f-0350-4147-ed5f-2c9f0845730d"
   },
   "outputs": [],
   "source": [
    "response.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MOo5le0TVFyN",
    "outputId": "a9987c71-e18e-47b0-992f-98838929dd07"
   },
   "outputs": [],
   "source": [
    "y_proba = np.array(response[\"predictions\"])\n",
    "y_proba.round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " **모델 배포 중 고려 사항**\n",
    "- **RAM 사용량**:\n",
    "  - 새 버전 로딩 시 기존 버전과 동시 로드되므로 RAM 사용량 증가.\n",
    "  - GPU 사용 시 특히 주의 필요.\n",
    "- **서비스 중단 방지**:\n",
    "  - 텐서플로 서빙은 요청을 모두 응답한 후 모델 전환을 완료하여 중단 방지.\n",
    "\n",
    "**배치 처리 옵션**\n",
    "- **`--enable_batching`**:\n",
    "  - 요청을 일정 시간 동안 배치로 처리하여 성능 향상 가능.\n",
    "  - 배치 매개변수를 설정하면 GPU 활용 극대화 가능.\n",
    "\n",
    "**롤백 방법**\n",
    "- 새 버전이 정상 작동하지 않는 경우:\n",
    "  - 새로운 디렉토리 삭제(예: `my_mnist_model/0002`).\n",
    "\n",
    "**고성능 배포를 위한 추가 설정**\n",
    "- **부하가 많은 경우**:\n",
    "  - 여러 서버에 텐서플로 서빙 설치 및 로드 밸런싱 사용.\n",
    "  - **쿠버네티스(Kubernetes)**:\n",
    "    - 다수의 컨테이너 기반 서버 관리.\n",
    "    - 클라우드 서비스(AWS, GCP 등)와 연계 가능.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/fig_19_02.png\" width=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LFmBWIlFVFyN"
   },
   "source": [
    "## 버텍스 AI에서 예측 서비스 만들기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**버텍스 AI(Vetex AI)란?**\n",
    "- Google Cloud Platform(GCP)의 AI 플랫폼.\n",
    "- 주요 기능:\n",
    "  - 데이터셋 업로드 및 관리(Feature Store).\n",
    "  - 자동 하이퍼파라미터 튜닝.\n",
    "  - AutoML을 통한 모델 아키텍처 탐색.\n",
    "  - GPU 및 TPU를 사용한 모델 학습.\n",
    "  - REST/gRPC를 통해 대규모 모델 서빙.\n",
    "  - Workbench로 데이터와 모델 실험 가능."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/fig_19_03.png\" width=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**GCP에서 설정해야 할 준비 사항**\n",
    "1. **GCP 계정 생성 및 콘솔 이동**:\n",
    "   - GCP 계정이 없다면 신규 계정을 만들어야 함.\n",
    "\n",
    "2. **GCP 무료 크레딧 제공**:\n",
    "   - 신규 사용자에게 $300 크레딧 제공(90일 사용 가능).\n",
    "   - 무료 체험 이후 서비스를 계속 사용하려면 결제 정보 업데이트 필요.\n",
    "\n",
    "3. **프로젝트 생성**:\n",
    "   - 모든 GCP 리소스(가상 서버, 데이터, 모델 등)는 프로젝트 단위로 관리됨.\n",
    "   - GCP 콘솔에서 새 프로젝트 생성 가능.\n",
    "   - 프로젝트 생성 후 결제 계정 활성화 필수.\n",
    "\n",
    "4. **결제 계정 확인**:\n",
    "   - GCP 사용 전 결제 계정이 활성화되어 있어야 함.\n",
    "   - 예상 비용 확인 및 불필요한 서비스 종료로 비용 절감 가능.\n",
    "\n",
    "5. **필요 API 활성화**:\n",
    "   - Vertex AI API 및 Cloud Storage API 활성화 필요.\n",
    "\n",
    "\n",
    "**버텍스 AI의 장점**\n",
    "- **대규모 데이터 처리**:\n",
    "  - 대량의 데이터를 효과적으로 학습 및 관리 가능.\n",
    "- **통합 관리**:\n",
    "  - 모델 학습, 관리, 서빙까지 GCP 플랫폼에서 원활히 수행.\n",
    "- **추가 서비스**:\n",
    "  - 컴퓨터 비전, 번역, 스피치-투-텍스트 등 다양한 AI API 포함.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TAWb6eAFVFyN"
   },
   "source": [
    "책의 가이드를 따라 구글 클라우드 플랫폼 계정을 생성하고 버텍스 AI 및 클라우드 스토리지 API를 활성화하세요. 그런 다음, 코랩에서 이 노트북을 실행하는 경우 다음 셀을 실행하여 구글 클라우드 플랫폼에서 사용한 것과 동일한 구글 계정으로 인증하고 코랩이 데이터에 액세스할 수 있도록 권한을 부여할 수 있습니다.\n",
    "\n",
    "**경고: 이 노트북을 신뢰하는 경우에만 이 작업을 수행하세요!**\n",
    "* https://github.com/rickiepark/handson-ml3 에 있는 공식 노트북이 아닌 경우 특히 주의하세요: 코랩 URL은 https://colab.research.google.com/github/rickiepark/handson-ml3 으로 시작합니다. 그렇지 않으면 이 코드가 여러분의 데이터로 원하는 모든 작업을 수행할 수 있습니다.\n",
    "\n",
    "코랩에서 이 노트북을 실행하지 않는 경우, 책의 가이드를 따라 서비스 계정을 만들고 해당 서비스 계정의 키를 생성한 다음, 이 노트북의 디렉터리에 다운로드하고 이름을 `my_service_account_key.json`으로 지정해야 합니다(또는 `GOOGLE_APPLICATION_CREDENTIALS` 환경 변수가 이 파일을 가리키도록 합니다)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2G24_xI5VFyN"
   },
   "outputs": [],
   "source": [
    "project_id = \"my_project\"  ##### 이를 프로젝트 ID로 변경합니다. #####\n",
    "\n",
    "if \"google.colab\" in sys.modules:\n",
    "    from google.colab import auth\n",
    "    auth.authenticate_user()\n",
    "else:\n",
    "    os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = \"my_service_account_key.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CZFTG7xVVFyN"
   },
   "outputs": [],
   "source": [
    "from google.cloud import storage\n",
    "\n",
    "bucket_name = \"my_bucket\"  ##### 고유한 버킷 이름으로 변경합니다. #####\n",
    "location = \"us-central1\"\n",
    "\n",
    "storage_client = storage.Client(project=project_id)\n",
    "bucket = storage_client.create_bucket(bucket_name, location=location)\n",
    "#bucket = storage_client.bucket(bucket_name)  # 버킷을 재사용하는 경우"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6lyxtLFvVFyN"
   },
   "outputs": [],
   "source": [
    "def upload_directory(bucket, dirpath):\n",
    "    dirpath = Path(dirpath)\n",
    "    for filepath in dirpath.glob(\"**/*\"):\n",
    "        if filepath.is_file():\n",
    "            blob = bucket.blob(filepath.relative_to(dirpath.parent).as_posix())\n",
    "            blob.upload_from_filename(filepath)\n",
    "\n",
    "upload_directory(bucket, \"my_mnist_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZQWOKJZyVFyN"
   },
   "outputs": [],
   "source": [
    "# 추가 코드 – upload_directory()의 훨씬 빠른 멀티 스레드 구현\n",
    "#           타깃 경로의 prefix를 받고 출력 기능도 있습니다.\n",
    "\n",
    "from concurrent import futures\n",
    "\n",
    "def upload_file(bucket, filepath, blob_path):\n",
    "    blob = bucket.blob(blob_path)\n",
    "    blob.upload_from_filename(filepath)\n",
    "\n",
    "def upload_directory(bucket, dirpath, prefix=None, max_workers=50):\n",
    "    dirpath = Path(dirpath)\n",
    "    prefix = prefix or dirpath.name\n",
    "    with futures.ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        future_to_filepath = {\n",
    "            executor.submit(\n",
    "                upload_file,\n",
    "                bucket, filepath,\n",
    "                f\"{prefix}/{filepath.relative_to(dirpath).as_posix()}\"\n",
    "            ): filepath\n",
    "            for filepath in sorted(dirpath.glob(\"**/*\"))\n",
    "            if filepath.is_file()\n",
    "        }\n",
    "        for future in futures.as_completed(future_to_filepath):\n",
    "            filepath = future_to_filepath[future]\n",
    "            try:\n",
    "                result = future.result()\n",
    "            except Exception as ex:\n",
    "                print(f\"{filepath!s:60} 업로드 에러: {ex}\")  # f!s is str(f)\n",
    "            else:\n",
    "                print(f\"{filepath!s:60} 업로드 완료\", end=\"\\r\")\n",
    "\n",
    "    print(f\"{dirpath!s:60} 업로드 완료\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 구글 클라우드 CLI와 셀\n",
    "\n",
    " **구글 클라우드 CLI**\n",
    "- **gcloud 명령줄 인터페이스**:\n",
    "  - GCP의 거의 모든 리소스를 제어 가능.\n",
    "  - 포함된 도구:\n",
    "    - **`gcloud`**: GCP 관리 명령어 제공.\n",
    "    - **`gsutil`**: 스토리지와 상호작용 도구.\n",
    "  - 예시 명령어:\n",
    "    ```bash\n",
    "    gcloud config list\n",
    "    ```\n",
    "    - 현재 설정을 표시.\n",
    "\n",
    "- **CLI 인증**:\n",
    "  - CLI에서 **`google.auth.authenticate_user()`**를 호출하여 인증.\n",
    "\n",
    "2. **구글 클라우드 셸 (Cloud Shell)**\n",
    "- **웹 브라우저 기반 환경**:\n",
    "  - GCP에서 제공하는 사전 설정된 셸 환경.\n",
    "  - 실행 환경:\n",
    "    - 무료 리눅스 가상 머신(데비안 기반).\n",
    "    - GCP SDK 사전 설치 및 설정 완료.\n",
    "  - 장점:\n",
    "    - 추가 인증 과정 불필요.\n",
    "    - GCP 어느 곳에서나 사용 가능.\n",
    "\n",
    "- **활성화 방법**:\n",
    "  - GCP 콘솔 오른쪽 상단 **Cloud Shell 아이콘 클릭**.\n",
    "\n",
    "3. **CLI 설치 및 초기화**\n",
    "- **CLI 설치**:\n",
    "  - [설치 링크](https://homl.info/gcloud) 방문.\n",
    "  - 설치 후 **`gcloud init`** 명령 실행.\n",
    "    - GCP 계정으로 로그인.\n",
    "    - GCP 리소스에 대한 액세스 권한 부여.\n",
    "    - 기본 프로젝트 및 리전(region) 선택."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/fig_19_04.png\" width=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KHoge3Q3VFyO"
   },
   "source": [
    "또는 구글 클라우드 CLI를 설치한 경우(코랩에는 이미 설치되어 있음) 다음 `gsutil` 명령을 사용할 수 있습니다:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HRuJkd_0VFyO"
   },
   "outputs": [],
   "source": [
    "#!gsutil -m cp -r my_mnist_model gs://{bucket_name}/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_VDUUbp5VFyO",
    "outputId": "c9db6903-1bf9-4599-87cc-3591f043965f"
   },
   "outputs": [],
   "source": [
    "from google.cloud import aiplatform\n",
    "\n",
    "server_image = \"gcr.io/cloud-aiplatform/prediction/tf2-gpu.2-8:latest\"\n",
    "\n",
    "aiplatform.init(project=project_id, location=location)\n",
    "mnist_model = aiplatform.Model.upload(\n",
    "    display_name=\"mnist\",\n",
    "    artifact_uri=f\"gs://{bucket_name}/my_mnist_model/0001\",\n",
    "    serving_container_image_uri=server_image,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "whzT_ehIVFyO"
   },
   "source": [
    "**경고**: 이 셀은 버텍스 AI가 컴퓨팅 노드를 프로비저닝할 때까지 기다리므로 실행하는 데 몇 분 정도 걸릴 수 있습니다:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "G1IyPtz1VFyO",
    "outputId": "8c69a01d-c4ee-4a75-d299-26648801d68e"
   },
   "outputs": [],
   "source": [
    "endpoint = aiplatform.Endpoint.create(display_name=\"mnist-endpoint\")\n",
    "\n",
    "endpoint.deploy(\n",
    "    mnist_model,\n",
    "    min_replica_count=1,\n",
    "    max_replica_count=5,\n",
    "    machine_type=\"n1-standard-4\",\n",
    "    accelerator_type=\"NVIDIA_TESLA_K80\",\n",
    "    accelerator_count=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sh-j5somVFyO"
   },
   "outputs": [],
   "source": [
    "response = endpoint.predict(instances=X_new.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AYxcRJwDVFyO",
    "outputId": "1febf806-62ff-4c47-eadf-695f75f6cb6a"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.round(response.predictions, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3LHeL9N3VFyP",
    "outputId": "3c24c16a-660f-4ee5-f491-b59ed228afde"
   },
   "outputs": [],
   "source": [
    "endpoint.undeploy_all()  # 엔드포인트에서 모든 모델 배포 취소\n",
    "endpoint.delete()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GCP에서의 인증 및 권한 부여\n",
    "\n",
    "1. **OAuth 2.0 인증**\n",
    "- **사용자 데이터를 대신 접근**하려면 OAuth 2.0 사용.\n",
    "  - 예: 애플리케이션이 사용자 구글 드라이브 데이터 접근 시 OAuth 인증 필요.\n",
    "  - 일반적으로 제한된 액세스 권한만 부여하며, 무제한 액세스는 허용되지 않음.\n",
    "  - 승인은 일정 시간이 지나면 만료되며, 취소 가능.\n",
    "\n",
    "2. **서비스 계정 (Service Account)**\n",
    "- 애플리케이션이 **사용자를 대신하지 않고 자체적으로 GCP 서비스**에 액세스할 경우 사용.\n",
    "- 예: Vertex AI 엔드포인트로 요청을 보내는 웹사이트.\n",
    "- **서비스 계정 생성 방법**:\n",
    "  1. GCP 콘솔 → **IAM 및 관리자** → **서비스 계정** → **서비스 계정 만들기** 클릭.\n",
    "  2. 이름, ID, 설명 입력 후 **계속**.\n",
    "  3. 필요한 역할(예: Vertex AI 사용자) 부여.\n",
    "  4. 생성 후 키(JSON 파일)를 다운로드하여 보관.\n",
    "\n",
    "3. **서비스 계정 인증**\n",
    "- GCP에서 실행되는 경우:\n",
    "  - 가상 머신 인스턴스나 앱 엔진과 같은 GCP 리소스에 자동 연결.\n",
    "- 로컬 환경에서 실행되는 경우:\n",
    "  - JSON 키 파일을 **`GOOGLE_APPLICATION_CREDENTIALS`** 환경 변수에 설정.\n",
    "- 쿠버네티스 사용 시:\n",
    "  - 워크로드 아이덴티티(WIF)를 통해 각 서비스 계정을 매칭하여 안전하게 인증.\n",
    "\n",
    "4. **JSON 키 파일 관리**\n",
    "- 키 파일은 서비스 계정의 **민감 정보**를 포함.\n",
    "- 반드시 안전한 위치에 저장하고, 외부에 노출되지 않도록 관리."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZG1aRQH8VFyP"
   },
   "source": [
    "## 버텍스 AI에서 배치 예측 작업 실행하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x7BYQamwVFyP",
    "outputId": "7e378644-5adf-4e1f-9efa-e69a655de3e6"
   },
   "outputs": [],
   "source": [
    "batch_path = Path(\"my_mnist_batch\")\n",
    "batch_path.mkdir(exist_ok=True)\n",
    "with open(batch_path / \"my_mnist_batch.jsonl\", \"w\") as jsonl_file:\n",
    "    for image in X_test[:100].tolist():\n",
    "        jsonl_file.write(json.dumps(image))\n",
    "        jsonl_file.write(\"\\n\")\n",
    "\n",
    "upload_directory(bucket, batch_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a1nJfIMmVFyP",
    "outputId": "5b1bbe76-0087-4a37-8d0a-ac2b231c8b6c"
   },
   "outputs": [],
   "source": [
    "batch_prediction_job = mnist_model.batch_predict(\n",
    "    job_display_name=\"my_batch_prediction_job\",\n",
    "    machine_type=\"n1-standard-4\",\n",
    "    starting_replica_count=1,\n",
    "    max_replica_count=5,\n",
    "    accelerator_type=\"NVIDIA_TESLA_K80\",\n",
    "    accelerator_count=1,\n",
    "    gcs_source=[f\"gs://{bucket_name}/{batch_path.name}/my_mnist_batch.jsonl\"],\n",
    "    gcs_destination_prefix=f\"gs://{bucket_name}/my_mnist_predictions/\",\n",
    "    sync=True  # 완료될 때까지 기다리지 않으려면 False로 설정합니다.\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4WZQ_8pSVFyP",
    "outputId": "ec40e240-6268-4d31-f840-c806a915031d"
   },
   "outputs": [],
   "source": [
    "batch_prediction_job.output_info  # 추가 코드 - 출력 디렉토리를 표시합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bR00V4WQVFyP",
    "outputId": "6ae8557f-e898-4719-c4e9-ff1c4872dba1"
   },
   "outputs": [],
   "source": [
    "y_probas = []\n",
    "for blob in batch_prediction_job.iter_outputs():\n",
    "    print(blob.name)  # 추가 코드\n",
    "    if \"prediction.results\" in blob.name:\n",
    "        for line in blob.download_as_text().splitlines():\n",
    "            y_proba = json.loads(line)[\"prediction\"]\n",
    "            y_probas.append(y_proba)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2d8g7tm2VFyP"
   },
   "outputs": [],
   "source": [
    "y_pred = np.argmax(y_probas, axis=1)\n",
    "accuracy = np.sum(y_pred == y_test[:100]) / 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XPdtKXXbVFyQ",
    "outputId": "198a316e-51ee-4744-82a1-c165358d60c3"
   },
   "outputs": [],
   "source": [
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x2zVcwXYVFyQ",
    "outputId": "a990da25-90b1-4053-a68d-aeec5f1a0712"
   },
   "outputs": [],
   "source": [
    "mnist_model.delete()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G6LZ70CtVFyQ"
   },
   "source": [
    "GCS에서 만든 모든 디렉토리(즉, 디렉토리 이름의 접두사를 가진 모든 블롭)를 삭제해 보겠습니다:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "82rmtTCOVFyQ",
    "outputId": "2c9c0b27-7edc-4bad-eef2-5fea74df6b0f"
   },
   "outputs": [],
   "source": [
    "for prefix in [\"my_mnist_model/\", \"my_mnist_batch/\", \"my_mnist_predictions/\"]:\n",
    "    blobs = bucket.list_blobs(prefix=prefix)\n",
    "    for blob in blobs:\n",
    "        blob.delete()\n",
    "\n",
    "#bucket.delete()  # 버킷 자체를 삭제하려면 주석을 제거하고 실행하세요.\n",
    "batch_prediction_job.delete()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8aWy9ECgVFyQ"
   },
   "source": [
    "# 모바일 또는 임베디드 디바이스에 모델 배포하기\n",
    "\n",
    "- **엣지 컴퓨팅 (Edge Computing):**\n",
    "  - 머신러닝 모델이 중앙 서버 대신 모바일 및 임베디드 디바이스와 같이 데이터 소스에 가까운 곳에서 실행되는 방식을 의미합니다.\n",
    "  - 장점:\n",
    "    - 인터넷 연결 없이도 디바이스에서 스마트한 기능 유지.\n",
    "    - 서버로 데이터 전송이 필요 없어서 지연 시간 감소 및 서버 부하 경감.\n",
    "    - 데이터가 디바이스에 남아 있어 개인 정보 보호에 유리.\n",
    "\n",
    "- **단점:**\n",
    "  - 디바이스의 컴퓨팅 자원이 GPU 서버에 비해 약함.\n",
    "  - 큰 모델은 디바이스에서 실행이 어려움.\n",
    "  - RAM과 CPU 사용량이 많아지면 배터리 소모 및 성능 저하.\n",
    "\n",
    "- **해결 방안:**\n",
    "  - TFLite(TensorFlow Lite) 라이브러리를 사용하여 모델을 디바이스에 맞게 최적화:\n",
    "    1. **모델 크기 줄이기**:\n",
    "       - 다운로드 시간 단축 및 RAM 사용량 감소.\n",
    "    2. **응답 속도 및 배터리 사용량 최적화**:\n",
    "       - 계산량을 줄여 예측 속도 개선.\n",
    "    3. **디바이스 제약 조건에 맞춤화**:\n",
    "       - 디바이스의 리소스와 제한 사항에 맞게 모델 조정.\n",
    "\n",
    "---\n",
    "\n",
    "**TFLite 모델 변환 과정**\n",
    "- **SavedModel → FlatBuffers 변환**:\n",
    "  - FlatBuffers는 경량 포맷으로, 모델을 압축하여 메모리 사용량과 로드 시간을 줄임.\n",
    "  - FlatBuffers의 특징:\n",
    "    - RAM으로 로드할 때 성능 최적화.\n",
    "    - 모바일 또는 임베디드 디바이스에서 효율적으로 실행 가능.\n",
    "  - 결과물은 `.tflite` 형식으로 저장.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "N-O3Sc0DVFyQ",
    "outputId": "faad2986-11a2-4b74-cb56-da7700ad00f0"
   },
   "outputs": [],
   "source": [
    "converter = tf.lite.TFLiteConverter.from_saved_model(str(model_path))\n",
    "tflite_model = converter.convert()\n",
    "with open(\"my_converted_savedmodel.tflite\", \"wb\") as f:\n",
    "    f.write(tflite_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**모델 최적화와 크기 축소**\n",
    "\n",
    "1. **연산 최적화**:\n",
    "   - 예측에 필요하지 않은 연산(예: 훈련 연산)을 제거하거나 가능한 연산을 단순화 및 결합하여 최적화.\n",
    "   - 예: \\(3a + 4x + a + 5x^2\\) → \\(12x^2 + 4a\\).\n",
    "\n",
    "2. **모델 압축 및 확인 도구**:\n",
    "   - 사전 훈련된 TFLite 모델(Inception_V1_quant)을 다운로드 후 압축 해제 가능.\n",
    "   - Netron(https://lutzroeder.github.io/netron)으로 `.pb` 파일이나 `.tflite` 모델의 구조를 그래프로 시각화."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "# 모델 다운로드 URL\n",
    "model_url = \"https://github.com/google-coral/edgetpu/raw/master/test_data/inception_v1_224_quant.tflite\"\n",
    "output_file = \"inception_v1_224_quant.tflite\"\n",
    "\n",
    "# 모델 다운로드\n",
    "response = requests.get(model_url)\n",
    "if response.status_code == 200:\n",
    "    with open(output_file, \"wb\") as file:\n",
    "        file.write(response.content)\n",
    "    print(f\"모델 다운로드 완료: {output_file}\")\n",
    "else:\n",
    "    print(f\"모델 다운로드 실패. 상태 코드: {response.status_code}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!netron inception_v1_224_quant.tflite"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**모델 크기 축소 방법**\n",
    "\n",
    "1. **반정밀도 숫자 사용**:\n",
    "   - 32비트 대신 16비트의 반정밀도 실수(half-float)를 사용하여 모델 크기를 절반으로 줄이고, 훈련 속도와 GPU RAM 사용량도 크게 절감.\n",
    "\n",
    "2. **가중치 양자화**:\n",
    "   - **Post-training Quantization**:\n",
    "     - 훈련 후 모델 가중치를 고정 소수점(8비트 정수)로 압축.\n",
    "     - 32비트 실수와 비교해 4배 더 작은 크기.\n",
    "   - 대략적인 과정:\n",
    "     1. 가장 큰 절댓값 \\(m\\) 찾기.\n",
    "     2. 값 범위를 \\(-m\\)에서 \\(+m\\)까지 고정.\n",
    "     3. 이 범위를 \\(-127\\)에서 \\(+127\\)로 매핑.\n",
    "   - 예:\n",
    "     - 원래 가중치 범위: \\(-1.5\\) ~ \\(+1.5\\).\n",
    "     - 매핑 후: \\(-127, 0, +127\\)로 정수 변환.\n",
    "\n",
    "<img src=\"./images/fig_19_05.png\" width=\"800\">\n",
    "\n",
    "3. **`convert()` 메서드 활용**:\n",
    "   - 변환 최적화 리스트에 `tf.lite.Optimize.DEFAULT`를 추가하여 기본적인 최적화를 수행.\n",
    "\n",
    "**효과**\n",
    "- 모델 크기 축소로 다운로드 및 저장 공간 절약.\n",
    "- 양자화 후 속도 감소는 거의 없으며 RAM 사용량 최소화.\n",
    "- 모델의 정확도 저하를 최소화하면서도 효율적인 실행 가능.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "UqtZBZ2bVFyR"
   },
   "outputs": [],
   "source": [
    "# 추가 코드 - 케라스 모델을 변환하는 방법을 보여줍니다.\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "lHCMe8ekVFyR"
   },
   "outputs": [],
   "source": [
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mXONZCdrVFyR",
    "outputId": "7df9e9bc-5507-45ca-a081-2593debddc22"
   },
   "outputs": [],
   "source": [
    "tflite_model = converter.convert()\n",
    "with open(\"my_converted_keras_model.tflite\", \"wb\") as f:\n",
    "    f.write(tflite_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!netron my_converted_keras_model.tflite"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**양자화(Quantization)의 효과와 활용**\n",
    "1. **활성화 출력의 양자화**\n",
    "   - **효과**:\n",
    "     - CPU 사용량 감소 및 전력 소비 절약.\n",
    "     - 정수 계산(예: 8비트 정수) 사용으로 속도와 에너지 효율 증가.\n",
    "   - **활용 사례**:\n",
    "     - TPU(Edge TPU) 같은 가속 장치는 정수 연산에 최적화되어 있어 효율적으로 처리 가능.\n",
    "\n",
    "2. **최대 활성화 절댓값 설정**\n",
    "   - 양자화를 수행하기 전 **활성화 값 통계**를 수집해 최적의 절댓값 범위를 설정.\n",
    "   - **Calibration 단계**: 대표 샘플 데이터를 모델에 입력해 활성화 범위를 결정.\n",
    "   - **Tip**: 적은 샘플 데이터로도 활성화 통계를 측정하여 모델을 통과시키는 양자화 작업 가능.\n",
    "\n",
    "3. **정확도와 손실 문제**\n",
    "   - 양자화는 **정확도를 약간 희생**하는 대가로 전력과 속도를 대폭 향상.\n",
    "   - **정확도 손실을 줄이는 방법**:\n",
    "     - \"양자화를 고려한 훈련(Quantization-aware training)\" 사용.\n",
    "     - 양자화된 환경을 고려해 추가 연산을 포함하여 훈련 진행.\n",
    "   - 최종 가중치가 양자화된 값에 안정적으로 수렴."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JI3RubYxVFyR"
   },
   "source": [
    "# 웹 페이지에서 모델 실행하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**1. 브라우저에서 모델 실행의 장점**\n",
    "- **느린 네트워크 상황에서 유용**:\n",
    "  - 사용자 연결이 제한적이거나 느린 상황에서, 클라이언트 측에서 모델을 실행하면 앱이 안정적으로 작동 가능.\n",
    "  - 예: 등산객을 위한 오프라인 웹사이트.\n",
    "  \n",
    "- **응답 속도 향상**:\n",
    "  - 예측 시 서버에 쿼리하지 않아도 되므로 웹사이트의 응답 속도가 크게 개선. (온라인 게임 등)\n",
    "\n",
    "- **데이터 보안**:\n",
    "  - 비공개 데이터를 클라이언트 측에서 처리하여, 개인 정보가 외부로 유출되지 않도록 보호.\n",
    "\n",
    "---\n",
    "\n",
    "**2. TensorFlow.js(TFJS)를 활용한 클라이언트 측 모델 실행**\n",
    "- **TensorFlow.js 소개**:\n",
    "  - 자바스크립트 라이브러리로, TFLite 모델을 로드하고 사용자 브라우저에서 실행 가능.\n",
    "  - 사전 훈련된 모델(MobileNet 등)을 로드하여 이미지 분류, 예측 등을 수행.\n",
    "\n",
    "- **실습 사이트**:\n",
    "  - [Glitch](https://homl.info/tfjswpa): 자바스크립트 코드 예제를 테스트할 수 있는 사이트.\n",
    "  - 사이트 내 `[PREVIEW]` 버튼 클릭으로 코드 실행 결과 확인 가능.\n",
    "\n",
    "```html\n",
    "\n",
    "import \"https://cdn.jsdelivr.net/npm/@tensorflow/tfjs@latest\";\n",
    "import \"https://cdn.jsdelivr.net/npm/@tensorflow-models/mobilenet@1.0.0\";\n",
    "\n",
    "const image = document.getElementById(\"image\");\n",
    "\n",
    "mobilenet.load().then(model => {\n",
    "    model.classify(image).then(predictions => {\n",
    "        for (var i = 0; i < predictions.length; i++) {\n",
    "            let className = predictions[i].className;\n",
    "            let proba = (predictions[i].probability * 100).toFixed(1);\n",
    "            console.log(className + \": \" + proba + \"%\");\n",
    "        }\n",
    "    });\n",
    "});\n",
    "\n",
    "\n",
    "```\n",
    "- 다음 명령어로 webserver 실행 필요.\n",
    "\n",
    "```bash\n",
    "python -m http.server 8000\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**3. 프로그레시브 웹 앱(PWA)으로의 확장**\n",
    "- **PWA란?**:\n",
    "  - 클라이언트 측에서 작동하며 독립 실행형 앱처럼 설치 및 실행 가능.\n",
    "  - 심지어 모바일 디바이스에서 독립 앱처럼 작동.\n",
    "\n",
    "- **PWA의 특징**:\n",
    "  - 홈 화면에 추가하여 독립적인 앱처럼 실행 가능.\n",
    "  - 오프라인에서도 작동 가능(서비스 워커 기능 사용).\n",
    "  - 백그라운드 작업 수행 및 푸시 메시지 지원.\n",
    "\n",
    "- **PWA 코드 샘플**:\n",
    "  - Glitch의 [PWA 코드 예제](https://homl.info/wpacode)에서 확인 가능.\n",
    "\n",
    "- **활용 예시**\n",
    "  - TFJS를 활용한 웹 애플리케이션 제작으로 이미지 분류, 예측, 데이터 분석 등 다양한 작업 가능.\n",
    "  - PWA로 확장하여 모바일 및 웹에서 일관된 사용자 경험 제공.\n",
    "\n",
    "---\n",
    "\n",
    "**4. TFJS와 GPU 활용**\n",
    "- **TFJS의 GPU 지원**:\n",
    "  - TFJS는 **WebGL**을 사용하여 GPU 가속을 지원.\n",
    "  - NVIDIA GPU뿐만 아니라 최신 웹 브라우저에서 광범위한 GPU 카드 지원.\n",
    "\n",
    "- **사용 사례**:\n",
    "  - 브라우저에서 모델 학습 가능.\n",
    "  - 클라이언트 측에서 사용자 데이터를 비공개로 유지하며 학습(연합 학습, Federated Learning).\n",
    "  - 모델 학습 후 로컬 데이터 기반으로 미세 튜닝 가능.\n",
    "\n",
    "- **관련 자료**:\n",
    "  - [연합 학습](https://tensorflow.org/federated) 참조."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OwuPe70DVFyR"
   },
   "source": [
    "이 섹션의 코드 예제는 웹 앱을 무료로 만들 수 있는 웹사이트인 glitch.com에서 호스팅됩니다.\n",
    "\n",
    "* https://homl.info/tfjscode: 사전 학습된 모델을 로드하고 이미지를 분류하는 간단한 TFJS 웹 앱입니다.\n",
    "* https://homl.info/tfjswpa: WPA로 설정된 동일한 웹 앱. 모바일 장치를 포함한 다양한 플랫폼에서 이 링크를 열어 보세요.\n",
    "** https://homl.info/wpacode: 이 WPA의 소스 코드입니다.\n",
    "* https://tensorflow.org/js: TFJS 라이브러리.\n",
    "** https://www.tensorflow.org/js/demos: 재미있는 데모 몇 가지를 소개합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y4VfgmwVVFyR"
   },
   "source": [
    "# 계산 속도를 높이기 위해 GPU 사용하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **배경:** \n",
    "  - 대규모 신경망을 CPU만으로 훈련 시 시간이 매우 오래 걸림.\n",
    "  - GPU 사용 시 훈련 시간을 크게 단축할 수 있음.\n",
    "\n",
    "- **GPU 사용의 이점:**\n",
    "  - 훈련 시간을 몇 시간 또는 몇 분으로 줄일 수 있음.\n",
    "  - 모델 실험 및 데이터 업데이트 주기를 더 빠르게 실행 가능.\n",
    "\n",
    "- **GPU 활용 방법:**\n",
    "  - 구글 코랩에서 GPU 런타임 변경으로 간단히 사용 가능.\n",
    "  - 코드 호환성 유지: GPU가 없는 환경에서도 동일하게 실행 가능.\n",
    "  - 여러 GPU가 포함된 노드에서도 작동.\n",
    "\n",
    "- **구체적인 활용:**\n",
    "  - Vertex AI 모델 배포 시 GPU 선택 가능 (`endpoint.deploy()`).\n",
    "  - 단일 머신의 CPU 및 GPU, 다수 GPU 장치 활용 방법 논의 예정.\n",
    "  - 서버 계산 분산 방법도 추가로 설명.\n",
    "\n",
    "<img src=\"./images/fig_19_06.png\" width=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPU 구매하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **구매 필요성 판단:**\n",
    "  - GPU를 장기적으로 많이 사용할 경우 직접 구매하는 것이 경제적으로 유리.\n",
    "  - 로컬 환경에서 데이터 처리 및 훈련을 원하는 경우 적합.\n",
    "\n",
    "- **GPU 구매 시 고려 사항:**\n",
    "  - 작업 특성에 따라 필요한 RAM 용량, 대역폭, 코어 수, 냉각 시스템 등 검토 필요.\n",
    "  - 예시: 이미지 처리 및 NLP는 최소 10GB RAM 권장.\n",
    "  - Tim Dettmery의 블로그(https://homl.info/66)에서 상세 정보 확인 가능.\n",
    "\n",
    "- **호환성 및 지원:**\n",
    "  - TensorFlow와 CUDA 계산 능력 3.5 이상의 Nvidia GPU만 지원.\n",
    "  - 다른 제조사의 장치를 지원하는 가능성도 있으므로 TensorFlow 문서 참고.\n",
    "\n",
    "- **Nvidia GPU 설정:**\n",
    "  - Nvidia 드라이버 및 CUDA (Compute Unified Device Architecture) 라이브러리 설치 필요.\n",
    "    - NVIDIA가 만든 **GPU를 위한 소프트웨어 개발 도구**\n",
    "  - cuDNN (CUDA Deep Neural Network Library)을 통해 DNN 연산 가속 가능 (추론 및 학습 속도 향상).\n",
    "    - CUDA 위에서 동작하는 **딥러닝 최적화 라이브러리**\n",
    "  - Nvidia 딥러닝 SDK 및 개발자 계정 필요.\n",
    "\n",
    "<img src=\"./images/fig_19_07.png\" width=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **설치 확인:**\n",
    "  - Nvidia 드라이버 및 라이브러리 설치 후 `nvidia-smi` 명령어를 사용하여 GPU 설치 상태 점검 가능.\n",
    "  - 이 명령어는 사용 가능한 GPU 카드, 실행 중인 프로세스, 카드 사양 등을 확인할 수 있음."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pWEx4a31VFyR"
   },
   "source": [
    "텐서플로우가 GPU를 볼 수 있는지 확인해 보겠습니다:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xVJNYyroVFyR",
    "outputId": "e5daa069-3b41-49cd-dee2-efbbf0a42d50"
   },
   "outputs": [],
   "source": [
    "physical_gpus = tf.config.list_physical_devices(\"GPU\")\n",
    "physical_gpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h20yT_y6VFyR"
   },
   "source": [
    "텐서플로 스크립트에서 GPU \\#0과 \\#1(PCI 순서 기준)만 사용하려면 스크립트를 시작하기 전에 환경 변수 `CUDA_DEVICE_ORDER=PCI_BUS_ID`와 `CUDA_VISIBLE_DEVICES=0,1`을 설정합니다. 또는 스크립트 자체에서 텐서플로를 사용하기 전에 지정할 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_zNPhsERVFyS"
   },
   "source": [
    "## GPU RAM 관리하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **기본 동작:**\n",
    "  - TensorFlow는 실행 시 자동으로 사용 가능한 GPU의 대부분 RAM을 할당함.\n",
    "  - 다른 프로그램이 GPU를 사용할 경우 RAM 부족 현상이 발생할 수 있음.\n",
    "\n",
    "- **다중 프로그램 실행 시 GPU RAM 분배:**\n",
    "  - 동일 컴퓨터에서 여러 TensorFlow 프로그램을 실행하려면 GPU RAM을 균등하게 나누어 사용해야 함.\n",
    "  - `CUDA_VISIBLE_DEVICES` 환경 변수를 설정해 특정 GPU를 각 프로세스에 할당 가능.\n",
    "  - `CUDA_DEVICE_ORDER` 환경 변수를 `PCI_BUS_ID`로 설정하여 GPU ID를 고정적으로 유지 가능.\n",
    "\n",
    "- **예시 명령어:**\n",
    "  - GPU 카드 4개 중 2개를 각기 다른 프로그램에 할당하는 경우:\n",
    "\n",
    "    ```bash\n",
    "    CUDA_DEVICE_ORDER=PCI_BUS_ID CUDA_VISIBLE_DEVICES=0,1 python3 program_1.py\n",
    "    CUDA_DEVICE_ORDER=PCI_BUS_ID CUDA_VISIBLE_DEVICES=2,3 python3 program_2.py\n",
    "    ```\n",
    "\n",
    "- **환경 변수 설정:**\n",
    "  - TensorFlow 실행 전에 아래와 같이 Python 스크립트에서 환경 변수를 설정 가능:\n",
    "  \n",
    "    ```python\n",
    "    os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1\"\n",
    "    ```\n",
    "\n",
    "<img src=\"./images/fig_19_08.png\" width=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HItL_tIeVFyS"
   },
   "source": [
    "- **GPU RAM 할당량 제한**\n",
    "  - TensorFlow가 GPU마다 특정 양의 RAM만 사용하도록 설정 가능.\n",
    "  - 예시 코드: 각 GPU에 2GiB 메모리 할당\n",
    "    ```python\n",
    "    for gpu in physical_gpus:\n",
    "        tf.config.set_logical_device_configuration(\n",
    "            gpu,\n",
    "            [tf.config.LogicalDeviceConfiguration(memory_limit=2048)]\n",
    "        )\n",
    "    ```\n",
    "  - 이를 통해 4GiB RAM을 가진 GPU 4개를 두 개의 프로그램에서 나누어 사용할 수 있음.\n",
    "\n",
    "  <img src=\"./images/fig_19_09.png\" width=\"800\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ByepjvQaVFyS"
   },
   "outputs": [],
   "source": [
    "#for gpu in physical_gpus:\n",
    "#    tf.config.set_logical_device_configuration(\n",
    "#        gpu,\n",
    "#        [tf.config.LogicalDeviceConfiguration(memory_limit=2048)]\n",
    "#    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cKiDo2ciVFyS"
   },
   "source": [
    "- **필요할 때만 메모리 점유**\n",
    "  - 점진적으로 텐서플로가 메모리를 점유하도록 하려면(프로세스가 종료될 때만 메모리를 해제합니다):\n",
    "  - 예시 코드:\n",
    "    ```python\n",
    "    for gpu in physical_gpus:\n",
    "        tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gQ2WN76YVFyS"
   },
   "outputs": [],
   "source": [
    "#for gpu in physical_gpus:\n",
    "#    tf.config.experimental.set_memory_growth(gpu, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Az0uZINuVFyS"
   },
   "source": [
    "- `TF_FORCE_GPU_ALLOW_GROWTH`를 `True`로 설정하면 TensorFlow가 필요에 따라 GPU 메모리를 점유 및 해제 가능.\n",
    "  - 이 방법은 메모리 고정으로 인한 충돌을 방지하지만, 결정적인 행동을 보장하기 어려움.\n",
    "  - 주로 코랩 런타임이나 단일 GPU를 다중 논리적 장치로 나누는 경우 유용.\n",
    "  - 예를 들어, GPU #0을 두 개의 논리적 GPU로 나누고 각 논리적 장치에 2GiB의 메모리 한도를 설정:\n",
    "    ```python\n",
    "    tf.config.set_logical_device_configuration(\n",
    "        physical_gpus[0],\n",
    "        [\n",
    "            tf.config.LogicalDeviceConfiguration(memory_limit=2048),\n",
    "            tf.config.LogicalDeviceConfiguration(memory_limit=2048)\n",
    "        ]\n",
    "    )\n",
    "    ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qU0QW2dEVFyS"
   },
   "source": [
    "- 물리적 GPU를 두 개의 논리적 GPU로 분할합니다:\n",
    "```python\n",
    "tf.config.set_logical_device_configuration(\n",
    "   physical_gpus[0],\n",
    "   [tf.config.LogicalDeviceConfiguration(memory_limit=2048),\n",
    "    tf.config.LogicalDeviceConfiguration(memory_limit=2048)]\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JJfMvtwmVFyT"
   },
   "outputs": [],
   "source": [
    "# tf.config.set_logical_device_configuration(\n",
    "#    physical_gpus[0],\n",
    "#    [tf.config.LogicalDeviceConfiguration(memory_limit=2048),\n",
    "#     tf.config.LogicalDeviceConfiguration(memory_limit=2048)]\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 생성된 논리적 GPU를 확인하는 코드:\n",
    "    ```python\n",
    "    logical_gpus = tf.config.list_logical_devices(\"GPU\")\n",
    "    print(logical_gpus)\n",
    "    ```\n",
    "- 결과: `/gpu:0`, `/gpu:1`와 같은 논리적 장치 이름 확인 가능.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "xZuKsErMVFyT",
    "outputId": "7049c814-01be-49f6-fb8e-94e6c010cd69"
   },
   "outputs": [],
   "source": [
    "# logical_gpus = tf.config.list_logical_devices(\"GPU\")\n",
    "# logical_gpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dHhdR7oKVFyT"
   },
   "source": [
    "## 디바이스에 연산 및 변수 배치하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IW9RswfNVFyT"
   },
   "source": [
    "- **기본 동작:**\n",
    "  - TensorFlow는 `tf.data`를 사용해 연산과 변수를 자동으로 적절히 배치.\n",
    "  - 세부적인 제어가 필요한 경우, 디바이스별로 연산과 변수를 수동 배치 가능.\n",
    "\n",
    "- **배치 원칙:**\n",
    "  1. 데이터 전처리는 CPU에서 처리, 신경망 연산은 GPU에서 수행.\n",
    "  2. GPU는 대역폭 제약이 있으므로 불필요한 데이터 전송을 최소화해야 함.\n",
    "  3. CPU RAM은 여유롭게 사용 가능하지만, GPU RAM은 제한적이므로 중요 변수만 배치 권장.\n",
    "\n",
    "- **기본 배치 규칙:**\n",
    "  - GPU 커널이 있는 변수와 연산은 첫 번째 GPU (`/gpu:0`)에 할당.\n",
    "  - GPU 커널이 없는 경우 CPU (`/cpu:0`)에 할당.\n",
    "\n",
    "- **변수 및 연산 배치 확인:**\n",
    "  - TensorFlow 변수의 `device` 속성을 통해 변수나 연산이 할당된 디바이스 확인 가능.\n",
    "    ```python\n",
    "    a = tf.Variable([1., 2., 3.])  # float32 변수는 GPU로 이동\n",
    "    print(a.device)  # '/job:localhost/replica:0/task:0/device:GPU:0'\n",
    "\n",
    "    b = tf.Variable([1, 2, 3])  # int32 변수는 CPU로 이동\n",
    "    print(b.device)  # '/job:localhost/replica:0/task:0/device:CPU:0'\n",
    "    ```\n",
    "\n",
    "- **특징:**\n",
    "  - 정수 변수 등 GPU 커널이 없는 연산은 기본적으로 CPU에 배치됨.\n",
    "  - `/job:localhost/replica:0/task:0`는 무시 가능하며 디바이스의 실제 위치를 나타냄.\n",
    "\n",
    "모든 변수 및 연산 배치를 기록하려면(이 작업은 텐서플로를 임포팅한 직후에 실행해야 합니다):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-RVXD-xJVFyT"
   },
   "outputs": [],
   "source": [
    "#tf.get_logger().setLevel(\"DEBUG\")  # 디폴트 로그 수준은 INFO입니다.\n",
    "#tf.debugging.set_log_device_placement(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "soMZkk53VFyT",
    "outputId": "0465ee9c-3213-41e3-f39f-0774ada29729"
   },
   "outputs": [],
   "source": [
    "a = tf.Variable([1., 2., 3.])  # float32 변수는 GPU로 이동합니다.\n",
    "a.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "daZNR01OVFyT",
    "outputId": "288233f9-aac0-448d-b798-1af4a8d4652b"
   },
   "outputs": [],
   "source": [
    "b = tf.Variable([1, 2, 3])  # int32 변수는 CPU로 이동합니다.\n",
    "b.device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CVMtpGUTVFyU"
   },
   "source": [
    "`tf.device()` 컨텍스트를 사용하여 원하는 장치에 변수 및 연산을 수동으로 배치할 수 있습니다:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GpJqrz-cVFyU",
    "outputId": "11f838a4-1c74-43a4-dcab-4683416d8d56"
   },
   "outputs": [],
   "source": [
    "with tf.device(\"/cpu:0\"):\n",
    "    c = tf.Variable([1., 2., 3.])\n",
    "\n",
    "c.device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3kFjB8ByVFyU"
   },
   "source": [
    "존재하지 않거나 커널이 없는 장치를 지정하면 텐서플로는 자동으로 기본 장치를 사용합니다:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ne5CKMS5VFyU",
    "outputId": "996d47d1-571d-40c4-8136-519482ed0ab7"
   },
   "outputs": [],
   "source": [
    "# 추가 코드\n",
    "\n",
    "with tf.device(\"/gpu:1234\"):\n",
    "    d = tf.Variable([1., 2., 3.])\n",
    "\n",
    "d.device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gikk9azaVFyU"
   },
   "source": [
    "텐서플로에서 존재하지 않는 장치를 사용하려고 할 때 기본 장치로 되돌아가지 않고 예외를 발생시키려면:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FxOJsmC6VFyU",
    "outputId": "185d0eb0-b705-402a-93f2-a836a4e2a2fd"
   },
   "outputs": [],
   "source": [
    "tf.config.set_soft_device_placement(False)\n",
    "\n",
    "# 추가 코드\n",
    "try:\n",
    "    with tf.device(\"/gpu:1000\"):\n",
    "        d = tf.Variable([1., 2., 3.])\n",
    "except tf.errors.InvalidArgumentError as ex:\n",
    "    print(ex)\n",
    "\n",
    "tf.config.set_soft_device_placement(True)  # 추가 코드 - 소프트 배치로 돌아가기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "axTbpgvtVFyU"
   },
   "source": [
    "## 다중 장치에서 병렬 실행하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "텐서플로(TensorFlow)는 병렬 처리가 가능하도록 설계되어 있어, 여러 장치를 활용해 연산을 분산시켜 효율적으로 실행할 수 있습니다.\n",
    "\n",
    "1. **작업 분석 및 평가**\n",
    "   - 텐서플로는 실행 시 그래프를 분석해 각 연산의 의존성을 계산합니다.\n",
    "   - 의존성이 없는 연산(소스 연산)은 평가 큐(evaluation queue)에 추가됩니다.\n",
    "   - 연산이 평가되면 해당 연산에 의존하는 다른 연산들의 의존성 카운터가 감소하여 실행 준비 상태가 됩니다.\n",
    "\n",
    "2. **CPU 병렬 처리**\n",
    "   - CPU는 `inter-op` 스레드 풀과 `intra-op` 스레드 풀을 사용합니다.\n",
    "     - **Inter-op 스레드 풀**: 여러 연산이 다른 CPU 코어에서 병렬로 실행됩니다.\n",
    "     - **Intra-op 스레드 풀**: 하나의 연산 내에서 병렬 처리를 수행합니다.\n",
    "   - 예를 들어, 다중 스레드 CPU 커널이 있는 연산은 `intra-op` 스레드 풀을 통해 효율적으로 병렬 처리됩니다.\n",
    "\n",
    "3. **GPU 병렬 처리**\n",
    "   - GPU는 텐서플로에서 CUDA 및 cuDNN 라이브러리를 사용해 병렬 처리를 수행합니다.\n",
    "   - GPU 자체적으로 스레드 풀을 가지고 있어 `inter-op` 스레드 풀이 필요 없습니다.\n",
    "   - GPU의 연산은 대부분 GPU 스레드를 사용해 병렬로 실행됩니다.\n",
    "\n",
    "4. **실행 과정 예시**\n",
    "   - [그림 19-10]을 참고로, 연산 A, B, C는 의존성이 없으므로 바로 평가됩니다.\n",
    "   - CPU와 GPU의 큐로 각각 분배된 후 병렬로 계산됩니다.\n",
    "   - D, E는 C의 완료를 기다린 후 평가됩니다.\n",
    "   - 의존성이 모두 해소된 F가 마지막으로 실행됩니다.\n",
    "\n",
    "5. **순차 실행 보장**\n",
    "   - 변수나 리소스를 수정하는 연산의 경우, 텐서플로는 코드 작성 순서를 보장해 순차적으로 실행합니다.\n",
    "\n",
    "\n",
    "<img src=\"./images/fig_19_10.png\" width=\"800\">\n",
    "\n",
    "\n",
    "6. **여러 GPU에서 모델 병렬 훈련**\n",
    "   - 각 GPU에 하나의 모델을 배치하여 병렬로 훈련 가능.\n",
    "   - CUDA_DEVICE_ORDER와 CUDA_VISIBLE_DEVICES를 사용하여 특정 GPU를 지정.\n",
    "   - 하이퍼파라미터 튜닝에 유용.\n",
    "   - 예: GPU 2개로 병렬 훈련 시, 단일 GPU로 하나의 모델을 학습 시키는데 1시간 걸릴 작업이,  2개 GPU로도 동일한 시간 내 두개의 모델을 병렬 수행 가능.\n",
    "\n",
    "7. **GPU와 CPU의 협업**\n",
    "   - GPU가 모델을 훈련하는 동안 CPU는 데이터를 전처리.\n",
    "   - `prefetch()` 메서드를 활용하여 GPU가 데이터를 요청하기 전에 미리 준비 가능.\n",
    "\n",
    "8. **CNN과 병렬 처리**\n",
    "   - 입력 데이터를 2개로 나누고 각 데이터를 다른 GPU에서 병렬 처리.\n",
    "   - 결과를 합쳐 더욱 빠르게 CNN 계산 가능.\n",
    "\n",
    "9. **효율적인 앙상블 모델 구축**\n",
    "   - GPU에서 개별 모델을 병렬 훈련 후, 앙상블 결과를 최종적으로 통합하여 예측 성능 향상.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4Ar1AawCVFyU"
   },
   "source": [
    "inter-op 또는 intra-op 스레드 수를 설정하려는 경우(CPU 포화를 방지하거나 완벽하게 재현 가능한 테스트 케이스를 실행하기 위해 텐서플로를 단일 스레드로 만들고자 하는 경우 유용할 수 있습니다):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7u9Yrl-SVFyV"
   },
   "outputs": [],
   "source": [
    "#tf.config.threading.set_inter_op_parallelism_threads(10)\n",
    "#tf.config.threading.set_intra_op_parallelism_threads(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uMH0X27cVFyV",
    "tags": []
   },
   "source": [
    "# 다중 장치에서 모델 훈련하기\n",
    "\n",
    "다중 장치에서 모델을 훈련하는 두 가지 방법:\n",
    "\n",
    "1. **모델 병렬화 (Model Parallelism)**  \n",
    "   - 하나의 모델을 여러 장치에 나누어 분할하여 각 장치가 모델의 일부를 처리.\n",
    "\n",
    "2. **데이터 병렬화 (Data Parallelism)**  \n",
    "   - 동일한 모델 복사본을 여러 장치에 배치하고, 데이터를 나누어 각 장치에서 병렬로 처리."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 모델 병렬화"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **개념**  \n",
    "  - 모델의 각 부분을 여러 장치에 나누어 병렬적으로 실행하는 방식.\n",
    "\n",
    "- **장점**  \n",
    "  - 특정 구조의 신경망(예: 합성곱 신경망)의 경우 효율적으로 병렬화 가능.\n",
    "\n",
    "<img src=\"./images/fig_19_11.png\" width=\"800\">\n",
    "\n",
    "- **단점**  \n",
    "  - **통신 지연**: 장치 간 통신이 빈번하며 특히 다른 머신 사이에서는 속도가 느려짐.  \n",
    "  - **의존성 문제**: 각 층이 이전 층의 출력을 기다려야 하므로 병렬 효율성이 저하됨.  \n",
    "  - **구현 복잡성**: 많은 셀을 병렬로 실행해야 하는 경우 설정과 조율이 복잡.\n",
    "\n",
    "<img src=\"./images/fig_19_12.png\" width=\"800\">\n",
    "\n",
    "- **적용 사례**  \n",
    "  - 순환 신경망(LSTM): 실질적으로 병렬화보다는 한 GPU에서 처리하는 것이 더 효율적.\n",
    "  - 합성곱 신경망: 일부 층이 부분적으로만 연결되어 있어 병렬화 효율이 비교적 높음.\n",
    "\n",
    "<img src=\"./images/fig_19_13.png\" width=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 데이터 병렬화\n",
    "\n",
    "- **개념**  \n",
    "  - 동일한 모델 복사본을 여러 장치에 배치하고, 각 장치에 다른 미니배치를 할당하여 병렬적으로 훈련.  \n",
    "  - 각 장치에서 계산된 그레디언트를 평균화하여 모델 파라미터를 업데이트.  \n",
    "  - **SPMD(Single Program, Multiple Data)** 접근 방식.\n",
    "\n",
    "- **장점**  \n",
    "  - **효율성:** 병렬화 구현이 비교적 간단하며, 통신 비용이 모델 병렬화보다 적음.  \n",
    "  - **확장성:** GPU나 장치를 추가할수록 병렬 처리 능력 향상.  \n",
    "\n",
    "- **단점**  \n",
    "  - 결과를 통합하고 평균화하는 단계에서 통신 오버헤드 발생 가능.  \n",
    "  - 데이터의 불균형이나 동기화 문제가 있을 경우 성능 저하 가능.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 미러드 전략을 사용한 데이터 병렬화\n",
    "\n",
    "- **개념**  \n",
    "  - 모든 GPU에 동일한 모델 파라미터 복사본을 생성하고 항상 동일한 상태를 유지하며 훈련.  \n",
    "  - 이를 **미러드 전략(Mirrored Strategy)** 이라고 부름.\n",
    "\n",
    "<img src=\"./images/fig_19_14.png\" width=\"800\">\n",
    "\n",
    "- **장점**  \n",
    "  - **효율성:** 단일 머신에서 사용할 때 매우 효과적.  \n",
    "  - **동일 상태 유지:** 모든 복제된 모델이 항상 동일한 파라미터를 유지.\n",
    "\n",
    "- **알고리즘**  \n",
    "  - **올리듀스(AllReduce):**  \n",
    "    - 모든 GPU에서 계산된 그레디언트의 평균을 효율적으로 계산하고 이를 모든 GPU에 배포.  \n",
    "    - 여러 노드가 협력하여 평균, 합, 최댓값 등을 계산하는 방식.  \n",
    "    - 리듀스(reduce) 연산을 효율적으로 수행.\n",
    "      - 리듀스(Reduce) 연산은 분산 컴퓨팅에서 여러 데이터를 하나의 값으로 집계하는 연산을 의미\n",
    "\n",
    "- **한계**  \n",
    "  - 올리듀스는 효율적이지만 노드 간 통신 비용이 발생할 수 있음.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 중앙 집중적인 파라미터를 사용한 데이터 병렬화\n",
    "\n",
    "- **개념**  \n",
    "  - 계산을 수행하는 GPU 장치(워커) 외에 **파라미터 서버**(Parameter Server)를 사용하여 모델 파라미터를 저장 및 업데이트.  \n",
    "  - 미러드 전략과 달리 중앙 집중적으로 파라미터를 관리하며, 동기 업데이트와 비동기 업데이트 방식 사용 가능.\n",
    "\n",
    "\n",
    "**동기 업데이트 (Synchronous Update)**  \n",
    "- **작동 방식**  \n",
    "  - 모든 워커에서 그레디언트를 계산한 후 이를 평균화하여 모델 파라미터를 업데이트.  \n",
    "  - 업데이트 완료 후 모델 파라미터를 모든 장치에 복사.\n",
    "\n",
    "<img src=\"./images/fig_19_15.png\" width=\"800\">\n",
    "\n",
    "- **장점** : 모든 워커가 동기화된 상태로 동일한 모델 파라미터를 사용 가능.\n",
    "\n",
    "- **단점** : 느린 워커가 다른 워커들의 처리를 지연시키는 병목현상 발생.  \n",
    "\n",
    "- **최적화 방법 (Tip)**  \n",
    "  - 느린 복제 모델(10% 정도)을 무시하고 빠른 모델들로만 평균 그레디언트를 계산.  \n",
    "  - 예: 복제 모델 20개 중 가장 느린 2개를 제외하고 나머지 18개로 작업 진행.  \n",
    "\n",
    " **비동기 업데이트 (Asynchronous Update) 요약**\n",
    "\n",
    "- **개념**  \n",
    "  - 복제 모델이 각자 독립적으로 그레디언트를 계산한 후 바로 모델 파라미터를 업데이트.  \n",
    "  - 동기화 과정이 없으며, 각 복제 모델이 다른 복제 모델을 기다리지 않음.  \n",
    "\n",
    "**장점**  \n",
    "1. **효율성** : 병렬 처리의 장점이 극대화되어 더 많은 훈련 스텝을 빠르게 실행 가능.  \n",
    "2. **대역폭 감소** : 동기화 지연이 없어 통신 비용 감소.  \n",
    "\n",
    "**단점**  \n",
    "1. **불안정성:**  \n",
    "   - 각 복제 모델이 독립적으로 작동하여 모델 상태가 불안정해질 위험.  \n",
    "   - 복제 모델 간의 업데이트 간격으로 인해 **낡은 그레디언트(stale gradient)** 문제가 발생.  \n",
    "   - 낡은 그레디언트는 훈련의 수렴 속도를 느리게 하고 학습 곡선의 불안정을 초래.\n",
    "\n",
    "<img src=\"./images/fig_19_16.png\" width=\"800\">\n",
    "\n",
    "2. **균형 문제:**  \n",
    "   - 특정 복제 모델이 파라미터를 자주 업데이트하여 다른 복제 모델과 방향성이 달라질 위험.\n",
    "\n",
    "**해결 방법**  \n",
    "1. **학습률 감소:**  \n",
    "   - 지나친 업데이트로 인한 불안정성을 완화.  \n",
    "2. **낡은 그레디언트 무시:**  \n",
    "   - 너무 오래된 그레디언트는 버림.  \n",
    "3. **미니배치 크기 조정:**  \n",
    "   - 데이터 샘플 크기를 조절해 균형을 맞춤.  \n",
    "4. **준비 단계 도입:**  \n",
    "   - 하나의 복제 모델만 사용하여 처음 몇 번의 에포크 시작, 초기 에포크 동안 안정적으로 작동하도록 설정.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### 대역폭 포화\n",
    "\n",
    "- **문제 정의**  \n",
    "  동기 업데이트나 비동기 업데이트 중 어떤 것을 사용하든, 훈련 시 모델의 파라미터와 그레이디언트를 중앙 서버와 GPU 간에 전송해야 하는 문제가 발생.  \n",
    "  특히, 병렬적으로 작업하는 경우 각 GPU에서 계산된 그레이디언트를 공유해야 하므로 대역폭 포화 문제가 생김.\n",
    "\n",
    "- **성능 한계**  \n",
    "  GPU를 추가하더라도 네트워크 전송 속도 제한으로 성능 향상이 어렵고, 대규모 딥러닝 모델에서는 이런 문제로 인해 훈련 속도가 느려짐.\n",
    "\n",
    "- **대규모 모델에서의 영향**  \n",
    "  - 그레이디언트의 양이 많아질수록 대역폭 문제가 더 심각해짐.\n",
    "  - 희소 모델의 규모를 늘리기가 더 쉽다는 구체적인 사례:\n",
    "    - **신경망 기계 번역:** GPU 8개에서 6배 속도 증가.\n",
    "    - **인셉션/이미지넷:** GPU 50개에서 32배 속도 증가.\n",
    "    - **랭크브레인:** GPU 500개에서 300배 속도 증가.\n",
    "\n",
    "**해결 방법: 병렬화 기법**\n",
    "- **PipeDream 시스템**  \n",
    "  모델 병렬화와 데이터 병렬화를 결합한 새로운 방식 제안.  \n",
    "  - **스테이지(Stage) 분할:** 모델을 여러 부분으로 나눠 각기 다른 머신에서 훈련.  \n",
    "  - **비동기 처리:** 데이터와 그레이디언트를 병렬로 처리하면서 대역폭 문제 완화.\n",
    "    - **작동 방식**  \n",
    "      - 입력 데이터를 작은 미니배치로 나눠 각각 처리.  \n",
    "      - 역전파 단계에서 그레이디언트를 순차적으로 업데이트.  \n",
    "      - 각 단계가 독립적으로 작동하며, 대역폭 부담을 줄이기 위해 \"미리보기 전략\" 사용 가능.\n",
    "\n",
    "    <img src=\"./images/fig_19_17.png\" width=\"800\">\n",
    "    \n",
    "    - **기대 효과**  \n",
    "      - PipeDream과 같은 새로운 병렬화 기술은 GPU 자원을 효율적으로 활용하면서 대규모 모델 훈련을 가속화 가능.\n",
    "     \n",
    "    - **PipeDream의 문제점**  \n",
    "      - 스테이지를 거치며 역전파 시 정확도가 떨어지고 발산 가능성 증가.  \n",
    "      - 특정 스테이지에서 계산된 그레이디언트가 다른 스테이지에서 제대로 반영되지 않는 문제.  \n",
    "      - 이러한 문제를 해결하기 위해 **가중치 스태싱(weight stashing)** 기법을 제안:\n",
    "        - 정방향 계산 중에 가중치를 저장하여 역전파 단계에서도 일관되게 사용.\n",
    "        - 이를 통해 정확도 문제를 해결하고 확장성 개선.\n",
    "   \n",
    "- **Pathways 시스템**  \n",
    "  구글 연구팀이 개발한 시스템으로, TPU 활용률을 최대화하고 대규모 모델을 효율적으로 훈련하기 위한 새로운 접근법.  \n",
    "  - **특징:** \n",
    "    - 자동화된 모델 병렬 처리.\n",
    "    - 비동기 갱 스케줄링(gang scheduling)을 통해 대규모 TPU 클러스터에서 작업 병렬화를 극대화.\n",
    "    - TPU 6,000개 이상에서 대규모 언어 모델 훈련에 사용.\n",
    "    - 하드웨어 활용률이 100%에 가까움.\n",
    "\n",
    "    - Pathways는 아직 공개되지 않았으나, 향후 베타스에서도 유사한 시스템이 대규모 모델 훈련에 사용될 가능성이 높음.  \n",
    "- **대역폭 문제 해결을 위한 방법:**  \n",
    "    - 저렴한 GPU를 여러 대 사용하는 대신, 강력한 GPU 소수를 활용.\n",
    "    - 네트워크 부하를 줄이기 위해 여러 GPU를 하나의 서버에 집중 연결.\n",
    "    - 데이터 전송 크기를 줄이기 위해 32비트(tf.float32)를 16비트(tf.float16)로 변환.\n",
    "    - **중앙집중식 파라미터 서버 활용**  \n",
    "        - 파라미터 서버 여러 대를 추가하여 각 서버에 네트워크 부하 분산.  \n",
    "        - 이를 통해 대역폭 포화 문제를 최소화하고 훈련 속도를 높일 수 있음."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GbS0MW8VVFyV"
   },
   "source": [
    "## 분산 전략 API를 사용한 대규모 훈련"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tlwMePdtVFyV"
   },
   "outputs": [],
   "source": [
    "# 추가 코드 - 케라스를 사용하여 MNIST용 CNN 모델을 생성합니다.\n",
    "def create_model():\n",
    "    return tf.keras.Sequential([\n",
    "        tf.keras.layers.Reshape([28, 28, 1], input_shape=[28, 28],\n",
    "                                dtype=tf.uint8),\n",
    "        tf.keras.layers.Rescaling(scale=1 / 255),\n",
    "        tf.keras.layers.Conv2D(filters=64, kernel_size=7, activation=\"relu\",\n",
    "                               padding=\"same\"),\n",
    "        tf.keras.layers.MaxPooling2D(pool_size=2),\n",
    "        tf.keras.layers.Conv2D(filters=128, kernel_size=3, activation=\"relu\",\n",
    "                               padding=\"same\"),\n",
    "        tf.keras.layers.Conv2D(filters=128, kernel_size=3, activation=\"relu\",\n",
    "                               padding=\"same\"),\n",
    "        tf.keras.layers.MaxPooling2D(pool_size=2),\n",
    "        tf.keras.layers.Flatten(),\n",
    "        tf.keras.layers.Dense(units=64, activation=\"relu\"),\n",
    "        tf.keras.layers.Dropout(0.5),\n",
    "        tf.keras.layers.Dense(units=10, activation=\"softmax\"),\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. 개요**\n",
    "- 텐서플로는 여러 장치와 머신에서 모델을 분산 처리하는 복잡성을 대신 처리해주는 **분산 전략 API**를 제공.\n",
    "- **MirroredStrategy**:\n",
    "  - 모든 GPU에서 데이터 병렬화를 사용해 동기화.\n",
    "  - **scope()** 메서드로 분산 컨텍스트를 설정한 뒤 모델 생성 및 컴파일 과정을 실행.\n",
    "  - 이후 **fit()** 메서드로 훈련."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zBJa8Mc3VFyV"
   },
   "outputs": [],
   "source": [
    "# 랜덤 시드를 42로 설정하여 재현성 확보\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# MirroredStrategy 객체 생성 - 모든 GPU에서 동기화된 데이터 병렬화 수행\n",
    "strategy = tf.distribute.MirroredStrategy()\n",
    "\n",
    "# strategy.scope() 컨텍스트에서 모델 생성 및 컴파일\n",
    "with strategy.scope():\n",
    "    # 일반적인 케라스 모델 생성 \n",
    "    model = create_model()\n",
    "    \n",
    "    # 모델 컴파일 - 손실 함수, 옵티마이저, 평가 지표 설정\n",
    "    model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "                  optimizer=tf.keras.optimizers.SGD(learning_rate=1e-2), \n",
    "                  metrics=[\"accuracy\"])\n",
    "\n",
    "# 배치 크기 설정 - GPU 복제본 수로 나누어지도록 설정\n",
    "batch_size = 100\n",
    "\n",
    "# 모델 훈련 - 10 에포크 동안 훈련 및 검증 데이터로 평가\n",
    "model.fit(X_train, y_train, epochs=10,\n",
    "          validation_data=(X_valid, y_valid), batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `batch_size`는 GPU 개수로 나누어져 각 복제 모델에 할당.\n",
    "- 모든 복제 모델에 동일한 크기의 배치가 전달되어 병렬 처리 속도가 증가.\n",
    "- `model.weights`는 `MirroredVariable` 타입으로 GPU 간 동기화를 지원."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yQhapE_hVFyV",
    "outputId": "3ea45e32-9fb5-48d3-f163-debbf038eda3"
   },
   "outputs": [],
   "source": [
    "type(model.weights[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **predict()**: 모든 복제 모델에 배치를 나눠 병렬로 예측을 수행.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KHjpfdYKVFyV",
    "outputId": "40334e50-512d-464d-f222-169065ce05ca"
   },
   "outputs": [],
   "source": [
    "model.predict(X_new).round(2)  # 추가 코드 - 배치가 모든 복제본에 분할됩니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **save()**: 일반적인 형태로 모델을 저장. 특정 GPU가 아닌 모든 장치에서 로드 가능."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MS9hGg7HVFyW",
    "outputId": "2b2287a1-951e-4e1e-fb83-d24cf51fe10e"
   },
   "outputs": [],
   "source": [
    "# 추가 코드 - 모델을 저장해도 분산 전략이 보존되지 않음을 보여줍니다.\n",
    "model.save(\"my_mirrored_model\", save_format=\"tf\")\n",
    "model = tf.keras.models.load_model(\"my_mirrored_model\")\n",
    "type(model.weights[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qhQRaMguVFyW"
   },
   "outputs": [],
   "source": [
    "with strategy.scope():\n",
    "    model = tf.keras.models.load_model(\"my_mirrored_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tRVoO84VVFyW",
    "outputId": "e7591f3a-af9a-4101-92d3-70cb703f0239"
   },
   "outputs": [],
   "source": [
    "type(model.weights[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WN3CledCVFyW"
   },
   "source": [
    "사용할 GPU 리스트를 지정하려는 경우:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**MirroredStrategy 옵션 변경**\n",
    "- **기본 설정**:\n",
    "  - NCCL(NVIDIA Collective Communications Library)을 사용해 GPU 간 평균 계산.\n",
    "- **다른 옵션**:\n",
    "  - **tf.distribute.HierarchicalCopyAllReduce**:\n",
    "    - NCCL 대신 GPU와 모델 구조에 따라 더 적합한 옵션.\n",
    "  - **tf.distribute.ReductionToOneDevice**:\n",
    "    - 모든 데이터를 한 장치로모아 처리.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cA4CHPHkVFyW",
    "outputId": "05fd42b0-f2ff-472d-a5f9-2793f365d7b7"
   },
   "outputs": [],
   "source": [
    "strategy = tf.distribute.MirroredStrategy(devices=[\"/gpu:0\", \"/gpu:1\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Wh-lT0kdVFyW",
    "outputId": "54cf5c98-f503-402f-b407-8f824bc95875"
   },
   "outputs": [],
   "source": [
    "strategy = tf.distribute.MirroredStrategy(\n",
    "    cross_device_ops=tf.distribute.HierarchicalCopyAllReduce())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**CentralStorageStrategy**\n",
    "- **MirroredStrategy**의 대안\n",
    "- 중앙 집중식 파라미터 서버에서 데이터 병렬화를 수행."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U-JeJwYMVFyX",
    "outputId": "593f93fa-d500-43e4-c9de-90cd70e9a67c"
   },
   "outputs": [],
   "source": [
    "strategy = tf.distribute.experimental.CentralStorageStrategy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3C4ZCAWbVFyX"
   },
   "outputs": [],
   "source": [
    "# Google Colab에서 TPU로 훈련하기:\n",
    "#if \"google.colab\" in sys.modules and \"COLAB_TPU_ADDR\" in os.environ:\n",
    "#  tpu_address = \"grpc://\" + os.environ[\"COLAB_TPU_ADDR\"]\n",
    "#else:\n",
    "#  tpu_address = \"\"\n",
    "#resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu_address)\n",
    "#tf.config.experimental_connect_to_cluster(resolver)\n",
    "#tf.tpu.experimental.initialize_tpu_system(resolver)\n",
    "#strategy = tf.distribute.experimental.TPUStrategy(resolver)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zRyn37DGVFyX"
   },
   "source": [
    "## 텐서플로 클러스터에서 모델 훈련하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**텐서플로 클러스터 구성 요소**\n",
    "\n",
    "텐서플로 클러스터는 일반적으로 서로 다른 컴퓨터에서 병렬로 실행되며 신경망 훈련이나 실행과 같은 작업을 완료하기 위해 서로 대화하는 텐서플로 프로세스의 그룹입니다. 클러스터의 각 TF 프로세스를 \"태스크\"(또는 \"TF 서버\")라고 합니다. IP 주소, 포트, 타입(역할 또는 작업이라고도 함)이 있습니다. 타입은 `\"worker\"`, `\"chief\"`, `\"ps\"`(파라미터 서버) 또는 `\"evaluator\"`가 될 수 있습니다:\n",
    "* **워커**는 일반적으로 하나 이상의 GPU가 있는 컴퓨터에서 계산을 수행합니다.\n",
    "* **치프**는 계산도 수행하지만, 텐서보드 로그 작성이나 체크포인트 저장과 같은 추가 작업도 처리합니다. 클러스터에는 하나의 치프가 있습니다. 정의되지 않은 경우 워커 #0이 치프가 됩니다.\n",
    "* **파라미터 서버**(ps)는 변수 값만 추적하며, 일반적으로 CPU 전용 머신에 있습니다.\n",
    "* **이밸류에이터**는 당연히 평가를 담당합니다. 일반적으로 클러스터에는 하나의 이밸류에이터가 있습니다.\n",
    "\n",
    "같은 유형의 태스크 집합을 흔히 \"작업(job)\"이라고 합니다. 예를 들어 \"워커\" 작업은 모든 워커 태스크의 집합입니다.\n",
    "\n",
    "텐서플로 클러스터를 시작하려면 먼저 클러스터를 정의해야 합니다. 즉, 모든 작업(IP 주소, TCP 포트 및 타입)을 지정해야 합니다. 예를 들어, 다음 클러스터 사양은 3개의 태스크(워커 2개, 파라미터 서버 1개)가 있는 클러스터를 정의합니다. 작업당 하나의 키가 있는 사전이며, 값은 태스크 주소 목록입니다:\n",
    "\n",
    "<img src=\"./images/fig_19_18.png\" width=\"800\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_5QYlq2OVFyX"
   },
   "outputs": [],
   "source": [
    "cluster_spec = {\n",
    "    \"worker\": [\n",
    "        \"machine-a.example.com:2222\",     # /job:worker/task:0\n",
    "        \"machine-b.example.com:2222\"      # /job:worker/task:1\n",
    "    ],\n",
    "    \"ps\": [\"machine-a.example.com:2221\"]  # /job:ps/task:0\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9eMw3p8GVFyX"
   },
   "source": [
    "클러스터의 모든 태스크는 서버의 다른 모든 태스크와 통신할 수 있으므로 방화벽이 해당 포트에서 컴퓨터 간의 모든 통신을 승인하도록 구성해야 합니다(일반적으로 모든 컴퓨터에서 동일한 포트를 사용하는 것이 간단합니다).\n",
    "\n",
    "태스크가 시작되면 어떤 태스크인지 타입과 인덱스(태스크 인덱스는 태스크 ID라고도 함)를 알려주어야 합니다. 클러스터 사양과 현재 태스크의 타입 및 아이디를 모두 한 번에 지정하는 일반적인 방법은 프로그램을 시작하기 전에 `TF_CONFIG` 환경 변수를 설정하는 것입니다. 클러스터 사양(``cluster`` 키 아래)과 시작할 태스크의 타입 및 인덱스(``task`` 키 아래)가 포함된 JSON 인코딩된 딕셔너리여야 합니다. 예를 들어, 다음 `TF_CONFIG` 환경 변수는 위와 동일한 클러스터(워커 2개, 파라미터 서버 1개)를 정의하고, 시작할 태스크를 워커 \\#0으로 지정합니다:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a_rTAxUlVFyY"
   },
   "outputs": [],
   "source": [
    "os.environ[\"TF_CONFIG\"] = json.dumps({\n",
    "    \"cluster\": cluster_spec,\n",
    "    \"task\": {\"type\": \"worker\", \"index\": 0}\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S6iYPbcFVFyY"
   },
   "source": [
    "일부 플랫폼(예: 구글 버텍스 AI)에서는 이 환경 변수를 자동으로 설정합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_m0-G1zhVFyY"
   },
   "source": [
    "텐서플로의 `TFConfigClusterResolver` 클래스는 이 환경 변수에서 클러스터 구성을 읽습니다:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BNmFZBt-VFyY",
    "outputId": "1bc329bb-0490-4f7f-a896-e76038aeca82"
   },
   "outputs": [],
   "source": [
    "resolver = tf.distribute.cluster_resolver.TFConfigClusterResolver()\n",
    "resolver.cluster_spec()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fFujWNXUVFyY",
    "outputId": "f8a415d7-cfea-435d-db1b-9deed53595f4"
   },
   "outputs": [],
   "source": [
    "resolver.task_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-KhEIZIRVFyY",
    "outputId": "e9e190ea-918a-493e-e3b7-2229b71cdc4d"
   },
   "outputs": [],
   "source": [
    "resolver.task_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UchJJ_e-VFyY"
   },
   "source": [
    "이제 로컬 컴퓨터에서 두 개의 워커 태스크를 가진 간단한 클러스터를 실행해 보겠습니다. `MultiWorkerMirroredStrategy`을 사용하여 두 태스크로 모델을 훈련하겠습니다.\n",
    "\n",
    "첫 번째 단계는 훈련 코드를 작성하는 것입니다. 이 코드를 사용해 두 워커에서 각각 고유한 프로세스로 실행하므로 별도의 파이썬 파일인 `my_mnist_multiworker_task.py`에 작성합니다. 코드는 비교적 간단하지만 주의해야 할 몇 가지 중요한 사항이 있습니다:\n",
    "* 텐서플로로 다른 작업을 수행하기 전에 `MultiWorkerMirroredStrategy`을 생성합니다.\n",
    "* 워커 중 하나만 텐서보드 로깅을 처리합니다. 앞서 언급했듯이 이 작업자를 *치프*라고 합니다. 명시적으로 정의되지 않은 경우 워커 #0이 치프가 됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x5wca2YyVFyY",
    "outputId": "ac475391-f228-4778-9ad9-d526ba8301c0"
   },
   "outputs": [],
   "source": [
    "%%writefile my_mnist_multiworker_task.py\n",
    "\n",
    "import tempfile\n",
    "import tensorflow as tf\n",
    "\n",
    "strategy = tf.distribute.MultiWorkerMirroredStrategy()  # 시작 부분에!\n",
    "resolver = tf.distribute.cluster_resolver.TFConfigClusterResolver()\n",
    "print(f\"Starting task {resolver.task_type} #{resolver.task_id}\")\n",
    "\n",
    "# 추가 코드 - MNIST 데이터셋 로드 및 분할\n",
    "mnist = tf.keras.datasets.mnist.load_data()\n",
    "(X_train_full, y_train_full), (X_test, y_test) = mnist\n",
    "X_valid, X_train = X_train_full[:5000], X_train_full[5000:]\n",
    "y_valid, y_train = y_train_full[:5000], y_train_full[5000:]\n",
    "\n",
    "with strategy.scope():\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Reshape([28, 28, 1], input_shape=[28, 28],\n",
    "                                dtype=tf.uint8),\n",
    "        tf.keras.layers.Rescaling(scale=1 / 255),\n",
    "        tf.keras.layers.Conv2D(filters=64, kernel_size=7, activation=\"relu\",\n",
    "                               padding=\"same\", input_shape=[28, 28, 1]),\n",
    "        tf.keras.layers.MaxPooling2D(pool_size=2),\n",
    "        tf.keras.layers.Conv2D(filters=128, kernel_size=3, activation=\"relu\",\n",
    "                               padding=\"same\"),\n",
    "        tf.keras.layers.Conv2D(filters=128, kernel_size=3, activation=\"relu\",\n",
    "                               padding=\"same\"),\n",
    "        tf.keras.layers.MaxPooling2D(pool_size=2),\n",
    "        tf.keras.layers.Flatten(),\n",
    "        tf.keras.layers.Dense(units=64, activation=\"relu\"),\n",
    "        tf.keras.layers.Dropout(0.5),\n",
    "        tf.keras.layers.Dense(units=10, activation=\"softmax\"),\n",
    "    ])\n",
    "    model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "                  optimizer=tf.keras.optimizers.SGD(learning_rate=1e-2),\n",
    "                  metrics=[\"accuracy\"])\n",
    "\n",
    "model.fit(X_train, y_train, validation_data=(X_valid, y_valid), epochs=10)\n",
    "\n",
    "if resolver.task_id == 0:  # 치프는 모델을 올바른 위치에 저장합니다.\n",
    "    model.save(\"my_mnist_multiworker_model\", save_format=\"tf\")\n",
    "else:\n",
    "    tmpdir = tempfile.mkdtemp()  # 다른 워커는 임시 디렉터리에 저장합니다.\n",
    "    model.save(tmpdir, save_format=\"tf\")\n",
    "    tf.io.gfile.rmtree(tmpdir)  # 마지막에 이 디렉터리를 삭제할 수 있습니다!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KyPpRrL2VFyZ"
   },
   "source": [
    "실제 애플리케이션에서는 일반적으로 머신당 하나의 워커가 있지만, 이 예제에서는 동일한 머신에서 두 워커를 모두 실행하고 있으므로 두 워커 모두 사용 가능한 모든 GPU RAM(이 머신에 GPU가 있는 경우)을 사용하려고 시도하므로 메모리 부족(OOM) 오류가 발생할 수 있습니다. 이를 방지하기 위해 `CUDA_VISIBLE_DEVICES` 환경 변수를 사용하여 각 워커에 다른 GPU를 할당할 수 있습니다. 또는 `CUDA_VISIBLE_DEVICES`를 빈 문자열로 설정하여 간단히 GPU 지원을 비활성화할 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "axQRK0fhVFyZ"
   },
   "source": [
    "이제 각각 고유한 프로세스에서 두 워커를 시작할 준비가 되었습니다. 태스크 인덱스가 변경된 것을 확인할 수 있습니다:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tHjSZiw1VFyZ"
   },
   "outputs": [],
   "source": [
    "%%bash --bg\n",
    "\n",
    "export CUDA_VISIBLE_DEVICES=''\n",
    "export TF_CONFIG='{\"cluster\": {\"worker\": [\"127.0.0.1:9901\", \"127.0.0.1:9902\"]},\n",
    "                   \"task\": {\"type\": \"worker\", \"index\": 0}}'\n",
    "python my_mnist_multiworker_task.py > my_worker_0.log 2>&1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gOd7b7jkVFyZ"
   },
   "outputs": [],
   "source": [
    "%%bash --bg\n",
    "\n",
    "export CUDA_VISIBLE_DEVICES=''\n",
    "export TF_CONFIG='{\"cluster\": {\"worker\": [\"127.0.0.1:9901\", \"127.0.0.1:9902\"]},\n",
    "                   \"task\": {\"type\": \"worker\", \"index\": 1}}'\n",
    "python my_mnist_multiworker_task.py > my_worker_1.log 2>&1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mwguA2F-VFyZ"
   },
   "source": [
    "**참고**: `AutoShardPolicy`에 대한 경고가 표시되면 무시해도 무방합니다. 자세한 내용은 [TF 이슈 #42146](https://github.com/tensorflow/tensorflow/issues/42146)을 참고하세요."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2oTt8nDlVFyZ"
   },
   "source": [
    "끝났습니다! 이제 텐서플로 클러스터가 실행 중이지만 별도의 프로세스에서 실행 중이므로 이 노트북에서는 볼 수 없습니다(하지만 진행 상황은 `my_worker_*.log`에서 확인할 수 있습니다).\n",
    "\n",
    "치프(워커 #0)가 텐서보드에 로그를 쓰기 때문에, 훈련 진행 상황을 보기 위해 텐서보드를 사용할 수 있습니다. 다음 셀을 실행한 다음, 설정 버튼(즉, 톱니바퀴 아이콘)을 클릭하고 \"Reload data\" 상자를 체크하여 텐서보드가 30초마다 자동으로 새로고침되도록 설정합니다. 첫 번째 훈련이 완료되고(몇 분 정도 소요될 수 있음) 텐서보드가 새로 고침되면 SCALARS 탭이 나타납니다. 이 탭을 클릭하면 모델의 훈련 진행 과정과 검증 정확도를 확인할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CYFTn6yXVFyZ"
   },
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir=./my_mnist_multiworker_logs --port=6006"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RsH23DuSVFya"
   },
   "outputs": [],
   "source": [
    "# strategy = tf.distribute.MultiWorkerMirroredStrategy(\n",
    "#     communication_options=tf.distribute.experimental.CommunicationOptions(\n",
    "#         implementation=tf.distribute.experimental.CollectiveCommunication.NCCL))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "do-XysUJVFya"
   },
   "source": [
    "## 버텍스 AI에서 대규모 훈련 작업 실행하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RzjR9YISVFya"
   },
   "source": [
    "훈련 스크립트를 복사하되 `import os`를 추가하고 저장 경로를 `AIP_MODEL_DIR` 환경 변수가 가리키는 GCS 경로로 변경해 보겠습니다:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3d4u1mtKVFya",
    "outputId": "d4c031c7-b1f3-4501-e3bc-bc19ea652584"
   },
   "outputs": [],
   "source": [
    "%%writefile my_vertex_ai_training_task.py\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "import tempfile\n",
    "import tensorflow as tf\n",
    "\n",
    "strategy = tf.distribute.MultiWorkerMirroredStrategy()  # 시작 부분에!\n",
    "resolver = tf.distribute.cluster_resolver.TFConfigClusterResolver()\n",
    "\n",
    "if resolver.task_type == \"chief\":\n",
    "    model_dir = os.getenv(\"AIP_MODEL_DIR\")  # 버텍스 AI가 제공하는 경로\n",
    "    tensorboard_log_dir = os.getenv(\"AIP_TENSORBOARD_LOG_DIR\")\n",
    "    checkpoint_dir = os.getenv(\"AIP_CHECKPOINT_DIR\")\n",
    "else:\n",
    "    tmp_dir = Path(tempfile.mkdtemp())  # 다른 워커는 임시 디렉터리를 사용합니다.\n",
    "    model_dir = tmp_dir / \"model\"\n",
    "    tensorboard_log_dir = tmp_dir / \"logs\"\n",
    "    checkpoint_dir = tmp_dir / \"ckpt\"\n",
    "\n",
    "callbacks = [tf.keras.callbacks.TensorBoard(tensorboard_log_dir),\n",
    "             tf.keras.callbacks.ModelCheckpoint(checkpoint_dir)]\n",
    "\n",
    "# 추가 코드 - MNIST 데이터셋 로드 및 준비\n",
    "mnist = tf.keras.datasets.mnist.load_data()\n",
    "(X_train_full, y_train_full), (X_test, y_test) = mnist\n",
    "X_valid, X_train = X_train_full[:5000], X_train_full[5000:]\n",
    "y_valid, y_train = y_train_full[:5000], y_train_full[5000:]\n",
    "\n",
    "# 추가 코드 - 분산 전략을 사용하여 케라스 모델을 빌드하고 컴파일합니다.\n",
    "with strategy.scope():\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Reshape([28, 28, 1], input_shape=[28, 28],\n",
    "                                dtype=tf.uint8),\n",
    "        tf.keras.layers.Lambda(lambda X: X / 255),\n",
    "        tf.keras.layers.Conv2D(filters=64, kernel_size=7, activation=\"relu\",\n",
    "                               padding=\"same\", input_shape=[28, 28, 1]),\n",
    "        tf.keras.layers.MaxPooling2D(pool_size=2),\n",
    "        tf.keras.layers.Conv2D(filters=128, kernel_size=3, activation=\"relu\",\n",
    "                               padding=\"same\"),\n",
    "        tf.keras.layers.Conv2D(filters=128, kernel_size=3, activation=\"relu\",\n",
    "                               padding=\"same\"),\n",
    "        tf.keras.layers.MaxPooling2D(pool_size=2),\n",
    "        tf.keras.layers.Flatten(),\n",
    "        tf.keras.layers.Dense(units=64, activation=\"relu\"),\n",
    "        tf.keras.layers.Dropout(0.5),\n",
    "        tf.keras.layers.Dense(units=10, activation=\"softmax\"),\n",
    "    ])\n",
    "    model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "                  optimizer=tf.keras.optimizers.SGD(learning_rate=1e-2),\n",
    "                  metrics=[\"accuracy\"])\n",
    "\n",
    "model.fit(X_train, y_train, validation_data=(X_valid, y_valid), epochs=10,\n",
    "          callbacks=callbacks)\n",
    "model.save(model_dir, save_format=\"tf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GYMAUq6xVFya"
   },
   "outputs": [],
   "source": [
    "custom_training_job = aiplatform.CustomTrainingJob(\n",
    "    display_name=\"my_custom_training_job\",\n",
    "    script_path=\"my_vertex_ai_training_task.py\",\n",
    "    container_uri=\"gcr.io/cloud-aiplatform/training/tf-gpu.2-4:latest\",\n",
    "    model_serving_container_image_uri=server_image,\n",
    "    requirements=[\"gcsfs==2022.3.0\"],  # 필요 없음, 이것은 단지 예일 뿐입니다.\n",
    "    staging_bucket=f\"gs://{bucket_name}/staging\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Qe_rzzvJVFya",
    "outputId": "14afb27c-ef23-4bb0-f0fc-2381499bdcaa"
   },
   "outputs": [],
   "source": [
    "mnist_model2 = custom_training_job.run(\n",
    "    machine_type=\"n1-standard-4\",\n",
    "    replica_count=2,\n",
    "    accelerator_type=\"NVIDIA_TESLA_K80\",\n",
    "    accelerator_count=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z8I1Nxs8VFyb"
   },
   "source": [
    "정리합니다:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kvcJVmwZVFyb"
   },
   "outputs": [],
   "source": [
    "mnist_model2.delete()\n",
    "custom_training_job.delete()\n",
    "blobs = bucket.list_blobs(prefix=f\"gs://{bucket_name}/staging/\")\n",
    "for blob in blobs:\n",
    "    blob.delete()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9sniORFuVFyb"
   },
   "source": [
    "## 버텍스 AI의 하이퍼파라미터 튜닝"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bjUAxCNtVFyb",
    "outputId": "878f2c27-1cac-4bcc-ed28-927bf7f3a4ea"
   },
   "outputs": [],
   "source": [
    "%%writefile my_vertex_ai_trial.py\n",
    "\n",
    "import argparse\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"--n_hidden\", type=int, default=2)\n",
    "parser.add_argument(\"--n_neurons\", type=int, default=256)\n",
    "parser.add_argument(\"--learning_rate\", type=float, default=1e-2)\n",
    "parser.add_argument(\"--optimizer\", default=\"adam\")\n",
    "args = parser.parse_args()\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "def build_model(args):\n",
    "    with tf.distribute.MirroredStrategy().scope():\n",
    "        model = tf.keras.Sequential()\n",
    "        model.add(tf.keras.layers.Flatten(input_shape=[28, 28], dtype=tf.uint8))\n",
    "        for _ in range(args.n_hidden):\n",
    "            model.add(tf.keras.layers.Dense(args.n_neurons, activation=\"relu\"))\n",
    "        model.add(tf.keras.layers.Dense(10, activation=\"softmax\"))\n",
    "        opt = tf.keras.optimizers.get(args.optimizer)\n",
    "        opt.learning_rate = args.learning_rate\n",
    "        model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=opt,\n",
    "                      metrics=[\"accuracy\"])\n",
    "        return model\n",
    "\n",
    "# 추가 코드 - 데이터셋 로드 및 분할\n",
    "mnist = tf.keras.datasets.mnist.load_data()\n",
    "(X_train_full, y_train_full), (X_test, y_test) = mnist\n",
    "X_valid, X_train = X_train_full[:5000], X_train_full[5000:]\n",
    "y_valid, y_train = y_train_full[:5000], y_train_full[5000:]\n",
    "\n",
    "# 추가 코드 - AIP_* 환경 변수를 사용하고 콜백을 만듭니다.\n",
    "import os\n",
    "model_dir = os.getenv(\"AIP_MODEL_DIR\")\n",
    "tensorboard_log_dir = os.getenv(\"AIP_TENSORBOARD_LOG_DIR\")\n",
    "checkpoint_dir = os.getenv(\"AIP_CHECKPOINT_DIR\")\n",
    "trial_id = os.getenv(\"CLOUD_ML_TRIAL_ID\")\n",
    "tensorboard_cb = tf.keras.callbacks.TensorBoard(tensorboard_log_dir)\n",
    "early_stopping_cb = tf.keras.callbacks.EarlyStopping(patience=5)\n",
    "callbacks = [tensorboard_cb, early_stopping_cb]\n",
    "\n",
    "model = build_model(args)\n",
    "history = model.fit(X_train, y_train, validation_data=(X_valid, y_valid),\n",
    "                    epochs=10, callbacks=callbacks)\n",
    "model.save(model_dir, save_format=\"tf\")  # 추가 코드\n",
    "\n",
    "import hypertune\n",
    "\n",
    "hypertune = hypertune.HyperTune()\n",
    "hypertune.report_hyperparameter_tuning_metric(\n",
    "    hyperparameter_metric_tag=\"accuracy\",  # 보고할 지표의 이름\n",
    "    metric_value=max(history.history[\"val_accuracy\"]),  # 최대 정확도 값\n",
    "    global_step=model.optimizer.iterations.numpy(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J_RyhTU0VFyb",
    "outputId": "ad8bd41c-55ee-45df-aeaa-94c76835e82b"
   },
   "outputs": [],
   "source": [
    "trial_job = aiplatform.CustomJob.from_local_script(\n",
    "    display_name=\"my_search_trial_job\",\n",
    "    script_path=\"my_vertex_ai_trial.py\",  # 훈련 스크립트 경로\n",
    "    container_uri=\"gcr.io/cloud-aiplatform/training/tf-gpu.2-4:latest\",\n",
    "    staging_bucket=f\"gs://{bucket_name}/staging\",\n",
    "    accelerator_type=\"NVIDIA_TESLA_K80\",\n",
    "    accelerator_count=2,  # 이 예제에서는 각 트라이얼에 2개의 GPU가 있습니다.\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Wwqanly9VFyb",
    "outputId": "c1374e88-0e76-4050-cf07-d0a0ee0490ea"
   },
   "outputs": [],
   "source": [
    "from google.cloud.aiplatform import hyperparameter_tuning as hpt\n",
    "\n",
    "hp_job = aiplatform.HyperparameterTuningJob(\n",
    "    display_name=\"my_hp_search_job\",\n",
    "    custom_job=trial_job,\n",
    "    metric_spec={\"accuracy\": \"maximize\"},\n",
    "    parameter_spec={\n",
    "        \"learning_rate\": hpt.DoubleParameterSpec(min=1e-3, max=10, scale=\"log\"),\n",
    "        \"n_neurons\": hpt.IntegerParameterSpec(min=1, max=300, scale=\"linear\"),\n",
    "        \"n_hidden\": hpt.IntegerParameterSpec(min=1, max=10, scale=\"linear\"),\n",
    "        \"optimizer\": hpt.CategoricalParameterSpec([\"sgd\", \"adam\"]),\n",
    "    },\n",
    "    max_trial_count=100,\n",
    "    parallel_trial_count=20,\n",
    ")\n",
    "hp_job.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NaDCcWu6VFyb"
   },
   "outputs": [],
   "source": [
    "def get_final_metric(trial, metric_id):\n",
    "    for metric in trial.final_measurement.metrics:\n",
    "        if metric.metric_id == metric_id:\n",
    "            return metric.value\n",
    "\n",
    "trials = hp_job.trials\n",
    "trial_accuracies = [get_final_metric(trial, \"accuracy\") for trial in trials]\n",
    "best_trial = trials[np.argmax(trial_accuracies)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xCt9NWjpVFyc",
    "outputId": "a1219223-94c0-447a-9ea0-b5eb39e57ada"
   },
   "outputs": [],
   "source": [
    "max(trial_accuracies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XR0d5UnSVFyc",
    "outputId": "8239662b-cf8e-47db-be18-15a1bf632b0d"
   },
   "outputs": [],
   "source": [
    "best_trial.id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ys_lvgbLVFyc",
    "outputId": "1690d27d-85d2-4d47-bbaa-14f38366daaa"
   },
   "outputs": [],
   "source": [
    "best_trial.parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0EjBOfRMVFyc"
   },
   "source": [
    "# 추가 자료 - 버텍스 AI의 분산 케라스 튜너"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DdoTyP82VFyc"
   },
   "source": [
    "버텍스 AI의 하이퍼파라미터 튜닝 서비스를 사용하는 대신, 10장에서 소개한 [케라스 튜너](https://keras.io/keras_tuner/)를 사용하여 버텍스 AI VM에서 실행할 수 있습니다. 케라스 튜너는 하이퍼파라미터 탐색을 여러 머신에 분산하여 간단하게 확장할 수 있는 방법을 제공합니다: 각 머신에서 세 개의 환경 변수를 설정한 다음 일반 케라스 튜너 코드를 실행하기만 하면 됩니다. 모든 머신에서 똑같은 스크립트를 사용할 수 있습니다. 머신 중 한 대는 치프 역할을 하고 다른 머신은 워커 역할을 합니다. 각 워커는 오라클 역할을 하는 치프에게 어떤 하이퍼파라미터 값을 시도할지 요청하고, 치프는 이 하이퍼파라미터 값을 사용하여 모델을 훈련시킨 다음, 최종적으로 모델의 성능을 치프에게 보고하면 치프는 다음에 워커가 시도할 하이퍼파라미터 값을 결정할 수 있습니다.\n",
    "\n",
    "각 머신에서 설정해야 하는 세 가지 환경 변수는 다음과 같습니다:\n",
    "\n",
    "* `KERASTUNER_TUNER_ID`: 치프 머신은 `\"chief\"`이고 워커 머신은 `\"worker0\"`, `\"worker1\"` 등과 같은 고유 식별자입니다.\n",
    "* `KERASTUNER_ORACLE_IP`: 치프 머신의 IP 주소 또는 호스트 이름입니다. 치프 자체는 일반적으로 `\"0.0.0.0\"`을 사용하여 머신의 모든 IP 주소에서 수신해야 합니다.\n",
    "* `KERASTUNER_ORACLE_PORT`: 치프가 수신 대기할 TCP 포트입니다.\n",
    "\n",
    "분산 케라스 튜너는 모든 머신 집합에서 사용할 수 있습니다. 버텍스 AI 머신에서 실행하려면 일반 훈련 작업을 생성하고 훈련 스크립트를 수정하여 세 가지 환경 변수를 적절히 설정한 후 케라스 튜너를 사용하면 됩니다.\n",
    "\n",
    "예를 들어, 아래 스크립트는 이전과 마찬가지로 먼저 버텍스 AI가 자동으로 설정하는 `TF_CONFIG` 환경 변수를 파싱합니다. `\"chief\"` 타입의 태스크 주소를 찾아서 IP 주소 또는 호스트 이름과 TCP 포트를 추출합니다. 그런 다음 튜너 ID를 태스크 타입과 태스크 인덱스로 정의합니다(예: `\"worker0\"`). 튜너 ID가 `\"chief0\"`이면 `\"chief\"`로 변경하고 IP를 `\"0.0.0.0\"`으로 설정하면 해당 컴퓨터의 모든 IPv4 주소에서 수신 대기하게 됩니다. 그런 다음 케라스 튜너에 대한 환경 변수를 정의합니다. 다음으로 스크립트는 10장에서와 마찬가지로 튜너를 생성하여 탐색을 수행한 다음 마지막으로 버텍스 AI가 지정한 위치에 최적의 모델을 저장합니다:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1gOrP79ZVFyc",
    "outputId": "47b01298-7b18-4a8b-f6b5-3a938e67e780"
   },
   "outputs": [],
   "source": [
    "%%writefile my_keras_tuner_search.py\n",
    "\n",
    "import json\n",
    "import os\n",
    "\n",
    "tf_config = json.loads(os.environ[\"TF_CONFIG\"])\n",
    "\n",
    "chief_ip, chief_port = tf_config[\"cluster\"][\"chief\"][0].rsplit(\":\", 1)\n",
    "tuner_id = f'{tf_config[\"task\"][\"type\"]}{tf_config[\"task\"][\"index\"]}'\n",
    "if tuner_id == \"chief0\":\n",
    "    tuner_id = \"chief\"\n",
    "    chief_ip = \"0.0.0.0\"\n",
    "    # 추가 코드 - 치프는 많은 작업을 수행하지 않으므로 동일한 컴퓨터에서 워커를 실행하여\n",
    "    # 컴퓨팅 리소스를 최적화할 수 있습니다. 이렇게 하려면\n",
    "    # TF_CONFIG 환경 변수를 조정하여 태스크 유형을 \"worker\"로 설정하고 태스크 인덱스를\n",
    "    # 고유한 값으로 설정한 후 치프가 또 다른 프로세스를 시작하도록 하면 됩니다.\n",
    "    # 다음 몇 줄의 주석 처리를 해제하고 시도해 보세요:\n",
    "    # import subprocess\n",
    "    # import sys\n",
    "    # tf_config[\"task\"][\"type\"] = \"workerX\"  # 치프 머신의 워커\n",
    "    # os.environ[\"TF_CONFIG\"] = json.dumps(tf_config)\n",
    "    # subprocess.Popen([sys.executable] + sys.argv,\n",
    "    #                  stdout=sys.stdout, stderr=sys.stderr)\n",
    "\n",
    "os.environ[\"KERASTUNER_TUNER_ID\"] = tuner_id\n",
    "os.environ[\"KERASTUNER_ORACLE_IP\"] = chief_ip\n",
    "os.environ[\"KERASTUNER_ORACLE_PORT\"] = chief_port\n",
    "\n",
    "from pathlib import Path\n",
    "import keras_tuner as kt\n",
    "import tensorflow as tf\n",
    "\n",
    "gcs_path = \"/gcs/my_bucket/my_hp_search\"  # 버킷 이름으로 바꾸기\n",
    "\n",
    "def build_model(hp):\n",
    "    n_hidden = hp.Int(\"n_hidden\", min_value=0, max_value=8, default=2)\n",
    "    n_neurons = hp.Int(\"n_neurons\", min_value=16, max_value=256)\n",
    "    learning_rate = hp.Float(\"learning_rate\", min_value=1e-4, max_value=1e-2,\n",
    "                             sampling=\"log\")\n",
    "    optimizer = hp.Choice(\"optimizer\", values=[\"sgd\", \"adam\"])\n",
    "    if optimizer == \"sgd\":\n",
    "        optimizer = tf.keras.optimizers.SGD(learning_rate=learning_rate)\n",
    "    else:\n",
    "        optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(tf.keras.layers.Flatten(input_shape=[28, 28], dtype=tf.uint8))\n",
    "    for _ in range(n_hidden):\n",
    "        model.add(tf.keras.layers.Dense(n_neurons, activation=\"relu\"))\n",
    "    model.add(tf.keras.layers.Dense(10, activation=\"softmax\"))\n",
    "    model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "                  optimizer=optimizer,\n",
    "                  metrics=[\"accuracy\"])\n",
    "    return model\n",
    "\n",
    "hyperband_tuner = kt.Hyperband(\n",
    "    build_model, objective=\"val_accuracy\", seed=42,\n",
    "    max_epochs=10, factor=3, hyperband_iterations=2,\n",
    "    distribution_strategy=tf.distribute.MirroredStrategy(),\n",
    "    directory=gcs_path, project_name=\"mnist\")\n",
    "\n",
    "# extra code – Load and split the MNIST dataset\n",
    "mnist = tf.keras.datasets.mnist.load_data()\n",
    "(X_train_full, y_train_full), (X_test, y_test) = mnist\n",
    "X_valid, X_train = X_train_full[:5000], X_train_full[5000:]\n",
    "y_valid, y_train = y_train_full[:5000], y_train_full[5000:]\n",
    "\n",
    "tensorboard_log_dir = os.environ[\"AIP_TENSORBOARD_LOG_DIR\"] + \"/\" + tuner_id\n",
    "tensorboard_cb = tf.keras.callbacks.TensorBoard(tensorboard_log_dir)\n",
    "early_stopping_cb = tf.keras.callbacks.EarlyStopping(patience=5)\n",
    "hyperband_tuner.search(X_train, y_train, epochs=10,\n",
    "                       validation_data=(X_valid, y_valid),\n",
    "                       callbacks=[tensorboard_cb, early_stopping_cb])\n",
    "\n",
    "if tuner_id == \"chief\":\n",
    "    best_hp = hyperband_tuner.get_best_hyperparameters()[0]\n",
    "    best_model = hyperband_tuner.hypermodel.build(best_hp)\n",
    "    best_model.save(os.getenv(\"AIP_MODEL_DIR\"), save_format=\"tf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ThpfJhRtVFyd"
   },
   "source": [
    "참고로 버텍스 AI는 오픈 소스 [GCS 퓨즈 어댑터](https://cloud.google.com/storage/docs/gcs-fuse)를 사용하여 `/gcs` 디렉터리를 GCS에 자동으로 마운트합니다. 이렇게 하면 워커와 치프 간에 공유 디렉터리가 제공되며, 이는 케라스 튜너에 필요합니다. 또한 배포 전략을 `MirroredStrategy`으로 설정했습니다. 이렇게 하면 각 워커가 자신의 머신에 있는 모든 GPU를 사용할 수 있습니다(GPU가 두 개 이상 있는 경우)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2bd7yE9nVFyd"
   },
   "source": [
    "`gcs/my_bucket/`를 <code>/gcs/<i>{bucket_name}</i>/</code>로 바꿉니다:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XNDrhUYdVFyd"
   },
   "outputs": [],
   "source": [
    "with open(\"my_keras_tuner_search.py\") as f:\n",
    "    script = f.read()\n",
    "\n",
    "with open(\"my_keras_tuner_search.py\", \"w\") as f:\n",
    "    f.write(script.replace(\"/gcs/my_bucket/\", f\"/gcs/{bucket_name}/\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HqEDdR4IVFyd"
   },
   "source": [
    "이제 이전 섹션에서와 똑같이 이 스크립트를 기반으로 사용자 정의 훈련 작업을 시작하기만 하면 됩니다. `requirements` 목록에 `keras-tuner`를 추가하는 것을 잊지 마세요:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g5uMNBiDVFyd"
   },
   "outputs": [],
   "source": [
    "hp_search_job = aiplatform.CustomTrainingJob(\n",
    "    display_name=\"my_hp_search_job\",\n",
    "    script_path=\"my_keras_tuner_search.py\",\n",
    "    container_uri=\"gcr.io/cloud-aiplatform/training/tf-gpu.2-4:latest\",\n",
    "    model_serving_container_image_uri=server_image,\n",
    "    requirements=[\"keras-tuner~=1.1.2\"],\n",
    "    staging_bucket=f\"gs://{bucket_name}/staging\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DDe_0b6WVFyd",
    "outputId": "3062c727-0e39-4184-9405-a4a019b53664"
   },
   "outputs": [],
   "source": [
    "mnist_model3 = hp_search_job.run(\n",
    "    machine_type=\"n1-standard-4\",\n",
    "    replica_count=3,\n",
    "    accelerator_type=\"NVIDIA_TESLA_K80\",\n",
    "    accelerator_count=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yVueoD4IVFye"
   },
   "source": [
    "모델을 찾았습니다!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "opovykC9VFye"
   },
   "source": [
    "정리:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ImHUNUPUVFye"
   },
   "outputs": [],
   "source": [
    "mnist_model3.delete()\n",
    "hp_search_job.delete()\n",
    "blobs = bucket.list_blobs(prefix=f\"gs://{bucket_name}/staging/\")\n",
    "for blob in blobs:\n",
    "    blob.delete()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GOnvNUY2VFye"
   },
   "source": [
    "# 추가 자료 - AutoML을 사용하여 모델 훈련하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j7egsPY4VFye"
   },
   "source": [
    "먼저 MNIST 데이터셋을 PNG 이미지로 내보내고, 각 이미지 파일과 분할(훈련, 검증 또는 테스트), 레이블을 나타내는 `import.csv`를 준비해 보겠습니다:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Y_MHPWghVFye",
    "outputId": "8900513c-d723-45d7-be50-77f2c5a5b264"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "mnist_path = Path(\"datasets/mnist\")\n",
    "mnist_path.mkdir(parents=True, exist_ok=True)\n",
    "idx = 0\n",
    "with open(mnist_path / \"import.csv\", \"w\") as import_csv:\n",
    "    for split, X, y in zip((\"training\", \"validation\", \"test\"),\n",
    "                           (X_train, X_valid, X_test),\n",
    "                           (y_train, y_valid, y_test)):\n",
    "        for image, label in zip(X, y):\n",
    "            print(f\"\\r{idx + 1}/70000\", end=\"\")\n",
    "            filename = f\"{idx:05d}.png\"\n",
    "            plt.imsave(mnist_path / filename, np.tile(image, 3))\n",
    "            line = f\"{split},gs://{bucket_name}/mnist/{filename},{label}\\n\"\n",
    "            import_csv.write(line)\n",
    "            idx += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PPipbXJ8VFye"
   },
   "source": [
    "이 데이터셋을 GCS에 업로드해 보겠습니다:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yc3RXmfVVFye",
    "outputId": "1728b1d3-8b6d-44ee-df83-fd9705b93706"
   },
   "outputs": [],
   "source": [
    "upload_directory(bucket, mnist_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E5WtBg_bVFyf"
   },
   "source": [
    "이제 버텍스 AI에서 관리용 이미지 데이터셋을 만들어 보겠습니다:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P_d7f2wUVFyf",
    "outputId": "9b9be52c-27f8-48d5-819a-9072b5db6ebe"
   },
   "outputs": [],
   "source": [
    "from aiplatform.schema.dataset.ioformat.image import single_label_classification\n",
    "\n",
    "mnist_dataset = aiplatform.ImageDataset.create(\n",
    "    display_name=\"mnist-dataset\",\n",
    "    gcs_source=[f\"gs://{bucket_name}/mnist/import.csv\"],\n",
    "    project=project_id,\n",
    "    import_schema_uri=single_label_classification,\n",
    "    sync=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pw9Uy5TPVFyf"
   },
   "source": [
    "이 데이터셋에 대해 AutoML 학습 작업을 생성합니다:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3D5g74g1VFyf"
   },
   "source": [
    "**TODO**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ai4FvYpKVFyf",
    "tags": []
   },
   "source": [
    "# 연습문제 해답"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aAf4fPN5VFyf"
   },
   "source": [
    "## 1. to 8."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xnLyD5xMVFyf"
   },
   "source": [
    "부록 A 참조"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mRo_LlvQVFyf"
   },
   "source": [
    "## 9.\n",
    "_문제: (원하는 어떤 모델이든) 모델을 훈련하고 TF 서빙이나 구글 버텍스 AI 플랫폼에 배포해보세요. REST API나 gRPC API를 사용해 쿼리하는 클라이언트 코드를 작성해보세요. 모델을 업데이트하고 새로운 버전을 배포해보세요. 클라이언트 코드가 새로운 버전으로 쿼리할 것입니다. 첫 번째 버전으로 롤백해보세요._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KA16G_wCVFyf"
   },
   "source": [
    "<a href=\"#텐서플로-모델-서빙하기\">텐서플로 모델 서빙하기</a> 절에 있는 단계를 따라해 보세요."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yoGwY-UMVFyg"
   },
   "source": [
    "# 10.\n",
    "_문제: 하나의 머신에 여러 개의 GPU에서 `MirroredStrategy` 전략으로 모델을 훈련해보세요(GPU를 준비하지 못하면 코랩의 GPU 런타임을 사용하여 가상 GPU 2개를 만들 수 있습니다). `CentralStorageStrategy` 전략으로 모델을 다시 훈련하고 훈련 시간을 비교해보세요._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GvoUvXYJVFyg"
   },
   "source": [
    "[여러 디바이스에서 모델 훈련하기](#여러-디바이스에서-모델-훈련하기) 절에 있는 단계를 따라해 보세요."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PLTko9_OVFyg"
   },
   "source": [
    "# 11.\n",
    "_문제: 케라스 튜너 또는 버텍스 AI의 하이퍼파라미터 튜닝 서비스를 사용하여 버텍스 AI에서 원하는 모델을 세부 튜닝해 보세요._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cP347D6UVFyg"
   },
   "source": [
    "이 책의 _케라스 튜너를 사용한 하이퍼파라미터 튜닝_ 섹션에 있는 지침을 따르세요."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fYqjiBKGVFyg"
   },
   "source": [
    "# 축하합니다!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tH2LQUP2VFyg"
   },
   "source": [
    "책의 마지막에 도달했습니다! 도움이 되셨기를 바랍니다. 😊"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
