{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. 벨만 방정식"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p align=\"center\"><img src=\"./images/fig_03_01.png\" width=500></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이번 장의 목표는 확률적 백업 다이어그램에서 상태 가치 함수를 구하는 것입니다. 이 때 핵심이 벨만 방정식(bellman equation) 입니다.  \n",
    "벨만 방정식은 마르코프 결정 과정에서 성립하는 가장 중요한 방정식이며 많은 강화학습 알고리즘에 중요한 기초를 제공합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 벨만 방정식 도출"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.1 확률과 기댓값(사전 준비)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "주사위 백업 다이어그램은 아래 그림과 같이 표현할 수 있습니다.\n",
    "\n",
    "<p align=\"center\"><img src=\"./images/fig_03_02.png\" width=300></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "기댓값을 계산하면\n",
    "\n",
    "<p align=\"center\"><img src=\"./images/eq_03_00_1.png\" width=500></p>  \n",
    "<p align=\"center\"><img src=\"./images/eq_03_00_2.png\" width=200></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "주사위와 동전을 순서대로 던지는 문제의 백업 다이그램\n",
    "<p align=\"center\"><img src=\"./images/fig_03_03.png\" width=500></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p align=\"center\"><img src=\"./images/fig_03_04.png\" width=500></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "보상의 기대값을 계산하면\n",
    "<p align=\"center\"><img src=\"./images/eq_03_00_3.png\" width=600></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "문자로 표현하면 주사위 눈을 $x$, 동전의 결과 $y$라고 할 때, 주사위 눈 개수에 따라 동전 앞면이 나올 확률이 달라집니다.  \n",
    "이 설정은 조건부 확률 $p(y|x)$로 표현하면 다음과 같습니다.\n",
    ">$p(y=앞|x=4)=0.8$  \n",
    "$p(y=뒤|x=4)=0.2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "또한 x와 y가 동시에 일어날 확률, 즉 '동시 확률'은 다음과 같습니다.\n",
    ">$p(x,y)=p(x)p(y|x)$  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "보상은 x와 y의 값에 의해 결정된다. 따라서 보상을 함수 r(x,y)로 나타냅니다.\n",
    ">$r(x=4,y=앞)=4$  \n",
    "$r(x=3,y=뒤)=0$  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "기대값 = 값 x 그 값이 발생할 확률의 합  \n",
    "<p align=\"center\"><img src=\"./images/eq_03_00_4.png\" width=500></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.2 벨만 방정식 도출"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "앞 장에서 '수익'을 다음과 같이 정의했습니다. 이번 절에서는 보상을 무한히 계속 받을 수 있는 지속적 과제를 가정합니다.  \n",
    "수익 $G_{t}$는 시간 $t$이후로 얻을 수 있는 보상의 총합으로 할인율 $\\gamma$에 따라 더 나중에 받는 보상일수록 기하급수적으로 감소합니다. \n",
    "<p align=\"center\"><img src=\"./images/eq_03_01.png\" width=700></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[식3.2]는 시간 $t+1$이후에 얻을 수 있는 보상의 총합으로 아래와 같이 나타낼 수 있습니다.\n",
    "<p align=\"center\"><img src=\"./images/eq_03_02.png\" width=700></p>  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[식3.3]으로부터 수익인 $G_{t}$와 $G_{t+1}$의 관계를 알 수 있습니다. 이 관계는 수많은 강화학습 이론과 알고리즘에서 사용됩니다.  \n",
    "\n",
    "<p align=\"center\"><img src=\"./images/eq_03_03.png\" width=700></p>  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이어서 [식3.3]을 상태 가치 함수의 수식에 대입해보겠습니다. 상태 가치 함수는 수익에 대한 기대값(기대수익)이며, 다음 식으로 정의됩니다.  \n",
    "\n",
    "<p align=\"center\"><img src=\"./images/eq_03_04.png\" width=700></p>  \n",
    "\n",
    "[식3.4]와 같이 상태 s의 상태 가치 함수가 $v_{\\pi}(s)$로 표현됩니다. 이 식에 $G_t$의 [식3.3]을 대입하면 다음과 같습니다.\n",
    "\n",
    "<p align=\"center\"><img src=\"./images/eq_03_05.png\" width=700></p>  \n",
    "\n",
    "마지막 식의 전개는 기댓값의 '선형성'덕분에 성립됩니다. 선형성이란 확률변수 X와 Y가 있을 때 $E[X+Y] = E[X]+E[Y]$가 성립함을 말합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> NOTE : 이 책에서는 에이전트의 정책을 확률적 정책 $\\pi(a|s)$로 가정합니다. 결정적 정책도 확률적 정책으로 표현할 수 있기 때문이죠. 마찬가지로 환경의 상태 전이도 확률적이라고, 즉 수식으로 $p(s'|s,a)$라고 가정합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 [식3.5]의 항을 하나씩 구해보겠습니다.  \n",
    "\n",
    "첫번째 항은 $E_\\pi[R_t|S_t=s]$입니다.\n",
    "<p align=\"center\"><img src=\"./images/fig_03_05.png\" width=500></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "현재 상태가 s이고, 에이전트는 정책 $\\pi(a|s)$에 따라 행동합니다. 예를 들어 다음의 세가지 행동을 취할 수 있다고 해봅시다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p align=\"center\">\n",
    "<img src=\"./images/fig_03_05_1.png\" width=200>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "에이전트는 이 확률 분포에 따라 행동을 선택합니다. 그러면 상태 전이 확률 $p(s'|s,a)$에 따라 새로운 상태 $s'$로 이동합니다.  \n",
    "예를 들어 행동 $a_1$을 수행했을 때 전이될 수 있는 상태 후보가 두개라면 다음과 같은 값을 취합니다.\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"./images/fig_03_05_2.png\" width=300>\n",
    "</p>\n",
    "\n",
    "그리고 마지막으로 보상은 $r(s,a,s')$ 함수로 결정됩니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 구체적인 예를 들어 계산해봅시다. 에이전트가 0.2의 확률로 행동 $a_1$을 선택하고 0.6의 확률로 상태 $s_1$로 전이한다고 가정하죠. 이 경우 얻게 되는 보상은 다음과 같습니다.\n",
    "\n",
    "- $\\pi(a=a_1|s)*p(s'=s_1|s,a=a_1)=0.2*0.6=0.12$ 의 확률로\n",
    "- $r(s,a=a_1,s'=s_1)$ 의 보상을 얻는다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "기댓값을 구하려면 모든 후보에 똑같은 계산을 수행하여 다 더하면 됩니다.\n",
    "<p align=\"center\"><img src=\"./images/fig_03_05_3.png\" width=500></p>\n",
    "\n",
    "이와 같이\n",
    "- '에이전트가 선택하는 행동의 확률' $\\pi(a|s)$\n",
    "- '전이되는 상태의 확률' $p(s'|s,a)$\n",
    "- '보상 함수' $r(s,a,s')$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p align=\"center\"><img src=\"./images/fig_03_06.png\" width=500></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "수익의 기댓값을 $v_\\pi(s)$인 **상태 가치 함수**  [식3.4]를 다시 살펴 보면\n",
    "<p align=\"center\"><img src=\"./images/eq_03_04.png\" width=700></p>  \n",
    "\n",
    "먼저 식[3.4]의 $t$에 $t+1$을 대입합니다. \n",
    "<p align=\"center\"><img src=\"./images/fig_03_06_1.png\" width=250></p> \n",
    "\n",
    "이 식은 $S_{t+1} = s$에서의 상태 가치 함수입니다.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 우리의 관심은 $E_\\pi[G_{t+1}|S_t=s]$ 입니다. 이 식은 현재 시간이 $t$일 때 한 단위 뒤 시간 $(t+1)$의 기대 수익을 뜻합니다.  \n",
    "\n",
    "문제 해결의 핵심은 조건인 $S_t=s$를 $S_{t+1}=s$ 형태로 바꾸는 것입니다. 즉, 시간을 한 단위만큼 흘려보내는 것입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "앞에서와 마찬가지로 구체적인 예를 들어 설명하겠습니다. 지금 에이전트의 상태는 $S_t=s$ 입니다.  \n",
    "그리고 에이전트가 0.2의 확률로 행동 $a_1$을 선택하고, 0.6의 확률로 상태 $s_1$로 전이한다고 해봅시다.  \n",
    "그러면 다음과 같이 나타낼 수 있습니다.  \n",
    "\n",
    "- $\\pi(a=a_1|s)*p(s'=s_1|s,a=a_1)=0.2*0.6=0.12$ 의 확률로  \n",
    "- $E_\\pi[G_{t+1}|S_{t+1}=s_1]=v_\\pi(s_1)$ 로 전이된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이와 같이 다음 단계의 시간을 '보는' 것으로 다음 상태의 가치함수를 얻을 수 있습니다. 이제 기댓값 $E_\\pi[G_{t+1}|S_{t}=s]$ 를 구하려면 모든 후보에 이 계산을 수행하여 다 더합니다.\n",
    "\n",
    "<p align=\"center\"><img src=\"./images/fig_03_06_2.png\" width=500></p> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "앞서 전개한 식에 대입하면 다음 식이 도출됩니다. 식[3.6]이 바로 **벨만방정식**입니다.\n",
    "\n",
    "<p align=\"center\"><img src=\"./images/eq_03_06.png\" width=700></p> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "벨만 방정식은 '상태 $s$의 상태 가치 함수'와 '다음에 취할 수 있는 상태 $s'$의 상태 가치 함수'의 관계를 나타낸 식으로, 모든 상태 s와 모든 정책 $\\pi$ 에 대해 성립합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 벨만 방정식의 예"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "벨만 방정식을 이용하면 상태 가치 함수를 구할 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.1 두 칸짜리 그리드 월드"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p align=\"center\"><img src=\"./images/fig_03_07.png\" width=500></p> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$v_{\\pi}(L1)$은 상태 $L1$에서 무작위 정책 $\\pi$에 따라 행동했을 때 얻을 수 있는 기대수익입니다. 이 기대 수익은 앞으로 무한히 지속되는 보상의 총합입니다.\n",
    "\n",
    "<p align=\"center\"><img src=\"./images/fig_03_08.png\" width=500></p> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[그림3-8]과 같이 지금 문제는 무한히 분기되어 뻗어나가는 계산입니다. 이처럼 무한히 분기하는 계산을 **벨만 방정식**을 이용하여 구할 수 있습니다.\n",
    "\n",
    "<p align=\"center\"><img src=\"./images/eq_03_07.png\" width=700></p> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이번 문제에서 상태는 결정적으로 전이됩니다. 즉 상태 전이는 확률 $f(s,a)$에 의해 결정됩니다. 이를 [식3.7]에 대입하면 다음과 같습니다.\n",
    "- $s'=f(s,a)$ 이면 $p(s'|s,a)=1$\n",
    "- $s'≠f(s,a)$ 이면 $p(s'|s,a)=0$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[식3.7]에서는 $s'=f(s,a)$를 만족하는 $s'$에 해당하는 항만 남습니다.  \n",
    "\n",
    "<p align=\"center\"><img src=\"./images/eq_03_08.png\" width=700></p> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p align=\"center\"><img src=\"./images/fig_03_09.png\" width=400></p> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[그림3-9]를 보면 백업 다이어그램이 두 갈래로 나뉘어 있습니다. 하나는 0.5의 확률로 행동 Left를 선택하고 상태는 전이되지 않습니다. 보상은 -1입니다. 이때 할인율 $\\gamma$를 0.9로 설정한다.\n",
    "\n",
    "상태 L1에서의 벨만 방정식을 구해보면\n",
    "\n",
    "- [식3.8]에서 Left를 선택하는 경우는 다음과 같습니다.\n",
    "<p align=\"center\"><img src=\"./images/fig_03_09_1.png\" width=240></p>  \n",
    "\n",
    "- Right를 선택했을 때를 계산하면\n",
    "<p align=\"center\"><img src=\"./images/fig_03_09_2.png\" width=200></p> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이 식을 벨만 방정식으로 나타내면  \n",
    "<p align=\"center\"><img src=\"./images/fig_03_09_3.png\" width=500></p> \n",
    "\n",
    "<p align=\"center\"><img src=\"./images/eq_03_09.png\" width=700></p> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "상태 L2에서의 벨만 방정식은 아래와 같다\n",
    "<p align=\"center\"><img src=\"./images/fig_03_10.png\" width=400></p> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p align=\"center\"><img src=\"./images/eq_03_10.png\" width=700></p> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이렇게 하여 모든 상태에서의 벨만 방정식을 구했습니다. 이제 알고 싶은 변수는 $v_{\\pi}(L1)$ 과  $v_{\\pi}(L2)$ 가 남았습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p align=\"center\"><img src=\"./images/eq_03_10_1.png\" width=300></p> \n",
    "<p align=\"center\"><img src=\"./images/fig_03_10_1.png\" width=200></p> \n",
    "<p align=\"center\"><img src=\"./images/eq_03_10_2.png\" width=200></p> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 이는 무작위 정책의 상태 가치 함수입니다.\n",
    "- 즉, 상태 L1에서 무작위로 행동하면 -2.25의 수익을 기대할 수 있다는 뜻입니다.\n",
    "- 무작위로 행동하다 보면 벽에 부딪힐 수도 있으니 미래의 보상이 마이너스가 될 수 있습니다.\n",
    "- 또한 $v_{\\pi}(L1)$의 값이 $v_{\\pi}(L2)$보다 큰 이유도 L1 옆에 사과가 있고, \n",
    "- 첫번째 행동에서 그 사과를 얻을 확률이 50%이기 때문입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.2 벨만 방정식의 의의"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p align=\"center\"><img src=\"./images/eq_03_01.png\" width=700></p>\n",
    "<p align=\"center\"><img src=\"./images/eq_03_04.png\" width=700></p>  \n",
    "<p align=\"center\"><img src=\"./images/eq_03_03.png\" width=700></p>  \n",
    "<p align=\"center\"><img src=\"./images/eq_03_05.png\" width=700></p>  \n",
    "<p align=\"center\"><img src=\"./images/eq_03_07.png\" width=700></p> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "벨만 방정식을 통해 무한히 계속되는 계산을 유한한 연립방정식으로 변환할 수 있었습니다. 이번처럼 행동이 무작위로 이루어지더라도 벨만 방정식을 이용하면 상태 가치 함수를 구할 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> NOTE : 상태 가치 함수는 기대 수익이며 '무한히 이어지는' 보상의 합으로 정의됩니다.\n",
    "하지만 [식3.6]에서 보듯 벨만 방정식에는 '무한'이라는 개념이 없습니다. 벨만 방정식 덕분에 무한의 굴레에서\n",
    "빠져나온 셈입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "또한 이번 문제는 매우 단순했지만 복잡한 문제라도 벨만 방정식을 이용해 연립방정식으로 표현할 수 있습니다. 그리고 연립방정식을 푸는 알고리즘을 이용하면 자동으로 상태 가치 함수를 구할 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 행동 가치 함수(Q 함수)와 벨만 방정식"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "행동 가치 함수(action-value function)\n",
    "- 행동 가치 함수의 정의식을 살펴본 후 행동 가치 함수를 이용한 벨만 방정식을 도출하겠습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> NOTE : 이 책에서는 설명을 간결하게 하고자 상태 가치 함수를 단순히 '가치 함수'로 줄여 쓰기도 합니다. 또한 행동 가치 함수는 관례적으로 Q함수라고 부릅니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.1 행동 가치 함수"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**상태 가치 함수**\n",
    "<p align=\"center\"><img src=\"./images/eq_03_04.png\" width=700></p>   \n",
    "\n",
    "상태 가치 함수의 조건은 '상태가 $s$ 일 것'과 '정책이 $\\pi$ 일 것' 두 가지 입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "그리고 조건에 '행동 a'를 추가할 수 있는데 이것이 바로 **행동 가치 함수**(Q 함수)입니다.  \n",
    "<p align=\"center\"><img src=\"./images/eq_03_10_3.png\" height=50></p> \n",
    "\n",
    "Q 함수는 시간 t일 때 상태 s에서 행동 a를 취하고, 시간 t+1부터는 $\\pi$에 따라 행동을 결정합니다. 이때 얻을 수 있는 기대수익이 $q_{\\pi}(s,a)$입니다.  \n",
    "\n",
    "Q 함수는 상태 가치 함수에 행동 a를 조건으로 추가한 것입니다.\n",
    "\n",
    "<p align=\"center\"><img src=\"./images/fig_03_11.png\" width=700></p> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "상태 가치 함수에서의 행동 a는 정책 $\\pi$에 따라 선택됩니다. 반면 Q 함수에서 행동 a는 자유롭게 선택할 수 있습니다. 상태 가치 함수와 Q함수의 차이는 바로 이 점입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "따라서 **만약 Q 함수의 행동 a를 정책 $\\pi$에 따라 선택하도록 설계하면 Q 함수와 상태 가치 함수는 완전히 같아집니다.** \n",
    "\n",
    "행동 후보 $\\{a_1,a_2,a_3\\}$ 이고, 정책 $\\pi$에 따라 행동한다고 가정하면\n",
    "- $\\pi(a_1|s)$의 확률로 행동 $a_1$ 을 선택하는 경우 Q 함수는 $q_{\\pi}(s,a_1)$\n",
    "- $\\pi(a_2|s)$의 확률로 행동 $a_2$ 을 선택하는 경우 Q 함수는 $q_{\\pi}(s,a_2)$\n",
    "- $\\pi(a_3|s)$의 확률로 행동 $a_3$ 을 선택하는 경우 Q 함수는 $q_{\\pi}(s,a_3)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이 경우 기대 수익은 Q 함수의 가중 합으로 구할 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p align=\"center\"><img src=\"./images/eq_03_10_4.png\" width=500></p> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이 식은 상태 가치 함수와 똑같은 조건에서의 기대 수익입니다. 따라서 다음 식이 성립합니다.\n",
    "\n",
    "<p align=\"center\"><img src=\"./images/eq_03_11.png\" width=700></p> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.2 행동 가치 함수를 이용한 벨만 방정식"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p align=\"center\"><img src=\"./images/eq_03_12.png\" width=700></p> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "여기서 상태 $s$와 행동 $a$는 정해져 있습니다. 그렇다면 다음상태 $s'$로의 전이 확률은 $p(s'|s,a)$이고 보상은 $r(s,a,s')$ 함수에 의해 주어집니다. 이 점을 고려하면 [식3.12]는 다음과 같이 전개됩니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p align=\"center\"><img src=\"./images/eq_03_13.png\" width=700></p> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**행동 가치 함수(Q 함수)를 이용한 벨만 방정식**  \n",
    "<p align=\"center\"><img src=\"./images/eq_03_14.png\" width=700></p> \n",
    "\n",
    "여기서 $a'$는 시간 t+1에서의 행동입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 벨만 최적 방정식"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "최적 정책이란 모든 상태에서 상태 가치 함수가 최대인 정책입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4.1 상태 가치 함수의 벨만 최적 방정식"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "벨만 방정식\n",
    "<p align=\"center\"><img src=\"./images/eq_03_07.png\" width=700></p> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "벨만 방정식은 어떠한 정책에서도 성립합니다. 따라서 최적 정책을 $\\pi_{*}(a|s)$ 라고 하면 다음과 같은 벨만 방정식이 성립합니다.\n",
    "\n",
    "<p align=\"center\"><img src=\"./images/eq_03_15.png\" width=700></p> \n",
    "\n",
    "이제 우리가 고민하고 싶은 문제는 최적 정책 $\\pi_{*}(a|s)$ 에 의해 선택되는 행동 a입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p align=\"center\"><img src=\"./images/fig_03_12.png\" width=500></p> \n",
    "\n",
    "이때 어떤 확률 분포로 행동을 선택해야 할까요?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "물론 최적 정책이기 때문에 값이 최대인 행동 $a_3$를 100% 확률로 선택해야 합니다.  \n",
    "결정적 정책인 셈이죠. 따라서 확률적 정책 $\\pi_{*}(a|s)$는 결정적 정책 $\\mu_{*}(s)$로 나타낼 수 있습니다.  \n",
    "그리고 항상 $a_3$를 선택하기 때문에 $v_{\\pi}(s)$의 값은 4가 됩니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "벨만 최적 방정식\n",
    "\n",
    "<p align=\"center\"><img src=\"./images/eq_03_16.png\" width=700></p> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4.2 행동 가치 함수의 벨만 최적 방정식"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "행동 가치 함수(Q 함수)에 대해서도 마찬가지로 벨만 최적 방정식을 구할 수 있습니다.  \n",
    "최적 정책에서의 행동 가치 함수를 최적 행동 가치 함수 $q_{*}$ 라고 합니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q 함수의 벨만 방정식\n",
    "\n",
    "<p align=\"center\"><img src=\"./images/eq_03_16_1.png\" width=500></p> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "최적 정책 $\\pi_{*}$를 대입하면\n",
    "\n",
    "<p align=\"center\"><img src=\"./images/eq_03_17.png\" width=700></p> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q 함수에 대한 벨만 최적 방정식**\n",
    "<p align=\"center\"><img src=\"./images/eq_03_18.png\" width=700></p> \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> NOTE : MDP에서는 결정적 최적 정책이 하나 이상 존재합니다. 결정적 정책이란 특정 상태에서는 반드시\n",
    "특정 행동을 선택하는 정책입니다. 따라서 최적 정책은 $\\mu_{*}(s)$ 와 같이 함수로 나타낼 수 있습니다. 또한 문제에 따라 최적 정책이 여러 개일 수도 있지만, 그 가치 함수들의 값은 모두 같습니다. 따라서 최적 정책의 가치함수는 $v_{*}(s)$ 라는 하나의 기호로 나타낼 수 있습니다. 마찬가지로 최적 정책의 Q 함수도 $q_{*}(s,a)$ 로 하나만 존재합니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.5 벨만 최적 방적식의 예"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p align=\"center\"><img src=\"./images/fig_03_13.png\" width=300></p> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5.1 벨만 최적 방정식 적용"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "벨만 최적 방정식은 \n",
    "\n",
    "<p align=\"center\"><img src=\"./images/eq_03_16.png\" width=700></p> \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "또한 상태 전이가 결정적이라면 다음과 같이 단순화할 수 있습니다. \n",
    "\n",
    "<p align=\"center\"><img src=\"./images/eq_03_19.png\" width=700></p> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이렇게 단순화할 수 있는 이유는 다음 식이 성립하기 때문입니다.\n",
    "- $s'=f(s,a)$ 이면 $p(s'|s,a)=1$\n",
    "- $s'≠f(s,a)$ 이면 $p(s'|s,a)=0$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p align=\"center\"><img src=\"./images/fig_03_14.png\" width=500></p> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[그림3-14]를 참고하여 할인율이 0.9일 때의 벨만 최적 방정식은 다음과 같습니다.\n",
    "\n",
    "<p align=\"center\"><img src=\"./images/fig_03_14_1.png\" width=300></p> \n",
    "\n",
    "$$v_*(L1) = 1 + 0.9v_*(L2)$$\n",
    "$$v_*(L2) = 0.9v_*(L1)$$\n",
    "\n",
    "<p align=\"center\"><img src=\"./images/fig_03_14_2.png\" width=150|></p> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> CAUTION : 앞의 연립방정식에서 max 연산을 하고 있는데, max는 비선형 연산입니다. 따라서 이 연립방정식을 푸는 알고리즘은 '선형 방정식 계산기'로는 풀 수 없지만, '비선형 방정식 계산기'를 사용하면 풀 수 있습니다. 물론 이번처럼 단순한 문제라면 직접 계산할 수도 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "두 칸짜리 그리드 월드처럼 단순한 문제라면 벨만 최적 방정식을 직접 손으로 계산하여 풀 수도 있습니다. 하지만 우리가 궁극적으로 알고 싶은 것은 최적 정책입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5.2 최적 정책 구하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "최적 행동 가치 함수 $q_{*}(s,a)$를 알고 있다고 가정합시다. 그렇다면 상태 s에서의 최적 행동은 다음과 같이 구할 수 있습니다.\n",
    "\n",
    "<p align=\"center\"><img src=\"./images/eq_03_20.png\" width=700|></p> \n",
    "\n",
    "argmax는 (최댓값이 아니라) 최댓값을 만들어내는 인수(이번에는 행동 a)를 반환합니다. 이 식과 같이 최적 행동 가치 함수를 알고 있는 경우, 함수의 값이 최대가 되는 행동을 선택하면 됩니다. 그 행동을 선택하는 것이 바로 최적 정책인 것입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p align=\"center\"><img src=\"./images/eq_03_13_1.png\" width=700></p> \n",
    "\n",
    "이 식의 $q_\\pi$와 $v_\\pi$에서 정책의 첨자 $\\pi$를 최적 정책인 첨자 *로 대체할 수 있습니다.  \n",
    "그리고 [식3.20]에 대입하면 다음 식이 만들어 집니다.\n",
    "\n",
    "<p align=\"center\"><img src=\"./images/eq_03_21.png\" width=700></p> \n",
    "\n",
    "[식3.21]과 같이 최적 상태 가치 함수 $v_*(s)$를 사용하여 최적 정책 $\\mu_*(s)$를 얻을 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> NOTE : [식3.20]과 [식3.21]은 '탐욕 정책'이라고도 할수 있습니다. 탐욕 정책은 국소적인 후보 중에서 최선의 행동을 찾습니다. 이번처럼 벨만 최적 방정식에서는 현재 상태(s)와 다음 상태(s')만이 관련 있으며, 단순히 다음 상태만을 고려하여 가치가 가장 큰 행동을 선택합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p align=\"center\"><img src=\"./images/fig_03_15.png\" width=500></p> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "그림에서 보듯 취할 수 있는 행동은 Left와 Right 두가지입니다. Left를 선택하면 상태 L1로 전이하여 보상 -1을 얻습니다.  \n",
    "그러면 [식3.21]의\n",
    "<p align=\"center\"><img src=\"./images/eq_03_21.png\" width=700></p>\n",
    "값은 다음과 같습니다. (0.9는 할인율)\n",
    "\n",
    "<p align=\"center\"><img src=\"./images/fig_03_15_1.png\" width=400></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "한편 행동 **Right**를 선택하면 상태 L2로 전이하여 보상 1을 얻습니다. 이 경우의 값은 다음과 같습니다.\n",
    "<p align=\"center\"><img src=\"./images/fig_03_15_2.png\" width=350></p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "따라서 값이 더 큰 행동은 Right입니다. **상태 L1에서의 최적 행동은 Right라는 뜻입니다.**  \n",
    "\n",
    "같은 방식으로 **상태 L2에서의 최적 행동을 찾으면 Left가 나옵니다.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**L1에서는 오른쪽으로 L2에서는 왼쪽으로 이동하는 행동이 최적 정책입니다.**\n",
    "<p align=\"center\"><img src=\"./images/fig_03_16.png\" width=400></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.6 정리"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "벨만 방정식을 도출하고, 연립 방정식을 얻고 이를 통해 가치 함수를 구할 수 있었습니다.  \n",
    "안타깝게도 실용적인 문제에서는 계산량이 너무 많아져서 연립방정식을 이용하는 방법은 적용할 수 없지만, 벨만 방정식은 많은 강화 학습 알고리즘에 중요한 기초를 제공합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "강화 학습의 궁극적인 목표는 최적 정책 찾기입니다.  \n",
    "이번 장에서는 벨만 최적 방정식에 대해 배웠고, 벨만 최적 방정식은 최적 정책에서 성립하는 특별한 벨만 방정식입니다. 최적 정책의 가치 함수를 구할 수 있다면 쉽게 최적 정책을 찾을 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p align=\"center\"><img src=\"./images/fig_03_16_1.png\" width=600></p>\n",
    "<p align=\"center\"><img src=\"./images/fig_03_16_2.png\" width=600></p>\n",
    "<p align=\"center\"><img src=\"./images/fig_03_16_3.png\" width=600></p>"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
