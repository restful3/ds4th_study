{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "RqusGuvMrhfP"
   },
   "outputs": [],
   "source": [
    "# pip install transformers==4.40.1 datasets==2.19.0 sentence-transformers==2.7.0 faiss-cpu==1.8.0 llama-index==0.10.34 llama-index-embeddings-huggingface==0.2.0 -qqq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 예제 10.1 문장 임베딩을 활용한 단어 간 유사도 계산"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bERp0FI_rlWK"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.99999994, 0.5950743 , 0.32537544], dtype=float32)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 문장을 벡터로 변환해주는 사전학습된 모델을 로딩/사용\n",
    "from sentence_transformers import SentenceTransformer \n",
    "\n",
    "# 두 벡터 간의 코사인 유사도를 계산하는 함수\n",
    "# 코사인 유사도는 두 벡터 사이의 각도를 기반으로 유사도를 측정\n",
    "# 코사인 유사도 범위 : -1 ~ 1 (1 매우 유사, 0 관련 없음, -1 정반대)\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "\n",
    "# snunlp/KR-SBERT-V40K-klueNLI-augSTS : 한국어 전용 SBERT(Sentence-BERT) 모델\n",
    "smodel = SentenceTransformer('snunlp/KR-SBERT-V40K-klueNLI-augSTS')\n",
    "\n",
    "dense_embeddings = smodel.encode(['학교', '공부', '운동'])\n",
    "cosine_similarity(dense_embeddings)[0] # 학교 기준\n",
    "\n",
    "\n",
    "#                  학교          공부        운동\n",
    "# array([학교 : [0.99999994, 0.5950743 , 0.32537544],\n",
    "#        공부 : [0.5950743 , 1.        , 0.54595673],\n",
    "#        운동 : [0.32537544, 0.54595673, 1.0000002 ]], dtype=float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 예제 10.2 원핫 인코딩의 한계"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "id": "vcMwxBo6rmbI"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine similarity (school, study): 0.0\n",
      "Cosine similarity (school, workout): 0.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# 원핫 인코딩된 벡터를 딕셔너리로 정의\n",
    "word_dict = {\n",
    "    \"school\": np.array([[1, 0, 0]]),\n",
    "    \"study\": np.array([[0, 1, 0]]),\n",
    "    \"workout\": np.array([[0, 0, 1]])\n",
    "}\n",
    "\n",
    "# school과 study 사이의 코사인 유사도 계산\n",
    "# 두 벡터는 완전히 직각(orthogonal)이므로 유사도는 0\n",
    "cosine_school_study = cosine_similarity(word_dict[\"school\"], word_dict['study'])  # 0.0\n",
    "\n",
    "# school과 workout 사이의 코사인 유사도 계산\n",
    "# 마찬가지로 전혀 관련 없는 차원 => 유사도는 0\n",
    "cosine_school_workout = cosine_similarity(word_dict['school'], word_dict['workout'])  # 0.0\n",
    "\n",
    "print(\"Cosine similarity (school, study):\", cosine_school_study[0][0])\n",
    "print(\"Cosine similarity (school, workout):\", cosine_school_workout[0][0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Using Embedding Vectors]\n",
      "Cosine similarity (school, study): 0.9957846018984899\n",
      "Cosine similarity (school, workout): 0.2925567851696626\n"
     ]
    }
   ],
   "source": [
    "word_embeddings = {\n",
    "    \"school\": np.array([[0.9, 0.8, 0.1]]),\n",
    "    \"study\":  np.array([[0.85, 0.75, 0.2]]),\n",
    "    \"workout\": np.array([[0.1, 0.2, 0.95]])\n",
    "}\n",
    "\n",
    "# 임베딩 기반 유사도 측정\n",
    "cosine_school_study_embed = cosine_similarity(word_embeddings[\"school\"], word_embeddings[\"study\"])  \n",
    "cosine_school_workout_embed = cosine_similarity(word_embeddings[\"school\"], word_embeddings[\"workout\"])\n",
    "\n",
    "print(\"\\n[Using Embedding Vectors]\")\n",
    "print(\"Cosine similarity (school, study):\", cosine_school_study_embed[0][0])      # e.g. ~0.99\n",
    "print(\"Cosine similarity (school, workout):\", cosine_school_workout_embed[0][0])  # e.g. ~0.4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 예제 10.2 Sentence-Transformers 라이브러리로 바이 인코더 생성하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "id": "sXcPv_Otrpqk"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\me\\anaconda3\\envs\\lang310\\lib\\site-packages\\huggingface_hub\\file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at klue/roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 768)\n",
      "[[ 4.52818088e-02 -3.72968256e-01  9.29080248e-02 -6.31986186e-02\n",
      "  -2.63903290e-01 -4.54857387e-02 -3.42952907e-01 -3.42025250e-01\n",
      "  -3.49420041e-01  2.75499463e-01 -3.32062133e-02 -1.40977010e-01\n",
      "   1.65380642e-01  3.45746964e-01  2.02794205e-02  6.19507954e-02\n",
      "   2.59422548e-02 -1.37198254e-01 -2.29651388e-02 -1.13240995e-01\n",
      "  -4.67441201e-01  1.21111870e-01  1.46921694e-01  4.28803802e-01\n",
      "  -2.53313899e-01 -4.21574473e-01 -1.64629921e-01 -6.55333325e-02\n",
      "  -2.41080523e-01 -2.53970772e-01 -1.90059230e-01 -2.43895695e-01\n",
      "  -1.38974324e-01  1.42828703e-01  6.01070106e-01  8.84372219e-02\n",
      "   3.58265527e-02  4.91743572e-02 -3.69422808e-02 -3.55697125e-01\n",
      "  -6.39413148e-02 -1.04777902e-01 -2.05031052e-01 -9.72695351e-02\n",
      "   3.87814522e-01 -6.81074038e-02 -2.31856316e-01  2.53421426e-01\n",
      "  -2.98822850e-01 -2.59820670e-01  2.99505025e-01 -3.51672858e-01\n",
      "   2.00901166e-01 -5.22441626e-01 -2.16984063e-01  5.24030440e-02\n",
      "  -6.53395951e-02  2.38249362e-01 -2.17821136e-01 -1.39164314e-01\n",
      "   5.51542789e-02 -2.12896824e-01 -1.29063189e-01  9.69747379e-02\n",
      "  -1.90719832e-02 -2.29477242e-01  1.88885093e-01 -2.81656206e-01\n",
      "  -2.37495992e-02  1.87896654e-01 -2.36526757e-01 -1.96538270e-02\n",
      "   4.58899915e-01  1.60962537e-01  4.79462087e-01  2.31308322e-02\n",
      "  -9.96688679e-02  8.01535323e-02 -3.42914835e-02 -1.70460165e-01\n",
      "   1.48327172e-01  1.35639131e-01 -4.34146285e-01 -1.23722479e-02\n",
      "  -8.01214799e-02 -2.92887300e-01  1.05904359e-02 -1.22300439e-01\n",
      "  -2.43388310e-01 -1.72992781e-01 -9.70578194e-01 -1.02329671e-01\n",
      "  -3.65841240e-01  7.61384219e-02  1.31221801e-01  1.35860339e-01\n",
      "  -5.07315844e-02  4.35788445e-02 -4.04099375e-02 -2.77913690e-01\n",
      "   1.45283207e-01  1.38125479e-01 -2.86662877e-01  2.44203173e-02\n",
      "  -2.15576023e-01 -4.29506637e-02 -1.75993815e-01  7.62605295e-02\n",
      "   2.08855599e-01  4.73189279e-02  5.18606044e-02  1.04495555e-01\n",
      "  -1.63434327e-01 -2.08472788e-01  3.93492609e-01 -1.45792305e-01\n",
      "   3.87896389e-01  2.97783166e-01  1.97419271e-01  2.13475883e-01\n",
      "  -8.70817527e-02 -1.54821560e-01 -1.91967055e-01  4.82617803e-02\n",
      "  -9.62742046e-02 -2.88319647e-01 -1.01837583e-01  5.48892468e-02\n",
      "  -3.49320024e-02  2.61463910e-01 -1.83838949e-01 -4.08275247e-01\n",
      "  -1.40459865e-01  1.37878880e-01  3.42035174e-01 -9.66503099e-02\n",
      "  -2.21561462e-01 -3.40706944e-01  7.80917481e-02  1.91803038e-01\n",
      "  -9.38364193e-02 -1.34226590e-01 -1.91083714e-01 -2.38268793e-01\n",
      "  -6.53141588e-02  5.47693223e-02 -7.73988292e-02  6.08376525e-02\n",
      "  -1.91791132e-02  6.45753089e-03 -2.37568378e-01 -3.02970916e-01\n",
      "   1.76697537e-01 -6.41714036e-02 -1.55047506e-01  4.96221371e-02\n",
      "   2.56951898e-01  1.80636913e-01  7.95138776e-02 -1.16461229e+00\n",
      "  -3.53081226e-01  3.81969780e-01 -4.61598724e-01  9.51247513e-02\n",
      "  -3.01191330e-01 -5.76593280e-02  1.69688240e-01  1.90117918e-02\n",
      "  -1.91380396e-01 -2.15502441e-01 -2.57921308e-01  9.78248566e-02\n",
      "  -7.79865757e-02 -5.76433539e-01 -1.47293717e-01  4.76900004e-02\n",
      "   1.87039211e-01 -2.77033716e-01 -1.12295032e-01 -1.91914156e-01\n",
      "  -1.30402520e-01  5.98066859e-02  1.29536718e-01  1.37602657e-01\n",
      "   1.72130227e-01 -1.49519682e-01  4.64421749e-01 -6.27169535e-02\n",
      "   2.54702508e-01  3.60174984e-01 -5.24580330e-02 -7.95985460e-02\n",
      "  -1.40870273e-01  1.07566074e-01  3.55348408e-01  1.36193573e-01\n",
      "  -1.70291647e-01  4.37442698e-02 -1.78297579e-01 -2.18863055e-01\n",
      "   5.68242148e-02 -2.81509995e-01 -8.11933503e-02 -1.56824678e-01\n",
      "   1.93416119e-01  5.11377037e-01  3.42732877e-01 -2.36595660e-01\n",
      "  -1.96078375e-01 -2.28500701e-02 -3.77307206e-01 -8.93888324e-02\n",
      "   1.08900137e-01 -2.36038312e-01 -1.74284101e-01 -4.62764323e-01\n",
      "   1.85485363e-01 -1.71383187e-01 -2.57583588e-01  2.94210911e-01\n",
      "  -1.69498678e-02 -1.73880067e-02  1.83191278e-03 -8.46083090e-02\n",
      "  -3.10387790e-01  6.05221465e-02 -1.29601881e-01 -8.63041803e-02\n",
      "   7.55730718e-02 -3.96544725e-01  2.15417251e-01 -1.04422845e-01\n",
      "  -3.56531531e-01 -1.67776451e-01  1.54076800e-01 -2.77291401e-03\n",
      "  -1.09563321e-01  1.03750296e-01 -1.86996385e-01 -2.46252358e-01\n",
      "  -2.77159084e-02 -1.86334997e-01  2.94123441e-01 -9.67307091e-02\n",
      "  -3.68626982e-01 -3.36539000e-02  2.04322219e-01 -2.33702689e-01\n",
      "   1.29498377e-01  9.96698439e-02 -9.23595503e-02  7.45451450e-03\n",
      "   5.67551404e-02 -1.14787985e-02  1.25009969e-01 -1.55926412e-02\n",
      "   2.67610192e-01  1.63227871e-01  1.57764465e-01 -4.16869633e-02\n",
      "   1.66487023e-01  8.51722285e-02 -4.22842741e-01  1.00026697e-01\n",
      "  -2.30417117e-01 -1.40100136e-01  1.77854434e-01 -1.21217303e-01\n",
      "  -3.26557934e-01 -2.22875774e-01  5.09889871e-02  5.52638769e-02\n",
      "  -5.19303858e-01  2.99711347e-01  3.11529934e-02 -1.93220183e-01\n",
      "  -4.09378111e-02  5.00237085e-02 -7.85064995e-02 -3.64761502e-02\n",
      "   2.57179826e-01 -1.80960014e-01 -6.30420893e-02  4.91125211e-02\n",
      "  -1.28821090e-01 -4.38764066e-01  5.47653615e-01  3.30089033e-01\n",
      "   2.38528833e-01 -1.01584978e-01  6.99879974e-02 -3.88291985e-01\n",
      "   8.01310614e-02 -2.57303166e+00  2.95366775e-02 -4.07085061e-01\n",
      "   6.76502213e-02  1.96892947e-01 -1.69931829e-01 -4.52539384e-01\n",
      "   3.06480434e-02  9.42320526e-02 -9.65041444e-02  2.52861343e-02\n",
      "  -2.21272841e-01 -1.09732680e-01 -2.06018332e-02 -5.12979180e-02\n",
      "  -2.80811816e-01 -9.60713625e-02  4.31242585e-01 -1.79846084e-03\n",
      "  -9.57220271e-02 -2.35656768e-01  2.19805568e-01 -1.36271313e-01\n",
      "  -3.69546354e-01  2.70387735e-02 -6.22257926e-02  1.98935166e-01\n",
      "   2.46854294e-02 -1.46350935e-01 -6.47987500e-02 -6.09083951e-01\n",
      "  -4.57478404e-01  2.49948904e-01 -8.64689797e-02 -1.30622704e-02\n",
      "   7.18449652e-01 -1.47373363e-01 -1.27725840e-01  2.07755804e-01\n",
      "  -1.96206778e-01  8.24463591e-02  2.84772962e-01  1.44470900e-01\n",
      "   1.03588998e-01  1.41747832e-01 -1.74842343e-01 -2.16042116e-01\n",
      "   1.64721027e-01 -6.37567043e-02  6.80918396e-02 -3.51479262e-01\n",
      "  -5.43950610e-02  3.89137530e+00 -2.84046143e-01 -2.59219073e-02\n",
      "  -3.30521204e-02 -1.79310262e-01 -4.85092476e-02  1.70686483e-01\n",
      "  -6.75558150e-02 -1.15322247e-01  1.18913837e-01 -2.93486267e-01\n",
      "  -1.95287943e-01 -1.51032016e-01 -7.04670548e-02  8.42267349e-02\n",
      "   1.48040548e-01  2.45955631e-01  1.39118265e-02  1.58049405e-01\n",
      "   9.82683823e-02 -7.06019104e-02  5.57505563e-02 -8.46261438e-03\n",
      "  -3.91287893e-01 -1.59064382e-01  1.00048214e-01 -6.30765319e-01\n",
      "  -2.60907859e-01  2.06779182e-01 -2.91278753e-02  2.66729712e-01\n",
      "   6.16541430e-02 -1.14388667e-01 -1.20355211e-01 -1.66342676e-01\n",
      "   2.20077202e-01 -2.60709226e-01  1.27928257e-01 -2.33516265e-02\n",
      "   2.46033117e-01  3.13900888e-01 -1.28820807e-01  5.86211756e-02\n",
      "   2.25495949e-01 -1.14001527e-01  1.40086442e-01 -4.14179832e-01\n",
      "   1.06393375e-01  5.63093945e-02 -1.14036016e-01  2.14686885e-01\n",
      "  -9.83431861e-02  1.13056861e-01 -2.26796418e-01 -1.47800460e-01\n",
      "  -1.35645464e-01  1.24887913e-01 -1.47927508e-01 -1.06440954e-01\n",
      "   3.21433216e-01 -2.69722104e-01  6.77491575e-02 -7.98047185e-02\n",
      "  -2.34878376e-01 -2.50728250e-01 -7.08658472e-02  2.96481550e-01\n",
      "   2.60087371e-01  3.78351241e-01  1.39981538e-01 -2.60407090e-01\n",
      "  -2.43240625e-01 -4.16234702e-01 -1.41896784e-01 -3.04196440e-02\n",
      "   1.52562305e-01 -1.52723223e-01 -2.97496051e-01  3.70735019e-01\n",
      "  -2.31497753e-02 -5.03465272e-02  3.19345802e-01 -4.33439553e-01\n",
      "  -3.08393180e-01 -1.89697996e-01 -1.02758110e-01 -1.05386153e-01\n",
      "  -1.42824948e-01  5.49948737e-02  1.35303795e-01 -9.30552408e-02\n",
      "   9.95991156e-02 -2.04039421e-02 -2.87245631e-01 -1.39370203e-01\n",
      "  -3.01597297e-01  3.49710256e-01 -4.07859832e-01 -1.52126268e-01\n",
      "   6.71461970e-02 -1.62762269e-01  2.54900098e-01 -2.71387637e-01\n",
      "  -5.21410704e-02 -2.04210863e-01 -3.01304877e-01 -6.69738725e-02\n",
      "  -2.12874606e-01 -1.50360502e-02  9.33904275e-02 -3.25290442e-01\n",
      "  -2.84453899e-01 -3.79886210e-01  1.35754138e-01 -1.19968213e-01\n",
      "  -1.92362994e-01  1.18782572e-01 -2.40727022e-01 -3.22645009e-01\n",
      "   4.26869333e-01 -2.79497243e-02 -5.18692851e-01  1.86039925e-01\n",
      "  -2.60920078e-01  5.74615449e-02  8.71473923e-02  2.64473390e-02\n",
      "  -2.18886867e-01 -1.49334967e-01  4.18725222e-01 -7.78179616e-02\n",
      "  -2.12306038e-01  2.31329143e-01 -5.69647476e-02  9.93173793e-02\n",
      "  -3.47409368e-01  1.76034063e-01 -2.95846257e-02  1.04068667e-01\n",
      "  -6.84402213e-02 -1.94965735e-01 -2.49748170e-01 -2.70450473e-01\n",
      "   1.45292252e-01  4.58231382e-02 -4.32195783e-01 -2.12006107e-01\n",
      "   1.57100245e-01 -5.06042652e-02 -3.29402089e-02 -3.06677222e-02\n",
      "   4.74827066e-02 -1.37527026e-02  3.24656665e-01  3.32025200e-01\n",
      "   9.26353782e-02  1.58113137e-01 -1.40334964e-01 -6.63744286e-02\n",
      "  -2.55249172e-01 -1.51029527e-01 -1.10634670e-01  3.67999189e-02\n",
      "   2.77579993e-01 -1.26384079e-01 -9.67100859e-02  1.27595365e-01\n",
      "  -2.20290735e-01  1.25603676e-01 -5.12109637e-01 -1.29514918e-01\n",
      "  -2.42589533e-01 -3.33225355e-02  7.28583634e-02  1.77565306e-01\n",
      "   5.33358812e-01  7.93434083e-02 -3.31257121e-03  1.51506901e-01\n",
      "  -4.04639661e-01 -1.31146371e-01  3.70572209e-01 -7.36065283e-02\n",
      "   9.14234444e-02 -1.78746149e-01  1.47439077e-01  8.34567025e-02\n",
      "  -2.72677779e-01  5.18037230e-02  9.22971740e-02 -1.10763185e-01\n",
      "   2.60301918e-01 -2.12532237e-01  2.83194333e-01  7.12605298e-01\n",
      "   7.94062316e-02  5.14800064e-02  1.63296461e-01  1.24641890e-02\n",
      "  -9.24146548e-02  2.34007835e-03  1.07452758e-02  6.97259083e-02\n",
      "   1.78007111e-01 -4.49806213e-01 -2.51264513e-01 -2.59234965e-01\n",
      "   1.17742315e-01 -1.62049159e-01  1.18001565e-01  9.47169065e-02\n",
      "   6.43650368e-02  3.40780407e-01 -1.03469633e-01 -3.36182374e-03\n",
      "   3.37425560e-01 -3.63703609e-01 -2.78676227e-02  1.22243375e-01\n",
      "  -9.34312716e-02  2.01877803e-01 -2.95642674e-01  2.93630183e-01\n",
      "   1.60938695e-01  1.50663033e-01 -3.45596820e-02  3.01461518e-02\n",
      "  -5.75091578e-02  2.42677048e-01 -1.00753091e-01  1.00773811e-01\n",
      "  -5.10349032e-03  1.05999835e-01  4.07997996e-01 -8.00175965e-02\n",
      "   1.52244538e-01 -2.46277884e-01 -1.34580970e-01  2.54871666e-01\n",
      "   5.92623614e-02 -7.03104064e-02 -3.51026237e-01 -7.67841265e-02\n",
      "  -1.91110373e-03  1.10269450e-01 -3.69041506e-03  1.67045649e-02\n",
      "   7.32011423e-02 -1.76932186e-01  4.16512266e-02 -1.73200831e-01\n",
      "   1.81154191e-01  8.47238954e-03 -5.50281890e-02 -6.95515722e-02\n",
      "  -2.46332958e-02  2.42013454e-01 -4.80218977e-02 -1.87668294e-01\n",
      "   6.27528876e-02 -8.39754790e-02  3.96472096e-01  9.44171324e-02\n",
      "  -3.79327349e-02 -9.58502144e-02 -6.46455660e-02  8.08105394e-02\n",
      "  -5.93563020e-01  3.95835042e-01 -2.31972218e-01  2.51358896e-01\n",
      "  -6.15718123e-03  3.27886522e-01 -1.73550763e-03  4.79901209e-02\n",
      "  -8.42220113e-02 -3.79709266e-02  2.12963060e-01 -1.60586089e-01\n",
      "  -2.40355968e-01  7.03760609e-02  4.58440721e-01  6.35354519e-02\n",
      "  -2.14999899e-01  4.04505804e-02  6.27515512e-03 -4.03910190e-01\n",
      "   8.71414244e-02 -1.73872888e-01 -2.18614802e-01 -3.69554043e-01\n",
      "  -1.88285150e-02 -9.03737247e-02 -4.24824595e-01 -1.68773249e-01\n",
      "  -1.60840034e-01 -1.62554145e-01 -2.52589554e-01  7.51851052e-02\n",
      "  -5.17411456e-02 -1.25019088e-01 -1.25972271e-01 -5.34262180e-01\n",
      "  -3.19471180e-01 -1.89293414e-01  2.85631679e-02  3.40597898e-01\n",
      "   1.06270276e-01 -3.93365175e-01 -6.04030937e-02  3.90618071e-02\n",
      "   5.25518470e-02  2.46218815e-01  1.41813010e-01 -8.26558992e-02\n",
      "   2.90588345e-02 -1.28809020e-01  1.08831130e-01  2.95007735e-01\n",
      "  -4.35679317e-01  9.86600965e-02 -2.99559712e-01 -5.98634221e-02\n",
      "   9.60608646e-02  1.43973306e-01  2.49614850e-01  1.39980083e-02\n",
      "  -3.20416600e-01  2.99368128e-02  9.04388539e-03 -1.40395120e-01\n",
      "  -7.78125152e-02 -3.71473968e-01 -1.07517466e-01 -4.09224361e-01\n",
      "   1.59193859e-01  2.68605441e-01  1.40748426e-01  8.69587883e-02\n",
      "   2.12225169e-01  1.83484226e-01  2.66485691e-01  8.35568383e-02\n",
      "   6.94504753e-02 -1.52057469e-01 -1.48578480e-01 -1.39344797e-01\n",
      "  -1.85614616e-01  1.52206719e-01 -1.90722331e-01 -1.45497009e-01\n",
      "  -2.30829507e-01 -1.35451138e-01 -1.60559848e-01  1.98110774e-01\n",
      "  -1.22772813e-01 -4.14912045e-01 -1.21921271e-01 -1.49119899e-01\n",
      "   1.62287652e-01  5.39476573e-02  8.84941034e-03 -4.19977665e-01\n",
      "  -3.65334630e-01  2.51699090e-01  1.41544059e-01 -2.12615892e-01\n",
      "  -1.31460071e-01 -1.25556439e-01  2.43378431e-01 -2.98835278e-01\n",
      "  -2.54022956e-01 -2.50814199e-01  1.94285184e-01 -2.73701310e-01\n",
      "  -1.52297392e-01 -8.19830969e-03  1.17058501e-01  5.05554164e-03\n",
      "   1.64904311e-01  1.77873746e-01  1.31179005e-01  2.24040166e-01\n",
      "  -3.32687974e-01  1.33433700e-01 -2.49716025e-02 -4.56919819e-02\n",
      "  -2.18214720e-01 -3.40247154e-02  1.10407814e-01  1.27000406e-01\n",
      "   1.96845874e-01  2.72609591e-01  2.89976478e-01  3.36233526e-02\n",
      "  -6.65685654e-01 -1.09441876e-01 -3.06979120e-01  2.44878922e-02\n",
      "   5.57986140e-01 -6.60175905e-02  8.46210569e-02  2.62152493e-01\n",
      "   8.66689309e-02 -1.01448365e-01 -3.69372010e-01 -1.24021187e-01\n",
      "  -9.39261243e-02  1.46622479e-01 -3.71644825e-01  2.98621714e-01\n",
      "   1.94585070e-01  1.79260880e-01 -1.96991235e-01  2.40807563e-01\n",
      "   1.72873601e-01 -1.43368602e-01  5.02749607e-02 -7.00074583e-02\n",
      "   1.85421050e-01 -2.85767317e-01 -4.31663841e-02  1.09059922e-01]]\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer, models\n",
    "# SentenceTransformer: Transformer + Pooling 등을 조합해 SBERT 모델을 구성하고,\n",
    "# 문장을 임베딩하는 클래스.\n",
    "\n",
    "# 1. 사용할 BERT 모델 로드\n",
    "# 이 모델은 입력 문장을 BERT 방식으로 토큰화한 후, 각 단어/토큰에 대해 벡터를 생성함\n",
    "word_embedding_model = models.Transformer('klue/roberta-base')\n",
    "\n",
    "# 2. 풀링층 정의\n",
    "# 문장 전체를 하나의 고정된 벡터로 만드는 풀링(Pooling) 레이어\n",
    "pooling_model = models.Pooling(word_embedding_model.get_word_embedding_dimension())\n",
    "\n",
    "# 3. 위 두 모듈(Transformer + Pooling)을 연결해 SentenceTransformer 모델로 구성\n",
    "model = SentenceTransformer(modules=[word_embedding_model, pooling_model])\n",
    "\n",
    "print(model.encode(['진정한 데이터 사이언스가 되는 길']).shape)\n",
    "print(model.encode(['진정한 데이터 사이언스가 되는 길']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 예제 10.2 코드로 살펴보는 평균 모드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xua7CHzorrWs"
   },
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "def mean_pooling(model_output, attention_mask):\n",
    "\n",
    "    # model_output[0]: BERT의 마지막 은닉층 출력 (batch_size, seq_len, hidden_size)\n",
    "    token_embeddings = model_output[0]\n",
    "\n",
    "    # attention_mask: 실제 토큰인지(1) 패딩인지(0)를 나타냄\n",
    "    # unsqueeze(-1): (batch_size, seq_len) → (batch_size, seq_len, 1)\n",
    "    # expand(): 토큰 임베딩과 크기 맞춤 (broadcasting)\n",
    "    # float(): 곱셈이 가능하도록 정수형 → 실수형으로 변환\n",
    "    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "\n",
    "    # 마스크된 토큰 임베딩만 남기고 모두 더함 (sum along seq_len axis)\n",
    "    # shape: (batch_size, hidden_size)\n",
    "    sum_embeddings = torch.sum(token_embeddings * input_mask_expanded, dim=1)\n",
    "\n",
    "    # 마스크 합을 통해 평균 낼 때 나눌 분모 계산 (0으로 나누는 걸 방지하기 위해 최소값 지정)\n",
    "    sum_mask = torch.clamp(input_mask_expanded.sum(dim=1), min=1e-9)\n",
    "\n",
    "    # 평균 벡터 구하기: (각 문장의 토큰 벡터 합 / 유효 토큰 수)\n",
    "    # shape: (batch_size, hidden_size)\n",
    "    return sum_embeddings / sum_mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\me\\anaconda3\\envs\\lang310\\lib\\site-packages\\huggingface_hub\\file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at klue/roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "문장 임베딩 벡터: tensor([[ 3.5191e-01, -4.1031e-01,  1.1834e-01,  5.3934e-02, -1.6616e-02,\n",
      "         -1.1009e-01, -3.7938e-02, -2.7568e-01,  1.3821e-02, -4.5810e-02,\n",
      "         -5.2823e-02,  9.9165e-02, -1.7758e-01,  4.5876e-02,  1.9208e-01,\n",
      "         -1.5788e-01,  7.2701e-02, -3.7021e-01, -1.8652e-01, -5.7619e-01,\n",
      "          2.0276e-01, -1.8577e-01,  7.0466e-02,  2.7253e-01, -1.3598e-01,\n",
      "         -2.3917e-02,  2.5444e-01, -3.4262e-02,  3.2451e-02, -2.5701e-01,\n",
      "         -6.7598e-01, -1.7985e-01,  2.0873e-01,  3.8534e-02,  5.8911e-01,\n",
      "          1.5793e-01,  2.2785e-01,  2.8802e-02, -6.5119e-02, -3.4388e-01,\n",
      "          2.4584e-01,  4.3585e-01,  2.2032e-01, -2.5319e-02,  3.2841e-01,\n",
      "          7.9404e-02, -4.5297e-02,  1.3635e-01, -5.2890e-01, -2.0794e-01,\n",
      "          2.5024e-01, -3.1030e-01,  1.8552e-02, -3.2511e-01, -8.5269e-02,\n",
      "          8.4016e-02,  1.0609e-01, -4.8365e-02, -4.3351e-03, -2.4898e-01,\n",
      "          3.8349e-02,  1.5115e-01,  2.2323e-02, -8.1496e-02, -1.3640e-01,\n",
      "         -1.1187e-01,  6.2421e-02, -3.6187e-01, -6.3271e-02,  3.6845e-01,\n",
      "         -7.6664e-02,  2.4745e-02,  2.3343e-01, -3.1963e-01,  4.2841e-01,\n",
      "          7.0227e-02,  9.8149e-02, -1.1464e-01,  3.2120e-01,  1.3784e-01,\n",
      "          4.2050e-01,  6.2831e-02, -9.1156e-02, -1.1723e-01, -2.3326e-01,\n",
      "         -1.9442e-01, -7.4141e-02, -1.7107e-01, -6.6276e-02, -7.3120e-02,\n",
      "         -4.0037e-01, -3.2247e-01, -1.7473e-01, -3.5086e-02,  8.4520e-02,\n",
      "          2.2342e-01,  4.2008e-01,  2.1725e-01,  2.1323e-01, -4.0297e-01,\n",
      "          6.6659e-02, -2.8076e-01,  1.1010e-01, -1.5253e-01, -5.6274e-01,\n",
      "         -1.7404e-01,  1.3321e-01, -3.9319e-02,  4.5955e-02, -2.8924e-01,\n",
      "         -5.7703e-01,  1.6520e-01, -1.6417e-01, -2.6557e-01, -1.4767e-01,\n",
      "          1.1071e-01,  6.4869e-02,  1.1427e-01,  1.3728e-01,  1.7059e-01,\n",
      "         -9.6658e-02, -2.5089e-02, -6.4245e-01, -1.9096e-01, -2.1980e-01,\n",
      "          4.5517e-02,  2.0502e-01, -5.1156e-01, -1.7904e-02, -3.0501e-01,\n",
      "          6.2241e-02, -2.7508e-01, -4.5646e-01,  5.4273e-02,  2.7568e-01,\n",
      "          2.5914e-01, -2.9940e-02, -2.4612e-01,  3.5828e-01, -1.0877e-01,\n",
      "          7.2110e-02,  2.7315e-01, -5.2151e-02, -2.9583e-01,  1.1994e-02,\n",
      "          7.0159e-03, -1.3603e-01,  1.4970e-01,  7.8821e-02, -1.1468e-01,\n",
      "         -4.9529e-01, -3.7405e-01,  1.0736e-01, -2.0770e-01, -3.1270e-01,\n",
      "         -1.8253e-01,  1.5913e-01,  8.2332e-02, -9.2896e-02, -1.4421e+00,\n",
      "         -1.3006e-01, -2.1132e-01, -7.7138e-02,  1.0044e-01, -1.8292e-01,\n",
      "          2.8985e-01, -1.8302e-02,  1.4676e-01, -8.2106e-02, -4.6423e-01,\n",
      "         -1.2964e-02, -1.6863e-01, -1.9412e-01,  3.0294e-01, -1.4249e-01,\n",
      "          5.9412e-02, -1.9980e-01,  7.8255e-02, -1.2804e-01, -5.3171e-02,\n",
      "         -9.3035e-02, -1.3479e-02, -2.0785e-01,  3.0606e-01,  1.6398e-01,\n",
      "          9.7029e-02, -8.6303e-03, -3.6064e-01, -1.5700e-01,  3.7904e-01,\n",
      "          7.7035e-02,  1.9262e-01, -1.6659e-01, -1.1227e-01,  4.2764e-01,\n",
      "          9.1218e-02,  3.0776e-02, -9.5731e-02, -3.1339e-02, -2.4197e-01,\n",
      "          1.5465e-01, -1.1967e-01, -2.0356e-01, -2.8404e-01,  4.4087e-01,\n",
      "         -5.2990e-02, -2.2733e-01, -3.1760e-01,  1.6912e-02,  1.7683e-01,\n",
      "         -2.0702e-01, -1.5460e-01, -2.6831e-01,  1.1524e-01, -2.8800e-01,\n",
      "         -3.9231e-01,  2.8387e-01,  1.5138e-02, -2.7481e-01,  3.1303e-02,\n",
      "          2.1770e-01, -2.2938e-02, -2.2090e-01, -6.1348e-03, -2.3928e-02,\n",
      "         -1.5970e-01,  1.8608e-01,  2.2059e-01, -1.1907e-01,  4.5516e-02,\n",
      "          2.4753e-01,  3.7745e-02,  1.0135e-01,  2.6452e-02, -2.7949e-01,\n",
      "          1.5280e-02,  8.1921e-02, -1.9925e-01, -1.0499e-01, -3.6285e-01,\n",
      "         -1.9533e-01, -2.6904e-01,  1.5878e-01,  8.1447e-02, -2.0744e-01,\n",
      "         -2.1246e-01,  2.3146e-01, -8.0128e-02,  2.0745e-01, -1.8758e-01,\n",
      "         -9.1481e-02, -2.4767e-01, -2.1082e-01, -3.9713e-01,  1.5437e-01,\n",
      "          9.9031e-02, -3.8144e-03,  2.6530e-01,  1.7173e-01,  4.2974e-01,\n",
      "          6.2433e-03, -5.1858e-02, -7.5847e-02,  4.6674e-03, -4.5252e-01,\n",
      "          1.3879e-02,  1.4612e-01, -2.7822e-01, -3.5774e-01, -2.1376e-01,\n",
      "         -1.7305e-01,  2.1157e-01, -4.7386e-01,  2.4095e-01, -2.8249e-01,\n",
      "         -2.0011e-01,  8.6605e-02,  1.1007e-01,  1.2504e-01, -1.0698e-01,\n",
      "          3.5524e-01,  8.9161e-03, -3.0502e-01,  1.6992e-01, -1.4801e-01,\n",
      "          3.0096e-02,  1.2775e-01,  3.7826e-01, -1.7317e-02,  1.8935e-01,\n",
      "          5.8679e-02, -5.1207e-01, -1.2481e-02, -2.5093e+00, -7.1691e-03,\n",
      "         -9.7445e-02, -1.9404e-01, -9.4497e-02, -4.1137e-01, -1.8036e-01,\n",
      "         -1.1278e-01,  1.5508e-01, -1.9481e-01,  1.1521e-01,  2.1580e-01,\n",
      "         -1.0661e-01,  3.3054e-01, -2.0286e-01, -7.4788e-01, -6.0020e-02,\n",
      "          2.8657e-01, -1.0272e-01,  3.5281e-01, -4.9931e-01, -1.7662e-01,\n",
      "         -2.2012e-01, -1.0653e-01, -4.8470e-03,  5.7146e-03,  1.8995e-01,\n",
      "         -1.1473e-01, -1.8651e-01,  1.4544e-02, -1.5874e-01,  6.2717e-02,\n",
      "          1.8369e-01,  1.4777e-01,  2.2253e-02,  9.0170e-02, -1.2569e-01,\n",
      "         -1.5017e-01,  4.7337e-01, -2.0709e-01,  1.8805e-01,  2.2781e-01,\n",
      "          3.8697e-02, -3.5599e-01, -2.0954e-03, -3.1903e-01, -1.6054e-01,\n",
      "         -1.0273e-01, -7.6486e-02,  2.7312e-01, -3.1455e-01, -3.6293e-02,\n",
      "          6.0552e+00, -2.4479e-01, -3.1660e-01,  6.9666e-02,  1.0119e-01,\n",
      "         -3.8671e-02,  2.0722e-01, -6.3777e-02, -8.1034e-02,  2.4834e-01,\n",
      "         -2.7616e-01,  7.0176e-02,  5.8517e-02, -9.0407e-03,  3.3455e-01,\n",
      "          3.1327e-02,  6.7542e-01,  1.5283e-01, -2.7407e-02, -1.0427e-03,\n",
      "         -4.1703e-01,  3.8673e-02,  1.9454e-01, -1.1234e-02, -8.0397e-02,\n",
      "         -1.9000e-01, -4.8947e-01, -1.3298e-01, -1.1422e-01, -1.2989e-01,\n",
      "         -9.3555e-02,  1.4033e-01, -2.4935e-01, -5.6482e-02, -1.0157e-01,\n",
      "         -4.3085e-03, -2.5874e-01,  4.3660e-02,  1.5399e-01,  1.3968e-01,\n",
      "          1.4329e-01, -1.1053e-01, -1.3902e-01,  2.3946e-01, -2.5249e-01,\n",
      "         -1.2429e-01, -1.0024e-02,  4.4616e-01, -6.6872e-02, -1.9728e-01,\n",
      "         -2.3489e-01, -1.2047e-02, -5.4862e-02,  1.5566e-01, -4.5058e-02,\n",
      "         -6.3700e-02,  2.8381e-02, -1.0670e-01,  6.7748e-02,  9.6001e-02,\n",
      "         -4.0663e-01,  6.8515e-02, -5.6241e-02, -1.2605e-01, -2.5933e-01,\n",
      "         -1.1949e-01,  2.5675e-02, -4.3088e-02, -1.9811e-01, -1.3577e-01,\n",
      "          9.0724e-02, -2.7974e-01, -4.0094e-01, -1.8831e-02, -2.0070e-02,\n",
      "          1.2320e-01,  1.0074e-01, -1.4226e-01,  2.5050e-01, -1.3050e-04,\n",
      "          2.4623e-01,  3.8027e-01, -3.2335e-01, -3.4546e-01,  2.3619e-01,\n",
      "         -2.5992e-01,  1.0728e-01, -8.4747e-02, -1.0282e-01,  3.8942e-01,\n",
      "          3.3134e-01,  8.1062e-02,  4.0010e-01, -2.8906e-01, -1.5082e-01,\n",
      "         -9.7744e-02,  3.4346e-01,  9.4285e-02,  4.0999e-02, -3.4047e-02,\n",
      "          2.9146e-04,  7.0124e-01, -4.1607e-01, -9.2573e-02,  1.6789e-02,\n",
      "         -2.4245e-01,  3.1494e-01, -5.6100e-02, -2.2667e-01, -2.9554e-01,\n",
      "         -2.3275e-02, -1.3830e-01, -2.1360e-01,  5.1151e-01, -3.6147e-01,\n",
      "         -2.3304e-01, -4.2397e-01, -3.7407e-01, -1.8215e-01, -1.4868e-02,\n",
      "         -3.7501e-01, -4.2132e-01,  1.3181e-01, -8.7751e-02, -4.6985e-02,\n",
      "          3.3998e-01, -1.6999e-01, -2.3460e-01, -5.0926e-01, -1.2191e-01,\n",
      "          9.7944e-02, -2.1250e-01,  4.2001e-02, -8.0157e-02, -9.3481e-02,\n",
      "         -1.9678e-01,  1.3254e-01,  8.3757e-03,  2.7726e-01,  1.2706e-01,\n",
      "         -3.0460e-01, -2.9947e-01,  9.8483e-02,  4.7673e-02,  6.1782e-02,\n",
      "         -3.9383e-01, -2.7727e-01, -4.8171e-02,  1.2034e-01,  2.1239e-01,\n",
      "         -1.3442e-01, -3.8281e-02, -2.5464e-01, -4.7874e-02,  1.1206e-01,\n",
      "          2.8818e-02, -7.4621e-02, -1.2699e-01,  3.1341e-02, -1.4606e-01,\n",
      "          9.0021e-02,  1.7586e-01,  3.5713e-01,  3.3497e-01,  6.3855e-02,\n",
      "          1.7828e-01, -3.1951e-01, -2.8167e-01,  6.0576e-02, -2.3391e-01,\n",
      "         -2.3858e-01, -1.0028e-01, -4.1549e-02, -7.1305e-03,  1.2321e-01,\n",
      "         -3.6766e-02,  3.7049e-03,  2.1582e-01, -8.7138e-02, -4.1897e-01,\n",
      "         -2.5635e-02, -1.4772e-01, -1.1817e-01, -3.0935e-01, -4.1882e-01,\n",
      "          1.9759e-01, -2.3109e-01,  4.6810e-02, -6.6620e-02,  4.8603e-02,\n",
      "          7.6925e-02,  4.5874e-02,  1.7804e-01,  5.6872e-01,  1.9777e-01,\n",
      "          7.3993e-02, -1.2623e-01,  6.7559e-02, -3.0689e-01, -5.3374e-01,\n",
      "          1.5159e-01, -9.2305e-03,  3.0893e-01,  3.0077e-02, -2.2211e-01,\n",
      "         -3.8017e-01, -4.0371e-01,  5.9294e-03,  1.6510e-01, -1.8047e-02,\n",
      "         -2.5714e-01,  1.5938e-01,  4.0399e-01,  6.5698e-02,  6.6693e-02,\n",
      "          2.3315e-01, -4.1246e-01, -2.9805e-01,  2.8434e-02,  3.9222e-01,\n",
      "          3.3010e-01, -2.5891e-01,  1.5149e-01, -1.1451e-01, -1.4484e-01,\n",
      "          1.5531e-01,  5.8873e-02,  4.3781e-02, -2.1060e-02,  6.9908e-02,\n",
      "          3.0775e-02, -2.7636e-01,  1.5496e-01,  1.7444e-01,  3.0916e-02,\n",
      "          5.4994e-02, -7.8292e-02,  3.5303e-01,  3.9850e-02,  5.3726e-02,\n",
      "          1.0068e-02, -2.0268e-01, -1.8874e-02, -2.3778e-01, -8.6939e-02,\n",
      "          1.2442e-01,  3.7854e-02,  3.2550e-02,  9.1003e-02,  4.2516e-01,\n",
      "         -2.0740e-01,  1.0796e-01, -1.7391e-01, -2.9479e-01, -2.7794e-01,\n",
      "         -2.8021e-02,  1.7860e-01, -9.3106e-02,  6.2798e-01,  4.5985e-01,\n",
      "          1.2218e-03,  3.6893e-01, -1.3815e-01,  3.9300e-01, -3.8448e-01,\n",
      "          3.1665e-01, -3.1909e-01, -2.8198e-01, -2.7047e-01, -3.2327e-02,\n",
      "          1.2550e-01,  2.3086e-01,  7.4432e-02,  1.3650e-01,  1.0986e-01,\n",
      "         -3.6910e-01,  1.7894e-01, -5.1158e-02, -4.9672e-02, -1.6298e-01,\n",
      "          8.3660e-02,  4.1240e-01, -2.0057e-02,  4.2629e-02,  7.0634e-02,\n",
      "          1.8343e-01, -7.2770e-02, -9.3779e-02, -9.5886e-02, -3.4263e-01,\n",
      "         -2.6030e-01,  1.8387e-01, -3.9411e-01, -3.8652e-01,  6.0681e-02,\n",
      "         -2.0884e-01,  1.3856e-01, -2.1768e-01,  3.1998e-01,  8.5354e-02,\n",
      "         -3.7228e-01, -5.1695e-01, -3.0768e-01, -3.3507e-01, -2.2946e-02,\n",
      "         -1.9218e-01,  1.0439e-01, -3.9240e-01, -8.3724e-02, -2.3377e-01,\n",
      "          2.9393e-02, -1.5834e-01,  1.2084e-01,  2.6471e-01,  3.1782e-01,\n",
      "          2.9338e-01,  3.2238e-02, -2.9624e-04,  4.5320e-02, -3.9335e-01,\n",
      "         -1.8700e-01,  6.8266e-02, -3.4452e-02, -1.0119e-01, -4.2398e-01,\n",
      "          2.8335e-02, -2.1006e-02, -3.6370e-01, -8.0460e-02,  1.9603e-01,\n",
      "         -2.2123e-01, -8.1864e-02,  1.4063e-02, -2.2570e-02, -4.5798e-01,\n",
      "          2.2307e-01,  4.3797e-01, -1.9310e-01, -2.1121e-01, -6.4615e-02,\n",
      "          4.6649e-01,  1.1712e-01,  1.2204e-01,  2.1703e-01, -6.3343e-02,\n",
      "         -3.5744e-01,  1.2584e-01,  6.5946e-02,  6.3770e-02, -3.4376e-01,\n",
      "          2.5028e-02, -2.8270e-01,  4.6113e-02,  7.3683e-02,  3.3766e-01,\n",
      "         -2.6565e-01,  2.6536e-02, -2.4465e-01,  1.0414e-01, -1.1544e-01,\n",
      "         -3.9379e-02, -3.9795e-02,  1.6514e-02, -7.1198e-02, -2.6880e-02,\n",
      "          5.1130e-01, -2.0901e-01, -3.3941e-02, -1.1255e-01,  3.0593e-01,\n",
      "         -3.4721e-01,  1.7132e-01, -1.0648e-01,  1.2075e-01, -2.7258e-01,\n",
      "         -1.7134e-03,  4.5226e-02, -2.7832e-02,  1.1845e-01,  1.7051e-01,\n",
      "         -1.2729e-01,  4.7367e-02,  2.4367e-01, -4.5886e-01, -2.1522e-01,\n",
      "         -2.3857e-01, -1.1048e-01, -2.2551e-01, -3.1901e-01,  1.1579e-01,\n",
      "         -2.1270e-01,  1.0715e-01, -2.8234e-01, -2.6966e-02, -3.0904e-01,\n",
      "         -2.1237e-01, -2.1157e-01, -2.3665e-01, -3.1080e-01,  3.3118e-01,\n",
      "         -2.6063e-01, -2.8657e-01, -9.3646e-02, -1.6483e-01, -1.7798e-01,\n",
      "         -5.3343e-02, -3.7289e-01,  1.5565e-01,  2.1826e-01,  1.5404e-01,\n",
      "          9.1260e-02,  1.3976e-01,  1.4812e-03, -2.0349e-01, -1.3579e-01,\n",
      "         -2.4160e-02, -4.6764e-03,  3.2223e-02, -1.0536e-01, -6.7182e-02,\n",
      "         -3.3092e-02, -5.5522e-02, -3.0353e-02]])\n",
      "벡터 차원: torch.Size([1, 768])\n"
     ]
    }
   ],
   "source": [
    "# 1. transformer 모델에 문장을 입력하여 단어별 벡터 반환\n",
    "# 2. 반환된 벡터를 평균내어 하나의 문장 벡터를 만듬\n",
    "\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "# 1. 모델과 토크나이저 로드\n",
    "model_name = \"klue/roberta-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "# 2. Mean Pooling 함수 정의\n",
    "def mean_pooling(model_output, attention_mask):\n",
    "    token_embeddings = model_output[0] \n",
    "    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "    sum_embeddings = torch.sum(token_embeddings * input_mask_expanded, dim=1)\n",
    "    sum_mask = torch.clamp(input_mask_expanded.sum(dim=1), min=1e-9)\n",
    "    return sum_embeddings / sum_mask\n",
    "\n",
    "# 3. 입력 문장\n",
    "sentence = \"학교에 갑니다\"\n",
    "\n",
    "# 4. 토크나이징 + 텐서 형태로 변환\n",
    "inputs = tokenizer(sentence, return_tensors=\"pt\")\n",
    "\n",
    "# 5. 모델 추론 (gradient 계산 없이)\n",
    "with torch.no_grad():\n",
    "    model_output = model(**inputs)\n",
    "\n",
    "# 6. Mean Pooling으로 문장 임베딩 추출\n",
    "sentence_embedding = mean_pooling(model_output, inputs['attention_mask'])\n",
    "\n",
    "# 7. 결과 출력\n",
    "print(\"문장 임베딩 벡터:\", sentence_embedding)\n",
    "print(\"벡터 차원:\", sentence_embedding.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 예제 10.2 코드로 살펴보는 최대 모드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "Q9GFRCtZrt4P"
   },
   "outputs": [],
   "source": [
    "def max_pooling(model_output, attention_mask):\n",
    "    token_embeddings = model_output[0]\n",
    "    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "    token_embeddings[input_mask_expanded == 0] = -1e9\n",
    "    return torch.max(token_embeddings, 1)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 예제 10.2 한국어 문장 임베딩 모델로 입력 문장 사이의 유사도 계산"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4r64uO9xrvJd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0000, 0.6410, 0.1887],\n",
      "        [0.6410, 1.0000, 0.2730],\n",
      "        [0.1887, 0.2730, 1.0000]])\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "# 1. 한국어 SBERT 모델 로드 (KLUE 기반, 한국어에 특화된 사전학습 모델)\n",
    "model = SentenceTransformer('snunlp/KR-SBERT-V40K-klueNLI-augSTS')\n",
    "\n",
    "# 2. 문장들을 SBERT로 인코딩 → 각 문장이 768차원 벡터로 변환됨\n",
    "embs = model.encode(['잠이 안 옵니다',\n",
    "                     '졸음이 옵니다',\n",
    "                     '기차가 옵니다'])\n",
    "\n",
    "# 3. 문장 간 코사인 유사도 계산 \n",
    "cos_scores = util.cos_sim(embs, embs)\n",
    "\n",
    "print(cos_scores)\n",
    "\n",
    "# 출력 예시:\n",
    "# tensor([[1.0000, 0.6410, 0.1887],\n",
    "#         [0.6410, 1.0000, 0.2730],\n",
    "#         [0.1887, 0.2730, 1.0000]])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 예제 10.2 CLIP 모델을 활용한 이미지와 텍스트 임베딩 유사도 계산"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tFpBCDhMrwxM"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2771, 0.1509],\n",
      "        [0.2071, 0.3180]])\n"
     ]
    }
   ],
   "source": [
    "# CLIP (Contrastive Language–Image Pretraining) 모델을 사용하여\n",
    "# 이미지와 텍스트 간의 의미적 유사도를 계산\n",
    "\n",
    "from PIL import Image \n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "# 1. CLIP 모델 로드 \n",
    "model = SentenceTransformer('clip-ViT-B-32')\n",
    "\n",
    "# 2. 이미지 2장 로드하고 임베딩 계산 (각 이미지 → 벡터로 변환)\n",
    "img_embs = model.encode([Image.open('dog.jpg'), Image.open('cat.jpg')])\n",
    "\n",
    "# 3. 텍스트 2개 임베딩 계산 (자연어 설명 → 벡터로 변환)\n",
    "text_embs = model.encode(['A dog on grass', 'Brown cat on yellow background'])\n",
    "\n",
    "# 4. 이미지와 텍스트 임베딩 간 코사인 유사도 계산\n",
    "cos_scores = util.cos_sim(img_embs, text_embs)\n",
    "print(cos_scores)\n",
    "\n",
    "# 예시 출력:\n",
    "# tensor([[0.2771, 0.1509],\n",
    "#         [0.2071, 0.3180]])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 예제 10.3 실습에 사용할 모델과 데이터셋 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "id": "TOrKBXzYryjQ"
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# 한국어 QA 데이터셋(KLUE MRC), train split만 사용\n",
    "klue_mrc_dataset = load_dataset('klue', 'mrc', split='train')\n",
    "\n",
    "# 2. 한국어 SBERT 모델 로드 (KLUE 기반으로 학습된 모델)\n",
    "sentence_model = SentenceTransformer('snunlp/KR-SBERT-V40K-klueNLI-augSTS')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 예제 10.3 실습 데이터에서 1,000개만 선택하고 문장 임베딩으로 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "id": "-PPCKioTr0VT"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 768)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "klue_mrc_dataset = klue_mrc_dataset.train_test_split(train_size=1000, shuffle=False)['train']\n",
    "embeddings = sentence_model.encode(klue_mrc_dataset['context'])\n",
    "embeddings.shape\n",
    "# 출력 결과\n",
    "# (1000, 768)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 예제 10.3 KNN 검색 인덱스를 생성하고 문장 임베딩 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uJsY0g30r2BQ"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<faiss.swigfaiss_avx2.IndexFlatL2; proxy of <Swig Object of type 'faiss::IndexFlatL2 *' at 0x0000013CD8DFEA00> >"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Facebook AI의 FAISS 라이브러리를 사용하여, 문장 임베딩 벡터에 대한 빠른 유사도 검색 인덱스를 구축\n",
    "\n",
    "import faiss\n",
    "\n",
    "# 인덱스 만들기 (L2 거리 기반의 평면 인덱스)\n",
    "index = faiss.IndexFlatL2(embeddings.shape[1])\n",
    "\n",
    "# 인덱스에 임베딩 추가\n",
    "index.add(embeddings) \n",
    "index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 예제 10.3 의미 검색의 장점"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "id": "TC5XgbD7r3LR"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "올여름 장마가 17일 제주도에서 시작됐다. 서울 등 중부지방은 예년보다 사나흘 정도 늦은 \n",
      "연구 결과에 따르면, 오리너구리의 눈은 대부분의 포유류보다는 어류인 칠성장어나 먹장어, 그\n",
      "연구 결과에 따르면, 오리너구리의 눈은 대부분의 포유류보다는 어류인 칠성장어나 먹장어, 그\n"
     ]
    }
   ],
   "source": [
    "query = \"이번 연도에는 언제 비가 많이 올까?\"\n",
    "\n",
    "# 1. 질문 문장을 SBERT로 벡터화함\n",
    "query_embedding = sentence_model.encode([query])\n",
    "\n",
    "# 2. FAISS 인덱스에서 가장 유사한 3개의 context를 검색\n",
    "# distances: 각 검색 결과의 거리 (작을수록 유사함)\n",
    "# indices: 유사한 context의 인덱스들\n",
    "distances, indices = index.search(query_embedding, 3)\n",
    "\n",
    "# 3. 상위 3개 검색 결과의 context 일부 출력\n",
    "for idx in indices[0]:\n",
    "    print(klue_mrc_dataset['context'][idx][:50])\n",
    "\n",
    "\n",
    "# 출력 결과\n",
    "# 올여름 장마가 17일 제주도에서 시작됐다. 서울 등 중부지방은 예년보다 사나흘 정도 늦은   (정답)\n",
    "# 연구 결과에 따르면, 오리너구리의 눈은 대부분의 포유류보다는 어류인 칠성장어나 먹장어, 그 (오답)\n",
    "# 연구 결과에 따르면, 오리너구리의 눈은 대부분의 포유류보다는 어류인 칠성장어나 먹장어, 그 (오답)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[문맥]  올여름 장마가 17일 제주도에서 시작됐다. 서울 등 중부지방은 예년보다 사나흘 정도 늦은 \n",
      "[질문]  북태평양 기단과 오호츠크해 기단이 만나 국내에 머무르는 기간은?\n",
      "[정답]  ['한 달가량', '한 달']\n",
      "\n",
      "[문맥]  연구 결과에 따르면, 오리너구리의 눈은 대부분의 포유류보다는 어류인 칠성장어나 먹장어, 그\n",
      "[질문]  오리 너구리의 신체 부위 중 크게 발달한 것은?\n",
      "[정답]  ['눈']\n",
      "\n",
      "[문맥]  연구 결과에 따르면, 오리너구리의 눈은 대부분의 포유류보다는 어류인 칠성장어나 먹장어, 그\n",
      "[질문]  오리너구리와 눈 구조가 가장 비슷한 어류는 무엇인가요?\n",
      "[정답]  ['태평양먹장어(Eptatretus stoutii)', '태평양먹장어', 'Eptatretus stoutii']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for idx in indices[0]:\n",
    "    print(\"[문맥] \", klue_mrc_dataset['context'][idx][:50])\n",
    "    print(\"[질문] \", klue_mrc_dataset['question'][idx])\n",
    "    print(\"[정답] \", klue_mrc_dataset['answers'][idx]['text'])\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 예제 10.3 의미 검색의 한계"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oGe6VK4dr4cT"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "태평양 전쟁 중 뉴기니 방면에서 진공 작전을 실시해 온 더글러스 맥아더 장군을 사령관으로 \n",
      "태평양 전쟁 중 뉴기니 방면에서 진공 작전을 실시해 온 더글러스 맥아더 장군을 사령관으로 \n",
      "미국 세인트루이스에서 태어났고, 프린스턴 대학교에서 학사 학위를 마치고 1939년에 로체스\n"
     ]
    }
   ],
   "source": [
    "# 1. KLUE MRC 데이터셋의 3번째 질문 가져오기\n",
    "query = klue_mrc_dataset[3]['question']\n",
    "# 예: \"로버트 헨리 딕이 1946년에 매사추세츠 연구소에서 개발한 것은 무엇인가?\"\n",
    "\n",
    "# 2. 질문 문장을 SBERT 모델로 임베딩\n",
    "query_embedding = sentence_model.encode([query])\n",
    "\n",
    "# 3. FAISS 인덱스를 사용하여 질문 벡터와 가장 유사한 문맥(context) 검색\n",
    "distances, indices = index.search(query_embedding, 3)\n",
    "\n",
    "# 4. 유사한 context 3개의 처음 50자만 출력\n",
    "for idx in indices[0]:\n",
    "    print(klue_mrc_dataset['context'][idx][:50])\n",
    "\n",
    "\n",
    "# 출력 결과\n",
    "# 태평양 전쟁 중 뉴기니 방면에서 진공 작전을 실시해 온 더글러스 맥아더 장군을 사령관으로 (오답)\n",
    "# 태평양 전쟁 중 뉴기니 방면에서 진공 작전을 실시해 온 더글러스 맥아더 장군을 사령관으로 (오답)\n",
    "# 미국 세인트루이스에서 태어났고, 프린스턴 대학교에서 학사 학위를 마치고 1939년에 로체스 (정답)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[질문] 로버트 헨리 딕이 1946년에 매사추세츠 연구소에서 개발한 것은 무엇인가?\n",
      "\n",
      "[문맥 요약]  태평양 전쟁 중 뉴기니 방면에서 진공 작전을 실시해 온 더글러스 맥아더 장군을 사령관으로 하는 미 육군 주체의 연합군 남서 태평양 방면군은 1944년 후반 마침내 필리핀을 진공하기\n",
      "[전체 문맥]  태평양 전쟁 중 뉴기니 방면에서 진공 작전을 실시해 온 더글러스 맥아더 장군을 사령관으로 하는 미 육군 주체의 연합군 남서 태평양 방면군은 1944년 후반 마침내 필리핀을 진공하기로 결정했다. 그 첫 단계로 필리핀 방면의 전략 거점의 확보가 필요하였으며, 뉴기니 서쪽에 위치한 말루쿠 제도의 모로타이 섬을 공격 목표로 정했다. 또한 동시에 팔라우 제도의 펠렐리우 섬과 앙가우르 섬에도 미국 해군 주도의 연합국 중부 태평양 방면군이 공략을 맡았다.(이때의 전략 결정의 경위에 대해서는 필리핀 전투 (1944 - 1945)#미국을 참조.)\n",
      "\n",
      "한편, 1942년에 네덜란드령 동인도의 일부였던 모로타이 섬을 점령한 일본군은 이후 수비 부대를 증강배치하지 않았다. 1944년 말루쿠 제도 방면의 방비 강화를 도모하고자 파견된 제32사단은 평야가 많은 비행장 건설에 적합한 주변의 할마헤라 섬을 방어의 중심으로 여겼다. 따라서 모로타이 섬에는 제32사단의 2개 대대가 비행장 건설을 추진했지만, 배수가 좋지 않아 건설을 포기했다. 이 2개 대대가 할마헤라 섬으로 철수한 이후에는 카와시마 타케노부(川島威伸) 중위를 지휘관으로 하는 제2유격대 소속의 2개 중대(주로 다카사고의용대)만 배치되어 있었다.\n",
      "\n",
      "연합군이 상륙했을 때, 섬에는 9000명의 현지인이 살고 있었다. 도민에 대한 선무공작을 수행하기 위해 연합군의 상륙 부대에는 네덜란드 군 민정반이 추가 되었다.\n",
      "[정답]      ['네덜란드']\n",
      "==================================================\n",
      "[문맥 요약]  태평양 전쟁 중 뉴기니 방면에서 진공 작전을 실시해 온 더글러스 맥아더 장군을 사령관으로 하는 미 육군 주체의 연합군 남서 태평양 방면군은 1944년 후반 마침내 필리핀을 진공하기\n",
      "[전체 문맥]  태평양 전쟁 중 뉴기니 방면에서 진공 작전을 실시해 온 더글러스 맥아더 장군을 사령관으로 하는 미 육군 주체의 연합군 남서 태평양 방면군은 1944년 후반 마침내 필리핀을 진공하기로 결정했다. 그 첫 단계로 필리핀 방면의 전략 거점의 확보가 필요하였으며, 뉴기니 서쪽에 위치한 말루쿠 제도의 모로타이 섬을 공격 목표로 정했다. 또한 동시에 팔라우 제도의 펠렐리우 섬과 앙가우르 섬에도 미국 해군 주도의 연합국 중부 태평양 방면군이 공략을 맡았다.(이때의 전략 결정의 경위에 대해서는 필리핀 전투 (1944 - 1945)#미국을 참조.)\n",
      "\n",
      "한편, 1942년에 네덜란드령 동인도의 일부였던 모로타이 섬을 점령한 일본군은 이후 수비 부대를 증강배치하지 않았다. 1944년 말루쿠 제도 방면의 방비 강화를 도모하고자 파견된 제32사단은 평야가 많은 비행장 건설에 적합한 주변의 할마헤라 섬을 방어의 중심으로 여겼다. 따라서 모로타이 섬에는 제32사단의 2개 대대가 비행장 건설을 추진했지만, 배수가 좋지 않아 건설을 포기했다. 이 2개 대대가 할마헤라 섬으로 철수한 이후에는 카와시마 타케노부(川島威伸) 중위를 지휘관으로 하는 제2유격대 소속의 2개 중대(주로 다카사고의용대)만 배치되어 있었다.\n",
      "\n",
      "연합군이 상륙했을 때, 섬에는 9000명의 현지인이 살고 있었다. 도민에 대한 선무공작을 수행하기 위해 연합군의 상륙 부대에는 네덜란드 군 민정반이 추가 되었다.\n",
      "[정답]      ['더글러스 맥아더 장군', '맥아더 장군']\n",
      "==================================================\n",
      "[문맥 요약]  미국 세인트루이스에서 태어났고, 프린스턴 대학교에서 학사 학위를 마치고 1939년에 로체스터 대학교에서 핵물리학으로 박사 학위를 마쳤다. 제2차 세계대전중에 그는 매사추세츠 공과대\n",
      "[전체 문맥]  미국 세인트루이스에서 태어났고, 프린스턴 대학교에서 학사 학위를 마치고 1939년에 로체스터 대학교에서 핵물리학으로 박사 학위를 마쳤다. 제2차 세계대전중에 그는 매사추세츠 공과대학교 방사선 연구소에서 일하였다. 그곳에서 그는 레이다를 개발하였고, 딕 복사계와 마이크로파 수신기를 설계하였다. 그는 방사선 연구소의 지붕에서 이를 20K보다 낮은 우주 마이크로파 배경의 한계 온도를 설정하는 데 사용하였다.\n",
      "\n",
      "전후 1946년 그는 프린스턴 대학교로 돌아왔고, 이곳에서 자신의 경력의 나머지를 보내게 되었다. 그는 원자 물리학에서 레이저와 전자의 회전 자기 비율을 측정하는 것을 연구하였다. 그의 분광학과 발관 전송 분야의 중대한 기여는 딕 좁아짐(Dicke narrowing, 원자의 자유평균행로가 원자의 방사 전이의 한 파장의 길이보다 짧아지고, 광자의 흡수나 분출하는 동안에 원자의 속도와 방향은 많이 변하는 현상)이라 불리는 현상에 대한 예견이었다. 딕 좁아짐은 단파와 마이크로파 영역에서 상대적으로 낮은 압력에서 발생한다. 딕 좁아짐은 감마선에서 뫼스바우어 효과에 비견된다.\n",
      "\n",
      "그는 등가 원리를 이용한 일반 상대성 이론의 정확한 측정 프로그램의 개발에 그의 남은 경력을 보냈다. 칼 브랜스(Carl H. Brans)와 함께 그는 중력의 브랜스-딕 이론, 폴 디랙의 큰 수 가설과 마흐의 원리에 의해 영감을 얻은 일반 상대성 이론의 등가 원리의 깨짐을 발전시켰다. 하이라이트는 Roll과 Krotkov와 Dicke에 의한 등가 원리의 고전적인 측정이었고, 이 실험에서 이전의 작업들 보다 100배나 더 정확한 측정치를 얻었다. 디키는 또한 일반 상대성 이론의 고전적 시험 가운데 하나인, 수성의 근일점을 이해하는 데 유용한 태양 편평도를 측정하였다. 폴 디랙은 중력 상수가 우주의 거꾸로 나이와 부정확하게 일치하며, 중력 상수는 지금의 평형을 유지하기 위해 바뀌어야만 한다고 추측했다. 딕은 디랙의 관계가 선택 효과일 수 있다는 것을 깨달았다. 평형이 깨어졌을 다른 어떠한 시대에는 그 모순을 알아차릴 수 있는 지적인 생명채가 없을 것이다. 이것은 소위 약한 인간 중심 원리의 첫 번째 현대적 적용이다.\n",
      "[정답]      ['레이다']\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "print(f\"[질문] {query}\\n\")\n",
    "for idx in indices[0]:\n",
    "    print(\"[문맥 요약] \", klue_mrc_dataset['context'][idx][:100])\n",
    "    print(\"[전체 문맥] \", klue_mrc_dataset['context'][idx])\n",
    "    print(\"[정답]     \", klue_mrc_dataset['answers'][idx]['text'])\n",
    "    print(\"=\"*50)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 예제 10.3 라마인덱스에서 Sentence-Transformers 임베딩 모델 활용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\me\\anaconda3\\envs\\lang310\\lib\\site-packages\\huggingface_hub\\file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Public\\Documents\\ESTsoft\\CreatorTemp\\ipykernel_38004\\745163680.py:13: DeprecationWarning: Call to deprecated class method from_defaults. (ServiceContext is deprecated, please use `llama_index.settings.Settings` instead.) -- Deprecated since version 0.10.0.\n",
      "  service_context = ServiceContext.from_defaults(embed_model=embed_model, llm=None)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM is explicitly disabled. Using MockLLM.\n"
     ]
    }
   ],
   "source": [
    "# llama_index의 핵심 클래스 임포트\n",
    "from llama_index.core import VectorStoreIndex, ServiceContext\n",
    "from llama_index.core import Document\n",
    "\n",
    "# HuggingFace의 SBERT 임베딩 모델 로드\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "\n",
    "# 1. 한국어 SBERT 모델을 임베딩으로 설정\n",
    "embed_model = HuggingFaceEmbedding(model_name=\"snunlp/KR-SBERT-V40K-klueNLI-augSTS\")\n",
    "\n",
    "# 2. llama_index에서 사용할 서비스 컨텍스트 생성\n",
    "# LLM은 생략하고 (None), 임베딩 모델만 지정\n",
    "service_context = ServiceContext.from_defaults(embed_model=embed_model, llm=None)\n",
    "\n",
    "# 3. KLUE MRC 데이터셋에서 context 텍스트 100개 가져오기\n",
    "text_list = klue_mrc_dataset[:100]['context']\n",
    "\n",
    "# 4. 각 텍스트를 llama_index용 Document 객체로 변환\n",
    "documents = [Document(text=t) for t in text_list]\n",
    "\n",
    "# 5. VectorStoreIndex에 문서들을 임베딩하고 저장\n",
    "index_llama = VectorStoreIndex.from_documents(\n",
    "    documents,\n",
    "    service_context=service_context,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context information is below.\n",
      "---------------------\n",
      "《존 윅》(John Wick)은 데이비드 리치와 채드 스타헬스키가 연출한 2014년 공개된 미국의 네오 누아르 액션 스릴러 영화이다. 키아누 리브스, 미카엘 뉘크비스트, 알피 앨런, 에이드리언 팰리키, 브리짓 모이나한, 딘 윈터스, 이언 맥셰인, 존 레귀자모, 윌럼 더포등이 출연했다. 존 윅의 첫 시작 작품이며, 빈티지 자동차와 최근에 사망한 아내가 선물로 남긴 강아지를 죽인것에 대한 복수를 하고자 하는 은퇴한 청부 살인업자 존 윅 (리브스)의 대한 이야기를 다루고 있다. 리치와 스타헬스키는 영화를 함께 연출했지만, 리치는 크레딧에 오르지 않았다. \n",
      "\n",
      "2012년에 각본을 완료했었던 데릭 콜스테드가 각본을 썼고 선더 로드 픽쳐스를 통해 제작되었다. 선더 로드 픽쳐스 사의 배질 이와닉, 리치, 에바 롱고리아, 마이클 위더릴이 제작에 참여했다. 스타헬스키와 리치에게는 제2 제작진 감독과 스턴트맨으로서 경력을 시작한 이후 첫 감독으로서의 데뷔작이다. 그들은 매트릭스 트릴로지에서 스턴트맨으로서 리브스와 함께 작업한 바가 있다.\n",
      "\n",
      "폴 디랙은 중력 상수가 우주의 거꾸로 나이와 부정확하게 일치하며, 중력 상수는 지금의 평형을 유지하기 위해 바뀌어야만 한다고 추측했다. 딕은 디랙의 관계가 선택 효과일 수 있다는 것을 깨달았다. 평형이 깨어졌을 다른 어떠한 시대에는 그 모순을 알아차릴 수 있는 지적인 생명채가 없을 것이다. 이것은 소위 약한 인간 중심 원리의 첫 번째 현대적 적용이다.\n",
      "---------------------\n",
      "Given the context information and not prior knowledge, answer the query.\n",
      "Query: 로버트 헨리 딕은 무엇을 개발했는가?\n",
      "Answer: \n"
     ]
    }
   ],
   "source": [
    "query_engine = index_llama.as_query_engine()\n",
    "response = query_engine.query(\"로버트 헨리 딕은 무엇을 개발했는가?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 예제 10.14 BM25 클래스 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "P2n3JJ2br_Aa"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "from typing import List\n",
    "from transformers import PreTrainedTokenizer\n",
    "from collections import defaultdict\n",
    "\n",
    "class BM25:\n",
    "  def __init__(self, corpus:List[List[str]], tokenizer:PreTrainedTokenizer):\n",
    "    self.tokenizer = tokenizer\n",
    "    self.corpus = corpus\n",
    "    self.tokenized_corpus = self.tokenizer(corpus, add_special_tokens=False)['input_ids']\n",
    "    self.n_docs = len(self.tokenized_corpus)\n",
    "    self.avg_doc_lens = sum(len(lst) for lst in self.tokenized_corpus) / len(self.tokenized_corpus)\n",
    "    self.idf = self._calculate_idf()\n",
    "    self.term_freqs = self._calculate_term_freqs()\n",
    "\n",
    "  def _calculate_idf(self):\n",
    "    idf = defaultdict(float)\n",
    "    for doc in self.tokenized_corpus:\n",
    "      for token_id in set(doc):\n",
    "        idf[token_id] += 1\n",
    "    for token_id, doc_frequency in idf.items():\n",
    "      idf[token_id] = math.log(((self.n_docs - doc_frequency + 0.5) / (doc_frequency + 0.5)) + 1)\n",
    "    return idf\n",
    "\n",
    "  def _calculate_term_freqs(self):\n",
    "    term_freqs = [defaultdict(int) for _ in range(self.n_docs)]\n",
    "    for i, doc in enumerate(self.tokenized_corpus):\n",
    "      for token_id in doc:\n",
    "        term_freqs[i][token_id] += 1\n",
    "    return term_freqs\n",
    "\n",
    "  def get_scores(self, query:str, k1:float = 1.2, b:float=0.75):\n",
    "    query = self.tokenizer([query], add_special_tokens=False)['input_ids'][0]\n",
    "    scores = np.zeros(self.n_docs)\n",
    "    for q in query:\n",
    "      idf = self.idf[q]\n",
    "      for i, term_freq in enumerate(self.term_freqs):\n",
    "        q_frequency = term_freq[q]\n",
    "        doc_len = len(self.tokenized_corpus[i])\n",
    "        score_q = idf * (q_frequency * (k1 + 1)) / ((q_frequency) + k1 * (1 - b + b * (doc_len / self.avg_doc_lens)))\n",
    "        scores[i] += score_q\n",
    "    return scores\n",
    "\n",
    "  def get_top_k(self, query:str, k:int):\n",
    "    scores = self.get_scores(query)\n",
    "    top_k_indices = np.argsort(scores)[-k:][::-1]\n",
    "    top_k_scores = scores[top_k_indices]\n",
    "    return top_k_scores, top_k_indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 예제 10.15 BM25 점수 계산 확인해 보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "oB3Ro5wtsAcL"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.44713859, 0.        , 0.52354835])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('klue/roberta-base')\n",
    "\n",
    "bm25 = BM25(['안녕하세요', '반갑습니다', '안녕 서울'], tokenizer)\n",
    "bm25.get_scores('안녕')\n",
    "# array([0.44713859, 0.        , 0.52354835])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 예제 10.16 BM25 검색 결과의 한계"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "8fBHx5U5sBiG"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (965 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "갤럭시S5 언제 발매한다는 건지언제는 “27일 판매한다”고 했다가 “이르면 26일 판매한다\n",
      "인구 비율당 노벨상을 세계에서 가장 많이 받은 나라, 과학 논문을 가장 많이 쓰고 의료 특\n",
      "올여름 장마가 17일 제주도에서 시작됐다. 서울 등 중부지방은 예년보다 사나흘 정도 늦은 \n"
     ]
    }
   ],
   "source": [
    "# BM25 검색 준비\n",
    "bm25 = BM25(klue_mrc_dataset['context'], tokenizer)\n",
    "\n",
    "query = \"이번 연도에는 언제 비가 많이 올까?\"\n",
    "_, bm25_search_ranking = bm25.get_top_k(query, 100)\n",
    "\n",
    "for idx in bm25_search_ranking[:3]:\n",
    "  print(klue_mrc_dataset['context'][idx][:50])\n",
    "\n",
    "# 출력 결과\n",
    "# 갤럭시S5 언제 발매한다는 건지언제는 “27일 판매한다”고 했다가 “이르면 26일 판매한다 (오답)\n",
    "# 인구 비율당 노벨상을 세계에서 가장 많이 받은 나라, 과학 논문을 가장 많이 쓰고 의료 특 (오답)\n",
    "# 올여름 장마가 17일 제주도에서 시작됐다. 서울 등 중부지방은 예년보다 사나흘 정도 늦은  (정답)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 예제 10.17 BM25 검색 결과의 장점"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "AA8336HjsC4n"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "미국 세인트루이스에서 태어났고, 프린스턴 대학교에서 학사 학위를 마치고 1939년에 로체스\n",
      ";메카동(メカドン)\n",
      ":성우 : 나라하시 미키(ならはしみき)\n",
      "길가에 버려져 있던 낡은 느티나\n",
      ";메카동(メカドン)\n",
      ":성우 : 나라하시 미키(ならはしみき)\n",
      "길가에 버려져 있던 낡은 느티나\n"
     ]
    }
   ],
   "source": [
    "query = klue_mrc_dataset[3]['question']  # 로버트 헨리 딕이 1946년에 매사추세츠 연구소에서 개발한 것은 무엇인가?\n",
    "_, bm25_search_ranking = bm25.get_top_k(query, 100)\n",
    "\n",
    "for idx in bm25_search_ranking[:3]:\n",
    "  print(klue_mrc_dataset['context'][idx][:50])\n",
    "\n",
    "# 출력 결과\n",
    "# 미국 세인트루이스에서 태어났고, 프린스턴 대학교에서 학사 학위를 마치고 1939년에 로체스 (정답)\n",
    "# ;메카동(メカドン)                                                      (오답)\n",
    "# :성우 : 나라하시 미키(ならはしみき)\n",
    "# 길가에 버려져 있던 낡은 느티나\n",
    "# ;메카동(メカドン)                                                      (오답)\n",
    "# :성우 : 나라하시 미키(ならはしみき)\n",
    "# 길가에 버려져 있던 낡은 느티나"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 예제 10.18 상호 순위 조합 함수 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "GM-kCJ0LsEMM"
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def reciprocal_rank_fusion(rankings:List[List[int]], k=5):\n",
    "    rrf = defaultdict(float)\n",
    "    for ranking in rankings:\n",
    "        for i, doc_id in enumerate(ranking, 1):\n",
    "            rrf[doc_id] += 1.0 / (k + i)\n",
    "    return sorted(rrf.items(), key=lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 예제 10.19 예시 데이터에 대한 상호 순위 조합 결과 확인하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "8l18YDEDsFaq"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 0.30952380952380953),\n",
       " (3, 0.25),\n",
       " (4, 0.24285714285714285),\n",
       " (6, 0.2111111111111111),\n",
       " (2, 0.16666666666666666),\n",
       " (5, 0.1111111111111111)]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rankings = [[1, 4, 3, 5, 6], [2, 1, 3, 6, 4]]\n",
    "reciprocal_rank_fusion(rankings)\n",
    "\n",
    "# [(1, 0.30952380952380953),\n",
    "#  (3, 0.25),\n",
    "#  (4, 0.24285714285714285),\n",
    "#  (6, 0.2111111111111111),\n",
    "#  (2, 0.16666666666666666),\n",
    "#  (5, 0.1111111111111111)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 예제 10.20 하이브리드 검색 구현하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "heGbTstMsG4v"
   },
   "outputs": [],
   "source": [
    "def dense_vector_search(query:str, k:int):\n",
    "  query_embedding = sentence_model.encode([query])\n",
    "  distances, indices = index.search(query_embedding, k)\n",
    "  return distances[0], indices[0]\n",
    "\n",
    "def hybrid_search(query, k=20):\n",
    "  _, dense_search_ranking = dense_vector_search(query, 100)\n",
    "  _, bm25_search_ranking = bm25.get_top_k(query, 100)\n",
    "\n",
    "  results = reciprocal_rank_fusion([dense_search_ranking, bm25_search_ranking], k=k)\n",
    "  return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 예제 10.21 예시 데이터에 대한 하이브리드 검색 결과 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "id": "K17VRLmQsISX"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "검색 쿼리 문장:  이번 연도에는 언제 비가 많이 올까?\n",
      "올여름 장마가 17일 제주도에서 시작됐다. 서울 등 중부지방은 예년보다 사나흘 정도 늦은 \n",
      "갤럭시S5 언제 발매한다는 건지언제는 “27일 판매한다”고 했다가 “이르면 26일 판매한다\n",
      "연구 결과에 따르면, 오리너구리의 눈은 대부분의 포유류보다는 어류인 칠성장어나 먹장어, 그\n",
      "================================================================================\n",
      "검색 쿼리 문장:  로버트 헨리 딕이 1946년에 매사추세츠 연구소에서 개발한 것은 무엇인가?\n",
      "미국 세인트루이스에서 태어났고, 프린스턴 대학교에서 학사 학위를 마치고 1939년에 로체스\n",
      "1950년대 말 매사추세츠 공과대학교의 동아리 테크모델철도클럽에서 ‘해커’라는 용어가 처음\n",
      "1950년대 말 매사추세츠 공과대학교의 동아리 테크모델철도클럽에서 ‘해커’라는 용어가 처음\n"
     ]
    }
   ],
   "source": [
    "query = \"이번 연도에는 언제 비가 많이 올까?\"\n",
    "print(\"검색 쿼리 문장: \", query)\n",
    "results = hybrid_search(query)\n",
    "for idx, score in results[:3]:\n",
    "  print(klue_mrc_dataset['context'][idx][:50])\n",
    "\n",
    "print(\"=\" * 80)\n",
    "query = klue_mrc_dataset[3]['question'] # 로버트 헨리 딕이 1946년에 매사추세츠 연구소에서 개발한 것은 무엇인가?\n",
    "print(\"검색 쿼리 문장: \", query)\n",
    "\n",
    "results = hybrid_search(query)\n",
    "for idx, score in results[:3]:\n",
    "  print(klue_mrc_dataset['context'][idx][:50])\n",
    "\n",
    "# 출력 결과\n",
    "# 검색 쿼리 문장:  이번 연도에는 언제 비가 많이 올까?\n",
    "# 올여름 장마가 17일 제주도에서 시작됐다. 서울 등 중부지방은 예년보다 사나흘 정도 늦은  (정답)\n",
    "# 갤럭시S5 언제 발매한다는 건지언제는 “27일 판매한다”고 했다가 “이르면 26일 판매한다  (오답)\n",
    "# 연구 결과에 따르면, 오리너구리의 눈은 대부분의 포유류보다는 어류인 칠성장어나 먹장어, 그 (오답)\n",
    "# ================================================================================\n",
    "# 검색 쿼리 문장:  로버트 헨리 딕이 1946년에 매사추세츠 연구소에서 개발한 것은 무엇인가?\n",
    "# 미국 세인트루이스에서 태어났고, 프린스턴 대학교에서 학사 학위를 마치고 1939년에 로체스 (정답)\n",
    "# 1950년대 말 매사추세츠 공과대학교의 동아리 테크모델철도클럽에서 ‘해커’라는 용어가 처음 (오답)\n",
    "# 1950년대 말 매사추세츠 공과대학교의 동아리 테크모델철도클럽에서 ‘해커’라는 용어가 처음 (오답)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "lang310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
