{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6장 sLLM 학습하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install transformers==4.40.1 bitsandbytes==0.43.1 accelerate==0.29.3 datasets==2.19.0 tiktoken==0.6.0 huggingface_hub==0.22.2 autotrain-advanced==0.7.77 -qqq\n",
    "# !pip install --upgrade huggingface-hub -qqq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 성능 평가 파이프라인 준비하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SQL 성능 프롬프트"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 예제 6.2. SQL 프롬프트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_prompt(ddl, question, query=''):\n",
    "    prompt = f\"\"\"당신은 SQL을 생성하는 SQL 봇입니다. DDL의 테이블을 활용한 Question을 해결할 수 있는 SQL 쿼리를 생성하세요.\n",
    "\n",
    "### DDL:\n",
    "{ddl}\n",
    "\n",
    "### Question:\n",
    "{question}\n",
    "\n",
    "### SQL:\n",
    "{query}\"\"\"\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPT-4 평가 프롬프트와 코드 준비"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 예제 6.4. 평가를 위한 요청 jsonl 작성 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 필요한 라이브러리 임포트\n",
    "import json\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "def make_requests_for_gpt_evaluation(df, filename, dir='results'):\n",
    "  \"\"\"\n",
    "  GPT 평가를 위한 요청 파일을 생성하는 함수\n",
    "  \n",
    "  Args:\n",
    "      df: 평가 데이터가 포함된 데이터프레임\n",
    "      filename: 저장할 파일 이름 \n",
    "      dir: 저장할 디렉토리 경로 (기본값: 'results')\n",
    "  \"\"\"\n",
    "  # 디렉토리가 없으면 생성\n",
    "  if not Path(dir).exists():\n",
    "      Path(dir).mkdir(parents=True)\n",
    "      \n",
    "  # 프롬프트 리스트 생성\n",
    "  prompts = []\n",
    "  for idx, row in df.iterrows():\n",
    "      prompts.append(\"\"\"Based on below DDL and Question, evaluate gen_sql can resolve Question. If gen_sql and gt_sql do equal job, return \"yes\" else return \"no\". Output JSON Format: {\"resolve_yn\": \"\"}\"\"\" + f\"\"\"\n",
    "\n",
    "DDL: {row['context']}\n",
    "Question: {row['question']}\n",
    "gt_sql: {row['answer']}\n",
    "gen_sql: {row['gen_sql']}\"\"\"\n",
    ")\n",
    "\n",
    "  # GPT-4 요청을 위한 job 리스트 생성\n",
    "  jobs = [{\"model\": \"gpt-4-turbo-preview\", \"response_format\" : { \"type\": \"json_object\" }, \"messages\": [{\"role\": \"system\", \"content\": prompt}]} for prompt in prompts]\n",
    "  \n",
    "  # 파일에 job을 jsonl 형식으로 저장\n",
    "  with open(Path(dir, filename), \"w\") as f:\n",
    "      for job in jobs:\n",
    "          json_string = json.dumps(job)\n",
    "          f.write(json_string + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 예제 6.5. 비동기 요청 명령"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# os.environ[\"OPENAI_API_KEY\"] = \"자신의 OpenAI API 키 입력\"\n",
    "\n",
    "python api_request_parallel_processor.py \\\n",
    "  --requests_filepath {요청 파일 경로} \\\n",
    "  --save_filepath {생성할 결과 파일 경로} \\\n",
    "  --request_url https://api.openai.com/v1/chat/completions \\\n",
    "  --max_requests_per_minute 300 \\\n",
    "  --max_tokens_per_minute 100000 \\\n",
    "  --token_encoding_name cl100k_base \\\n",
    "  --max_attempts 5 \\\n",
    "  --logging_level 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 예제 6.6. 결과 jsonl 파일을 csv로 변환하는 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def change_jsonl_to_csv(input_file, output_file, prompt_column=\"prompt\", response_column=\"response\"):\n",
    "#     prompts = []\n",
    "#     responses = []\n",
    "#     with open(input_file, 'r') as json_file:\n",
    "#         for data in json_file:\n",
    "#             prompts.append(json.loads(data)[0]['messages'][0]['content'])\n",
    "#             responses.append(json.loads(data)[1]['choices'][0]['message']['content'])\n",
    "\n",
    "#     df = pd.DataFrame({prompt_column: prompts, response_column: responses})\n",
    "#     df.to_csv(output_file, index=False)\n",
    "#     return df\n",
    "\n",
    "def change_jsonl_to_csv(input_file, output_file, prompt_column=\"prompt\", response_column=\"response\"):\n",
    "    prompts = []\n",
    "    responses = []\n",
    "    with open(input_file, 'r') as json_file:\n",
    "        for line in json_file:\n",
    "            # 각 줄은 배열 형태로 요청과 응답을 포함\n",
    "            data = json.loads(line)\n",
    "            # 첫 번째 요소(인덱스 0)가 요청 정보\n",
    "            prompts.append(data[0][\"messages\"][0][\"content\"])\n",
    "            # 두 번째 요소(인덱스 1)가 응답 정보\n",
    "            responses.append(data[1][\"choices\"][0][\"message\"][\"content\"])\n",
    "\n",
    "    df = pd.DataFrame({prompt_column: prompts, response_column: responses})\n",
    "    df.to_csv(output_file, index=False)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 실습: 미세 조정 수행하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 기초 모델 평가하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyTorch와 Transformers 라이브러리 임포트\n",
    "import torch\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# 추론 파이프라인 생성 함수 정의\n",
    "def make_inference_pipeline(model_id):\n",
    "  # 토크나이저 로드\n",
    "  tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "  # 4비트 양자화를 적용한 모델 로드 \n",
    "  model = AutoModelForCausalLM.from_pretrained(model_id, device_map=\"auto\", load_in_4bit=True, bnb_4bit_compute_dtype=torch.float16)\n",
    "  # 텍스트 생성 파이프라인 생성\n",
    "  pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n",
    "  return pipe\n",
    "\n",
    "# Yi-Ko-6B 모델 ID 설정\n",
    "model_id = 'beomi/Yi-Ko-6B'\n",
    "# 추론 파이프라인 생성\n",
    "hf_pipe = make_inference_pipeline(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SQL 생성을 위한 예제 프롬프트\n",
    "example = \"\"\"당신은 SQL을 생성하는 SQL 봇입니다. DDL의 테이블을 활용한 Question을 해결할 수 있는 SQL 쿼리를 생성하세요.\n",
    "\n",
    "### DDL:\n",
    "CREATE TABLE players (\n",
    "  player_id INT PRIMARY KEY AUTO_INCREMENT,\n",
    "  username VARCHAR(255) UNIQUE NOT NULL,\n",
    "  email VARCHAR(255) UNIQUE NOT NULL,\n",
    "  password_hash VARCHAR(255) NOT NULL,\n",
    "  date_joined DATETIME NOT NULL,\n",
    "  last_login DATETIME\n",
    ");\n",
    "\n",
    "### Question:\n",
    "사용자 이름에 'admin'이 포함되어 있는 계정의 수를 알려주세요.\n",
    "\n",
    "### SQL:\n",
    "\"\"\"\n",
    "\n",
    "# 모델을 사용하여 SQL 쿼리 생성\n",
    "hf_pipe(example, do_sample=False,\n",
    "    return_full_text=False, max_length=512, truncation=True)\n",
    "#  SELECT COUNT(*) FROM players WHERE username LIKE '%admin%';\n",
    "\n",
    "# ### SQL 봇:\n",
    "# SELECT COUNT(*) FROM players WHERE username LIKE '%admin%';\n",
    "\n",
    "# ### SQL 봇의 결과:\n",
    "# SELECT COUNT(*) FROM players WHERE username LIKE '%admin%'; (생략)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 예제 6.8. 기초 모델 성능 측정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 데이터셋 라이브러리 임포트\n",
    "# from datasets import load_dataset\n",
    "\n",
    "# # ko_text2sql 데이터셋의 테스트 세트 불러오기\n",
    "# df = load_dataset(\"shangrilar/ko_text2sql\", \"origin\")['test']\n",
    "# # pandas DataFrame으로 변환\n",
    "# df = df.to_pandas()\n",
    "\n",
    "# # 각 데이터에 대해 프롬프트 생성\n",
    "# for idx, row in df.iterrows():\n",
    "#   # context와 question을 조합하여 프롬프트 생성\n",
    "#   prompt = make_prompt(row['context'], row['question'])\n",
    "#   # 생성된 프롬프트를 DataFrame에 저장\n",
    "#   df.loc[idx, 'prompt'] = prompt\n",
    "\n",
    "# # 모델을 사용하여 SQL 쿼리 생성\n",
    "# gen_sqls = hf_pipe(df['prompt'].tolist(), do_sample=False,\n",
    "#                    return_full_text=False, max_length=512, truncation=True)\n",
    "# # 생성된 SQL 쿼리 추출\n",
    "# gen_sqls = [x[0]['generated_text'] for x in gen_sqls]\n",
    "# # 생성된 SQL을 DataFrame에 저장\n",
    "# df['gen_sql'] = gen_sqls\n",
    "\n",
    "# # GPT 평가를 위한 JSONL 파일 생성\n",
    "# eval_filepath = \"./results/text2sql_evaluation.jsonl\"\n",
    "# make_requests_for_gpt_evaluation(df, eval_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터셋 라이브러리 임포트\n",
    "from datasets import load_dataset\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "# ko_text2sql 데이터셋의 테스트 세트 불러오기\n",
    "df = load_dataset(\"shangrilar/ko_text2sql\", \"origin\")['test']\n",
    "# pandas DataFrame으로 변환\n",
    "df = df.to_pandas()\n",
    "\n",
    "# 각 데이터에 대해 프롬프트 생성 (tqdm 추가)\n",
    "for idx, row in df.iterrows():\n",
    "  # context와 question을 조합하여 프롬프트 생성\n",
    "  prompt = make_prompt(row['context'], row['question'])\n",
    "  # 생성된 프롬프트를 DataFrame에 저장\n",
    "  df.loc[idx, 'prompt'] = prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 청크 크기 설정\n",
    "CHUNK_SIZE = 10  # 필요에 따라 조정하세요\n",
    "\n",
    "# 결과를 저장할 리스트\n",
    "gen_sqls_list = []\n",
    "\n",
    "# 데이터를 청크로 나누어 모델로 SQL 쿼리 생성 (진행 상황 표시)\n",
    "for i in tqdm(range(0, len(df), CHUNK_SIZE), desc=\"SQL 쿼리 생성 중\"):\n",
    "    # 현재 청크의 프롬프트 가져오기\n",
    "    prompts_chunk = df['prompt'][i:i+CHUNK_SIZE].tolist()\n",
    "    \n",
    "    # 빈 리스트면 건너뛰기\n",
    "    if not prompts_chunk:\n",
    "        continue\n",
    "        \n",
    "    # 모델을 사용하여 현재 청크의 SQL 쿼리 생성\n",
    "    chunk_results = hf_pipe(prompts_chunk, do_sample=False,\n",
    "                           return_full_text=False, max_length=512, truncation=True)\n",
    "    \n",
    "    # 생성된 SQL 쿼리 추출 후 리스트에 추가\n",
    "    chunk_gen_sqls = [x[0]['generated_text'] for x in chunk_results]\n",
    "    gen_sqls_list.extend(chunk_gen_sqls)\n",
    "\n",
    "# 생성된 SQL을 DataFrame에 저장\n",
    "df['gen_sql'] = gen_sqls_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPT 평가를 위한 JSONL 파일 생성\n",
    "eval_filepath = \"text2sql_evaluation.jsonl\"\n",
    "make_requests_for_gpt_evaluation(df, eval_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_filepath = \"text2sql_result.jsonl\"\n",
    "\n",
    "# GPT-4 평가 수행\n",
    "!python api_request_parallel_processor.py \\\n",
    "--requests_filepath results/{eval_filepath}  \\\n",
    "--save_filepath results/{result_filepath} \\\n",
    "--request_url https://api.openai.com/v1/chat/completions \\\n",
    "--max_requests_per_minute 100 \\\n",
    "--max_tokens_per_minute 20000 \\\n",
    "--token_encoding_name cl100k_base \\\n",
    "--max_attempts 5 \\\n",
    "--logging_level 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_filepath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_eval = change_jsonl_to_csv(f\"results/{result_filepath}\", \"results/yi_ko_6b_eval.csv\", \"prompt\", \"resolve_yn\")\n",
    "base_eval['resolve_yn'] = base_eval['resolve_yn'].apply(lambda x: json.loads(x)['resolve_yn'])\n",
    "num_correct_answers = base_eval.query(\"resolve_yn == 'yes'\").shape[0]\n",
    "print(f\"정확한 답변 개수: {num_correct_answers}/{len(base_eval)} ({num_correct_answers/len(base_eval)*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 미세 조정 수행"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 예제 6.9. 학습 데이터 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터셋 라이브러리 임포트\n",
    "from datasets import load_dataset\n",
    "\n",
    "# ko_text2sql 데이터셋 불러오기 \n",
    "df_sql = load_dataset(\"shangrilar/ko_text2sql\", \"origin\")[\"train\"]\n",
    "# pandas DataFrame으로 변환\n",
    "df_sql = df_sql.to_pandas()\n",
    "# 결측치 제거 및 랜덤 셔플링 \n",
    "df_sql = df_sql.dropna().sample(frac=1, random_state=42)\n",
    "# db_id가 1인 데이터 제외\n",
    "df_sql = df_sql.query(\"db_id != 1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 각 행에 대해 프롬프트 생성\n",
    "for idx, row in df_sql.iterrows():\n",
    "  df_sql.loc[idx, 'text'] = make_prompt(row['context'], row['question'], row['answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data 디렉토리가 없는 경우에만 생성\n",
    "import os\n",
    "if not os.path.exists('data'):\n",
    "    os.makedirs('data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습 데이터를 CSV 파일로 저장\n",
    "df_sql.to_csv('data/train.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 예제 6.10. 미세 조정 명령어\n",
    "\n",
    ">**autotrain-advanced**\n",
    ">- Hugging Face에서 제공하는 CLI 기반의 모델 학습 도구입니다.\n",
    ">- 복잡한 코드 작성 없이 명령어만으로 모델 학습을 수행할 수 있습니다.\n",
    ">- LoRA, QLoRA 등 다양한 파라미터 튜닝 기법을 지원합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = 'beomi/Yi-Ko-6B'\n",
    "finetuned_model = './models/yi-ko-6b-text2sql'\n",
    "\n",
    "!autotrain llm \\\n",
    "--train \\\n",
    "--model {base_model} \\\n",
    "--project-name {finetuned_model} \\\n",
    "--data-path data/ \\\n",
    "--text-column text \\\n",
    "--lr 2e-4 \\\n",
    "--batch-size 8 \\\n",
    "--epochs 1 \\\n",
    "--block-size 1024 \\\n",
    "--warmup-ratio 0.1 \\\n",
    "--lora-r 16 \\\n",
    "--lora-alpha 32 \\\n",
    "--lora-dropout 0.05 \\\n",
    "--weight-decay 0.01 \\\n",
    "--gradient-accumulation 8 \\\n",
    "--mixed-precision fp16 \\\n",
    "--use-peft \\\n",
    "--quantization int4 \\\n",
    "--trainer sft"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 예제 6.11. LoRA 어댑터 결합 및 허깅페이스 허브 업로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 필요한 라이브러리 임포트\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import LoraConfig, PeftModel\n",
    "\n",
    "# 모델 이름과 디바이스 설정\n",
    "model_name = base_model\n",
    "finetuned_model = './models/yi-ko-6b-text2sql'\n",
    "\n",
    "device_map = {\"\": 0}  # GPU 0번 디바이스 사용\n",
    "\n",
    "# 기초 모델 불러오기\n",
    "# - low_cpu_mem_usage: CPU 메모리 사용량 최소화\n",
    "# - return_dict: 모델 출력을 딕셔너리 형태로 반환\n",
    "# - torch_dtype: FP16 정밀도 사용\n",
    "# - device_map: GPU 디바이스 매핑\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    low_cpu_mem_usage=True,\n",
    "    return_dict=True,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=device_map,\n",
    ")\n",
    "\n",
    "# LoRA 어댑터를 기초 모델에 결합\n",
    "model = PeftModel.from_pretrained(base_model, finetuned_model)\n",
    "model = model.merge_and_unload()  # LoRA 가중치를 기초 모델에 병합"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 토크나이저 설정\n",
    "# - trust_remote_code: 원격 코드 신뢰 옵션 활성화\n",
    "# - pad_token: 패딩 토큰을 EOS 토큰으로 설정\n",
    "# - padding_side: 오른쪽 패딩 적용\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 허깅페이스 허브에 모델과 토크나이저 업로드\n",
    "# - use_temp_dir=False: 임시 디렉토리 사용하지 않음\n",
    "\n",
    "# 허깅페이스 허브에 모델과 토크나이저 업로드\n",
    "hub_model_id = 'restful3/yi-ko-6b-text2sql'\n",
    "model.push_to_hub(hub_model_id, use_temp_dir=True)  # use_temp_dir를 True로 변경\n",
    "tokenizer.push_to_hub(hub_model_id, use_temp_dir=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 미세 조정한 모델로 예시 데이터에 대한 SQL 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터셋 라이브러리 임포트\n",
    "from datasets import load_dataset\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# 추론 파이프라인 생성 함수 정의\n",
    "def make_inference_pipeline(model_id):\n",
    "  # 토크나이저 로드\n",
    "  tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "  # 4비트 양자화를 적용한 모델 로드 \n",
    "  model = AutoModelForCausalLM.from_pretrained(model_id, device_map=\"auto\", load_in_4bit=True, bnb_4bit_compute_dtype=torch.float16)\n",
    "  # 텍스트 생성 파이프라인 생성\n",
    "  pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n",
    "  return pipe\n",
    "\n",
    "def make_prompt(ddl, question, query=''):\n",
    "    prompt = f\"\"\"당신은 SQL을 생성하는 SQL 봇입니다. DDL의 테이블을 활용한 Question을 해결할 수 있는 SQL 쿼리를 생성하세요.\n",
    "\n",
    "### DDL:\n",
    "{ddl}\n",
    "\n",
    "### Question:\n",
    "{question}\n",
    "\n",
    "### SQL:\n",
    "{query}\"\"\"\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"restful3/yi-ko-6b-text2sql\"\n",
    "hf_pipe = make_inference_pipeline(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ko_text2sql 데이터셋의 테스트 세트 불러오기\n",
    "df = load_dataset(\"shangrilar/ko_text2sql\", \"origin\")['test']\n",
    "# pandas DataFrame으로 변환\n",
    "df = df.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 각 데이터에 대해 프롬프트 생성 (tqdm 추가)\n",
    "for idx, row in df.iterrows():\n",
    "  # context와 question을 조합하여 프롬프트 생성\n",
    "  prompt = make_prompt(row['context'], row['question'])\n",
    "  # 생성된 프롬프트를 DataFrame에 저장\n",
    "  df.loc[idx, 'prompt'] = prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 청크 크기 설정\n",
    "CHUNK_SIZE = 10  # 필요에 따라 조정하세요\n",
    "\n",
    "# 결과를 저장할 리스트\n",
    "gen_sqls_list = []\n",
    "\n",
    "# 데이터를 청크로 나누어 모델로 SQL 쿼리 생성 (진행 상황 표시)\n",
    "for i in tqdm(range(0, len(df), CHUNK_SIZE), desc=\"SQL 쿼리 생성 중\"):\n",
    "    # 현재 청크의 프롬프트 가져오기\n",
    "    prompts_chunk = df['prompt'][i:i+CHUNK_SIZE].tolist()\n",
    "    \n",
    "    # 빈 리스트면 건너뛰기\n",
    "    if not prompts_chunk:\n",
    "        continue\n",
    "        \n",
    "    # 모델을 사용하여 현재 청크의 SQL 쿼리 생성\n",
    "    chunk_results = hf_pipe(prompts_chunk, do_sample=False,\n",
    "                           return_full_text=False, max_length=512, truncation=True)\n",
    "    \n",
    "    # 생성된 SQL 쿼리 추출 후 리스트에 추가\n",
    "    chunk_gen_sqls = [x[0]['generated_text'] for x in chunk_results]\n",
    "    gen_sqls_list.extend(chunk_gen_sqls)\n",
    "\n",
    "# 생성된 SQL을 DataFrame에 저장\n",
    "df['gen_sql'] = gen_sqls_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 예제 6.13. 미세 조정한 모델 성능 측정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 필요한 라이브러리 임포트\n",
    "import json\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "def make_requests_for_gpt_evaluation(df, filename, dir='results'):\n",
    "  \"\"\"\n",
    "  GPT 평가를 위한 요청 파일을 생성하는 함수\n",
    "  \n",
    "  Args:\n",
    "      df: 평가 데이터가 포함된 데이터프레임\n",
    "      filename: 저장할 파일 이름 \n",
    "      dir: 저장할 디렉토리 경로 (기본값: 'results')\n",
    "  \"\"\"\n",
    "  # 디렉토리가 없으면 생성\n",
    "  if not Path(dir).exists():\n",
    "      Path(dir).mkdir(parents=True)\n",
    "      \n",
    "  # 프롬프트 리스트 생성\n",
    "  prompts = []\n",
    "  for idx, row in df.iterrows():\n",
    "      prompts.append(\"\"\"Based on below DDL and Question, evaluate gen_sql can resolve Question. If gen_sql and gt_sql do equal job, return \"yes\" else return \"no\". Output JSON Format: {\"resolve_yn\": \"\"}\"\"\" + f\"\"\"\n",
    "\n",
    "DDL: {row['context']}\n",
    "Question: {row['question']}\n",
    "gt_sql: {row['answer']}\n",
    "gen_sql: {row['gen_sql']}\"\"\"\n",
    ")\n",
    "\n",
    "  # GPT-4 요청을 위한 job 리스트 생성\n",
    "  jobs = [{\"model\": \"gpt-4-turbo-preview\", \"response_format\" : { \"type\": \"json_object\" }, \"messages\": [{\"role\": \"system\", \"content\": prompt}]} for prompt in prompts]\n",
    "  \n",
    "  # 파일에 job을 jsonl 형식으로 저장\n",
    "  with open(Path(dir, filename), \"w\") as f:\n",
    "      for job in jobs:\n",
    "          json_string = json.dumps(job)\n",
    "          f.write(json_string + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 평가를 위한 requests.jsonl 생성\n",
    "ft_eval_filepath = \"text2sql_evaluation_finetuned.jsonl\"\n",
    "ft_result_filepath = \"text2sql_result_finetuned.jsonl\"\n",
    "\n",
    "make_requests_for_gpt_evaluation(df, ft_eval_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPT-4 평가 수행\n",
    "!python api_request_parallel_processor.py \\\n",
    "  --requests_filepath results/{ft_eval_filepath} \\\n",
    "  --save_filepath results/{ft_result_filepath} \\\n",
    "  --request_url https://api.openai.com/v1/chat/completions \\\n",
    "  --max_requests_per_minute 100 \\\n",
    "  --max_tokens_per_minute 30000 \\\n",
    "  --token_encoding_name cl100k_base \\\n",
    "  --max_attempts 5 \\\n",
    "  --logging_level 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def change_jsonl_to_csv(input_file, output_file, prompt_column=\"prompt\", response_column=\"response\"):\n",
    "    prompts = []\n",
    "    responses = []\n",
    "    with open(input_file, 'r') as json_file:\n",
    "        for line in json_file:\n",
    "            # 각 줄은 배열 형태로 요청과 응답을 포함\n",
    "            data = json.loads(line)\n",
    "            # 첫 번째 요소(인덱스 0)가 요청 정보\n",
    "            prompts.append(data[0][\"messages\"][0][\"content\"])\n",
    "            # 두 번째 요소(인덱스 1)가 응답 정보\n",
    "            responses.append(data[1][\"choices\"][0][\"message\"][\"content\"])\n",
    "\n",
    "    df = pd.DataFrame({prompt_column: prompts, response_column: responses})\n",
    "    df.to_csv(output_file, index=False)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_eval = change_jsonl_to_csv(f\"results/{ft_result_filepath}\", \"results/yi_ko_6b_eval.csv\", \"prompt\", \"resolve_yn\")\n",
    "base_eval['resolve_yn'] = base_eval['resolve_yn'].apply(lambda x: json.loads(x)['resolve_yn'])\n",
    "num_correct_answers = base_eval.query(\"resolve_yn == 'yes'\").shape[0]\n",
    "print(f\"정확한 답변 개수: {num_correct_answers}/{len(base_eval)} ({num_correct_answers/len(base_eval)*100:.2f}%)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
