{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V100",
      "mount_file_id": "1YbMvIBRycktUn48ThxKsQzePS9Qy3ApI",
      "authorship_tag": "ABX9TyMBXCYnoZCiWbKfhPwDd97t",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/restful3/ds4th_study/blob/main/source/%ED%85%90%EC%B4%88%EC%9D%98%20%ED%8C%8C%EC%9D%B4%ED%86%A0%EC%B9%98%20%EB%94%A5%EB%9F%AC%EB%8B%9D%20%ED%8A%B9%EA%B0%95/LSTM%ED%85%8D%EC%8A%A4%ED%8A%B8%EC%83%9D%EC%84%B1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9lTA_n7z1Uy7",
        "outputId": "fb3c28fc-82fd-44c7-8927-84c5fc00fcae"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Index(['abstract', 'articleID', 'articleWordCount', 'byline', 'documentType',\n",
            "       'headline', 'keywords', 'multimedia', 'newDesk', 'printPage', 'pubDate',\n",
            "       'sectionName', 'snippet', 'source', 'typeOfMaterial', 'webURL'],\n",
            "      dtype='object')\n",
            "(886, 16)\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "import string\n",
        "df= pd.read_csv('/content/drive/MyDrive/SelfStudy/딥러닝기초/data/CH10/ArticlesApril2017.csv')\n",
        "print(df.columns)\n",
        "print(df.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "헤드라인만 가져와서 텍스트분석을 진행한다.\n",
        "\n",
        "![](https://drive.google.com/uc?id=15kGb1FM8HLEiLeypYdOGnu4sF35A2GsU)"
      ],
      "metadata": {
        "id": "qz7smLhnyOfE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- BOW (Bag of Words)\n",
        "> 모든 단어를 겹치지 않도록 고유번호로 인식한다.\n",
        "- [corpus ](https://ko.wikipedia.org/wiki/%EB%A7%90%EB%AD%89%EC%B9%98)\n",
        "> 말뭉치라고도 하며, 자연어처리를 위해 구성되는 기본집단\n",
        "\n",
        "![](https://drive.google.com/uc?id=1uqEw3liB1LOmktYxf-T7C613xskD196j)\n"
      ],
      "metadata": {
        "id": "teYndNttxlqd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "help(string)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IP9LmothUOe6",
        "outputId": "6e9c38c0-0563-4c2f-d467-4ba7404714c3"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Help on module string:\n",
            "\n",
            "NAME\n",
            "    string - A collection of string constants.\n",
            "\n",
            "MODULE REFERENCE\n",
            "    https://docs.python.org/3.10/library/string.html\n",
            "    \n",
            "    The following documentation is automatically generated from the Python\n",
            "    source files.  It may be incomplete, incorrect or include features that\n",
            "    are considered implementation detail and may vary between Python\n",
            "    implementations.  When in doubt, consult the module reference at the\n",
            "    location listed above.\n",
            "\n",
            "DESCRIPTION\n",
            "    Public module variables:\n",
            "    \n",
            "    whitespace -- a string containing all ASCII whitespace\n",
            "    ascii_lowercase -- a string containing all ASCII lowercase letters\n",
            "    ascii_uppercase -- a string containing all ASCII uppercase letters\n",
            "    ascii_letters -- a string containing all ASCII letters\n",
            "    digits -- a string containing all ASCII decimal digits\n",
            "    hexdigits -- a string containing all ASCII hexadecimal digits\n",
            "    octdigits -- a string containing all ASCII octal digits\n",
            "    punctuation -- a string containing all ASCII punctuation characters\n",
            "    printable -- a string containing all ASCII characters considered printable\n",
            "\n",
            "CLASSES\n",
            "    builtins.object\n",
            "        Formatter\n",
            "        Template\n",
            "    \n",
            "    class Formatter(builtins.object)\n",
            "     |  Methods defined here:\n",
            "     |  \n",
            "     |  check_unused_args(self, used_args, args, kwargs)\n",
            "     |  \n",
            "     |  convert_field(self, value, conversion)\n",
            "     |  \n",
            "     |  format(self, format_string, /, *args, **kwargs)\n",
            "     |  \n",
            "     |  format_field(self, value, format_spec)\n",
            "     |  \n",
            "     |  get_field(self, field_name, args, kwargs)\n",
            "     |      # given a field_name, find the object it references.\n",
            "     |      #  field_name:   the field being looked up, e.g. \"0.name\"\n",
            "     |      #                 or \"lookup[3]\"\n",
            "     |      #  used_args:    a set of which args have been used\n",
            "     |      #  args, kwargs: as passed in to vformat\n",
            "     |  \n",
            "     |  get_value(self, key, args, kwargs)\n",
            "     |  \n",
            "     |  parse(self, format_string)\n",
            "     |      # returns an iterable that contains tuples of the form:\n",
            "     |      # (literal_text, field_name, format_spec, conversion)\n",
            "     |      # literal_text can be zero length\n",
            "     |      # field_name can be None, in which case there's no\n",
            "     |      #  object to format and output\n",
            "     |      # if field_name is not None, it is looked up, formatted\n",
            "     |      #  with format_spec and conversion and then used\n",
            "     |  \n",
            "     |  vformat(self, format_string, args, kwargs)\n",
            "     |  \n",
            "     |  ----------------------------------------------------------------------\n",
            "     |  Data descriptors defined here:\n",
            "     |  \n",
            "     |  __dict__\n",
            "     |      dictionary for instance variables (if defined)\n",
            "     |  \n",
            "     |  __weakref__\n",
            "     |      list of weak references to the object (if defined)\n",
            "    \n",
            "    class Template(builtins.object)\n",
            "     |  Template(template)\n",
            "     |  \n",
            "     |  A string class for supporting $-substitutions.\n",
            "     |  \n",
            "     |  Methods defined here:\n",
            "     |  \n",
            "     |  __init__(self, template)\n",
            "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
            "     |  \n",
            "     |  safe_substitute(self, mapping={}, /, **kws)\n",
            "     |  \n",
            "     |  substitute(self, mapping={}, /, **kws)\n",
            "     |  \n",
            "     |  ----------------------------------------------------------------------\n",
            "     |  Class methods defined here:\n",
            "     |  \n",
            "     |  __init_subclass__() from builtins.type\n",
            "     |      This method is called when a class is subclassed.\n",
            "     |      \n",
            "     |      The default implementation does nothing. It may be\n",
            "     |      overridden to extend subclasses.\n",
            "     |  \n",
            "     |  ----------------------------------------------------------------------\n",
            "     |  Data descriptors defined here:\n",
            "     |  \n",
            "     |  __dict__\n",
            "     |      dictionary for instance variables (if defined)\n",
            "     |  \n",
            "     |  __weakref__\n",
            "     |      list of weak references to the object (if defined)\n",
            "     |  \n",
            "     |  ----------------------------------------------------------------------\n",
            "     |  Data and other attributes defined here:\n",
            "     |  \n",
            "     |  braceidpattern = None\n",
            "     |  \n",
            "     |  delimiter = '$'\n",
            "     |  \n",
            "     |  flags = re.IGNORECASE\n",
            "     |  \n",
            "     |  idpattern = '(?a:[_a-z][_a-z0-9]*)'\n",
            "     |  \n",
            "     |  pattern = re.compile('\\n            \\\\$(?:\\n              ...identifie...\n",
            "\n",
            "FUNCTIONS\n",
            "    capwords(s, sep=None)\n",
            "        capwords(s [,sep]) -> string\n",
            "        \n",
            "        Split the argument into words using split, capitalize each\n",
            "        word using capitalize, and join the capitalized words using\n",
            "        join.  If the optional second argument sep is absent or None,\n",
            "        runs of whitespace characters are replaced by a single space\n",
            "        and leading and trailing whitespace are removed, otherwise\n",
            "        sep is used to split and join the words.\n",
            "\n",
            "DATA\n",
            "    __all__ = ['ascii_letters', 'ascii_lowercase', 'ascii_uppercase', 'cap...\n",
            "    ascii_letters = 'abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ'\n",
            "    ascii_lowercase = 'abcdefghijklmnopqrstuvwxyz'\n",
            "    ascii_uppercase = 'ABCDEFGHIJKLMNOPQRSTUVWXYZ'\n",
            "    digits = '0123456789'\n",
            "    hexdigits = '0123456789abcdefABCDEF'\n",
            "    octdigits = '01234567'\n",
            "    printable = '0123456789abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTU...\n",
            "    punctuation = '!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'\n",
            "    whitespace = ' \\t\\n\\r\\x0b\\x0c'\n",
            "\n",
            "FILE\n",
            "    /usr/lib/python3.10/string.py\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import glob\n",
        "from torch.utils.data.dataset import Dataset\n",
        "\n",
        "class TextGeneration(Dataset):\n",
        "    def clean_text(self, txt):\n",
        "        # 모든 단어를 소문자로 바꾸고 특수문자를 제거\n",
        "        txt = \"\".join(v for v in txt if v not in string.punctuation).lower()\n",
        "        return txt\n",
        "\n",
        "    def __init__(self):\n",
        "        all_headlines = []\n",
        "\n",
        "        # 모든 헤드라인의 텍스트를 불러옴\n",
        "        for filename in glob.glob(\"/content/drive/MyDrive/SelfStudy/딥러닝기초/data/CH10/*.csv\"):\n",
        "            if 'Articles' in filename:\n",
        "                article_df = pd.read_csv(filename)\n",
        "\n",
        "                # 데이터셋의 headline의 값을 all_headlines에 추가\n",
        "                all_headlines.extend(list(article_df.headline.values))\n",
        "                break\n",
        "\n",
        "        # headline 중 unknown 값은 제거\n",
        "        all_headlines = [h for h in all_headlines if h != \"Unknown\"]\n",
        "\n",
        "        # 구두점 제거 및 전처리가 된 문장들을 리스트로 반환\n",
        "        self.corpus = [self.clean_text(x) for x in all_headlines]\n",
        "        self.BOW = {}\n",
        "\n",
        "        # 모든 문장의 단어를 추출해 고유번호 지정\n",
        "        for line in self.corpus: # 전처리된 문장 하나씩 불러와서\n",
        "            for word in line.split(): # 쪼갠 word를\n",
        "                if word not in self.BOW.keys(): # 아직 키로 저장 안된것만 골라서\n",
        "                    self.BOW[word] = len(self.BOW.keys()) # 새로운 키로 저장함\n",
        "\n",
        "        # 모델의 입력으로 사용할 데이터\n",
        "        self.data = self.generate_sequence(self.corpus)\n",
        "\n",
        "\n",
        "    def generate_sequence(self, txt):\n",
        "        seq = []\n",
        "        for line in txt:\n",
        "            line = line.split() # line 분해리스트\n",
        "            line_bow = [self.BOW[word] for word in line] #BOW[word]의 고유번호 리스트\n",
        "            # 단어 2개를 입력으로, 그다음 단어를 정답으로\n",
        "            data = [([line_bow[i], line_bow[i+1]], line_bow[i+2])\n",
        "            for i in range(len(line_bow)-2)]\n",
        "            seq.extend(data)\n",
        "        return seq\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "    def __getitem__(self, i):\n",
        "        data = np.array(self.data[i][0])  # ❶ 입력 데이터\n",
        "        label = np.array(self.data[i][1]).astype(np.float32)  # ❷ 출력 데이터\n",
        "\n",
        "        return data, label"
      ],
      "metadata": {
        "id": "rGzibkUt2Hxn"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "LSTM 모델 정의<br>\n",
        "![](https://drive.google.com/uc?id=1CHLvEcJBBaxhvw08oRyHaPuZZJ50IhUA)\n"
      ],
      "metadata": {
        "id": "Tm_2SPLl8NSO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "class LSTM(nn.Module):\n",
        "  def __init__(self, num_embeddings):\n",
        "    super(LSTM, self).__init__()\n",
        "\n",
        "    # 밀집표현을 위한 임베딩 층\n",
        "    self.embed = nn.Embedding(\n",
        "        num_embeddings = num_embeddings, embedding_dim = 16\n",
        "    )\n",
        "\n",
        "    # LSTM을 5개 층으로 쌓음\n",
        "    self.lstm = nn.LSTM(\n",
        "        input_size = 16,\n",
        "        hidden_size = 64,\n",
        "        num_layers = 5,\n",
        "        batch_first = True\n",
        "    )\n",
        "\n",
        "    # 분류를 위한 MLP층\n",
        "    self.fc1 = nn.Linear(128, num_embeddings)\n",
        "    self.fc2 = nn.Linear(num_embeddings,num_embeddings)\n",
        "    # 활성화함수\n",
        "    self.relu = nn.ReLU()\n",
        "\n",
        "  def forward(self, x):\n",
        "      x = self.embed(x)\n",
        "\n",
        "      # LSTM 모델의 예측값\n",
        "      x, _ = self.lstm(x)\n",
        "      x = torch.reshape(x, (x.shape[0], -1))\n",
        "      x = self.fc1(x)\n",
        "      x = self.relu(x)\n",
        "      x = self.fc2(x)\n",
        "\n",
        "      return x"
      ],
      "metadata": {
        "id": "nbmtBDhS8IbV"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tqdm\n",
        "from torch.utils.data.dataloader import DataLoader\n",
        "from torch.optim.adam import Adam\n",
        "import torch\n",
        "\n",
        "# 학습을 진행할 프로세서 정의\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "dataset = TextGeneration() # 데이터셋 정의\n",
        "model = LSTM(num_embeddings=len(dataset.BOW)).to(device) # 모델정의\n",
        "loader = DataLoader(dataset, batch_size=64)\n",
        "optim = Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "for epoch in range(200):\n",
        "  iterator = tqdm.tqdm(loader)\n",
        "  for data, label in iterator:\n",
        "    # 기울기 초기화\n",
        "    optim.zero_grad()\n",
        "    # 모델의 예측값\n",
        "    pred = model(torch.tensor(data, dtype=torch.long).to(device))\n",
        "    # 정답레이블은 long텐서로반환필요ㅕ\n",
        "    loss = nn.CrossEntropyLoss()(\n",
        "        pred, torch.tensor(label,dtype=torch.long).to(device))\n",
        "\n",
        "    # 오차역전파\n",
        "    loss.backward()\n",
        "    optim.step()\n",
        "    iterator.set_description(f'epoch{epoch} loss: {loss.item()}')\n",
        "torch.save(model.state_dict(), 'lstm.pth')"
      ],
      "metadata": {
        "id": "r_kTyxgp9coh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "67b9a513-151e-4ded-a134-9f5ef70b4efb"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/104 [00:00<?, ?it/s]<ipython-input-5-77141e9f149f>:20: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  pred =model(torch.tensor(data, dtype=torch.long).to(device))\n",
            "<ipython-input-5-77141e9f149f>:23: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  pred, torch.tensor(label,dtype=torch.long).to(device))\n",
            "epoch0 loss: 7.4215617179870605: 100%|██████████| 104/104 [00:02<00:00, 49.21it/s]\n",
            "epoch1 loss: 6.937582492828369: 100%|██████████| 104/104 [00:00<00:00, 119.61it/s]\n",
            "epoch2 loss: 6.2258195877075195: 100%|██████████| 104/104 [00:00<00:00, 122.64it/s]\n",
            "epoch3 loss: 5.8591108322143555: 100%|██████████| 104/104 [00:00<00:00, 122.64it/s]\n",
            "epoch4 loss: 5.5691680908203125: 100%|██████████| 104/104 [00:00<00:00, 117.60it/s]\n",
            "epoch5 loss: 5.846491813659668: 100%|██████████| 104/104 [00:00<00:00, 122.71it/s]\n",
            "epoch6 loss: 5.572712421417236: 100%|██████████| 104/104 [00:00<00:00, 117.64it/s]\n",
            "epoch7 loss: 5.594508647918701: 100%|██████████| 104/104 [00:00<00:00, 113.33it/s]\n",
            "epoch8 loss: 5.73337984085083: 100%|██████████| 104/104 [00:00<00:00, 113.97it/s]\n",
            "epoch9 loss: 5.402045726776123: 100%|██████████| 104/104 [00:01<00:00, 86.38it/s]\n",
            "epoch10 loss: 5.920941352844238: 100%|██████████| 104/104 [00:00<00:00, 109.01it/s]\n",
            "epoch11 loss: 5.919926166534424: 100%|██████████| 104/104 [00:01<00:00, 101.40it/s]\n",
            "epoch12 loss: 6.0732951164245605: 100%|██████████| 104/104 [00:00<00:00, 104.39it/s]\n",
            "epoch13 loss: 6.718099594116211: 100%|██████████| 104/104 [00:01<00:00, 98.35it/s]\n",
            "epoch14 loss: 6.516963005065918: 100%|██████████| 104/104 [00:01<00:00, 93.23it/s]\n",
            "epoch15 loss: 6.492096424102783: 100%|██████████| 104/104 [00:00<00:00, 109.14it/s]\n",
            "epoch16 loss: 6.432387828826904: 100%|██████████| 104/104 [00:00<00:00, 117.80it/s]\n",
            "epoch17 loss: 6.375779628753662: 100%|██████████| 104/104 [00:00<00:00, 118.30it/s]\n",
            "epoch18 loss: 6.3109941482543945: 100%|██████████| 104/104 [00:00<00:00, 120.47it/s]\n",
            "epoch19 loss: 6.260670185089111: 100%|██████████| 104/104 [00:00<00:00, 118.35it/s]\n",
            "epoch20 loss: 6.197169780731201: 100%|██████████| 104/104 [00:00<00:00, 114.47it/s]\n",
            "epoch21 loss: 6.1428422927856445: 100%|██████████| 104/104 [00:00<00:00, 117.23it/s]\n",
            "epoch22 loss: 6.064089775085449: 100%|██████████| 104/104 [00:00<00:00, 121.76it/s]\n",
            "epoch23 loss: 5.990910053253174: 100%|██████████| 104/104 [00:00<00:00, 122.28it/s]\n",
            "epoch24 loss: 5.915824890136719: 100%|██████████| 104/104 [00:00<00:00, 123.14it/s]\n",
            "epoch25 loss: 5.898956775665283: 100%|██████████| 104/104 [00:00<00:00, 114.92it/s]\n",
            "epoch26 loss: 5.8815999031066895: 100%|██████████| 104/104 [00:01<00:00, 102.75it/s]\n",
            "epoch27 loss: 5.849090576171875: 100%|██████████| 104/104 [00:00<00:00, 105.09it/s]\n",
            "epoch28 loss: 5.862652778625488: 100%|██████████| 104/104 [00:01<00:00, 103.22it/s]\n",
            "epoch29 loss: 5.80088996887207: 100%|██████████| 104/104 [00:01<00:00, 101.58it/s]\n",
            "epoch30 loss: 5.745202541351318: 100%|██████████| 104/104 [00:00<00:00, 104.78it/s]\n",
            "epoch31 loss: 5.7270965576171875: 100%|██████████| 104/104 [00:00<00:00, 109.94it/s]\n",
            "epoch32 loss: 5.679845809936523: 100%|██████████| 104/104 [00:00<00:00, 117.36it/s]\n",
            "epoch33 loss: 5.527682304382324: 100%|██████████| 104/104 [00:00<00:00, 115.80it/s]\n",
            "epoch34 loss: 5.50202751159668: 100%|██████████| 104/104 [00:00<00:00, 117.21it/s]\n",
            "epoch35 loss: 5.424776554107666: 100%|██████████| 104/104 [00:00<00:00, 119.14it/s]\n",
            "epoch36 loss: 5.405639171600342: 100%|██████████| 104/104 [00:00<00:00, 116.30it/s]\n",
            "epoch37 loss: 5.434586048126221: 100%|██████████| 104/104 [00:00<00:00, 116.37it/s]\n",
            "epoch38 loss: 5.29288911819458: 100%|██████████| 104/104 [00:00<00:00, 115.11it/s]\n",
            "epoch39 loss: 5.166983127593994: 100%|██████████| 104/104 [00:00<00:00, 121.33it/s]\n",
            "epoch40 loss: 4.979467391967773: 100%|██████████| 104/104 [00:00<00:00, 120.84it/s]\n",
            "epoch41 loss: 4.913801670074463: 100%|██████████| 104/104 [00:00<00:00, 121.09it/s]\n",
            "epoch42 loss: 4.850804805755615: 100%|██████████| 104/104 [00:00<00:00, 108.71it/s]\n",
            "epoch43 loss: 4.650565147399902: 100%|██████████| 104/104 [00:00<00:00, 114.61it/s]\n",
            "epoch44 loss: 4.536182403564453: 100%|██████████| 104/104 [00:00<00:00, 105.18it/s]\n",
            "epoch45 loss: 4.45197057723999: 100%|██████████| 104/104 [00:00<00:00, 104.33it/s]\n",
            "epoch46 loss: 4.320582866668701: 100%|██████████| 104/104 [00:00<00:00, 107.00it/s]\n",
            "epoch47 loss: 4.443624019622803: 100%|██████████| 104/104 [00:00<00:00, 109.53it/s]\n",
            "epoch48 loss: 4.172617435455322: 100%|██████████| 104/104 [00:00<00:00, 119.87it/s]\n",
            "epoch49 loss: 4.08314323425293: 100%|██████████| 104/104 [00:00<00:00, 117.28it/s]\n",
            "epoch50 loss: 4.0409345626831055: 100%|██████████| 104/104 [00:00<00:00, 113.57it/s]\n",
            "epoch51 loss: 4.323919773101807: 100%|██████████| 104/104 [00:00<00:00, 119.51it/s]\n",
            "epoch52 loss: 4.372236251831055: 100%|██████████| 104/104 [00:00<00:00, 118.29it/s]\n",
            "epoch53 loss: 4.252533912658691: 100%|██████████| 104/104 [00:00<00:00, 114.65it/s]\n",
            "epoch54 loss: 4.365299701690674: 100%|██████████| 104/104 [00:00<00:00, 120.05it/s]\n",
            "epoch55 loss: 4.435546398162842: 100%|██████████| 104/104 [00:00<00:00, 117.93it/s]\n",
            "epoch56 loss: 4.5167388916015625: 100%|██████████| 104/104 [00:00<00:00, 116.94it/s]\n",
            "epoch57 loss: 4.407713890075684: 100%|██████████| 104/104 [00:00<00:00, 117.59it/s]\n",
            "epoch58 loss: 4.295466899871826: 100%|██████████| 104/104 [00:00<00:00, 115.98it/s]\n",
            "epoch59 loss: 4.114965438842773: 100%|██████████| 104/104 [00:00<00:00, 110.02it/s]\n",
            "epoch60 loss: 4.068853378295898: 100%|██████████| 104/104 [00:00<00:00, 114.42it/s]\n",
            "epoch61 loss: 3.9888510704040527: 100%|██████████| 104/104 [00:00<00:00, 107.29it/s]\n",
            "epoch62 loss: 3.818411350250244: 100%|██████████| 104/104 [00:01<00:00, 69.93it/s]\n",
            "epoch63 loss: 3.7674689292907715: 100%|██████████| 104/104 [00:01<00:00, 103.18it/s]\n",
            "epoch64 loss: 3.6615450382232666: 100%|██████████| 104/104 [00:00<00:00, 104.27it/s]\n",
            "epoch65 loss: 3.565065860748291: 100%|██████████| 104/104 [00:00<00:00, 116.18it/s]\n",
            "epoch66 loss: 3.5172102451324463: 100%|██████████| 104/104 [00:00<00:00, 112.36it/s]\n",
            "epoch67 loss: 3.542896032333374: 100%|██████████| 104/104 [00:00<00:00, 113.76it/s]\n",
            "epoch68 loss: 3.5002896785736084: 100%|██████████| 104/104 [00:00<00:00, 113.52it/s]\n",
            "epoch69 loss: 3.383939266204834: 100%|██████████| 104/104 [00:00<00:00, 114.47it/s]\n",
            "epoch70 loss: 3.4065122604370117: 100%|██████████| 104/104 [00:00<00:00, 115.49it/s]\n",
            "epoch71 loss: 3.3589398860931396: 100%|██████████| 104/104 [00:00<00:00, 116.35it/s]\n",
            "epoch72 loss: 3.2720601558685303: 100%|██████████| 104/104 [00:00<00:00, 117.88it/s]\n",
            "epoch73 loss: 3.498626470565796: 100%|██████████| 104/104 [00:00<00:00, 114.63it/s]\n",
            "epoch74 loss: 3.5160446166992188: 100%|██████████| 104/104 [00:00<00:00, 117.71it/s]\n",
            "epoch75 loss: 3.349963426589966: 100%|██████████| 104/104 [00:00<00:00, 113.33it/s]\n",
            "epoch76 loss: 3.347987174987793: 100%|██████████| 104/104 [00:00<00:00, 104.38it/s]\n",
            "epoch77 loss: 3.3301122188568115: 100%|██████████| 104/104 [00:01<00:00, 99.06it/s]\n",
            "epoch78 loss: 3.243244171142578: 100%|██████████| 104/104 [00:00<00:00, 105.36it/s]\n",
            "epoch79 loss: 3.045488119125366: 100%|██████████| 104/104 [00:01<00:00, 68.15it/s]\n",
            "epoch80 loss: 2.975626230239868: 100%|██████████| 104/104 [00:00<00:00, 105.33it/s]\n",
            "epoch81 loss: 2.990476131439209: 100%|██████████| 104/104 [00:01<00:00, 101.33it/s]\n",
            "epoch82 loss: 2.8625409603118896: 100%|██████████| 104/104 [00:00<00:00, 109.24it/s]\n",
            "epoch83 loss: 2.838844060897827: 100%|██████████| 104/104 [00:00<00:00, 109.36it/s]\n",
            "epoch84 loss: 2.7770705223083496: 100%|██████████| 104/104 [00:01<00:00, 97.60it/s]\n",
            "epoch85 loss: 2.775161027908325: 100%|██████████| 104/104 [00:01<00:00, 92.26it/s]\n",
            "epoch86 loss: 2.856140375137329: 100%|██████████| 104/104 [00:00<00:00, 104.54it/s]\n",
            "epoch87 loss: 2.6911520957946777: 100%|██████████| 104/104 [00:00<00:00, 105.36it/s]\n",
            "epoch88 loss: 2.730297565460205: 100%|██████████| 104/104 [00:01<00:00, 94.66it/s]\n",
            "epoch89 loss: 2.660236358642578: 100%|██████████| 104/104 [00:01<00:00, 102.27it/s]\n",
            "epoch90 loss: 2.5936522483825684: 100%|██████████| 104/104 [00:01<00:00, 98.96it/s]\n",
            "epoch91 loss: 2.4975016117095947: 100%|██████████| 104/104 [00:00<00:00, 104.26it/s]\n",
            "epoch92 loss: 2.5178275108337402: 100%|██████████| 104/104 [00:01<00:00, 95.04it/s]\n",
            "epoch93 loss: 2.574615955352783: 100%|██████████| 104/104 [00:01<00:00, 93.59it/s]\n",
            "epoch94 loss: 2.548133373260498: 100%|██████████| 104/104 [00:01<00:00, 101.31it/s]\n",
            "epoch95 loss: 2.6066770553588867: 100%|██████████| 104/104 [00:00<00:00, 111.74it/s]\n",
            "epoch96 loss: 2.7541210651397705: 100%|██████████| 104/104 [00:01<00:00, 76.73it/s]\n",
            "epoch97 loss: 2.6125402450561523: 100%|██████████| 104/104 [00:00<00:00, 115.73it/s]\n",
            "epoch98 loss: 2.606097459793091: 100%|██████████| 104/104 [00:00<00:00, 109.47it/s]\n",
            "epoch99 loss: 2.5479791164398193: 100%|██████████| 104/104 [00:00<00:00, 111.54it/s]\n",
            "epoch100 loss: 2.691166400909424: 100%|██████████| 104/104 [00:00<00:00, 115.62it/s]\n",
            "epoch101 loss: 2.5739500522613525: 100%|██████████| 104/104 [00:00<00:00, 115.65it/s]\n",
            "epoch102 loss: 2.7321102619171143: 100%|██████████| 104/104 [00:00<00:00, 111.91it/s]\n",
            "epoch103 loss: 2.643519639968872: 100%|██████████| 104/104 [00:00<00:00, 111.39it/s]\n",
            "epoch104 loss: 2.80410099029541: 100%|██████████| 104/104 [00:00<00:00, 109.25it/s]\n",
            "epoch105 loss: 2.690488576889038: 100%|██████████| 104/104 [00:00<00:00, 106.07it/s]\n",
            "epoch106 loss: 2.3779070377349854: 100%|██████████| 104/104 [00:00<00:00, 104.66it/s]\n",
            "epoch107 loss: 2.3687071800231934: 100%|██████████| 104/104 [00:01<00:00, 95.48it/s]\n",
            "epoch108 loss: 2.650507926940918: 100%|██████████| 104/104 [00:00<00:00, 104.77it/s]\n",
            "epoch109 loss: 1.920021891593933: 100%|██████████| 104/104 [00:00<00:00, 110.79it/s]\n",
            "epoch110 loss: 1.9970368146896362: 100%|██████████| 104/104 [00:01<00:00, 96.80it/s]\n",
            "epoch111 loss: 2.275099039077759: 100%|██████████| 104/104 [00:01<00:00, 96.57it/s]\n",
            "epoch112 loss: 2.232494592666626: 100%|██████████| 104/104 [00:01<00:00, 102.71it/s]\n",
            "epoch113 loss: 2.2513129711151123: 100%|██████████| 104/104 [00:01<00:00, 99.61it/s]\n",
            "epoch114 loss: 2.1479547023773193: 100%|██████████| 104/104 [00:01<00:00, 99.08it/s]\n",
            "epoch115 loss: 1.9414870738983154: 100%|██████████| 104/104 [00:00<00:00, 109.03it/s]\n",
            "epoch116 loss: 1.941760540008545: 100%|██████████| 104/104 [00:00<00:00, 108.15it/s]\n",
            "epoch117 loss: 1.9807721376419067: 100%|██████████| 104/104 [00:00<00:00, 112.47it/s]\n",
            "epoch118 loss: 2.041402816772461: 100%|██████████| 104/104 [00:00<00:00, 121.80it/s]\n",
            "epoch119 loss: 1.5831801891326904: 100%|██████████| 104/104 [00:00<00:00, 115.79it/s]\n",
            "epoch120 loss: 1.5448741912841797: 100%|██████████| 104/104 [00:00<00:00, 117.45it/s]\n",
            "epoch121 loss: 1.2601432800292969: 100%|██████████| 104/104 [00:00<00:00, 117.27it/s]\n",
            "epoch122 loss: 1.2938017845153809: 100%|██████████| 104/104 [00:00<00:00, 114.47it/s]\n",
            "epoch123 loss: 1.3588074445724487: 100%|██████████| 104/104 [00:00<00:00, 115.47it/s]\n",
            "epoch124 loss: 1.524742603302002: 100%|██████████| 104/104 [00:00<00:00, 118.01it/s]\n",
            "epoch125 loss: 1.6903644800186157: 100%|██████████| 104/104 [00:00<00:00, 108.46it/s]\n",
            "epoch126 loss: 1.5127899646759033: 100%|██████████| 104/104 [00:00<00:00, 113.08it/s]\n",
            "epoch127 loss: 1.4242812395095825: 100%|██████████| 104/104 [00:00<00:00, 104.93it/s]\n",
            "epoch128 loss: 1.29500150680542: 100%|██████████| 104/104 [00:01<00:00, 103.02it/s]\n",
            "epoch129 loss: 1.3451619148254395: 100%|██████████| 104/104 [00:00<00:00, 108.10it/s]\n",
            "epoch130 loss: 1.423364520072937: 100%|██████████| 104/104 [00:00<00:00, 113.57it/s]\n",
            "epoch131 loss: 1.4685680866241455: 100%|██████████| 104/104 [00:00<00:00, 110.01it/s]\n",
            "epoch132 loss: 1.7804561853408813: 100%|██████████| 104/104 [00:00<00:00, 110.52it/s]\n",
            "epoch133 loss: 1.9337973594665527: 100%|██████████| 104/104 [00:00<00:00, 113.10it/s]\n",
            "epoch134 loss: 1.2631630897521973: 100%|██████████| 104/104 [00:00<00:00, 114.59it/s]\n",
            "epoch135 loss: 0.9054529070854187: 100%|██████████| 104/104 [00:00<00:00, 117.13it/s]\n",
            "epoch136 loss: 1.1623657941818237: 100%|██████████| 104/104 [00:00<00:00, 114.66it/s]\n",
            "epoch137 loss: 1.4134222269058228: 100%|██████████| 104/104 [00:00<00:00, 112.21it/s]\n",
            "epoch138 loss: 0.8177657723426819: 100%|██████████| 104/104 [00:00<00:00, 113.71it/s]\n",
            "epoch139 loss: 1.0377246141433716: 100%|██████████| 104/104 [00:00<00:00, 117.87it/s]\n",
            "epoch140 loss: 1.49741792678833: 100%|██████████| 104/104 [00:00<00:00, 115.26it/s]\n",
            "epoch141 loss: 0.6786180138587952: 100%|██████████| 104/104 [00:00<00:00, 114.06it/s]\n",
            "epoch142 loss: 0.5079020857810974: 100%|██████████| 104/104 [00:00<00:00, 118.09it/s]\n",
            "epoch143 loss: 0.5417585968971252: 100%|██████████| 104/104 [00:00<00:00, 117.82it/s]\n",
            "epoch144 loss: 0.4444586932659149: 100%|██████████| 104/104 [00:00<00:00, 109.95it/s]\n",
            "epoch145 loss: 0.4066001772880554: 100%|██████████| 104/104 [00:00<00:00, 107.91it/s]\n",
            "epoch146 loss: 0.4661453366279602: 100%|██████████| 104/104 [00:00<00:00, 109.52it/s]\n",
            "epoch147 loss: 0.4973793923854828: 100%|██████████| 104/104 [00:01<00:00, 103.94it/s]\n",
            "epoch148 loss: 0.3655834197998047: 100%|██████████| 104/104 [00:01<00:00, 103.45it/s]\n",
            "epoch149 loss: 0.35289111733436584: 100%|██████████| 104/104 [00:01<00:00, 102.16it/s]\n",
            "epoch150 loss: 0.42421531677246094: 100%|██████████| 104/104 [00:01<00:00, 103.23it/s]\n",
            "epoch151 loss: 0.36861446499824524: 100%|██████████| 104/104 [00:01<00:00, 103.14it/s]\n",
            "epoch152 loss: 0.2822594940662384: 100%|██████████| 104/104 [00:00<00:00, 116.23it/s]\n",
            "epoch153 loss: 0.33030787110328674: 100%|██████████| 104/104 [00:00<00:00, 112.48it/s]\n",
            "epoch154 loss: 0.4613282382488251: 100%|██████████| 104/104 [00:00<00:00, 117.16it/s]\n",
            "epoch155 loss: 0.44369688630104065: 100%|██████████| 104/104 [00:00<00:00, 115.33it/s]\n",
            "epoch156 loss: 0.293830543756485: 100%|██████████| 104/104 [00:00<00:00, 115.07it/s]\n",
            "epoch157 loss: 0.29602426290512085: 100%|██████████| 104/104 [00:00<00:00, 111.43it/s]\n",
            "epoch158 loss: 0.3308158814907074: 100%|██████████| 104/104 [00:00<00:00, 114.26it/s]\n",
            "epoch159 loss: 0.2639673054218292: 100%|██████████| 104/104 [00:00<00:00, 119.42it/s]\n",
            "epoch160 loss: 0.4499535858631134: 100%|██████████| 104/104 [00:00<00:00, 114.09it/s]\n",
            "epoch161 loss: 0.7396833896636963: 100%|██████████| 104/104 [00:00<00:00, 113.09it/s]\n",
            "epoch162 loss: 0.5039339661598206: 100%|██████████| 104/104 [00:00<00:00, 112.54it/s]\n",
            "epoch163 loss: 0.36172616481781006: 100%|██████████| 104/104 [00:00<00:00, 116.21it/s]\n",
            "epoch164 loss: 0.25961825251579285: 100%|██████████| 104/104 [00:00<00:00, 115.20it/s]\n",
            "epoch165 loss: 0.3116806149482727: 100%|██████████| 104/104 [00:00<00:00, 120.20it/s]\n",
            "epoch166 loss: 0.16542424261569977: 100%|██████████| 104/104 [00:00<00:00, 107.17it/s]\n",
            "epoch167 loss: 0.1702953726053238: 100%|██████████| 104/104 [00:00<00:00, 112.95it/s]\n",
            "epoch168 loss: 0.19807228446006775: 100%|██████████| 104/104 [00:00<00:00, 106.68it/s]\n",
            "epoch169 loss: 0.3318645656108856: 100%|██████████| 104/104 [00:00<00:00, 110.32it/s]\n",
            "epoch170 loss: 0.29430586099624634: 100%|██████████| 104/104 [00:01<00:00, 101.44it/s]\n",
            "epoch171 loss: 0.7203240394592285: 100%|██████████| 104/104 [00:00<00:00, 119.85it/s]\n",
            "epoch172 loss: 0.24873977899551392: 100%|██████████| 104/104 [00:00<00:00, 118.86it/s]\n",
            "epoch173 loss: 0.270549476146698: 100%|██████████| 104/104 [00:00<00:00, 117.03it/s]\n",
            "epoch174 loss: 0.246781125664711: 100%|██████████| 104/104 [00:00<00:00, 117.87it/s]\n",
            "epoch175 loss: 0.19837108254432678: 100%|██████████| 104/104 [00:00<00:00, 113.47it/s]\n",
            "epoch176 loss: 0.1982080340385437: 100%|██████████| 104/104 [00:00<00:00, 113.84it/s]\n",
            "epoch177 loss: 0.1492403745651245: 100%|██████████| 104/104 [00:00<00:00, 115.01it/s]\n",
            "epoch178 loss: 0.24713851511478424: 100%|██████████| 104/104 [00:00<00:00, 116.45it/s]\n",
            "epoch179 loss: 0.2608035206794739: 100%|██████████| 104/104 [00:00<00:00, 117.95it/s]\n",
            "epoch180 loss: 0.1884046494960785: 100%|██████████| 104/104 [00:00<00:00, 112.10it/s]\n",
            "epoch181 loss: 0.0755079910159111: 100%|██████████| 104/104 [00:00<00:00, 113.44it/s]\n",
            "epoch182 loss: 0.16199640929698944: 100%|██████████| 104/104 [00:01<00:00, 98.82it/s]\n",
            "epoch183 loss: 0.4183712899684906: 100%|██████████| 104/104 [00:00<00:00, 111.43it/s]\n",
            "epoch184 loss: 0.32595908641815186: 100%|██████████| 104/104 [00:00<00:00, 108.93it/s]\n",
            "epoch185 loss: 0.12313451617956161: 100%|██████████| 104/104 [00:00<00:00, 105.93it/s]\n",
            "epoch186 loss: 0.18832950294017792: 100%|██████████| 104/104 [00:00<00:00, 108.11it/s]\n",
            "epoch187 loss: 0.7269530296325684: 100%|██████████| 104/104 [00:00<00:00, 107.36it/s]\n",
            "epoch188 loss: 0.4596260190010071: 100%|██████████| 104/104 [00:00<00:00, 109.02it/s]\n",
            "epoch189 loss: 1.0063257217407227: 100%|██████████| 104/104 [00:01<00:00, 97.35it/s]\n",
            "epoch190 loss: 0.06266025453805923: 100%|██████████| 104/104 [00:00<00:00, 108.91it/s]\n",
            "epoch191 loss: 0.036743052303791046: 100%|██████████| 104/104 [00:00<00:00, 114.58it/s]\n",
            "epoch192 loss: 0.07557140290737152: 100%|██████████| 104/104 [00:00<00:00, 108.59it/s]\n",
            "epoch193 loss: 0.05866124480962753: 100%|██████████| 104/104 [00:00<00:00, 114.96it/s]\n",
            "epoch194 loss: 0.18136362731456757: 100%|██████████| 104/104 [00:00<00:00, 114.43it/s]\n",
            "epoch195 loss: 0.5875227451324463: 100%|██████████| 104/104 [00:00<00:00, 117.00it/s]\n",
            "epoch196 loss: 0.17209464311599731: 100%|██████████| 104/104 [00:00<00:00, 110.50it/s]\n",
            "epoch197 loss: 0.3794628977775574: 100%|██████████| 104/104 [00:00<00:00, 118.71it/s]\n",
            "epoch198 loss: 0.31886205077171326: 100%|██████████| 104/104 [00:00<00:00, 116.97it/s]\n",
            "epoch199 loss: 0.22730398178100586: 100%|██████████| 104/104 [00:00<00:00, 116.16it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def generate(model, BOW, string=\"finding an \", strlen=10):\n",
        "   device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "   print(f\"input word: {string}\")\n",
        "\n",
        "   with torch.no_grad():\n",
        "       for p in range(strlen):\n",
        "           # 입력 문장을 텐서로 변경\n",
        "           words = torch.tensor(\n",
        "               [BOW[w] for w in string.split()], dtype=torch.long).to(device)\n",
        "\n",
        "           #\n",
        "           input_tensor = torch.unsqueeze(words[-2:], dim=0)\n",
        "           output = model(input_tensor)  # 모델을 이용해 예측\n",
        "           output_word = (torch.argmax(output).cpu().numpy())\n",
        "           string += list(BOW.keys())[output_word]  # 문장에 예측된 단어를 추가\n",
        "           string += \" \"\n",
        "\n",
        "   print(f\"predicted sentence: {string}\")\n",
        "\n",
        "model.load_state_dict(torch.load(\"lstm.pth\", map_location=device))\n",
        "pred = generate(model, dataset.BOW)"
      ],
      "metadata": {
        "id": "3mbRbM55B5D5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "62c3dea5-d226-4779-d819-14db61cd5016"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "input word: finding an \n",
            "predicted sentence: finding an york france at award on gets webs be attacker ready \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "generate(model, dataset.BOW, string = \"a church \")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PLWrF5Fvw6Eh",
        "outputId": "dc24383c-5c35-4f7d-d6f7-a6e5f310f761"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "input word: a church \n",
            "predicted sentence: a church immigrants sprint a new spider family tree tries to untangle \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "generate(model, dataset.BOW, string = \"girl school \")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oerkJTWUvttn",
        "outputId": "f3740354-74d4-4ddd-a63a-30c90806b460"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "input word: girl school \n",
            "predicted sentence: girl school says he was too rude’ more can trump’s lawman ledge \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cp_WKUIfvuob"
      },
      "execution_count": 8,
      "outputs": []
    }
  ]
}