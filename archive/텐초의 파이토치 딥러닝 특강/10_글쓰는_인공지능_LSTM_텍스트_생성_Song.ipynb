{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1301a4d4",
   "metadata": {},
   "source": [
    "# 데이터 살펴보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d8f7ab9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['abstract', 'articleID', 'articleWordCount', 'byline', 'documentType',\n",
       "       'headline', 'keywords', 'multimedia', 'newDesk', 'printPage', 'pubDate',\n",
       "       'sectionName', 'snippet', 'source', 'typeOfMaterial', 'webURL'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "data_path = '/home/restful3/workspaces/study/ds4th_study/source/텐초의 파이토치 딥러닝 특강/datasets/CH10/'\n",
    "\n",
    "df = pd.read_csv(data_path + 'ArticlesApril2017.csv')\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4be1b50",
   "metadata": {},
   "source": [
    "# 학습용 데이터 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7d9f707c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import glob\n",
    "import string\n",
    "\n",
    "from torch.utils.data.dataset import Dataset\n",
    "\n",
    "class TextGeneration(Dataset):\n",
    "    def clean_text(self, txt):\n",
    "        txt = \"\".join(v for v in txt if v not in string.punctuation).lower()\n",
    "        return txt\n",
    "    \n",
    "    def __init__(self):\n",
    "        all_headlines = []\n",
    "        \n",
    "        for filename in glob.glob(data_path+'*.csv'):\n",
    "            if 'Articles' in filename:\n",
    "                article_df = pd.read_csv(filename)\n",
    "                all_headlines.extend(list(article_df.headline.values))\n",
    "#                 break\n",
    "                \n",
    "        all_headlines = [h for h in all_headlines if h!= 'Unknown']\n",
    "        \n",
    "        self.corpus = [self.clean_text(x) for x in all_headlines]\n",
    "        \n",
    "        self.BOW = {}\n",
    "        \n",
    "        for line in self.corpus:\n",
    "            for word in line.split():\n",
    "                if word not in self.BOW.keys():\n",
    "                    self.BOW[word] = len(self.BOW.keys())\n",
    "                    \n",
    "        self.data = self.generate_sequence(self.corpus)\n",
    "        \n",
    "    def generate_sequence(self, txt):\n",
    "        seq = []\n",
    "        \n",
    "        for line in txt:\n",
    "            line = line.split()\n",
    "            line_bow = [self.BOW[word] for word in line]\n",
    "            \n",
    "            data = [([line_bow[i], line_bow[i+1]], line_bow[i+2]) for i in range(len(line_bow)-2)]\n",
    "            \n",
    "            seq.extend(data)\n",
    "            \n",
    "        return seq\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self,i):\n",
    "        data = np.array(self.data[i][0])\n",
    "        label = np.array(self.data[i][1]).astype(np.float32)\n",
    "        \n",
    "        return data, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fb11862d",
   "metadata": {},
   "outputs": [],
   "source": [
    "txt = TextGeneration()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bb620832",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12148"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(txt.BOW)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b197492",
   "metadata": {},
   "source": [
    "# LSTM 모델 정의하기\n",
    "- num_embeddings: 이 매개변수는 임베딩 층에 총 몇 개의 임베딩 벡터가 있는지를 지정합니다. 일반적으로 이 값은 단어나 토큰의 총 개수와 동일합니다. 예를 들어, 텍스트 데이터에서 고유한 단어의 수가 10,000개라면 num_embeddings은 10,000이 될 것입니다.\n",
    "\n",
    "- embedding_dim: 이 매개변수는 임베딩 벡터의 차원을 지정합니다. 임베딩 벡터는 각각의 단어나 토큰을 나타내는 고정된 길이의 실수 벡터입니다. embedding_dim을 설정하면 임베딩 벡터의 차원이 결정됩니다. 일반적으로 이 값은 사전에 정의된 임베딩 차원의 크기로 설정됩니다. 예를 들어, embedding_dim을 300으로 설정하면 각각의 임베딩 벡터는 300차원의 실수 벡터가 됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e7576473",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class LSTM(nn.Module):\n",
    "    def __init__(self, num_embeddings):\n",
    "        super(LSTM, self).__init__()\n",
    "        \n",
    "        self.embed = nn.Embedding(num_embeddings=num_embeddings, embedding_dim=16)\n",
    "        \n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=16,\n",
    "            hidden_size=64,\n",
    "            num_layers=5,\n",
    "            batch_first=True\n",
    "        )\n",
    "        \n",
    "        self.fc1 = nn.Linear(128, num_embeddings)\n",
    "        self.fc2 = nn.Linear(num_embeddings, num_embeddings)\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.embed(x)\n",
    "        \n",
    "        x, _ = self.lstm(x)\n",
    "        x = torch.reshape(x, (x.shape[0], -1))\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d4422d1",
   "metadata": {},
   "source": [
    "# 학습하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ded4aba8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import tqdm\n",
    "\n",
    "import torch\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "from torch.optim.adam import Adam\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a9c715dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = TextGeneration()\n",
    "model = LSTM(num_embeddings=len(dataset.BOW)).to(device)\n",
    "loader = DataLoader(dataset, batch_size=64)\n",
    "optim = Adam(model.parameters(), lr=0.001)\n",
    "loss_fn = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "633ea77a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                   | 0/677 [00:00<?, ?it/s]/tmp/ipykernel_3399/3492746132.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  pred = model(torch.tensor(data, dtype=torch.long).to(device))\n",
      "/tmp/ipykernel_3399/3492746132.py:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  loss = loss_fn(pred, torch.tensor(label, dtype=torch.long).to(device))\n",
      "epoch 0, loss : 7.570168495178223: 100%|██████| 677/677 [00:15<00:00, 43.26it/s]\n",
      "epoch 1, loss : 7.290179252624512: 100%|██████| 677/677 [00:15<00:00, 43.70it/s]\n",
      "epoch 2, loss : 7.001972198486328: 100%|██████| 677/677 [00:15<00:00, 43.70it/s]\n",
      "epoch 3, loss : 6.691782474517822: 100%|██████| 677/677 [00:15<00:00, 43.63it/s]\n",
      "epoch 4, loss : 6.396479606628418: 100%|██████| 677/677 [00:15<00:00, 43.84it/s]\n",
      "epoch 5, loss : 6.211499214172363: 100%|██████| 677/677 [00:15<00:00, 43.99it/s]\n",
      "epoch 6, loss : 5.9702534675598145: 100%|█████| 677/677 [00:15<00:00, 43.83it/s]\n",
      "epoch 7, loss : 5.700667858123779: 100%|██████| 677/677 [00:15<00:00, 43.69it/s]\n",
      "epoch 8, loss : 5.5047807693481445: 100%|█████| 677/677 [00:15<00:00, 43.76it/s]\n",
      "epoch 9, loss : 5.298403263092041: 100%|██████| 677/677 [00:15<00:00, 43.82it/s]\n",
      "epoch 10, loss : 5.163243293762207: 100%|█████| 677/677 [00:15<00:00, 43.71it/s]\n",
      "epoch 11, loss : 5.155560493469238: 100%|█████| 677/677 [00:15<00:00, 43.82it/s]\n",
      "epoch 12, loss : 5.0775322914123535: 100%|████| 677/677 [00:15<00:00, 43.88it/s]\n",
      "epoch 13, loss : 4.87265682220459: 100%|██████| 677/677 [00:15<00:00, 43.80it/s]\n",
      "epoch 14, loss : 4.562154293060303: 100%|█████| 677/677 [00:15<00:00, 43.69it/s]\n",
      "epoch 15, loss : 4.336092472076416: 100%|█████| 677/677 [00:15<00:00, 43.27it/s]\n",
      "epoch 16, loss : 4.104661464691162: 100%|█████| 677/677 [00:15<00:00, 43.27it/s]\n",
      "epoch 17, loss : 4.0196075439453125: 100%|████| 677/677 [00:15<00:00, 43.20it/s]\n",
      "epoch 18, loss : 3.830610752105713: 100%|█████| 677/677 [00:15<00:00, 43.27it/s]\n",
      "epoch 19, loss : 3.7652747631073: 100%|███████| 677/677 [00:15<00:00, 43.28it/s]\n",
      "epoch 20, loss : 3.6436524391174316: 100%|████| 677/677 [00:15<00:00, 43.20it/s]\n",
      "epoch 21, loss : 3.671555757522583: 100%|█████| 677/677 [00:15<00:00, 43.33it/s]\n",
      "epoch 22, loss : 3.7505226135253906: 100%|████| 677/677 [00:15<00:00, 43.27it/s]\n",
      "epoch 23, loss : 3.6912529468536377: 100%|████| 677/677 [00:15<00:00, 43.07it/s]\n",
      "epoch 24, loss : 3.5849850177764893: 100%|████| 677/677 [00:15<00:00, 43.06it/s]\n",
      "epoch 25, loss : 3.5602188110351562: 100%|████| 677/677 [00:15<00:00, 43.11it/s]\n",
      "epoch 26, loss : 3.297079086303711: 100%|█████| 677/677 [00:15<00:00, 43.33it/s]\n",
      "epoch 27, loss : 3.3256378173828125: 100%|████| 677/677 [00:16<00:00, 42.20it/s]\n",
      "epoch 28, loss : 3.1524405479431152: 100%|████| 677/677 [00:15<00:00, 43.75it/s]\n",
      "epoch 29, loss : 3.076266288757324: 100%|█████| 677/677 [00:15<00:00, 43.93it/s]\n",
      "epoch 30, loss : 3.1164238452911377: 100%|████| 677/677 [00:15<00:00, 43.89it/s]\n",
      "epoch 31, loss : 3.1955370903015137: 100%|████| 677/677 [00:15<00:00, 43.94it/s]\n",
      "epoch 32, loss : 3.196369171142578: 100%|█████| 677/677 [00:15<00:00, 43.65it/s]\n",
      "epoch 33, loss : 3.0917487144470215: 100%|████| 677/677 [00:15<00:00, 43.53it/s]\n",
      "epoch 34, loss : 3.1156046390533447: 100%|████| 677/677 [00:15<00:00, 43.69it/s]\n",
      "epoch 35, loss : 2.973822832107544: 100%|█████| 677/677 [00:15<00:00, 43.92it/s]\n",
      "epoch 36, loss : 3.0146844387054443: 100%|████| 677/677 [00:15<00:00, 43.90it/s]\n",
      "epoch 37, loss : 2.8781070709228516: 100%|████| 677/677 [00:15<00:00, 43.72it/s]\n",
      "epoch 38, loss : 2.7097818851470947: 100%|████| 677/677 [00:15<00:00, 43.80it/s]\n",
      "epoch 39, loss : 2.54726243019104: 100%|██████| 677/677 [00:15<00:00, 43.58it/s]\n",
      "epoch 40, loss : 2.4463162422180176: 100%|████| 677/677 [00:15<00:00, 43.59it/s]\n",
      "epoch 41, loss : 2.578150510787964: 100%|█████| 677/677 [00:15<00:00, 43.26it/s]\n",
      "epoch 42, loss : 2.3570146560668945: 100%|████| 677/677 [00:15<00:00, 43.25it/s]\n",
      "epoch 43, loss : 2.253589630126953: 100%|█████| 677/677 [00:15<00:00, 43.31it/s]\n",
      "epoch 44, loss : 2.3176653385162354: 100%|████| 677/677 [00:15<00:00, 43.29it/s]\n",
      "epoch 45, loss : 2.2653815746307373: 100%|████| 677/677 [00:15<00:00, 43.41it/s]\n",
      "epoch 46, loss : 2.2487449645996094: 100%|████| 677/677 [00:15<00:00, 43.68it/s]\n",
      "epoch 47, loss : 2.2511439323425293: 100%|████| 677/677 [00:15<00:00, 43.75it/s]\n",
      "epoch 48, loss : 2.2999281883239746: 100%|████| 677/677 [00:15<00:00, 43.72it/s]\n",
      "epoch 49, loss : 2.0866899490356445: 100%|████| 677/677 [00:15<00:00, 43.77it/s]\n",
      "epoch 50, loss : 2.224658489227295: 100%|█████| 677/677 [00:15<00:00, 43.80it/s]\n",
      "epoch 51, loss : 2.2185215950012207: 100%|████| 677/677 [00:15<00:00, 43.84it/s]\n",
      "epoch 52, loss : 2.130887508392334: 100%|█████| 677/677 [00:15<00:00, 43.64it/s]\n",
      "epoch 53, loss : 2.054297685623169: 100%|█████| 677/677 [00:15<00:00, 43.62it/s]\n",
      "epoch 54, loss : 2.0985653400421143: 100%|████| 677/677 [00:15<00:00, 43.62it/s]\n",
      "epoch 55, loss : 2.0621724128723145: 100%|████| 677/677 [00:15<00:00, 43.73it/s]\n",
      "epoch 56, loss : 2.0089364051818848: 100%|████| 677/677 [00:15<00:00, 43.79it/s]\n",
      "epoch 57, loss : 1.9273631572723389: 100%|████| 677/677 [00:15<00:00, 43.76it/s]\n",
      "epoch 58, loss : 1.8983452320098877: 100%|████| 677/677 [00:15<00:00, 43.58it/s]\n",
      "epoch 59, loss : 2.1297271251678467: 100%|████| 677/677 [00:15<00:00, 43.60it/s]\n",
      "epoch 60, loss : 2.0664143562316895: 100%|████| 677/677 [00:15<00:00, 43.72it/s]\n",
      "epoch 61, loss : 2.1083920001983643: 100%|████| 677/677 [00:15<00:00, 43.51it/s]\n",
      "epoch 62, loss : 1.9400774240493774: 100%|████| 677/677 [00:15<00:00, 43.50it/s]\n",
      "epoch 63, loss : 1.7656368017196655: 100%|████| 677/677 [00:15<00:00, 43.40it/s]\n",
      "epoch 64, loss : 1.8680307865142822: 100%|████| 677/677 [00:15<00:00, 43.55it/s]\n",
      "epoch 65, loss : 2.142402410507202: 100%|█████| 677/677 [00:15<00:00, 43.46it/s]\n",
      "epoch 66, loss : 1.8342618942260742: 100%|████| 677/677 [00:15<00:00, 43.73it/s]\n",
      "epoch 67, loss : 1.7592397928237915: 100%|████| 677/677 [00:15<00:00, 43.72it/s]\n",
      "epoch 68, loss : 1.7179027795791626: 100%|████| 677/677 [00:15<00:00, 43.73it/s]\n",
      "epoch 69, loss : 1.8631458282470703: 100%|████| 677/677 [00:15<00:00, 43.81it/s]\n",
      "epoch 70, loss : 1.5287419557571411: 100%|████| 677/677 [00:15<00:00, 43.77it/s]\n",
      "epoch 71, loss : 1.6371166706085205: 100%|████| 677/677 [00:15<00:00, 43.75it/s]\n",
      "epoch 72, loss : 1.6485284566879272: 100%|████| 677/677 [00:15<00:00, 43.78it/s]\n",
      "epoch 73, loss : 1.704450249671936: 100%|█████| 677/677 [00:15<00:00, 43.78it/s]\n",
      "epoch 74, loss : 1.860553503036499: 100%|█████| 677/677 [00:15<00:00, 43.75it/s]\n",
      "epoch 75, loss : 1.5398390293121338: 100%|████| 677/677 [00:15<00:00, 43.80it/s]\n",
      "epoch 76, loss : 1.6314315795898438: 100%|████| 677/677 [00:15<00:00, 43.80it/s]\n",
      "epoch 77, loss : 1.813852310180664: 100%|█████| 677/677 [00:15<00:00, 43.70it/s]\n",
      "epoch 78, loss : 1.8258413076400757: 100%|████| 677/677 [00:15<00:00, 43.49it/s]\n",
      "epoch 79, loss : 1.8373171091079712: 100%|████| 677/677 [00:15<00:00, 43.42it/s]\n",
      "epoch 80, loss : 1.6526978015899658: 100%|████| 677/677 [00:15<00:00, 43.55it/s]\n",
      "epoch 81, loss : 1.5999103784561157: 100%|████| 677/677 [00:15<00:00, 43.67it/s]\n",
      "epoch 82, loss : 1.649643898010254: 100%|█████| 677/677 [00:15<00:00, 43.72it/s]\n",
      "epoch 83, loss : 1.396308183670044: 100%|█████| 677/677 [00:15<00:00, 43.66it/s]\n",
      "epoch 84, loss : 1.5372755527496338: 100%|████| 677/677 [00:15<00:00, 43.64it/s]\n",
      "epoch 85, loss : 1.3606804609298706: 100%|████| 677/677 [00:15<00:00, 43.58it/s]\n",
      "epoch 86, loss : 1.415626883506775: 100%|█████| 677/677 [00:15<00:00, 43.61it/s]\n",
      "epoch 87, loss : 1.1836779117584229: 100%|████| 677/677 [00:15<00:00, 43.64it/s]\n",
      "epoch 88, loss : 1.4017630815505981: 100%|████| 677/677 [00:15<00:00, 43.63it/s]\n",
      "epoch 89, loss : 1.3161264657974243: 100%|████| 677/677 [00:15<00:00, 43.37it/s]\n",
      "epoch 90, loss : 1.258000135421753: 100%|█████| 677/677 [00:15<00:00, 43.56it/s]\n",
      "epoch 91, loss : 1.2732083797454834: 100%|████| 677/677 [00:15<00:00, 43.58it/s]\n",
      "epoch 92, loss : 1.2875479459762573: 100%|████| 677/677 [00:15<00:00, 43.58it/s]\n",
      "epoch 93, loss : 1.2728325128555298: 100%|████| 677/677 [00:15<00:00, 43.47it/s]\n",
      "epoch 94, loss : 1.119249701499939: 100%|█████| 677/677 [00:15<00:00, 43.70it/s]\n",
      "epoch 95, loss : 1.194252848625183: 100%|█████| 677/677 [00:15<00:00, 43.66it/s]\n",
      "epoch 96, loss : 1.3074363470077515: 100%|████| 677/677 [00:15<00:00, 43.66it/s]\n",
      "epoch 97, loss : 1.0440142154693604: 100%|████| 677/677 [00:15<00:00, 43.65it/s]\n",
      "epoch 98, loss : 1.0708692073822021: 100%|████| 677/677 [00:15<00:00, 43.60it/s]\n",
      "epoch 99, loss : 1.0219323635101318: 100%|████| 677/677 [00:15<00:00, 43.70it/s]\n",
      "epoch 100, loss : 1.1229121685028076: 100%|███| 677/677 [00:15<00:00, 43.71it/s]\n",
      "epoch 101, loss : 1.0633838176727295: 100%|███| 677/677 [00:15<00:00, 43.72it/s]\n",
      "epoch 102, loss : 1.2466068267822266: 100%|███| 677/677 [00:15<00:00, 43.66it/s]\n",
      "epoch 103, loss : 1.06509530544281: 100%|█████| 677/677 [00:15<00:00, 43.64it/s]\n",
      "epoch 104, loss : 1.0802195072174072: 100%|███| 677/677 [00:15<00:00, 43.48it/s]\n",
      "epoch 105, loss : 1.0005911588668823: 100%|███| 677/677 [00:15<00:00, 43.49it/s]\n",
      "epoch 106, loss : 1.0407161712646484: 100%|███| 677/677 [00:15<00:00, 43.58it/s]\n",
      "epoch 107, loss : 0.9962992668151855: 100%|███| 677/677 [00:15<00:00, 43.65it/s]\n",
      "epoch 108, loss : 1.0386914014816284: 100%|███| 677/677 [00:15<00:00, 43.46it/s]\n",
      "epoch 109, loss : 0.9870147705078125: 100%|███| 677/677 [00:15<00:00, 43.66it/s]\n",
      "epoch 110, loss : 1.144160509109497: 100%|████| 677/677 [00:15<00:00, 43.48it/s]\n",
      "epoch 111, loss : 1.0587198734283447: 100%|███| 677/677 [00:15<00:00, 43.57it/s]\n",
      "epoch 112, loss : 0.8184008598327637: 100%|███| 677/677 [00:15<00:00, 43.77it/s]\n",
      "epoch 113, loss : 1.121423363685608: 100%|████| 677/677 [00:15<00:00, 43.63it/s]\n",
      "epoch 114, loss : 0.730265200138092: 100%|████| 677/677 [00:15<00:00, 43.71it/s]\n",
      "epoch 115, loss : 0.9763047099113464: 100%|███| 677/677 [00:15<00:00, 43.63it/s]\n",
      "epoch 116, loss : 0.852939784526825: 100%|████| 677/677 [00:15<00:00, 43.61it/s]\n",
      "epoch 117, loss : 1.036298155784607: 100%|████| 677/677 [00:15<00:00, 44.00it/s]\n",
      "epoch 118, loss : 1.0924761295318604: 100%|███| 677/677 [00:15<00:00, 44.03it/s]\n",
      "epoch 119, loss : 1.005792498588562: 100%|████| 677/677 [00:15<00:00, 43.94it/s]\n",
      "epoch 120, loss : 0.8730862736701965: 100%|███| 677/677 [00:15<00:00, 43.96it/s]\n",
      "epoch 121, loss : 0.6771969199180603: 100%|███| 677/677 [00:15<00:00, 44.01it/s]\n",
      "epoch 122, loss : 0.8032118082046509: 100%|███| 677/677 [00:15<00:00, 43.97it/s]\n",
      "epoch 123, loss : 0.9949105381965637: 100%|███| 677/677 [00:15<00:00, 43.64it/s]\n",
      "epoch 124, loss : 0.9274028539657593: 100%|███| 677/677 [00:15<00:00, 43.72it/s]\n",
      "epoch 125, loss : 0.9639161825180054: 100%|███| 677/677 [00:15<00:00, 43.85it/s]\n",
      "epoch 126, loss : 1.2249687910079956: 100%|███| 677/677 [00:15<00:00, 43.87it/s]\n",
      "epoch 127, loss : 0.9047938585281372: 100%|███| 677/677 [00:15<00:00, 43.90it/s]\n",
      "epoch 128, loss : 0.9151374697685242: 100%|███| 677/677 [00:15<00:00, 43.74it/s]\n",
      "epoch 129, loss : 0.9070213437080383: 100%|███| 677/677 [00:15<00:00, 43.89it/s]\n",
      "epoch 130, loss : 1.2728855609893799: 100%|███| 677/677 [00:15<00:00, 43.57it/s]\n",
      "epoch 131, loss : 0.8875962495803833: 100%|███| 677/677 [00:15<00:00, 43.68it/s]\n",
      "epoch 132, loss : 0.7537631988525391: 100%|███| 677/677 [00:15<00:00, 43.89it/s]\n",
      "epoch 133, loss : 0.6032381653785706: 100%|███| 677/677 [00:15<00:00, 43.84it/s]\n",
      "epoch 134, loss : 1.0294311046600342: 100%|███| 677/677 [00:15<00:00, 43.83it/s]\n",
      "epoch 135, loss : 0.7135205268859863: 100%|███| 677/677 [00:15<00:00, 43.88it/s]\n",
      "epoch 136, loss : 0.924331784248352: 100%|████| 677/677 [00:15<00:00, 43.86it/s]\n",
      "epoch 137, loss : 0.729693591594696: 100%|████| 677/677 [00:15<00:00, 43.84it/s]\n",
      "epoch 138, loss : 0.602488100528717: 100%|████| 677/677 [00:15<00:00, 43.84it/s]\n",
      "epoch 139, loss : 0.842480480670929: 100%|████| 677/677 [00:15<00:00, 43.87it/s]\n",
      "epoch 140, loss : 0.7979404926300049: 100%|███| 677/677 [00:15<00:00, 43.87it/s]\n",
      "epoch 141, loss : 0.7030384540557861: 100%|███| 677/677 [00:15<00:00, 43.80it/s]\n",
      "epoch 142, loss : 0.7009907960891724: 100%|███| 677/677 [00:15<00:00, 43.85it/s]\n",
      "epoch 143, loss : 0.6601109504699707: 100%|███| 677/677 [00:15<00:00, 43.79it/s]\n",
      "epoch 144, loss : 0.6021696329116821: 100%|███| 677/677 [00:15<00:00, 43.85it/s]\n",
      "epoch 145, loss : 0.6628481149673462: 100%|███| 677/677 [00:15<00:00, 43.80it/s]\n",
      "epoch 146, loss : 0.7424972653388977: 100%|███| 677/677 [00:15<00:00, 43.82it/s]\n",
      "epoch 147, loss : 0.5497962236404419: 100%|███| 677/677 [00:15<00:00, 43.86it/s]\n",
      "epoch 148, loss : 0.5949610471725464: 100%|███| 677/677 [00:15<00:00, 43.83it/s]\n",
      "epoch 149, loss : 0.5270916223526001: 100%|███| 677/677 [00:15<00:00, 43.85it/s]\n",
      "epoch 150, loss : 0.4963105320930481: 100%|███| 677/677 [00:15<00:00, 43.87it/s]\n",
      "epoch 151, loss : 0.6593413949012756: 100%|███| 677/677 [00:15<00:00, 43.82it/s]\n",
      "epoch 152, loss : 0.7082265019416809: 100%|███| 677/677 [00:15<00:00, 43.89it/s]\n",
      "epoch 153, loss : 0.5573034286499023: 100%|███| 677/677 [00:15<00:00, 43.88it/s]\n",
      "epoch 154, loss : 0.5771611928939819: 100%|███| 677/677 [00:15<00:00, 43.88it/s]\n",
      "epoch 155, loss : 0.5775333642959595: 100%|███| 677/677 [00:15<00:00, 43.84it/s]\n",
      "epoch 156, loss : 0.49857693910598755: 100%|██| 677/677 [00:15<00:00, 43.81it/s]\n",
      "epoch 157, loss : 0.5967727899551392: 100%|███| 677/677 [00:15<00:00, 43.84it/s]\n",
      "epoch 158, loss : 0.9134028553962708: 100%|███| 677/677 [00:15<00:00, 43.84it/s]\n",
      "epoch 159, loss : 0.36869144439697266: 100%|██| 677/677 [00:15<00:00, 43.84it/s]\n",
      "epoch 160, loss : 0.7933760285377502: 100%|███| 677/677 [00:15<00:00, 43.81it/s]\n",
      "epoch 161, loss : 0.5389257669448853: 100%|███| 677/677 [00:15<00:00, 43.82it/s]\n",
      "epoch 162, loss : 0.6553839445114136: 100%|███| 677/677 [00:15<00:00, 43.81it/s]\n",
      "epoch 163, loss : 0.739051342010498: 100%|████| 677/677 [00:15<00:00, 43.79it/s]\n",
      "epoch 164, loss : 0.7477408647537231: 100%|███| 677/677 [00:15<00:00, 43.76it/s]\n",
      "epoch 165, loss : 0.49180111289024353: 100%|██| 677/677 [00:15<00:00, 43.83it/s]\n",
      "epoch 166, loss : 0.7434660792350769: 100%|███| 677/677 [00:15<00:00, 43.82it/s]\n",
      "epoch 167, loss : 0.5759693384170532: 100%|███| 677/677 [00:15<00:00, 43.86it/s]\n",
      "epoch 168, loss : 0.5358909964561462: 100%|███| 677/677 [00:15<00:00, 43.80it/s]\n",
      "epoch 169, loss : 0.5722726583480835: 100%|███| 677/677 [00:15<00:00, 43.62it/s]\n",
      "epoch 170, loss : 0.5956324338912964: 100%|███| 677/677 [00:15<00:00, 43.48it/s]\n",
      "epoch 171, loss : 0.6475764513015747: 100%|███| 677/677 [00:15<00:00, 43.72it/s]\n",
      "epoch 172, loss : 1.014832854270935: 100%|████| 677/677 [00:15<00:00, 43.73it/s]\n",
      "epoch 173, loss : 0.7999772429466248: 100%|███| 677/677 [00:15<00:00, 43.82it/s]\n",
      "epoch 174, loss : 0.6074783802032471: 100%|███| 677/677 [00:15<00:00, 43.80it/s]\n",
      "epoch 175, loss : 0.6956261396408081: 100%|███| 677/677 [00:15<00:00, 43.84it/s]\n",
      "epoch 176, loss : 0.4481872022151947: 100%|███| 677/677 [00:15<00:00, 43.74it/s]\n",
      "epoch 177, loss : 0.6931759715080261: 100%|███| 677/677 [00:15<00:00, 43.88it/s]\n",
      "epoch 178, loss : 0.7329219579696655: 100%|███| 677/677 [00:15<00:00, 43.82it/s]\n",
      "epoch 179, loss : 0.6520247459411621: 100%|███| 677/677 [00:15<00:00, 43.59it/s]\n",
      "epoch 180, loss : 0.7520669102668762: 100%|███| 677/677 [00:15<00:00, 43.91it/s]\n",
      "epoch 181, loss : 0.426727831363678: 100%|████| 677/677 [00:15<00:00, 43.96it/s]\n",
      "epoch 182, loss : 0.6751219034194946: 100%|███| 677/677 [00:15<00:00, 43.87it/s]\n",
      "epoch 183, loss : 0.5023108720779419: 100%|███| 677/677 [00:15<00:00, 43.59it/s]\n",
      "epoch 184, loss : 0.41078710556030273: 100%|██| 677/677 [00:15<00:00, 43.78it/s]\n",
      "epoch 185, loss : 0.535458505153656: 100%|████| 677/677 [00:15<00:00, 43.85it/s]\n",
      "epoch 186, loss : 0.5892013907432556: 100%|███| 677/677 [00:15<00:00, 43.81it/s]\n",
      "epoch 187, loss : 0.5629206895828247: 100%|███| 677/677 [00:15<00:00, 43.74it/s]\n",
      "epoch 188, loss : 0.8289071917533875: 100%|███| 677/677 [00:15<00:00, 43.50it/s]\n",
      "epoch 189, loss : 0.4334643483161926: 100%|███| 677/677 [00:15<00:00, 43.84it/s]\n",
      "epoch 190, loss : 0.700942873954773: 100%|████| 677/677 [00:15<00:00, 43.85it/s]\n",
      "epoch 191, loss : 0.7751623392105103: 100%|███| 677/677 [00:15<00:00, 43.89it/s]\n",
      "epoch 192, loss : 0.597791314125061: 100%|████| 677/677 [00:15<00:00, 44.00it/s]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 193, loss : 0.4252092242240906: 100%|███| 677/677 [00:15<00:00, 44.06it/s]\n",
      "epoch 194, loss : 0.4041644036769867: 100%|███| 677/677 [00:15<00:00, 44.13it/s]\n",
      "epoch 195, loss : 0.8233386278152466: 100%|███| 677/677 [00:15<00:00, 43.91it/s]\n",
      "epoch 196, loss : 0.668724000453949: 100%|████| 677/677 [00:15<00:00, 43.92it/s]\n",
      "epoch 197, loss : 0.4234485626220703: 100%|███| 677/677 [00:15<00:00, 44.04it/s]\n",
      "epoch 198, loss : 0.5207594037055969: 100%|███| 677/677 [00:15<00:00, 44.09it/s]\n",
      "epoch 199, loss : 0.32682323455810547: 100%|██| 677/677 [00:15<00:00, 44.04it/s]\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(200):\n",
    "    iterator = tqdm.tqdm(loader)\n",
    "    for data, label in iterator:\n",
    "        data, label = data.to(device), label.to(device)\n",
    "        \n",
    "        optim.zero_grad()\n",
    "        pred = model(torch.tensor(data, dtype=torch.long).to(device))\n",
    "        loss = loss_fn(pred, torch.tensor(label, dtype=torch.long).to(device))\n",
    "        \n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "        \n",
    "        iterator.set_description(f'epoch {epoch}, loss : {loss.item()}')\n",
    "        \n",
    "torch.save(model.state_dict(), './models/LSTM.pth')        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a08654aa",
   "metadata": {},
   "source": [
    "# 모델 성능 평가하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9b089297",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(model, BOW, string='finding an ', strlen=10, device=device):    \n",
    "    \n",
    "    print(f'Input word : {string}')\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for p in range(strlen):\n",
    "            words = torch.tensor([BOW[w] for w in string.split()], dtype=torch.long).to(device)\n",
    "            \n",
    "            input_tensor = torch.unsqueeze(words[-2: ], dim=0)\n",
    "            output = model(input_tensor)\n",
    "            output_word = (torch.argmax(output).cpu().numpy())\n",
    "            string += list(BOW.keys())[output_word]\n",
    "            string += \" \"\n",
    "            \n",
    "    print(f'predicted sentence: {string}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7e9e5bdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input word : finding an \n",
      "predicted sentence: finding an workers sheep subject with laughing if on whisperer the — \n"
     ]
    }
   ],
   "source": [
    "\n",
    "model.load_state_dict(torch.load(\"./models/LSTM.pth\", map_location=device))\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "pred = generate(model, dataset.BOW, string='finding an ', strlen=10, device=device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
