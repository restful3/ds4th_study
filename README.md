# **ds4th study**

---

### **1. 스터디 목적**

* **Sebastian Raschka의 "Build a Large Language Model (From Scratch)" 책을 통해 대형 언어 모델(LLM)의 내부 작동 원리를 완전히 이해하고, GPT 유사 모델을 처음부터 구현하는 실무 능력을 강화하는 것**을 목표로 한다.
* '클로드 코드 완벽 가이드' 스터디를 통해 LLM 기반 개발 실무 능력(Agentic Workflow, Tool Use, Automation)을 함께 강화하여 이론과 실전을 모두 갖춘 AI 엔지니어링 역량을 구축한다.
  
---

### **2. 스터디 시간**

* 매주 토요일 오전 9시부터 1시간

---

### **3. 스터디 장소**

* Webex

---

### **4. 스터디 운영 계획 (2025년 9월 ~ 2025년 11월)**

---

#### **2025년 9월 13일**: [Webex](https://lgehq.webex.com/lgehq-en/j.php?MTID=m8a945c481103c4a418d4670b103f74be	)
* 발표자 - C성진
  * **Chapter 1:** Understanding Large Language Models
    * LLM 기본 개념 및 트랜스포머 아키텍처 소개
    * 사전 훈련과 파인튜닝 단계 이해
    * GPT 시리즈 모델의 발전 과정

#### **2025년 9월 20일**: [Webex](https://lgehq.webex.com/lgehq-en/j.php?MTID=m7ce242843e71db115e8b2369986a04aa	)
* 발표자 - 경연
  * **Chapter 2:** Working with Text Data
    * 텍스트 토큰화 및 토큰 ID 변환
    * 바이트 페어 인코딩(BPE) 구현
    * 토큰 임베딩과 위치 인코딩 생성
    * 슬라이딩 윈도우를 통한 훈련 데이터 생성
* 발표자 - K성진, 태호
  * **Kaggle**
    * MAP - Charting Student Math Misunderstandings(태호)
    * ARC Prize 2025 (K성진)
       
#### **2025년 9월 27일**: [Webex](https://lgehq.webex.com/lgehq-en/j.php?MTID=m794c4e50f5ff0e9aa332e21523485655	)
* 발표자 - S종훈
  * **Chapter 3:** Coding Attention Mechanisms
    * 셀프 어텐션 메커니즘 구현
    * 인과적 어텐션 마스크 적용
    * 멀티헤드 어텐션 구조 구축
    * 드롭아웃을 통한 정규화   
* 발표자 - 재익
  * **Kaggle**
    * [ARC Prize 2025](https://github.com/restful3/ds4th_study/blob/main/source/20250927_arc2025_jishin.pdf)

#### **✅ 2025년 10월 4일**: 휴일 (추석 연휴 - 스터디 없음)

#### **✅ 2025년 10월 11일**: 휴일 (추석 연휴 - 스터디 없음)

#### **2025년 10월 18일**: [Webex](https://lgehq.webex.com/lgehq-en/j.php?MTID=me926d907650c9b1a34a5cf2a56fbea7c	)
* 발표자 - 태영
  * [**Chapter 4:** Implementing a GPT Model from Scratch to Generate Text](https://github.com/restful3/ds4th_study/blob/main/source/build-a-large-language-model-from-scratch/CH4%20%ED%85%8D%EC%8A%A4%ED%8A%B8%20%EC%83%9D%EC%84%B1%EC%9D%84%20%EC%9C%84%ED%95%9C%20GPT%20%EB%AA%A8%EB%8D%B8%EC%9D%84%20%EC%B2%98%EC%9D%8C%EB%B6%80%ED%84%B0%20%EA%B5%AC%ED%98%84%ED%95%98%EA%B8%B0.pdf)
    * GPT 아키텍처 전체 구현
    * 레이어 정규화 및 피드포워드 네트워크
    * 트랜스포머 블록 조립
    * 텍스트 생성 및 디코딩 전략   
* 발표자 - 영재
  * **Kaggle**
    * ARC Prize 2025

#### **2025년 10월 25일**: [Webex](https://lgehq.webex.com/lgehq-en/j.php?MTID=m18b99f88356090f9f2ce9d29adefecc3	)
* 발표자 - 우석
  * **Chapter 5:** Pretraining on Unlabeled Data
    * 모델 성능 평가 지표 구현
    * 훈련 루프 및 검증 프로세스
    * OpenAI 사전 훈련 가중치 로드
    * 온도 스케일링 및 top-k 샘플링
    * 모델 저장 및 로드 방법   
* 발표자 - K성진, 태호
  * **Kaggle**
    * ARC Prize 2025

#### **2025년 11월 1일**: [Webex](https://lgehq.webex.com/lgehq-en/j.php?MTID=m54969345967403cc4743ce696daeb9f0	)
* 발표자 - 태호
  * [**Chapter 6:** Finetuning for Text Classification](https://github.com/restful3/ds4th_study/blob/main/source/build-a-large-language-model-from-scratch/ch06_Finetuning_for_classification_Teo.ipynb)
    * 분류 작업을 위한 모델 헤드 수정
    * 파인튜닝 데이터셋 준비 방법
    * 다양한 파인튜닝 전략 비교
    * 성능 평가 및 결과 분석
    * 스팸 분류기 구현 실습
* 발표자 - 태영
  * **Kaggle**
    * LLM Classification Finetuning

#### **2025년 11월 8일**: [Webex](https://lgehq.webex.com/lgehq-en/j.php?MTID=me13617ac007993e928d591c2d3dc036e	)
* 발표자 - 재익
  * **Chapter 7:** Finetuning to Follow Instructions
    * 지시사항 파인튜닝 데이터셋 구성
    * 인간 피드백 학습(RLHF) 개념
    * 직접 선호도 최적화(DPO) 구현
    * 모델 정렬 및 안전성 확보
    * 대화형 AI 구축
* 발표자 - 두균
  * **Kaggle**
    * LLM Classification Finetuning

#### ✅ **요즘 바이브 코딩 클로드 코드 완벽 가이드로 교재 변경**
   
#### **2025년 11월 15일**: [Webex](https://lgehq.webex.com/lgehq-en/j.php?MTID=m2f1c9bd81d1f13fd9a638b4092d599b2)
* 발표자 - 핀조이
  * **claude-code-router**    
* 발표자 - 보현
  * **Kaggle**
    * LLM Classification Finetuning

#### **2025년 11월 22일**: [Webex](https://lgehq.webex.com/lgehq-en/j.php?MTID=mb7eadd3edf33c21a979fa92ff7f8da7c)
* 발표자 - TBD
  * **Chapter 4~9 (p.44–93)**
    * **Chapter 4:** 클로드 코드 기본 인터페이스 이해하기
    * **Chapter 5:** 슬래시 명령어 제대로 알아보기
    * **Chapter 6:** CLAUDE.md 파일에 대한 모든 것
    * **Chapter 7:** 클로드 코드의 3가지 모드 알아보기
    * **Chapter 8:** 모델 선택, 사용량 관리
    * **Chapter 9:** 생각 과정 제어하기
* 발표자 - TBD
  * **Kaggle**
    * TBD

#### **2025년 11월 29일**: [Webex](https://lgehq.webex.com/lgehq-en/j.php?MTID=mc1203b072b82d970ac091b79ca6c03f6)
* 발표자 - TBD
  * **Chapter 10~14 (p.94–149)**
    * **Chapter 10:** 커스텀 슬래시 커맨드
    * **Chapter 11:** MCP 사용하기
    * **Chapter 12:** PRD와 실행 계획
    * **Chapter 13:** 에이전트 병렬 실행
    * **Chapter 14:** GitHub 워크플로
* 발표자 - TBD
  * **Kaggle**
    * TBD

#### **2025년 12월 6일**: [Webex](https://lgehq.webex.com/lgehq-en/j.php?MTID=m440b55ecb6412ea23b0d388326e11581)
* 발표자 - TBD
  * **Chapter 15~18 (p.150–202)**
    * **Chapter 15:** 아이디어 구체화
    * **Chapter 16:** UI 프로토타이핑
    * **Chapter 17:** 인증 구현하기
    * **Chapter 18:** 기능 작업
* 발표자 - TBD
  * **Kaggle**
    * TBD

#### **2025년 12월 13일**: [Webex](https://lgehq.webex.com/lgehq-en/j.php?MTID=m4043e4d9f8ed7a669db2e90873cfb9cc)
* 발표자 - TBD
  * **Chapter 19~23 (p.203–249)**
    * **Chapter 19:** 데이터베이스 연동하기
    * **Chapter 20:** 테스트 작성하기
    * **Chapter 21:** 배포하기
    * **Chapter 22:** Super Claude
    * **Chapter 23:** Claudia
* 발표자 - TBD
  * **Kaggle**
    * TBD

#### **2025년 12월 20일**: [Webex](https://lgehq.webex.com/lgehq-en/j.php?MTID=m144d92d270fea25298c353e9cc001b28)
* 발표자 - TBD
  * **Chapter 24~27 + 부록 (p.250–274)**
    * **Chapter 24:** Claude Squad
    * **Chapter 25:** ccusage
    * **Chapter 26:** Claude Code Action
    * **Chapter 27:** Claude Code Hooks
    * **부록:** 99가지 유용한 팁
* 발표자 - TBD
  * **Kaggle**
    * TBD

---

### **5. 스터디 운영 방법**

* **주교재**:
  * [Build a Large Language Model (From Scratch) - Sebastian Raschka](https://www.manning.com/books/build-a-large-language-model-from-scratch)
  * **공식 GitHub 저장소**: [rasbt/LLMs-from-scratch](https://github.com/rasbt/LLMs-from-scratch)

* **참고 자료**:
  * [Hands-On Large Language Models - Jay Alammar & Maarten Grootendorst](https://www.oreilly.com/library/view/hands-on-large-language/9781098150952/)
  * [LLM을 활용한 실전 AI 애플리케이션 개발 - 허정준](https://github.com/onlybooks/llm)

* **학습 공유**: 매주 학습한 내용을 발표자료와 함께 GitHub에 공유
* **발표 방식**:
  * 각 챕터에 대한 50분 발표 + 10분 Q&A
  * 이론 설명과 실제 구현 코드 시연
  * 참고 자료의 관련 내용도 함께 다룰 수 있음

* **운영 규칙**:
  * 스터디 운영 규칙 (별도 문서 참조)

---

### **6. 학습 목표 및 성과물**

* **기초 단계 (9월)**: LLM 기본 개념과 텍스트 처리 파이프라인 이해
* **핵심 단계 (10월)**: 어텐션 메커니즘과 GPT 모델 완전 구현  
* **응용 단계 (11월)**: 모델 훈련, 파인튜닝 및 실용화

* **최종 성과물**: 
  * 처음부터 구현한 완전한 GPT 유사 모델
  * 개인별 특화된 LLM 프로젝트
  * 학습 과정과 구현 코드가 체계적으로 정리된 GitHub 저장소

---

### **7. 기타**

* **참가 희망 요청**: [Email](mailto:restful3@gmail.com)
* **이제까지 다룬 내용**: [archive 확인](https://github.com/restful3/ds4th_study/tree/main/archive)
* **교재 구매 링크**: 
  * [Manning 주교재](https://www.manning.com/books/build-a-large-language-model-from-scratch)
  * [O'Reilly 참고서](https://www.oreilly.com/library/view/hands-on-large-language/9781098150952/)
  * [국내서 참고자료](https://github.com/onlybooks/llm)
  * [요즘 바이브 코딩 클로드 코드 완벽 가이드](https://ridibooks.com/books/4547000075?_rdt_sid=category_bestsellers&_rdt_idx=0&_rdt_arg=2220)
* **Kaggle 대회 링크**: 
  * [ARC Prize 2025](https://www.kaggle.com/competitions/arc-prize-2025)
  * [LLM Classification Finetuning](https://www.kaggle.com/competitions/llm-classification-finetuning)

---



















