{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 데이터 경로\n",
    "data_path = 'input/porto-seguro-safe-driver-prediction/'\n",
    "\n",
    "train = pd.read_csv(data_path + 'train.csv', index_col='id')\n",
    "test = pd.read_csv(data_path + 'test.csv', index_col='id')\n",
    "submission = pd.read_csv(data_path + 'sample_submission.csv', index_col='id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = pd.concat([train, test], ignore_index=True)\n",
    "all_data = all_data.drop('target', axis=1) # 타깃값 제거\n",
    "\n",
    "all_features = all_data.columns # 전체 피처"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "cat_features = [feature for feature in all_features if 'cat' in feature] # 명목형 피처\n",
    "\n",
    "# 원-핫 인코딩 적용\n",
    "onehot_encoder = OneHotEncoder()\n",
    "encoded_cat_matrix = onehot_encoder.fit_transform(all_data[cat_features]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# '데이터 하나당 결측값 개수'를 파생 피로 추가\n",
    "all_data['num_missing'] = (all_data==-1).sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 명목형 피처, calc 분류 피처를 제외한 피처\n",
    "remaining_features = [feature for feature in all_features \n",
    "                      if ('cat' not in feature and 'calc' not in feature)] \n",
    "# num_missing을 remaining_features에 추가\n",
    "remaining_features.append('num_missing')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 분류가 ind인 피처\n",
    "ind_features = [feature for feature in all_features if 'ind' in feature]\n",
    "\n",
    "is_first_feature = True\n",
    "for ind_feature in ind_features:\n",
    "    if is_first_feature:\n",
    "        all_data['mix_ind'] = all_data[ind_feature].astype(str) + '_'\n",
    "        is_first_feature = False\n",
    "    else:\n",
    "        all_data['mix_ind'] += all_data[ind_feature].astype(str) + '_'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_count_features = []\n",
    "for feature in cat_features+['mix_ind']:\n",
    "    val_counts_dict = all_data[feature].value_counts().to_dict()\n",
    "    all_data[f'{feature}_count'] = all_data[feature].apply(lambda x: \n",
    "                                                           val_counts_dict[x])\n",
    "    cat_count_features.append(f'{feature}_count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import sparse\n",
    "\n",
    "# 필요 없는 피처들\n",
    "drop_features = ['ps_ind_14', 'ps_ind_10_bin', 'ps_ind_11_bin', \n",
    "                 'ps_ind_12_bin', 'ps_ind_13_bin', 'ps_car_14']\n",
    "\n",
    "# remaining_features, cat_count_features에서 drop_features를 제거한 데이터\n",
    "all_data_remaining = all_data[remaining_features+cat_count_features].drop(drop_features, axis=1)\n",
    "\n",
    "# 데이터 합치기\n",
    "all_data_sprs = sparse.hstack([sparse.csr_matrix(all_data_remaining),\n",
    "                               encoded_cat_matrix],\n",
    "                              format='csr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_train = len(train) # 훈련 데이터 개수\n",
    "\n",
    "# 훈련 데이터와 테스트 데이터 나누기\n",
    "X = all_data_sprs[:num_train]\n",
    "X_test = all_data_sprs[num_train:]\n",
    "\n",
    "y = train['target'].values"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "하이퍼 파라미터 최적화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def eval_gini(y_true, y_pred):\n",
    "    # 실제값과 예측값의 크기가 같은지 확인 (값이 다르면 오류 발생)\n",
    "    assert y_true.shape == y_pred.shape\n",
    "\n",
    "    n_samples = y_true.shape[0]                      # 데이터 개수\n",
    "    L_mid = np.linspace(1 / n_samples, 1, n_samples) # 대각선 값\n",
    "\n",
    "    # 1) 예측값에 대한 지니계수\n",
    "    pred_order = y_true[y_pred.argsort()] # y_pred 크기순으로 y_true 값 정렬\n",
    "    L_pred = np.cumsum(pred_order) / np.sum(pred_order) # 로렌츠 곡선\n",
    "    G_pred = np.sum(L_mid - L_pred)       # 예측 값에 대한 지니계수\n",
    "\n",
    "    # 2) 예측이 완벽할 때 지니계수\n",
    "    true_order = y_true[y_true.argsort()] # y_true 크기순으로 y_true 값 정렬\n",
    "    L_true = np.cumsum(true_order) / np.sum(true_order) # 로렌츠 곡선\n",
    "    G_true = np.sum(L_mid - L_true)       # 예측이 완벽할 때 지니계수\n",
    "\n",
    "    # 정규화된 지니계수\n",
    "    return G_pred / G_true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LightGBM용 gini() 함수\n",
    "def gini_lgb(preds, dtrain):\n",
    "    labels = dtrain.get_label()\n",
    "    return 'gini', eval_gini(labels, preds), True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGBoost용 gini() 함수\n",
    "def gini_xgb(preds, dtrain):\n",
    "    labels = dtrain.get_label()\n",
    "    return 'gini', eval_gini(labels, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 8:2 비율로 훈련 데이터, 검증 데이터 분리 (베이지안 최적화 수행용)\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, \n",
    "                                                      test_size=0.2, \n",
    "                                                      random_state=0)\n",
    "\n",
    "# 베이지안 최적화용 데이터셋\n",
    "bayes_dtrain = lgb.Dataset(X_train, y_train)\n",
    "bayes_dvalid = lgb.Dataset(X_valid, y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 베이지안 최적화를 위한 하이퍼파라미터 범위\n",
    "param_bounds = {'num_leaves': (30, 40),\n",
    "                'lambda_l1': (0.7, 0.9),\n",
    "                'lambda_l2': (0.9, 1),\n",
    "                'feature_fraction': (0.6, 0.7),\n",
    "                'bagging_fraction': (0.6, 0.9),\n",
    "                'min_child_samples': (6, 10),\n",
    "                'min_child_weight': (10, 40)}\n",
    "\n",
    "# 값이 고정된 하이퍼파라미터\n",
    "fixed_params = {'objective': 'binary',\n",
    "                'learning_rate': 0.005,\n",
    "                'bagging_freq': 1,\n",
    "                'force_row_wise': True,\n",
    "                'random_state': 1991}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_function(num_leaves, lambda_l1, lambda_l2, feature_fraction,\n",
    "                  bagging_fraction, min_child_samples, min_child_weight):\n",
    "    '''최적화하려는 평가지표(지니계수) 계산 함수'''\n",
    "    \n",
    "    # 베이지안 최적화를 수행할 하이퍼파라미터 \n",
    "    params = {'num_leaves': int(round(num_leaves)),\n",
    "              'lambda_l1': lambda_l1,\n",
    "              'lambda_l2': lambda_l2,\n",
    "              'feature_fraction': feature_fraction,\n",
    "              'bagging_fraction': bagging_fraction,\n",
    "              'min_child_samples': int(round(min_child_samples)),\n",
    "              'min_child_weight': min_child_weight,\n",
    "              'feature_pre_filter': False}\n",
    "    # 고정된 하이퍼파라미터도 추가\n",
    "    params.update(fixed_params)\n",
    "    \n",
    "    print('하이퍼파라미터:', params)    \n",
    "    \n",
    "    # LightGBM 모델 훈련\n",
    "    lgb_model = lgb.train(params=params, \n",
    "                           train_set=bayes_dtrain,\n",
    "                           num_boost_round=2500,\n",
    "                           valid_sets=bayes_dvalid,\n",
    "                           feval=gini_lgb,\n",
    "                           early_stopping_rounds=300,\n",
    "                           verbose_eval=False)\n",
    "    # 검증 데이터로 예측 수행\n",
    "    preds = lgb_model.predict(X_valid) \n",
    "    # 지니계수 계산\n",
    "    gini_score = eval_gini(y_valid, preds)\n",
    "    print(f'지니계수 : {gini_score}\\n')\n",
    "    \n",
    "    return gini_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bayes_opt import BayesianOptimization\n",
    "\n",
    "# 베이지안 최적화 객체 생성\n",
    "optimizer = BayesianOptimization(f=eval_function,      # 평가지표 계산 함수\n",
    "                                 pbounds=param_bounds, # 하이퍼파라미터 범위\n",
    "                                 random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|   iter    |  target   | baggin... | featur... | lambda_l1 | lambda_l2 | min_ch... | min_ch... | num_le... |\n",
      "-------------------------------------------------------------------------------------------------------------\n",
      "하이퍼파라미터: {'num_leaves': 34, 'lambda_l1': 0.8205526752143287, 'lambda_l2': 0.9544883182996897, 'feature_fraction': 0.6715189366372419, 'bagging_fraction': 0.7646440511781974, 'min_child_samples': 8, 'min_child_weight': 29.376823391999682, 'feature_pre_filter': False, 'objective': 'binary', 'learning_rate': 0.005, 'bagging_freq': 1, 'force_row_wise': True, 'random_state': 1991}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jung0\\miniconda3\\envs\\kaggle\\lib\\site-packages\\lightgbm\\engine.py:181: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "c:\\Users\\jung0\\miniconda3\\envs\\kaggle\\lib\\site-packages\\lightgbm\\engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 17383, number of negative: 458786\n",
      "[LightGBM] [Info] Total Bins 1555\n",
      "[LightGBM] [Info] Number of data points in the train set: 476169, number of used features: 217\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.036506 -> initscore=-3.273091\n",
      "[LightGBM] [Info] Start training from score -3.273091\n",
      "지니계수 : 0.2855811556220905\n",
      "\n",
      "| \u001b[0m1        \u001b[0m | \u001b[0m0.2856   \u001b[0m | \u001b[0m0.7646   \u001b[0m | \u001b[0m0.6715   \u001b[0m | \u001b[0m0.8206   \u001b[0m | \u001b[0m0.9545   \u001b[0m | \u001b[0m7.695    \u001b[0m | \u001b[0m29.38    \u001b[0m | \u001b[0m34.38    \u001b[0m |\n",
      "하이퍼파라미터: {'num_leaves': 39, 'lambda_l1': 0.7766883037651555, 'lambda_l2': 0.9791725038082665, 'feature_fraction': 0.6963662760501029, 'bagging_fraction': 0.867531900234624, 'min_child_samples': 8, 'min_child_weight': 27.04133683281797, 'feature_pre_filter': False, 'objective': 'binary', 'learning_rate': 0.005, 'bagging_freq': 1, 'force_row_wise': True, 'random_state': 1991}\n",
      "[LightGBM] [Info] Number of positive: 17383, number of negative: 458786\n",
      "[LightGBM] [Info] Total Bins 1555\n",
      "[LightGBM] [Info] Number of data points in the train set: 476169, number of used features: 217\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.036506 -> initscore=-3.273091\n",
      "[LightGBM] [Info] Start training from score -3.273091\n",
      "지니계수 : 0.2837380537005777\n",
      "\n",
      "| \u001b[0m2        \u001b[0m | \u001b[0m0.2837   \u001b[0m | \u001b[0m0.8675   \u001b[0m | \u001b[0m0.6964   \u001b[0m | \u001b[0m0.7767   \u001b[0m | \u001b[0m0.9792   \u001b[0m | \u001b[0m8.116    \u001b[0m | \u001b[0m27.04    \u001b[0m | \u001b[0m39.26    \u001b[0m |\n",
      "하이퍼파라미터: {'num_leaves': 40, 'lambda_l1': 0.7040436794880651, 'lambda_l2': 0.9832619845547939, 'feature_fraction': 0.608712929970154, 'bagging_fraction': 0.6213108174593661, 'min_child_samples': 9, 'min_child_weight': 36.10036444740457, 'feature_pre_filter': False, 'objective': 'binary', 'learning_rate': 0.005, 'bagging_freq': 1, 'force_row_wise': True, 'random_state': 1991}\n",
      "[LightGBM] [Info] Number of positive: 17383, number of negative: 458786\n",
      "[LightGBM] [Info] Total Bins 1555\n",
      "[LightGBM] [Info] Number of data points in the train set: 476169, number of used features: 217\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.036506 -> initscore=-3.273091\n",
      "[LightGBM] [Info] Start training from score -3.273091\n",
      "지니계수 : 0.2857848354322048\n",
      "\n",
      "| \u001b[95m3        \u001b[0m | \u001b[95m0.2858   \u001b[0m | \u001b[95m0.6213   \u001b[0m | \u001b[95m0.6087   \u001b[0m | \u001b[95m0.704    \u001b[0m | \u001b[95m0.9833   \u001b[0m | \u001b[95m9.113    \u001b[0m | \u001b[95m36.1     \u001b[0m | \u001b[95m39.79    \u001b[0m |\n",
      "하이퍼파라미터: {'num_leaves': 30, 'lambda_l1': 0.8444997594874222, 'lambda_l2': 0.9234023852202012, 'feature_fraction': 0.6593983245038058, 'bagging_fraction': 0.8977977822397395, 'min_child_samples': 9, 'min_child_weight': 10.549362495448534, 'feature_pre_filter': False, 'objective': 'binary', 'learning_rate': 0.005, 'bagging_freq': 1, 'force_row_wise': True, 'random_state': 1991}\n",
      "[LightGBM] [Info] Number of positive: 17383, number of negative: 458786\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jung0\\miniconda3\\envs\\kaggle\\lib\\site-packages\\lightgbm\\engine.py:181: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "c:\\Users\\jung0\\miniconda3\\envs\\kaggle\\lib\\site-packages\\lightgbm\\engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Total Bins 1555\n",
      "[LightGBM] [Info] Number of data points in the train set: 476169, number of used features: 217\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.036506 -> initscore=-3.273091\n",
      "[LightGBM] [Info] Start training from score -3.273091\n",
      "지니계수 : 0.2828993761731121\n",
      "\n",
      "| \u001b[0m4        \u001b[0m | \u001b[0m0.2829   \u001b[0m | \u001b[0m0.8978   \u001b[0m | \u001b[0m0.6594   \u001b[0m | \u001b[0m0.8445   \u001b[0m | \u001b[0m0.9234   \u001b[0m | \u001b[0m8.619    \u001b[0m | \u001b[0m10.55    \u001b[0m | \u001b[0m30.09    \u001b[0m |\n",
      "하이퍼파라미터: {'num_leaves': 37, 'lambda_l1': 0.7738449330497988, 'lambda_l2': 0.9032695189818599, 'feature_fraction': 0.6606341064409726, 'bagging_fraction': 0.7666713964943057, 'min_child_samples': 9, 'min_child_weight': 29.306172421380474, 'feature_pre_filter': False, 'objective': 'binary', 'learning_rate': 0.005, 'bagging_freq': 1, 'force_row_wise': True, 'random_state': 1991}\n",
      "[LightGBM] [Info] Number of positive: 17383, number of negative: 458786\n",
      "[LightGBM] [Info] Total Bins 1555\n",
      "[LightGBM] [Info] Number of data points in the train set: 476169, number of used features: 217\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.036506 -> initscore=-3.273091\n",
      "[LightGBM] [Info] Start training from score -3.273091\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jung0\\miniconda3\\envs\\kaggle\\lib\\site-packages\\lightgbm\\engine.py:181: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "c:\\Users\\jung0\\miniconda3\\envs\\kaggle\\lib\\site-packages\\lightgbm\\engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "지니계수 : 0.28513273331754563\n",
      "\n",
      "| \u001b[0m5        \u001b[0m | \u001b[0m0.2851   \u001b[0m | \u001b[0m0.7667   \u001b[0m | \u001b[0m0.6606   \u001b[0m | \u001b[0m0.7738   \u001b[0m | \u001b[0m0.9033   \u001b[0m | \u001b[0m8.769    \u001b[0m | \u001b[0m29.31    \u001b[0m | \u001b[0m36.6     \u001b[0m |\n",
      "하이퍼파라미터: {'num_leaves': 33, 'lambda_l1': 0.8177421971241917, 'lambda_l2': 0.9, 'feature_fraction': 0.6, 'bagging_fraction': 0.6, 'min_child_samples': 10, 'min_child_weight': 35.79642473813153, 'feature_pre_filter': False, 'objective': 'binary', 'learning_rate': 0.005, 'bagging_freq': 1, 'force_row_wise': True, 'random_state': 1991}\n",
      "[LightGBM] [Info] Number of positive: 17383, number of negative: 458786\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jung0\\miniconda3\\envs\\kaggle\\lib\\site-packages\\lightgbm\\engine.py:181: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "c:\\Users\\jung0\\miniconda3\\envs\\kaggle\\lib\\site-packages\\lightgbm\\engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Total Bins 1555\n",
      "[LightGBM] [Info] Number of data points in the train set: 476169, number of used features: 217\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.036506 -> initscore=-3.273091\n",
      "[LightGBM] [Info] Start training from score -3.273091\n",
      "지니계수 : 0.28531769938044\n",
      "\n",
      "| \u001b[0m6        \u001b[0m | \u001b[0m0.2853   \u001b[0m | \u001b[0m0.6      \u001b[0m | \u001b[0m0.6      \u001b[0m | \u001b[0m0.8177   \u001b[0m | \u001b[0m0.9      \u001b[0m | \u001b[0m9.968    \u001b[0m | \u001b[0m35.8     \u001b[0m | \u001b[0m32.6     \u001b[0m |\n",
      "하이퍼파라미터: {'num_leaves': 37, 'lambda_l1': 0.8433793375135147, 'lambda_l2': 0.9479651949974717, 'feature_fraction': 0.6859622896374784, 'bagging_fraction': 0.8362539818721497, 'min_child_samples': 6, 'min_child_weight': 39.77484183530247, 'feature_pre_filter': False, 'objective': 'binary', 'learning_rate': 0.005, 'bagging_freq': 1, 'force_row_wise': True, 'random_state': 1991}\n",
      "[LightGBM] [Info] Number of positive: 17383, number of negative: 458786\n",
      "[LightGBM] [Info] Total Bins 1555\n",
      "[LightGBM] [Info] Number of data points in the train set: 476169, number of used features: 217\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.036506 -> initscore=-3.273091\n",
      "[LightGBM] [Info] Start training from score -3.273091\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jung0\\miniconda3\\envs\\kaggle\\lib\\site-packages\\lightgbm\\engine.py:181: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "c:\\Users\\jung0\\miniconda3\\envs\\kaggle\\lib\\site-packages\\lightgbm\\engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "지니계수 : 0.2854766974907317\n",
      "\n",
      "| \u001b[0m7        \u001b[0m | \u001b[0m0.2855   \u001b[0m | \u001b[0m0.8363   \u001b[0m | \u001b[0m0.686    \u001b[0m | \u001b[0m0.8434   \u001b[0m | \u001b[0m0.948    \u001b[0m | \u001b[0m6.002    \u001b[0m | \u001b[0m39.77    \u001b[0m | \u001b[0m36.8     \u001b[0m |\n",
      "하이퍼파라미터: {'num_leaves': 30, 'lambda_l1': 0.7294042178793255, 'lambda_l2': 0.9, 'feature_fraction': 0.6, 'bagging_fraction': 0.6, 'min_child_samples': 10, 'min_child_weight': 27.983666143579423, 'feature_pre_filter': False, 'objective': 'binary', 'learning_rate': 0.005, 'bagging_freq': 1, 'force_row_wise': True, 'random_state': 1991}\n",
      "[LightGBM] [Info] Number of positive: 17383, number of negative: 458786\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jung0\\miniconda3\\envs\\kaggle\\lib\\site-packages\\lightgbm\\engine.py:181: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "c:\\Users\\jung0\\miniconda3\\envs\\kaggle\\lib\\site-packages\\lightgbm\\engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Total Bins 1555\n",
      "[LightGBM] [Info] Number of data points in the train set: 476169, number of used features: 217\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.036506 -> initscore=-3.273091\n",
      "[LightGBM] [Info] Start training from score -3.273091\n",
      "지니계수 : 0.2845166879193859\n",
      "\n",
      "| \u001b[0m8        \u001b[0m | \u001b[0m0.2845   \u001b[0m | \u001b[0m0.6      \u001b[0m | \u001b[0m0.6      \u001b[0m | \u001b[0m0.7294   \u001b[0m | \u001b[0m0.9      \u001b[0m | \u001b[0m10.0     \u001b[0m | \u001b[0m27.98    \u001b[0m | \u001b[0m30.0     \u001b[0m |\n",
      "하이퍼파라미터: {'num_leaves': 36, 'lambda_l1': 0.9, 'lambda_l2': 1.0, 'feature_fraction': 0.7, 'bagging_fraction': 0.9, 'min_child_samples': 6, 'min_child_weight': 33.94526279251536, 'feature_pre_filter': False, 'objective': 'binary', 'learning_rate': 0.005, 'bagging_freq': 1, 'force_row_wise': True, 'random_state': 1991}\n",
      "[LightGBM] [Info] Number of positive: 17383, number of negative: 458786\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jung0\\miniconda3\\envs\\kaggle\\lib\\site-packages\\lightgbm\\engine.py:181: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "c:\\Users\\jung0\\miniconda3\\envs\\kaggle\\lib\\site-packages\\lightgbm\\engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Total Bins 1555\n",
      "[LightGBM] [Info] Number of data points in the train set: 476169, number of used features: 217\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.036506 -> initscore=-3.273091\n",
      "[LightGBM] [Info] Start training from score -3.273091\n",
      "지니계수 : 0.2844361274211447\n",
      "\n",
      "| \u001b[0m9        \u001b[0m | \u001b[0m0.2844   \u001b[0m | \u001b[0m0.9      \u001b[0m | \u001b[0m0.7      \u001b[0m | \u001b[0m0.9      \u001b[0m | \u001b[0m1.0      \u001b[0m | \u001b[0m6.0      \u001b[0m | \u001b[0m33.95    \u001b[0m | \u001b[0m35.86    \u001b[0m |\n",
      "=============================================================================================================\n"
     ]
    }
   ],
   "source": [
    "# 베이지안 최적화 수행\n",
    "optimizer.maximize(init_points=3, n_iter=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bagging_fraction': 0.6213108174593661,\n",
       " 'feature_fraction': 0.608712929970154,\n",
       " 'lambda_l1': 0.7040436794880651,\n",
       " 'lambda_l2': 0.9832619845547939,\n",
       " 'min_child_samples': 9.112627003799401,\n",
       " 'min_child_weight': 36.10036444740457,\n",
       " 'num_leaves': 39.78618342232764}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 평가함수 점수가 최대일 때 하이퍼파라미터\n",
    "max_params = optimizer.max['params']\n",
    "max_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 정수형 하이퍼파라미터 변환\n",
    "max_params['num_leaves'] = int(round(max_params['num_leaves']))\n",
    "max_params['min_child_samples'] = int(round(max_params['min_child_samples']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 값이 고정된 하이퍼파라미터 추가\n",
    "max_params.update(fixed_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bagging_fraction': 0.6213108174593661,\n",
       " 'feature_fraction': 0.608712929970154,\n",
       " 'lambda_l1': 0.7040436794880651,\n",
       " 'lambda_l2': 0.9832619845547939,\n",
       " 'min_child_samples': 9,\n",
       " 'min_child_weight': 36.10036444740457,\n",
       " 'num_leaves': 40,\n",
       " 'objective': 'binary',\n",
       " 'learning_rate': 0.005,\n",
       " 'bagging_freq': 1,\n",
       " 'force_row_wise': True,\n",
       " 'random_state': 1991}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 8:2 비율로 훈련 데이터, 검증 데이터 분리 (베이지안 최적화 수행용)\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, \n",
    "                                                      test_size=0.2, \n",
    "                                                      random_state=0)\n",
    "# 베이지안 최적화용 데이터셋\n",
    "bayes_dtrain = xgb.DMatrix(X_train, y_train)\n",
    "bayes_dvalid = xgb.DMatrix(X_valid, y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 베이지안 최적화를 위한 하이퍼파라미터 범위\n",
    "param_bounds = {'max_depth': (4, 8),\n",
    "                'subsample': (0.6, 0.9),\n",
    "                'colsample_bytree': (0.7, 1.0),\n",
    "                'min_child_weight': (5, 7),\n",
    "                'gamma': (8, 11),\n",
    "                'reg_alpha': (7, 9),\n",
    "                'reg_lambda': (1.1, 1.5),\n",
    "                'scale_pos_weight': (1.4, 1.6)}\n",
    "\n",
    "# 값이 고정된 하이퍼파라미터\n",
    "fixed_params = {'objective': 'binary:logistic',\n",
    "                'learning_rate': 0.02,\n",
    "                'random_state': 1991}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_function(max_depth, subsample, colsample_bytree, min_child_weight,\n",
    "                 reg_alpha, gamma, reg_lambda, scale_pos_weight):\n",
    "    '''최적화하려는 평가지표(지니계수) 계산 함수'''\n",
    "    # 베이지안 최적화를 수행할 하이퍼파라미터\n",
    "    params = {'max_depth': int(round(max_depth)),\n",
    "              'subsample': subsample,\n",
    "              'colsample_bytree': colsample_bytree,\n",
    "              'min_child_weight': min_child_weight,\n",
    "              'gamma': gamma,\n",
    "              'reg_alpha':reg_alpha,\n",
    "              'reg_lambda': reg_lambda,\n",
    "              'scale_pos_weight': scale_pos_weight}\n",
    "    # 값이 고정된 하이퍼파라미터도 추가\n",
    "    params.update(fixed_params)\n",
    "    \n",
    "    print('하이퍼파라미터 :', params)    \n",
    "        \n",
    "    # XGBoost 모델 훈련\n",
    "    xgb_model = xgb.train(params=params, \n",
    "                          dtrain=bayes_dtrain,\n",
    "                          num_boost_round=2000,\n",
    "                          evals=[(bayes_dvalid, 'bayes_dvalid')],\n",
    "                          maximize=True,\n",
    "                          feval=gini_xgb,\n",
    "                          early_stopping_rounds=200,\n",
    "                          verbose_eval=False)\n",
    "                           \n",
    "    best_iter = xgb_model.best_iteration # 최적 반복 횟수\n",
    "    # 검증 데이터로 예측 수행\n",
    "    preds = xgb_model.predict(bayes_dvalid, \n",
    "                              iteration_range=(0, best_iter))\n",
    "    # 지니계수 계산\n",
    "    gini_score = eval_gini(y_valid, preds)\n",
    "    print(f'지니계수 : {gini_score}\\n')\n",
    "    \n",
    "    return gini_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|   iter    |  target   | colsam... |   gamma   | max_depth | min_ch... | reg_alpha | reg_la... | scale_... | subsample |\n",
      "-------------------------------------------------------------------------------------------------------------------------\n",
      "하이퍼파라미터 : {'max_depth': 6, 'subsample': 0.867531900234624, 'colsample_bytree': 0.8646440511781974, 'min_child_weight': 6.0897663659937935, 'gamma': 10.14556809911726, 'reg_alpha': 7.84730959867781, 'reg_lambda': 1.3583576452266626, 'scale_pos_weight': 1.4875174422525386, 'objective': 'binary:logistic', 'learning_rate': 0.02, 'random_state': 1991}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jung0\\miniconda3\\envs\\kaggle\\lib\\site-packages\\xgboost\\training.py:39: UserWarning: `feval` is deprecated, use `custom_metric` instead.  They have different behavior when custom objective is also used.See https://xgboost.readthedocs.io/en/latest/tutorials/custom_metric_obj.html for details on the `custom_metric`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "지니계수 : 0.2852804659784522\n",
      "\n",
      "| \u001b[0m1        \u001b[0m | \u001b[0m0.2853   \u001b[0m | \u001b[0m0.8646   \u001b[0m | \u001b[0m10.15    \u001b[0m | \u001b[0m6.411    \u001b[0m | \u001b[0m6.09     \u001b[0m | \u001b[0m7.847    \u001b[0m | \u001b[0m1.358    \u001b[0m | \u001b[0m1.488    \u001b[0m | \u001b[0m0.8675   \u001b[0m |\n",
      "하이퍼파라미터 : {'max_depth': 7, 'subsample': 0.6261387899104622, 'colsample_bytree': 0.9890988281503088, 'min_child_weight': 6.0577898395058085, 'gamma': 9.150324556477333, 'reg_alpha': 8.136089122187865, 'reg_lambda': 1.4702386553170643, 'scale_pos_weight': 1.4142072116395774, 'objective': 'binary:logistic', 'learning_rate': 0.02, 'random_state': 1991}\n",
      "지니계수 : 0.2848088779310762\n",
      "\n",
      "| \u001b[0m2        \u001b[0m | \u001b[0m0.2848   \u001b[0m | \u001b[0m0.9891   \u001b[0m | \u001b[0m9.15     \u001b[0m | \u001b[0m7.167    \u001b[0m | \u001b[0m6.058    \u001b[0m | \u001b[0m8.136    \u001b[0m | \u001b[0m1.47     \u001b[0m | \u001b[0m1.414    \u001b[0m | \u001b[0m0.6261   \u001b[0m |\n",
      "하이퍼파라미터 : {'max_depth': 7, 'subsample': 0.8341587528859367, 'colsample_bytree': 0.7060655192320977, 'min_child_weight': 6.7400242964936385, 'gamma': 10.497859536643814, 'reg_alpha': 8.957236684465528, 'reg_lambda': 1.4196634256866894, 'scale_pos_weight': 1.4922958724505864, 'objective': 'binary:logistic', 'learning_rate': 0.02, 'random_state': 1991}\n",
      "지니계수 : 0.28504908755804076\n",
      "\n",
      "| \u001b[0m3        \u001b[0m | \u001b[0m0.285    \u001b[0m | \u001b[0m0.7061   \u001b[0m | \u001b[0m10.5     \u001b[0m | \u001b[0m7.113    \u001b[0m | \u001b[0m6.74     \u001b[0m | \u001b[0m8.957    \u001b[0m | \u001b[0m1.42     \u001b[0m | \u001b[0m1.492    \u001b[0m | \u001b[0m0.8342   \u001b[0m |\n",
      "하이퍼파라미터 : {'max_depth': 7, 'subsample': 0.7001630536555632, 'colsample_bytree': 0.8843124587484356, 'min_child_weight': 6.494091293383359, 'gamma': 10.452246227672624, 'reg_alpha': 8.551838810159788, 'reg_lambda': 1.3814765995549108, 'scale_pos_weight': 1.423280772455086, 'objective': 'binary:logistic', 'learning_rate': 0.02, 'random_state': 1991}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jung0\\miniconda3\\envs\\kaggle\\lib\\site-packages\\xgboost\\training.py:39: UserWarning: `feval` is deprecated, use `custom_metric` instead.  They have different behavior when custom objective is also used.See https://xgboost.readthedocs.io/en/latest/tutorials/custom_metric_obj.html for details on the `custom_metric`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "지니계수 : 0.28372532851421195\n",
      "\n",
      "| \u001b[0m4        \u001b[0m | \u001b[0m0.2837   \u001b[0m | \u001b[0m0.8843   \u001b[0m | \u001b[0m10.45    \u001b[0m | \u001b[0m6.838    \u001b[0m | \u001b[0m6.494    \u001b[0m | \u001b[0m8.552    \u001b[0m | \u001b[0m1.381    \u001b[0m | \u001b[0m1.423    \u001b[0m | \u001b[0m0.7002   \u001b[0m |\n",
      "하이퍼파라미터 : {'max_depth': 7, 'subsample': 0.8535233675350644, 'colsample_bytree': 0.92975858050776, 'min_child_weight': 6.249564429359247, 'gamma': 9.95563546750357, 'reg_alpha': 8.411512219837842, 'reg_lambda': 1.424460008293778, 'scale_pos_weight': 1.5416807226581535, 'objective': 'binary:logistic', 'learning_rate': 0.02, 'random_state': 1991}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jung0\\miniconda3\\envs\\kaggle\\lib\\site-packages\\xgboost\\training.py:39: UserWarning: `feval` is deprecated, use `custom_metric` instead.  They have different behavior when custom objective is also used.See https://xgboost.readthedocs.io/en/latest/tutorials/custom_metric_obj.html for details on the `custom_metric`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "지니계수 : 0.28530004505451917\n",
      "\n",
      "| \u001b[95m5        \u001b[0m | \u001b[95m0.2853   \u001b[0m | \u001b[95m0.9298   \u001b[0m | \u001b[95m9.956    \u001b[0m | \u001b[95m6.809    \u001b[0m | \u001b[95m6.25     \u001b[0m | \u001b[95m8.412    \u001b[0m | \u001b[95m1.424    \u001b[0m | \u001b[95m1.542    \u001b[0m | \u001b[95m0.8535   \u001b[0m |\n",
      "하이퍼파라미터 : {'max_depth': 7, 'subsample': 0.6462619019069298, 'colsample_bytree': 0.80929192865947, 'min_child_weight': 6.079999276892042, 'gamma': 9.553916776586505, 'reg_alpha': 8.860396362258099, 'reg_lambda': 1.4050740023119348, 'scale_pos_weight': 1.4668544695338273, 'objective': 'binary:logistic', 'learning_rate': 0.02, 'random_state': 1991}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jung0\\miniconda3\\envs\\kaggle\\lib\\site-packages\\xgboost\\training.py:39: UserWarning: `feval` is deprecated, use `custom_metric` instead.  They have different behavior when custom objective is also used.See https://xgboost.readthedocs.io/en/latest/tutorials/custom_metric_obj.html for details on the `custom_metric`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "지니계수 : 0.2852898511572112\n",
      "\n",
      "| \u001b[0m6        \u001b[0m | \u001b[0m0.2853   \u001b[0m | \u001b[0m0.8093   \u001b[0m | \u001b[0m9.554    \u001b[0m | \u001b[0m6.532    \u001b[0m | \u001b[0m6.08     \u001b[0m | \u001b[0m8.86     \u001b[0m | \u001b[0m1.405    \u001b[0m | \u001b[0m1.467    \u001b[0m | \u001b[0m0.6463   \u001b[0m |\n",
      "하이퍼파라미터 : {'max_depth': 7, 'subsample': 0.6931141936797243, 'colsample_bytree': 0.8817801730078565, 'min_child_weight': 6.992334203641873, 'gamma': 9.013424730095146, 'reg_alpha': 7.640858389939128, 'reg_lambda': 1.3562805915715632, 'scale_pos_weight': 1.449446257931491, 'objective': 'binary:logistic', 'learning_rate': 0.02, 'random_state': 1991}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jung0\\miniconda3\\envs\\kaggle\\lib\\site-packages\\xgboost\\training.py:39: UserWarning: `feval` is deprecated, use `custom_metric` instead.  They have different behavior when custom objective is also used.See https://xgboost.readthedocs.io/en/latest/tutorials/custom_metric_obj.html for details on the `custom_metric`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "지니계수 : 0.28467572646560496\n",
      "\n",
      "| \u001b[0m7        \u001b[0m | \u001b[0m0.2847   \u001b[0m | \u001b[0m0.8818   \u001b[0m | \u001b[0m9.013    \u001b[0m | \u001b[0m6.927    \u001b[0m | \u001b[0m6.992    \u001b[0m | \u001b[0m7.641    \u001b[0m | \u001b[0m1.356    \u001b[0m | \u001b[0m1.449    \u001b[0m | \u001b[0m0.6931   \u001b[0m |\n",
      "하이퍼파라미터 : {'max_depth': 5, 'subsample': 0.6261564417044092, 'colsample_bytree': 0.8763145220620449, 'min_child_weight': 5.135323353557588, 'gamma': 8.39495450163982, 'reg_alpha': 8.950443047087845, 'reg_lambda': 1.4235649099168255, 'scale_pos_weight': 1.5217625173811569, 'objective': 'binary:logistic', 'learning_rate': 0.02, 'random_state': 1991}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jung0\\miniconda3\\envs\\kaggle\\lib\\site-packages\\xgboost\\training.py:39: UserWarning: `feval` is deprecated, use `custom_metric` instead.  They have different behavior when custom objective is also used.See https://xgboost.readthedocs.io/en/latest/tutorials/custom_metric_obj.html for details on the `custom_metric`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "지니계수 : 0.28457181017567373\n",
      "\n",
      "| \u001b[0m8        \u001b[0m | \u001b[0m0.2846   \u001b[0m | \u001b[0m0.8763   \u001b[0m | \u001b[0m8.395    \u001b[0m | \u001b[0m4.561    \u001b[0m | \u001b[0m5.135    \u001b[0m | \u001b[0m8.95     \u001b[0m | \u001b[0m1.424    \u001b[0m | \u001b[0m1.522    \u001b[0m | \u001b[0m0.6262   \u001b[0m |\n",
      "하이퍼파라미터 : {'max_depth': 6, 'subsample': 0.857971740304964, 'colsample_bytree': 0.9583821245229369, 'min_child_weight': 6.158305055403563, 'gamma': 9.305332775334449, 'reg_alpha': 8.200928434091152, 'reg_lambda': 1.2571039588093065, 'scale_pos_weight': 1.4700266933495618, 'objective': 'binary:logistic', 'learning_rate': 0.02, 'random_state': 1991}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jung0\\miniconda3\\envs\\kaggle\\lib\\site-packages\\xgboost\\training.py:39: UserWarning: `feval` is deprecated, use `custom_metric` instead.  They have different behavior when custom objective is also used.See https://xgboost.readthedocs.io/en/latest/tutorials/custom_metric_obj.html for details on the `custom_metric`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "지니계수 : 0.28538973194158385\n",
      "\n",
      "| \u001b[95m9        \u001b[0m | \u001b[95m0.2854   \u001b[0m | \u001b[95m0.9584   \u001b[0m | \u001b[95m9.305    \u001b[0m | \u001b[95m5.594    \u001b[0m | \u001b[95m6.158    \u001b[0m | \u001b[95m8.201    \u001b[0m | \u001b[95m1.257    \u001b[0m | \u001b[95m1.47     \u001b[0m | \u001b[95m0.858    \u001b[0m |\n",
      "=========================================================================================================================\n"
     ]
    }
   ],
   "source": [
    "from bayes_opt import BayesianOptimization\n",
    "\n",
    "# 베이지안 최적화 객체 생성\n",
    "optimizer = BayesianOptimization(f=eval_function, \n",
    "                                 pbounds=param_bounds, \n",
    "                                 random_state=0)\n",
    "\n",
    "# 베이지안 최적화 수행\n",
    "optimizer.maximize(init_points=3, n_iter=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'colsample_bytree': 0.9583821245229369,\n",
       " 'gamma': 9.305332775334449,\n",
       " 'max_depth': 5.594282602920541,\n",
       " 'min_child_weight': 6.158305055403563,\n",
       " 'reg_alpha': 8.200928434091152,\n",
       " 'reg_lambda': 1.2571039588093065,\n",
       " 'scale_pos_weight': 1.4700266933495618,\n",
       " 'subsample': 0.857971740304964}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 평가함수 점수가 최대일 때 하이퍼파라미터\n",
    "max_params = optimizer.max['params']\n",
    "max_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'colsample_bytree': 0.9583821245229369,\n",
       " 'gamma': 9.305332775334449,\n",
       " 'max_depth': 6,\n",
       " 'min_child_weight': 6.158305055403563,\n",
       " 'reg_alpha': 8.200928434091152,\n",
       " 'reg_lambda': 1.2571039588093065,\n",
       " 'scale_pos_weight': 1.4700266933495618,\n",
       " 'subsample': 0.857971740304964,\n",
       " 'objective': 'binary:logistic',\n",
       " 'learning_rate': 0.02,\n",
       " 'random_state': 1991}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 정수형 하이퍼파라미터 변환\n",
    "max_params['max_depth'] = int(round(max_params['max_depth']))\n",
    "\n",
    "# 값이 고정된 하이퍼파라미터 추가\n",
    "max_params.update(fixed_params)\n",
    "max_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "# 층화 K 폴드 교차 검증기 생성\n",
    "folds = StratifiedKFold(n_splits=5, shuffle=True, random_state=1991)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_params_lgb = {\n",
    "    'bagging_fraction': 0.6213108174593661,\n",
    "    'feature_fraction': 0.608712929970154,\n",
    "    'lambda_l1': 0.7040436794880651,\n",
    "    'lambda_l2': 0.9832619845547939,\n",
    "    'min_child_samples': 9,\n",
    "    'min_child_weight': 36.10036444740457,\n",
    "    'num_leaves': 40,\n",
    "    'objective': 'binary',\n",
    "    'learning_rate': 0.005,\n",
    "    'bagging_freq': 1,\n",
    "    'force_row_wise': True,\n",
    "    'random_state': 1991\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "######################################## 폴드 1 / 폴드 5 ########################################\n",
      "[LightGBM] [Info] Number of positive: 17355, number of negative: 458814\n",
      "[LightGBM] [Info] Total Bins 1554\n",
      "[LightGBM] [Info] Number of data points in the train set: 476169, number of used features: 216\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.036447 -> initscore=-3.274764\n",
      "[LightGBM] [Info] Start training from score -3.274764\n",
      "Training until validation scores don't improve for 300 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[2458]\tvalid_0's binary_logloss: 0.151355\tvalid_0's gini: 0.29865\n",
      "폴드 1 지니계수 : 0.2986504843987991\n",
      "\n",
      "######################################## 폴드 2 / 폴드 5 ########################################\n",
      "[LightGBM] [Info] Number of positive: 17355, number of negative: 458814\n",
      "[LightGBM] [Info] Total Bins 1560\n",
      "[LightGBM] [Info] Number of data points in the train set: 476169, number of used features: 216\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.036447 -> initscore=-3.274764\n",
      "[LightGBM] [Info] Start training from score -3.274764\n",
      "Training until validation scores don't improve for 300 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[2334]\tvalid_0's binary_logloss: 0.151736\tvalid_0's gini: 0.285929\n",
      "폴드 2 지니계수 : 0.2859292916021393\n",
      "\n",
      "######################################## 폴드 3 / 폴드 5 ########################################\n",
      "[LightGBM] [Info] Number of positive: 17356, number of negative: 458814\n",
      "[LightGBM] [Info] Total Bins 1558\n",
      "[LightGBM] [Info] Number of data points in the train set: 476170, number of used features: 217\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.036449 -> initscore=-3.274707\n",
      "[LightGBM] [Info] Start training from score -3.274707\n",
      "Training until validation scores don't improve for 300 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1478]\tvalid_0's binary_logloss: 0.151568\tvalid_0's gini: 0.284492\n",
      "폴드 3 지니계수 : 0.2844916047790675\n",
      "\n",
      "######################################## 폴드 4 / 폴드 5 ########################################\n",
      "[LightGBM] [Info] Number of positive: 17355, number of negative: 458815\n",
      "[LightGBM] [Info] Total Bins 1555\n",
      "[LightGBM] [Info] Number of data points in the train set: 476170, number of used features: 216\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.036447 -> initscore=-3.274766\n",
      "[LightGBM] [Info] Start training from score -3.274766\n",
      "Training until validation scores don't improve for 300 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1852]\tvalid_0's binary_logloss: 0.15179\tvalid_0's gini: 0.280514\n",
      "폴드 4 지니계수 : 0.2805136229288192\n",
      "\n",
      "######################################## 폴드 5 / 폴드 5 ########################################\n",
      "[LightGBM] [Info] Number of positive: 17355, number of negative: 458815\n",
      "[LightGBM] [Info] Total Bins 1558\n",
      "[LightGBM] [Info] Number of data points in the train set: 476170, number of used features: 217\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.036447 -> initscore=-3.274766\n",
      "[LightGBM] [Info] Start training from score -3.274766\n",
      "Training until validation scores don't improve for 300 rounds\n",
      "Early stopping, best iteration is:\n",
      "[2045]\tvalid_0's binary_logloss: 0.151689\tvalid_0's gini: 0.295553\n",
      "폴드 5 지니계수 : 0.29555250456072807\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import lightgbm as lgb\n",
    "\n",
    "# OOF 방식으로 훈련된 모델로 검증 데이터 타깃값을 예측한 확률을 담을 1차원 배열\n",
    "oof_val_preds_lgb = np.zeros(X.shape[0]) \n",
    "# OOF 방식으로 훈련된 모델로 테스트 데이터 타깃값을 예측한 확률을 담을 1차원 배열\n",
    "oof_test_preds_lgb = np.zeros(X_test.shape[0]) \n",
    "\n",
    "# OOF 방식으로 모델 훈련, 검증, 예측\n",
    "for idx, (train_idx, valid_idx) in enumerate(folds.split(X, y)):\n",
    "    # 각 폴드를 구분하는 문구 출력\n",
    "    print('#'*40, f'폴드 {idx+1} / 폴드 {folds.n_splits}', '#'*40)\n",
    "    \n",
    "    # 훈련용 데이터, 검증용 데이터 설정\n",
    "    X_train, y_train = X[train_idx], y[train_idx] # 훈련용 데이터\n",
    "    X_valid, y_valid = X[valid_idx], y[valid_idx] # 검증용 데이터\n",
    "\n",
    "    # LightGBM 전용 데이터셋 생성\n",
    "    dtrain = lgb.Dataset(X_train, y_train) # LightGBM 전용 훈련 데이터셋\n",
    "    dvalid = lgb.Dataset(X_valid, y_valid) # LightGBM 전용 검증 데이터셋\n",
    "                          \n",
    "    # LightGBM 모델 훈련\n",
    "    lgb_model = lgb.train(params=max_params_lgb,     # 최적 하이퍼파라미터\n",
    "                          train_set=dtrain,          # 훈련 데이터셋\n",
    "                          num_boost_round=2500,      # 부스팅 반복 횟수\n",
    "                          valid_sets=dvalid,         # 성능 평가용 검증 데이터셋\n",
    "                          feval=gini_lgb,            # 검증용 평가지표\n",
    "                          callbacks=[lgb.early_stopping(300)])\n",
    "    \n",
    "    # 테스트 데이터를 활용해 OOF 예측\n",
    "    oof_test_preds_lgb += lgb_model.predict(X_test)/folds.n_splits\n",
    "    \n",
    "    # 모델 성능 평가를 위한 검증 데이터 타깃값 예측 \n",
    "    oof_val_preds_lgb[valid_idx] += lgb_model.predict(X_valid)\n",
    "    \n",
    "    # 검증 데이터 예측확률에 대한 정규화 지니계수\n",
    "    gini_score = eval_gini(y_valid, oof_val_preds_lgb[valid_idx])\n",
    "    print(f'폴드 {idx+1} 지니계수 : {gini_score}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_params_xgb = {\n",
    "    'colsample_bytree': 0.9583821245229369,\n",
    "    'gamma': 9.305332775334449,\n",
    "    'max_depth': 6,\n",
    "    'min_child_weight': 6.158305055403563,\n",
    "    'reg_alpha': 8.200928434091152,\n",
    "    'reg_lambda': 1.2571039588093065,\n",
    "    'scale_pos_weight': 1.4700266933495618,\n",
    "    'subsample': 0.857971740304964,\n",
    "    'objective': 'binary:logistic',\n",
    "    'learning_rate': 0.02,\n",
    "    'random_state': 1991\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "######################################## 폴드 1 / 폴드 5 ########################################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jung0\\miniconda3\\envs\\kaggle\\lib\\site-packages\\xgboost\\training.py:39: UserWarning: `feval` is deprecated, use `custom_metric` instead.  They have different behavior when custom objective is also used.See https://xgboost.readthedocs.io/en/latest/tutorials/custom_metric_obj.html for details on the `custom_metric`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalid-logloss:0.67671\tvalid-gini:0.16215\n",
      "[100]\tvalid-logloss:0.19182\tvalid-gini:0.24941\n",
      "[200]\tvalid-logloss:0.15837\tvalid-gini:0.27811\n",
      "[300]\tvalid-logloss:0.15506\tvalid-gini:0.28780\n",
      "[400]\tvalid-logloss:0.15452\tvalid-gini:0.29319\n",
      "[500]\tvalid-logloss:0.15439\tvalid-gini:0.29533\n",
      "[600]\tvalid-logloss:0.15433\tvalid-gini:0.29681\n",
      "[700]\tvalid-logloss:0.15427\tvalid-gini:0.29816\n",
      "[800]\tvalid-logloss:0.15425\tvalid-gini:0.29872\n",
      "[900]\tvalid-logloss:0.15422\tvalid-gini:0.29916\n",
      "[1000]\tvalid-logloss:0.15422\tvalid-gini:0.29949\n",
      "[1100]\tvalid-logloss:0.15422\tvalid-gini:0.29951\n",
      "[1200]\tvalid-logloss:0.15422\tvalid-gini:0.29941\n",
      "[1300]\tvalid-logloss:0.15420\tvalid-gini:0.29947\n",
      "[1338]\tvalid-logloss:0.15421\tvalid-gini:0.29945\n",
      "폴드 1 지니계수 : 0.2996383794197772\n",
      "\n",
      "######################################## 폴드 2 / 폴드 5 ########################################\n",
      "[0]\tvalid-logloss:0.67670\tvalid-gini:0.15338\n",
      "[100]\tvalid-logloss:0.19190\tvalid-gini:0.24056\n",
      "[200]\tvalid-logloss:0.15862\tvalid-gini:0.26445\n",
      "[300]\tvalid-logloss:0.15542\tvalid-gini:0.27408\n",
      "[400]\tvalid-logloss:0.15493\tvalid-gini:0.27824\n",
      "[500]\tvalid-logloss:0.15479\tvalid-gini:0.28058\n",
      "[600]\tvalid-logloss:0.15474\tvalid-gini:0.28193\n",
      "[700]\tvalid-logloss:0.15471\tvalid-gini:0.28279\n",
      "[800]\tvalid-logloss:0.15471\tvalid-gini:0.28313\n",
      "[900]\tvalid-logloss:0.15468\tvalid-gini:0.28360\n",
      "[1000]\tvalid-logloss:0.15467\tvalid-gini:0.28425\n",
      "[1100]\tvalid-logloss:0.15467\tvalid-gini:0.28435\n",
      "[1200]\tvalid-logloss:0.15465\tvalid-gini:0.28475\n",
      "[1300]\tvalid-logloss:0.15465\tvalid-gini:0.28490\n",
      "[1400]\tvalid-logloss:0.15464\tvalid-gini:0.28515\n",
      "[1500]\tvalid-logloss:0.15464\tvalid-gini:0.28503\n",
      "[1600]\tvalid-logloss:0.15465\tvalid-gini:0.28503\n",
      "[1635]\tvalid-logloss:0.15464\tvalid-gini:0.28506\n",
      "폴드 2 지니계수 : 0.2852264715520086\n",
      "\n",
      "######################################## 폴드 3 / 폴드 5 ########################################\n",
      "[0]\tvalid-logloss:0.67670\tvalid-gini:0.15662\n",
      "[100]\tvalid-logloss:0.19179\tvalid-gini:0.24702\n",
      "[200]\tvalid-logloss:0.15838\tvalid-gini:0.27073\n",
      "[300]\tvalid-logloss:0.15509\tvalid-gini:0.27895\n",
      "[400]\tvalid-logloss:0.15462\tvalid-gini:0.28197\n",
      "[500]\tvalid-logloss:0.15454\tvalid-gini:0.28295\n",
      "[600]\tvalid-logloss:0.15452\tvalid-gini:0.28350\n",
      "[700]\tvalid-logloss:0.15451\tvalid-gini:0.28375\n",
      "[800]\tvalid-logloss:0.15447\tvalid-gini:0.28444\n",
      "[900]\tvalid-logloss:0.15448\tvalid-gini:0.28427\n",
      "[1000]\tvalid-logloss:0.15448\tvalid-gini:0.28452\n",
      "[1100]\tvalid-logloss:0.15448\tvalid-gini:0.28462\n",
      "[1200]\tvalid-logloss:0.15448\tvalid-gini:0.28461\n",
      "[1300]\tvalid-logloss:0.15449\tvalid-gini:0.28438\n",
      "[1363]\tvalid-logloss:0.15447\tvalid-gini:0.28470\n",
      "폴드 3 지니계수 : 0.28469150190156506\n",
      "\n",
      "######################################## 폴드 4 / 폴드 5 ########################################\n",
      "[0]\tvalid-logloss:0.67670\tvalid-gini:0.16821\n",
      "[100]\tvalid-logloss:0.19173\tvalid-gini:0.23869\n",
      "[200]\tvalid-logloss:0.15846\tvalid-gini:0.26465\n",
      "[300]\tvalid-logloss:0.15526\tvalid-gini:0.27270\n",
      "[400]\tvalid-logloss:0.15482\tvalid-gini:0.27515\n",
      "[500]\tvalid-logloss:0.15472\tvalid-gini:0.27660\n",
      "[600]\tvalid-logloss:0.15467\tvalid-gini:0.27774\n",
      "[700]\tvalid-logloss:0.15464\tvalid-gini:0.27846\n",
      "[800]\tvalid-logloss:0.15462\tvalid-gini:0.27907\n",
      "[900]\tvalid-logloss:0.15461\tvalid-gini:0.27949\n",
      "[1000]\tvalid-logloss:0.15459\tvalid-gini:0.27973\n",
      "[1100]\tvalid-logloss:0.15460\tvalid-gini:0.27975\n",
      "[1200]\tvalid-logloss:0.15460\tvalid-gini:0.27984\n",
      "[1300]\tvalid-logloss:0.15460\tvalid-gini:0.27984\n",
      "[1400]\tvalid-logloss:0.15460\tvalid-gini:0.27981\n",
      "[1425]\tvalid-logloss:0.15461\tvalid-gini:0.27974\n",
      "폴드 4 지니계수 : 0.27991724318908323\n",
      "\n",
      "######################################## 폴드 5 / 폴드 5 ########################################\n",
      "[0]\tvalid-logloss:0.67671\tvalid-gini:0.17061\n",
      "[100]\tvalid-logloss:0.19185\tvalid-gini:0.24638\n",
      "[200]\tvalid-logloss:0.15860\tvalid-gini:0.27270\n",
      "[300]\tvalid-logloss:0.15535\tvalid-gini:0.28310\n",
      "[400]\tvalid-logloss:0.15484\tvalid-gini:0.28760\n",
      "[500]\tvalid-logloss:0.15469\tvalid-gini:0.29034\n",
      "[600]\tvalid-logloss:0.15462\tvalid-gini:0.29223\n",
      "[700]\tvalid-logloss:0.15458\tvalid-gini:0.29313\n",
      "[800]\tvalid-logloss:0.15455\tvalid-gini:0.29412\n",
      "[900]\tvalid-logloss:0.15453\tvalid-gini:0.29438\n",
      "[1000]\tvalid-logloss:0.15452\tvalid-gini:0.29502\n",
      "[1100]\tvalid-logloss:0.15451\tvalid-gini:0.29513\n",
      "[1200]\tvalid-logloss:0.15448\tvalid-gini:0.29535\n",
      "[1300]\tvalid-logloss:0.15449\tvalid-gini:0.29533\n",
      "[1400]\tvalid-logloss:0.15447\tvalid-gini:0.29586\n",
      "[1500]\tvalid-logloss:0.15446\tvalid-gini:0.29596\n",
      "[1600]\tvalid-logloss:0.15446\tvalid-gini:0.29603\n",
      "[1700]\tvalid-logloss:0.15445\tvalid-gini:0.29610\n",
      "[1800]\tvalid-logloss:0.15444\tvalid-gini:0.29622\n",
      "[1900]\tvalid-logloss:0.15445\tvalid-gini:0.29619\n",
      "[1999]\tvalid-logloss:0.15444\tvalid-gini:0.29641\n",
      "폴드 5 지니계수 : 0.2964012751575175\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "# OOF 방식으로 훈련된 모델로 검증 데이터 타깃값을 예측한 확률을 담을 1차원 배열\n",
    "oof_val_preds_xgb = np.zeros(X.shape[0]) \n",
    "# OOF 방식으로 훈련된 모델로 테스트 데이터 타깃값을 예측한 확률을 담을 1차원 배열\n",
    "oof_test_preds_xgb = np.zeros(X_test.shape[0]) \n",
    "\n",
    "# OOF 방식으로 모델 훈련, 검증, 예측\n",
    "for idx, (train_idx, valid_idx) in enumerate(folds.split(X, y)):\n",
    "    # 각 폴드를 구분하는 문구 출력\n",
    "    print('#'*40, f'폴드 {idx+1} / 폴드 {folds.n_splits}', '#'*40)\n",
    "    \n",
    "    # 훈련용 데이터, 검증용 데이터 설정\n",
    "    X_train, y_train = X[train_idx], y[train_idx]\n",
    "    X_valid, y_valid = X[valid_idx], y[valid_idx]\n",
    "\n",
    "    # XGBoost 전용 데이터셋 생성 \n",
    "    dtrain = xgb.DMatrix(X_train, y_train)\n",
    "    dvalid = xgb.DMatrix(X_valid, y_valid)\n",
    "    dtest = xgb.DMatrix(X_test)\n",
    "\n",
    "    # XGBoost 모델 훈련\n",
    "    xgb_model = xgb.train(params=max_params_xgb, \n",
    "                          dtrain=dtrain,\n",
    "                          num_boost_round=2000,\n",
    "                          evals=[(dvalid, 'valid')],\n",
    "                          maximize=True,\n",
    "                          feval=gini_xgb,\n",
    "                          early_stopping_rounds=200,\n",
    "                          verbose_eval=100)\n",
    "\n",
    "    # 모델 성능이 가장 좋을 때의 부스팅 반복 횟수 저장\n",
    "    best_iter = xgb_model.best_iteration\n",
    "\n",
    "    # 테스트 데이터를 활용해 OOF 예측\n",
    "    oof_test_preds_xgb += xgb_model.predict(dtest,\n",
    "                                            iteration_range=(0, best_iter))/folds.n_splits\n",
    "    \n",
    "    # 모델 성능 평가를 위한 검증 데이터 타깃값 예측 \n",
    "    oof_val_preds_xgb[valid_idx] += xgb_model.predict(dvalid, \n",
    "                                                      iteration_range=(0, best_iter))\n",
    "    \n",
    "    # 검증 데이터 예측확률에 대한 정규화 지니계수\n",
    "    gini_score = eval_gini(y_valid, oof_val_preds_xgb[valid_idx])\n",
    "    print(f'폴드 {idx+1} 지니계수 : {gini_score}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LightGBM OOF 검증 데이터 지니계수 : 0.2889651000887542\n"
     ]
    }
   ],
   "source": [
    "print('LightGBM OOF 검증 데이터 지니계수 :', eval_gini(y, oof_val_preds_lgb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost OOF 검증 데이터 지니계수 : 0.28905038063620453\n"
     ]
    }
   ],
   "source": [
    "print('XGBoost OOF 검증 데이터 지니계수 :', eval_gini(y, oof_val_preds_xgb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission['target'] = oof_test_preds_lgb\n",
    "submission.to_csv('submission_lightgbm.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission['target'] = oof_test_preds_xgb\n",
    "submission.to_csv('submission_xgboost.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "oof_test_preds = oof_test_preds_lgb * 0.5 + oof_test_preds_xgb * 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission['target'] = oof_test_preds\n",
    "submission.to_csv('submission.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "snowflakes",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
