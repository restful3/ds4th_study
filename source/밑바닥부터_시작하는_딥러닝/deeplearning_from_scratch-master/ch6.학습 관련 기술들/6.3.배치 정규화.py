"""
배치 정규화 : 앞 절에서는 가중치의 초깃값을 조절해 활성화값을 퍼트렸다.
각 층이 활성화 값을 적당히 퍼트리는 것을 강제하도록 하는 것을 배치 정규화Batch Normalization
이라고 한다.
"""

# 6.3.1 배치 정규화 알고리즘
"""
배치 정규화는 2015년에 제안되었으나 많은 연구자가 사용하고 있다. 배치 정규화의 장점은 다음과 같다.
 * 학습을 빨리 진행할 수 있다.
 * 초깃값에 크게 의존하지 않는다.
 * 오버피팅을 억제한다.(드롭아웃 등의 필요성 감소)

 데이터 분포를 정규화하는 배치 정규화Batch Norm 계층을 신경망에 삽입한다.
 ex) Affine → (BN) → ReLU → Affine → (BN) → ReLU → Affine → Softmax →
학습 시 미니배치를 단위로 데이터 분포가 평균 0, 분산 1이 되도록 정규화한다.
μB ← 1/m * ∑(i=1 to m)(xi)
σB² ← 1/m * ∑(i=1 to m)(xi - μB)²
σB² ← 1/m * ∑(i=1 to m)(xi - μB)²
^xi ← (xi - μB) / √(σB² + ε)

미니배치 B = {x1, ..., xm}의 m개의 입력 데이터 집합에 대해 평균μB와 분산σB²을 구한 뒤
이를 정규화한다. ε(입실론)은 작은 값(10e-7 등)으로 0으로 나누는 것을 예방한다.
이렇게 정규화된 데이터(^xi)를 활성화 함수의 앞(또는 뒤)에 삽입한다.(배치 정규화를 활성화 함수의
앞, 뒤 어느 쪽에 삽입할지에 관한 실험이 진행되고 있다.)

또 배치 정규화 계층마다 이 정규화된 데이터에 고유한 확대scale와 이동shift 변환을 수행한다.
yi ← γ * ^xi + β
γ : 확대
β : 이동
두 값은 처음에는 1, 0(1배 확대, 이동 없음=원본 그대로)에서 시작해서 학습하며 적합한 값으로
조정해간다.
배치 정규화의 계산 그래프는 그림 6-17 참고. 역전파 유도는 생략.
"""

# 6.3.2 배치 정규화의 효과
"""
MNIST 데이터셋을 사용하여 학습진도 차이를 확인한다.
batch_norm_test.py 참고.
대부분의 초깃값 표준편차에서 학습 진도가 빠르다. 배치 정규화를 이용하지 않은 경우
초깃값이 잘 분포되지 않으면 학습이 전혀 되지 않는 경우도 있다.
"""
