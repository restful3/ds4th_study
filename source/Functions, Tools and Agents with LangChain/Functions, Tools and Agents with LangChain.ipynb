{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cff11160-195a-4349-8b24-355079568d94",
   "metadata": {},
   "source": [
    "# OpenAI Function Calling\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5a26f23-b068-431b-8ab5-e92cb6fa3c8e",
   "metadata": {},
   "source": [
    "<img src='./images/fig_01_01.png' width=800>\n",
    "- 몇 달 전에 OpenAI API에 추가된 새로운 기능입니다.\n",
    "- 이 기능을 정확히 어떻게 사용하는지, 그리고 최고의 결과를 얻기 위한 몇 가지 팁과 요령에 대해 알아보겠습니다. \n",
    "- OpenAI는 함수 호출을 위한 추가 매개변수를 수락하도록 최신 모델 중 일부를 미세 조정했습니다.\n",
    "- 이 모델들은 함수 호출이 관련이 있는지 여부를 결정하기 위해 미세 조정되었습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "44f1cc34-64d2-4869-a1e6-3bf42e5b6f5c",
   "metadata": {
    "height": 115,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bfe5563-6266-438f-915c-894e2c13c0ab",
   "metadata": {},
   "source": [
    "- 이 강의에서는 OpenAI SDK를 직접 사용할 것입니다.\n",
    "- 이 예제를 진행하기 위해 언어 모델에 제공하기에 흥미로운 함수가 있다고 가정해 보겠습니다.\n",
    "- 나중에 흥미로운 것이 무엇을 의미하는지 설명하겠지만, 이 새로운 매개변수에는 다양한 사용 사례가 있습니다.\n",
    "- 여기에서 getCurrentWeather 함수를 정의할 것입니다.\n",
    "- 이는 OpenAI가 이 기능을 처음 출시할 때 제공한 예제입니다.\n",
    "- 현재 날씨를 얻는 것은 언어 모델이 자체적으로 할 수 없는 작업이기 때문에 사용하기 좋은 예제입니다.\n",
    "- 따라서 언어 모델을 함수에 연결하고 싶을 때가 많습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cd972358-0ee6-4650-b1db-cc05398c1d5c",
   "metadata": {
    "height": 234
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# 예시 더미 함수는 항상 동일한 날씨 정보를 반환하도록 하드코딩\n",
    "# 실제 환경에서는 백엔드 API 또는 외부 API를 호출할 수 있습니다.\n",
    "def get_current_weather(location, unit=\"fahrenheit\"):\n",
    "    \"\"\"Get the current weather in a given location\"\"\"\n",
    "    weather_info = {\n",
    "        \"location\": location,\n",
    "        \"temperature\": \"72\",\n",
    "        \"unit\": unit,\n",
    "        \"forecast\": [\"sunny\", \"windy\"],\n",
    "    }\n",
    "    return json.dumps(weather_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6499e20a-2424-44f5-aef8-d9264e26203a",
   "metadata": {},
   "source": [
    "- 이 예제에서는 반환되는 정보를 하드 코딩하지만, 실제 환경에서는 날씨 API 또는 외부 지식 소스를 사용할 수 있습니다.\n",
    "- OpenAI는 함수 정의 목록을 전달할 수 있는 함수라는 새로운 매개변수를 노출했습니다.\n",
    "- 위의 전체 함수 정의는 바로 여기에 있습니다. 보시다시피 목록이고 목록에 있는 요소는 하나뿐이며, 여기에는 몇 가지 다른 매개변수를 가진 JSON 객체가 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4513f985-9cf9-4d0b-a142-ef8198d6cfaa",
   "metadata": {
    "height": 319
   },
   "outputs": [],
   "source": [
    "# define a function\n",
    "functions = [\n",
    "    {\n",
    "        \"name\": \"get_current_weather\",\n",
    "        \"description\": \"Get the current weather in a given location\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"location\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"The city and state, e.g. San Francisco, CA\",\n",
    "                },\n",
    "                \"unit\": {\"type\": \"string\", \"enum\": [\"celsius\", \"fahrenheit\"]},\n",
    "            },\n",
    "            \"required\": [\"location\"],\n",
    "        },\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bf67af17-c4d3-4d5a-be3c-771fff2813ff",
   "metadata": {
    "height": 115
   },
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"What's the weather like in Boston?\"\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "32219afd-b6d4-4692-990c-087f845e0124",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fa91c86-f67f-4da0-836b-743587b74cb2",
   "metadata": {},
   "source": [
    "- OpenAI SDK를 가져오고 채팅 완료 엔드포인트를 호출합니다.\n",
    "- 먼저 모델을 지정할 것입니다. 이 기능이 있는 최신 모델을 지정해야 합니다.\n",
    "- 그런 다음 위에서 정의한 메시지를 전달할 것입니다. 실행해보고 결과를 확인해봅시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "13afb042-a9b6-4cde-8808-64ee356518d4",
   "metadata": {
    "height": 98
   },
   "outputs": [],
   "source": [
    "client = OpenAI()\n",
    "response = client.chat.completions.create(\n",
    "      model=\"gpt-3.5-turbo\",\n",
    "    messages=messages,\n",
    "    functions=functions\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e00603f-9f3b-4c16-88fc-6aec4af26747",
   "metadata": {},
   "source": [
    "- 전체 응답을 봅시다. 반환된 메시지에는 assistant 역할이 있고, content는 null이며 대신 name 및 arguments라는 두 개의 객체가 있는 함수 호출 매개변수가 있습니다.\n",
    "- name은 getCurrentWeather입니다. 이는 우리가 전달한 함수의 이름과 동일하며, arguments는 이 JSON blob입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9829e25a-5301-4606-b377-5a7d8655f540",
   "metadata": {
    "height": 30
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatCompletion(id='chatcmpl-9dBE12hQLlJPyy62XXxdlXr6ADlLJ', choices=[Choice(finish_reason='function_call', index=0, logprobs=None, message=ChatCompletionMessage(content=None, role='assistant', function_call=FunctionCall(arguments='{\"location\":\"Boston\"}', name='get_current_weather'), tool_calls=None))], created=1719124969, model='gpt-3.5-turbo-0125', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=15, prompt_tokens=82, total_tokens=97))\n"
     ]
    }
   ],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca917560-ff30-4d85-a163-a864df5597f0",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f9f94e09-8871-445d-ae23-9a41a7f10d95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatCompletionMessage(content=None, role='assistant', function_call=FunctionCall(arguments='{\"location\":\"Boston\"}', name='get_current_weather'), tool_calls=None)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.choices[0].message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cfae511c-4975-4bc2-81b6-404870cca467",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "response_message =response.choices[0].message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5f0cce7b-31b4-4e9d-b14d-477711393197",
   "metadata": {
    "height": 30
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatCompletionMessage(content=None, role='assistant', function_call=FunctionCall(arguments='{\"location\":\"Boston\"}', name='get_current_weather'), tool_calls=None)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response_message"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a995ef2-6359-4325-b4d0-dafa95b6dbd8",
   "metadata": {},
   "source": [
    "- 응답 메시지를 자세히 살펴보겠습니다. 다시 content는 비어 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1a495f0d-a234-4d77-b193-656e67d4f3b2",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "response_message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88a4762d-aaef-43dd-b759-ddd4e9324865",
   "metadata": {},
   "source": [
    "- 함수 호출은 이 사전입니다. 함수 호출의 arguments 매개변수 자체는 JSON 사전이므로 JSON.loads를 사용하여 이를 Python 사전으로 로드할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ce2b8624-bc1e-4b34-96a5-b448c3eeddf6",
   "metadata": {
    "height": 30
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FunctionCall(arguments='{\"location\":\"Boston\"}', name='get_current_weather')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response_message.function_call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b49ced4e-59ee-4c54-b190-b54cdded17d8",
   "metadata": {
    "height": 30
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'location': 'Boston'}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json.loads(response_message.function_call.arguments)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "826d0e48-800a-4b18-b61a-7b9b09e9da4e",
   "metadata": {},
   "source": [
    "- 반환된 arguments를 직접 전달할 수 있습니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "112e8359-6b3d-4099-a0b3-9dc945d41966",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "args = json.loads(response_message.function_call.arguments)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc034d3d-80e4-4fd0-a764-ecf20e29f01c",
   "metadata": {},
   "source": [
    "- OpenAI는 함수를 직접 호출하지 않습니다.\n",
    "- 우리가 직접 해야 합니다. 대신 호출할 함수의 이름과 해당 함수에 전달할 인수를 알려줍니다.\n",
    "- 또한 이것이 JSON blob을 반환하도록 학습되었지만 엄격히 적용되지는 않는다는 점도 주목할 가치가 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e0947c57-6074-49a4-864a-eb19b0764da3",
   "metadata": {
    "height": 30
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"location\": {\"location\": \"Boston\"}, \"temperature\": \"72\", \"unit\": \"fahrenheit\", \"forecast\": [\"sunny\", \"windy\"]}'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_current_weather(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d88ee9ee-62e3-4b30-ae0e-0977ad80c5a2",
   "metadata": {},
   "source": [
    "- 전달한 메시지가 함수와 관련이 없는 경우에는 어떻게 될까요? 한번 시도해보고 어떤 일이 일어나는지 봅시다.\n",
    "- 이번에는 반환된 메시지에 일반적인 내용이 있고 함수 호출 매개변수가 없는 새 메시지 집합을 만들어 보겠습니다.\n",
    "- 내부적으로 모델이 함수를 사용할지 여부를 결정하고 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5aba763a-a364-410f-8882-980c2d5ac5d3",
   "metadata": {
    "height": 115
   },
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"hi!\",\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5322f212-dccb-44e5-a5fa-c9773d6b2911",
   "metadata": {
    "height": 98
   },
   "outputs": [],
   "source": [
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-3.5-turbo-0613\",\n",
    "    messages=messages,\n",
    "    functions=functions,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "aa8c200d-8ced-4067-bc10-5d45a9b0c6f1",
   "metadata": {
    "height": 30
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello! How can I assist you today?\n"
     ]
    }
   ],
   "source": [
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "507abf6d-1279-4503-a8bb-91e1f55db8e7",
   "metadata": {},
   "source": [
    "- 함수를 사용하거나 사용하지 않도록 모델을 강제할 수 있는 추가 매개변수가 있습니다.\n",
    "- 이를 살펴보겠습니다. 추가 매개변수는 함수 호출 매개변수입니다. 기본적으로 auto로 설정되어 있습니다. 이는 언어 모델이 선택함을 의미합니다. 우리가 지금까지 해온 일입니다.\n",
    "- 다른 두 가지 모드가 있습니다. 첫 번째 모드에서는 함수를 호출하도록 강제할 수 있습니다. 항상 함수를 반환하고 싶을 때 유용합니다. 나중에 이 수업에서 그 사용 사례를 살펴보겠습니다.\n",
    "- auto를 사용하고 언어 모델이 무엇을 할지 선택하도록 하므로 여기서 함수 호출이 필요하지 않음을 인식하고 이전과 같이 역할과 내용만 응답하고 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3ba7d504-2c89-4451-954c-7dd86d2c3f81",
   "metadata": {
    "height": 234
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello! How can I assist you today?\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"hi!\",\n",
    "    }\n",
    "]\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-3.5-turbo-0613\",\n",
    "    messages=messages,\n",
    "    functions=functions,\n",
    "    function_call=\"auto\",\n",
    ")\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df966ac4-d538-4f1b-b141-118d00420790",
   "metadata": {},
   "source": [
    "- 함수 호출에 사용할 수 있는 또 다른 모드는 none입니다.\n",
    "- 제공된 함수 중 아무 것도 사용하지 않도록 언어 모델을 강제합니다.\n",
    "- 이 예제에서는 내용이 함수 호출을 필요로 하지 않기 때문에 상관없습니다.\n",
    "- 그러나 메시지가 여기서 함수를 호출해야 할 때는 어떻게 될까요? getCurrentWeather 함수를 호출해야 합니다.\n",
    "- 그러나 아래를 보면 여전히 일반적인 역할과 내용이 보입니다. 이는 함수를 호출하지 않도록 강제하고 있기 때문입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "499bb6b1-2530-47ce-b4fc-f04c0c0f697c",
   "metadata": {
    "height": 234
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello! How can I assist you today?\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"hi!\",\n",
    "    }\n",
    "]\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-3.5-turbo-0613\",\n",
    "    messages=messages,\n",
    "    functions=functions,\n",
    "    function_call=\"none\",\n",
    ")\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "776e3d8c-d592-4994-bc23-b64d3627077b",
   "metadata": {
    "height": 234
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please wait a moment while I check the current weather in Boston.\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"What's the weather in Boston?\",\n",
    "    }\n",
    "]\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-3.5-turbo-0613\",\n",
    "    messages=messages,\n",
    "    functions=functions,\n",
    "    function_call=\"none\",\n",
    ")\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "577aa1b4-cd54-4224-ab76-3e14d154d937",
   "metadata": {},
   "source": [
    "- 함수 호출 매개변수의 마지막 옵션은 함수를 호출하도록 강제하는 것입니다.\n",
    "- 이를 수행하는 방법을 살펴보겠습니다. 응답을 보면 실제로 함수 호출 객체가 반환되었으며 이름이 getCurrentWeather이고 몇 가지 인수가 포함되어 있습니다.\n",
    "- 재미로 함수 호출이 필요 없는 메시지를 전달하지만 강제로 호출하도록 하면 어떻게 되는지 봅시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0b75905b-49be-4811-8d2c-7a365134124a",
   "metadata": {
    "height": 234
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatCompletion(id='chatcmpl-9dBHoehvG6F2DJhSnqX6Lz2ubGD8y', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content=None, role='assistant', function_call=FunctionCall(arguments='{\\n  \"location\": \"San Francisco, CA\"\\n}', name='get_current_weather'), tool_calls=None))], created=1719125204, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=12, prompt_tokens=83, total_tokens=95))\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"hi!\",\n",
    "    }\n",
    "]\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-3.5-turbo-0613\",\n",
    "    messages=messages,\n",
    "    functions=functions,\n",
    "    function_call={\"name\": \"get_current_weather\"},\n",
    ")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "82092993-51e7-47b8-b24c-d72ecb87d469",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b84aed6f-1f80-41d2-bffa-dc3bd3bd8a4f",
   "metadata": {},
   "source": [
    "- 여기서 San Francisco, California를 생성하고 있습니다. 다시 실행하면 계속해서 San Francisco, California를 호출합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "93ab043a-4b16-43b7-9742-350c83172e6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"location\": \"San Francisco, CA\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(response.choices[0].message.function_call.arguments)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "459513d3-0ba3-4192-9dbd-35a9f526058d",
   "metadata": {},
   "source": [
    "- 마지막으로 몇 가지 사항을 정리하겠습니다.\n",
    "- 첫째, 함수 자체와 설명이 OpenAI에 전달하는 토큰 사용 한도에 포함됩니다.\n",
    "- 함수와 함수 호출을 주석 처리하면 프롬프트 토큰 수가 15로 감소하는 것을 볼 수 있습니다.\n",
    "- 이 특정 예제에서는 함수 정의와 함수 설명이 많은 토큰을 차지하고 있습니다.\n",
    "- 이는 OpenAI 모델에 토큰 한도가 있기 때문에 중요합니다.\n",
    "- 따라서 OpenAI에 전달할 메시지를 구성할 때 이를 염두에 두어야 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4d26452c-3db2-452a-aa1b-e1a7ceaec8f5",
   "metadata": {
    "height": 234
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatCompletion(id='chatcmpl-9dBKcaRgxv6AbtK1dEvVns2WMK7O0', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content=None, role='assistant', function_call=FunctionCall(arguments='{\\n\"location\": \"Boston, MA\"\\n}', name='get_current_weather'), tool_calls=None))], created=1719125378, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=10, prompt_tokens=89, total_tokens=99))\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"What's the weather like in Boston!\",\n",
    "    }\n",
    "]\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-3.5-turbo-0613\",\n",
    "    messages=messages,\n",
    "    functions=functions,\n",
    "    function_call={\"name\": \"get_current_weather\"},\n",
    ")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ed86945f-6230-466a-9a1b-f0e751ffb4e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "\"location\": \"Boston, MA\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(response.choices[0].message.function_call.arguments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "96761cf0-fce7-4491-9b16-42efd0700503",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "messages.append(response.choices[0].message)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30f46337-0ad5-41cb-9cd1-6bc4371f47b9",
   "metadata": {},
   "source": [
    "- 마지막으로 이러한 함수 호출 중 일부와 실제로 함수 호출을 수행한 결과를 언어 모델에 다시 전달하는 방법을 살펴보겠습니다.\n",
    "- 이는 종종 언어 모델을 사용하여 호출할 함수를 결정한 다음 해당 함수를 실행하고 그 결과를 언어 모델에 다시 전달하여 최종 응답을 얻고 싶을 때 중요합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "81352f48-5962-4b05-8665-c09a372f3211",
   "metadata": {
    "height": 47
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"location\": {\"location\": \"Boston, MA\"}, \"temperature\": \"72\", \"unit\": \"fahrenheit\", \"forecast\": [\"sunny\", \"windy\"]}\n"
     ]
    }
   ],
   "source": [
    "args = json.loads(response.choices[0].message.function_call.arguments)\n",
    "observation = get_current_weather(args)\n",
    "print(observation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d226e41a-12aa-4c34-af82-27d8d2344e25",
   "metadata": {},
   "source": [
    "- 이 메시지를 가져와 메시지 목록에 추가할 것입니다.\n",
    "- 언어 모델이 제공하는 인수로 getCurrentWeather 함수를 호출하는 것을 시뮬레이션할 수 있습니다.\n",
    "- 이를 변수에 저장한 다음 방금 호출한 함수의 응답을 나타내는 새 메시지를 목록에 추가할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "419608a4-7ccf-4e45-937a-ca8cf771d047",
   "metadata": {
    "height": 132
   },
   "outputs": [],
   "source": [
    "messages.append(\n",
    "        {\n",
    "            \"role\": \"function\",\n",
    "            \"name\": \"get_current_weather\",\n",
    "            \"content\": observation,\n",
    "        }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6088c47-d91e-47d2-9a8f-4d77b20f6fa9",
   "metadata": {},
   "source": [
    "- 새로운 유형의 메시지로 이를 수행합니다. 역할이 함수와 동일합니다.\n",
    "- 이는 함수 호출의 응답임을 언어 모델에 전달하는 데 사용됩니다. 그런 다음 이름도 전달하는데, 이는 함수의 이름입니다.\n",
    "- 그리고 위에서 계산한 observation과 동일하게 설정할 수 있는 content 변수를 전달합니다.\n",
    "- 이 메시지 목록으로 언어 모델을 호출하면 언어 모델이 observation의 응답이 생성됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "44cdd955-e341-44e9-9959-629e6836cc05",
   "metadata": {
    "height": 98
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The current weather in Boston, MA is sunny and windy with a temperature of 72°F.\n"
     ]
    }
   ],
   "source": [
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-3.5-turbo-0613\",\n",
    "    messages=messages,\n",
    ")\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e2b7c78-f4c8-4e2e-b678-8cb409ffeb9a",
   "metadata": {},
   "source": [
    "# LangChain Expression Language (LCEL)\n",
    "\n",
    "<img src='./images/fig_02_01.png' width=800>\n",
    "\n",
    "<img src='./images/fig_02_03.png' width=800>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d471af55-8555-4126-9e98-500c440e6ee0",
   "metadata": {
    "height": 115,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "# import openai\n",
    "\n",
    "# from dotenv import load_dotenv, find_dotenv\n",
    "# _ = load_dotenv(find_dotenv()) # read local .env file\n",
    "# openai.api_key = os.environ['OPENAI_API_KEY']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "975b0204-4e8f-44a0-bc8f-54e6d7c51eea",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "#!pip install pydantic==1.10.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2cf21f0d-aa45-46f4-95f0-e87b11a05d6d",
   "metadata": {
    "height": 64
   },
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.schema.output_parser import StrOutputParser"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fffd74b-406d-4a1e-b7f0-4c863087fda6",
   "metadata": {},
   "source": [
    "## Simple Chain\n",
    "\n",
    "- 단순 체인을 설정합니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d38fa769-2ba3-4977-8565-bcbbd7da0c01",
   "metadata": {
    "height": 98
   },
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"{topic}에 대한 짧은 농담을 말해줘.\"\n",
    ")\n",
    "model = ChatOpenAI()\n",
    "output_parser = StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e56ff396-583b-4441-b1e8-d9356f257bf6",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "chain = prompt | model | output_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8742ae79-c67b-4090-8bf8-cd3f42243394",
   "metadata": {
    "height": 30
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "왜 곰들은 항상 춥죠? \n",
      "\n",
      "왜냐하면 그들은 언제나 겨울모드에 있거든요!\n"
     ]
    }
   ],
   "source": [
    "response = chain.invoke({\"topic\": \"bears\"})\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86672128-055c-4f1f-9fdc-23d38a0fd608",
   "metadata": {},
   "source": [
    "## More complex chain\n",
    "\n",
    "- And Runnable Map to supply user-provided inputs to the prompt.\n",
    "- 이제 더 복잡한 체인을 만들어 보겠습니다. 여기서는 검색 기능이 추가된 생성을 구현합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "199978db-1704-4167-8ea6-0fc0d2c8862f",
   "metadata": {
    "height": 47
   },
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain.vectorstores import DocArrayInMemorySearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5c1819a5-4724-47ac-ac5e-407e2761d935",
   "metadata": {
    "height": 98
   },
   "outputs": [],
   "source": [
    "vectorstore = DocArrayInMemorySearch.from_texts(\n",
    "    [\"harrison worked at kensho\", \"bears like to eat honey\"],\n",
    "    embedding=OpenAIEmbeddings()\n",
    ")\n",
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a88dcadb-7989-46f1-aa54-15dda2f1dfc4",
   "metadata": {
    "height": 30
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(page_content='harrison worked at kensho'), Document(page_content='bears like to eat honey')]\n"
     ]
    }
   ],
   "source": [
    "results = retriever.invoke( \"where did harrison work?\")\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6d21c3bc-909d-45ee-8fe7-d8bc4969c532",
   "metadata": {
    "height": 30
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(page_content='bears like to eat honey'), Document(page_content='harrison worked at kensho')]\n"
     ]
    }
   ],
   "source": [
    "results = retriever.invoke(\"what do bears like to eat?\")\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0afbde46-b034-4f1e-9da2-6cd9d593bb1b",
   "metadata": {
    "height": 115
   },
   "outputs": [],
   "source": [
    "template = \"\"\"Answer the question based only on the following context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "af73d0a8-d78e-4177-b70e-fd6fc63212f2",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "from langchain.schema.runnable import RunnableMap"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15c828dd-afd4-47ef-baca-5355ee5f3acf",
   "metadata": {},
   "source": [
    "- RunnableMap을 사용하여 검색된 문맥과 질문을 매핑합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f378fbe0-027d-4b3a-aed6-333361bb5bf6",
   "metadata": {
    "height": 81
   },
   "outputs": [],
   "source": [
    "chain = RunnableMap({\n",
    "    \"context\": lambda x: retriever.invoke(x[\"question\"]),\n",
    "    \"question\": lambda x: x[\"question\"]\n",
    "}) | prompt | model | output_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b7b6f2cc-1357-4a06-a2e3-c779c03c97b9",
   "metadata": {
    "height": 30
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Harrison worked at Kensho.'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({\"question\": \"where did harrison work?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "32b2c53b-38d8-48c3-986f-831c3aecabfe",
   "metadata": {
    "height": 81
   },
   "outputs": [],
   "source": [
    "inputs = RunnableMap({\n",
    "    \"context\": lambda x: retriever.invoke(x[\"question\"]),\n",
    "    \"question\": lambda x: x[\"question\"]\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "86b99448-17b5-4b09-a21f-815e95eb1de6",
   "metadata": {
    "height": 30
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'context': [Document(page_content='harrison worked at kensho'),\n",
       "  Document(page_content='bears like to eat honey')],\n",
       " 'question': 'where did harrison work?'}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs.invoke({\"question\": \"where did harrison work?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "278d82cc-35de-4829-83f0-791dc459199a",
   "metadata": {},
   "source": [
    "## Bind\n",
    "\n",
    "and OpenAI Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f5c67401-5430-4382-9561-1a39b8d15d78",
   "metadata": {
    "height": 285
   },
   "outputs": [],
   "source": [
    "functions = [\n",
    "    {\n",
    "      \"name\": \"weather_search\",\n",
    "      \"description\": \"Search for weather given an airport code\",\n",
    "      \"parameters\": {\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "          \"airport_code\": {\n",
    "            \"type\": \"string\",\n",
    "            \"description\": \"The airport code to get the weather for\"\n",
    "          },\n",
    "        },\n",
    "        \"required\": [\"airport_code\"]\n",
    "      }\n",
    "    }\n",
    "  ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "03daf6ef-d3f9-44ca-b3a0-59c4bce8fc45",
   "metadata": {
    "height": 115
   },
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"human\", \"{input}\")\n",
    "    ]\n",
    ")\n",
    "model = ChatOpenAI(temperature=0).bind(functions=functions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "81305f2a-442c-44c5-98ad-16ad2438f0fa",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "runnable = prompt | model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "29acdd06-e928-4dfa-93ff-489e6d4fa9ba",
   "metadata": {
    "height": 30
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='', additional_kwargs={'function_call': {'arguments': '{\"airport_code\":\"SFO\"}', 'name': 'weather_search'}}, response_metadata={'token_usage': {'completion_tokens': 16, 'prompt_tokens': 64, 'total_tokens': 80}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': None, 'finish_reason': 'function_call', 'logprobs': None}, id='run-9f9a285c-36f9-43ed-9e48-6d41f1bcf27e-0', usage_metadata={'input_tokens': 64, 'output_tokens': 16, 'total_tokens': 80})"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "runnable.invoke({\"input\": \"what is the weather in sf\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "32608694-3b58-482d-b1ba-0991630471d2",
   "metadata": {
    "height": 523
   },
   "outputs": [],
   "source": [
    "functions = [\n",
    "    {\n",
    "      \"name\": \"weather_search\",\n",
    "      \"description\": \"Search for weather given an airport code\",\n",
    "      \"parameters\": {\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "          \"airport_code\": {\n",
    "            \"type\": \"string\",\n",
    "            \"description\": \"The airport code to get the weather for\"\n",
    "          },\n",
    "        },\n",
    "        \"required\": [\"airport_code\"]\n",
    "      }\n",
    "    },\n",
    "        {\n",
    "      \"name\": \"sports_search\",\n",
    "      \"description\": \"Search for news of recent sport events\",\n",
    "      \"parameters\": {\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "          \"team_name\": {\n",
    "            \"type\": \"string\",\n",
    "            \"description\": \"The sports team to search for\"\n",
    "          },\n",
    "        },\n",
    "        \"required\": [\"team_name\"]\n",
    "      }\n",
    "    }\n",
    "  ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f00dd814-1dd8-4a65-81ba-be845dd9ff9e",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "model = model.bind(functions=functions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "58cbaf33-74d3-496b-955c-db6fa058ca0a",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "runnable = prompt | model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e080c5f4-03ae-48be-8d70-87625a20fe7a",
   "metadata": {
    "height": 30
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='', additional_kwargs={'function_call': {'arguments': '{\"team_name\":\"patriots\"}', 'name': 'sports_search'}}, response_metadata={'token_usage': {'completion_tokens': 18, 'prompt_tokens': 99, 'total_tokens': 117}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': None, 'finish_reason': 'function_call', 'logprobs': None}, id='run-600262c2-ed8a-4adc-859c-3647da56f043-0', usage_metadata={'input_tokens': 99, 'output_tokens': 18, 'total_tokens': 117})"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "runnable.invoke({\"input\": \"how did the patriots do yesterday?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daa8571b-9823-426f-a828-d6a074e7b0bb",
   "metadata": {},
   "source": [
    "- 패트리어츠(Patriots)는 미국의 프로 미식축구 팀인 뉴잉글랜드 패트리어츠(New England Patriots)를 줄여서 부르는 말입니다. 이 팀은 NFL(National Football League)에 속해 있으며, 매사추세츠주 폭스버러에 기반을 두고 있습니다. 패트리어츠는 NFL에서 매우 성공적인 팀 중 하나로, 여러 차례 슈퍼볼 우승을 차지한 경력이 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7993cbe-b7ac-48e5-9113-a9652e83b5ec",
   "metadata": {},
   "source": [
    "## Fallbacks\n",
    "- 이제 에러 처리를 위한 폴백 체인을 설정하겠습니다.\n",
    "- 첫 번째 모델이 실패할 경우 폴백 모델을 사용합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "f9c8219b-96d7-41f3-b5c4-d42610eca331",
   "metadata": {
    "height": 47
   },
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI # 옛날 모델 \n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f884812-7dee-4f05-a61c-968582aa4010",
   "metadata": {},
   "source": [
    "**Note**: Due to the deprecation of OpenAI's model `text-davinci-001` on 4 January 2024, you'll be using OpenAI's recommended replacement model `gpt-3.5-turbo-instruct` instead."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "425709c5-df93-4364-ac93-bbcc20257d1f",
   "metadata": {},
   "source": [
    "- 단순 체인을 설정합니다. 이 체인은 언어 모델의 출력을 JSON으로 디코딩합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "c850a126-1c23-46e3-8858-12bd626c8e13",
   "metadata": {
    "height": 115
   },
   "outputs": [],
   "source": [
    "simple_model = OpenAI(\n",
    "    temperature=0, \n",
    "    max_tokens=1000, \n",
    "    model=\"gpt-3.5-turbo-instruct\"\n",
    ")\n",
    "\n",
    "simple_chain = simple_model | json.loads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "db988a48-2ed1-4e30-bf27-8d3053d08d66",
   "metadata": {},
   "outputs": [],
   "source": [
    "challenge = \"세 개의 시를 JSON 블롭으로 작성하세요. 각 시는 제목, 작가, 첫 줄로 구성된 JSON 블롭입니다.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "9dbd157f-0cf9-4bee-98d8-5418f7249919",
   "metadata": {
    "height": 30
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "{\n",
      "  \"title\": \"봄\",\n",
      "  \"author\": \"김소월\",\n",
      "  \"first_line\": \"봄이 오면 산들도 꽃피고\"\n",
      "}\n",
      "\n",
      "{\n",
      "  \"title\": \"가을\",\n",
      "  \"author\": \"윤동주\",\n",
      "  \"first_line\": \"가을이 오면 나는 더 사랑하리라\"\n",
      "}\n",
      "\n",
      "{\n",
      "  \"title\": \"겨울\",\n",
      "  \"author\": \"정지용\",\n",
      "  \"first_line\": \"겨울이 오면 나는 더 추억하리라\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "response = simple_model.invoke(challenge)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aa40bee-7c0a-4c5b-8ca2-6d797b6090c3",
   "metadata": {},
   "source": [
    "- json 형식 생성에 실패"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "f28d1dfd-67c0-4388-b433-50618c7cac2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c43fcaf7-2cbb-4c7c-90c4-752397fe607c",
   "metadata": {},
   "source": [
    "**Note**: The next line is expected to fail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "8ec2dea7-e730-43b1-82d5-dd9f56b67b3a",
   "metadata": {
    "height": 47
   },
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI # 최근 모델\n",
    "\n",
    "model = ChatOpenAI(temperature=0)\n",
    "chain = model | StrOutputParser() | json.loads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "e7a4ee27-d748-4c4d-ae1c-5fcd4bca2b7c",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "response = chain.invoke(challenge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "43f86d3b-8f83-4be3-85fd-22b2c87c470e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'title': '봄', 'author': '윤동주', 'first_line': '봄이 오나 봄이 오나 봄이 오나'}, {'title': '가을', 'author': '김소월', 'first_line': '하늘 높이 구름 없이 맑은 가을하늘'}, {'title': '겨울', 'author': '이상', 'first_line': '눈 내리는 밤이면 눈 내리는 밤이면'}]\n"
     ]
    }
   ],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83835cf9-da14-4c5c-9dc4-38ed7c0118e9",
   "metadata": {},
   "source": [
    "- 최근 모델 성공"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "85a92c08-57e3-4776-a0af-039671d31e1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(response[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e503a356-1059-4ca4-882a-7282421fa850",
   "metadata": {},
   "source": [
    "- fallbacks 를 쓰면 에러가 나도 원래 chain로 돌아가서 실행을 한다.\n",
    "- 첫 번째 모델이 실패하면 폴백 모델이 호출됨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "36a9b9b1-c4f8-4f2d-94f2-b409f5ae6a42",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "final_chain = simple_chain.with_fallbacks([chain])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "a01ed29a-bcf6-4d00-86ef-8bbb63cc8eb3",
   "metadata": {
    "height": 30
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'title': '봄', 'author': '윤동주', 'first_line': '봄이 오나 봄이 오나 봄이 오나'}, {'title': '가을', 'author': '김소월', 'first_line': '하늘 높이 구름 없이 맑은 가을하늘'}, {'title': '겨울', 'author': '이상', 'first_line': '눈 내리는 밤이면 눈 내리는 밤이면'}]\n"
     ]
    }
   ],
   "source": [
    "response = final_chain.invoke(challenge)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b5dad7a-45e8-4eaf-a125-fb57e1a9240e",
   "metadata": {},
   "source": [
    "## Interface\n",
    "- LCEL을 사용하면 다양한 방식으로 런너블을 호출할 수 있습니다. 여기에는 invoke, stream, batch 메서드가 포함됩니다.\n",
    "  \n",
    "<img src='./images/fig_02_02.png' width=800>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "27cfb0a6-b832-4fe6-9840-c4935e178ce9",
   "metadata": {
    "height": 132
   },
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"Tell me a short joke about {topic}\"\n",
    ")\n",
    "model = ChatOpenAI()\n",
    "output_parser = StrOutputParser()\n",
    "\n",
    "chain = prompt | model | output_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "546b1d66-7ce4-4746-ab95-6f476c34b810",
   "metadata": {
    "height": 30
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Why don't bears wear shoes? \\n\\nBecause they have bear feet!\""
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({\"topic\": \"bears\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3053313-7983-4b0f-94db-50f9a18ff703",
   "metadata": {},
   "source": [
    "- batch 메서드는 여러 입력에 대해 런너블을 호출합니다. 이는 병렬 처리를 통해 성능을 향상시킬 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "781506fa-4e8e-4f57-b929-61265eb9769b",
   "metadata": {
    "height": 30
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"Why did the bear break up with his girlfriend?\\nBecause he couldn't bear the relationship any longer!\",\n",
       " 'Why are frogs so happy?\\n\\nBecause they eat whatever bugs them!']"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.batch([{\"topic\": \"bears\"}, {\"topic\": \"frogs\"}])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16e790e5-ad74-4277-bdcc-29c557dc0de9",
   "metadata": {},
   "source": [
    "- stream 메서드는 스트리밍 방식으로 응답을 반환합니다. 이 방법은 응답을 점진적으로 처리할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "4de381d9-c630-475d-9e89-fef67ce7e101",
   "metadata": {
    "height": 47
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Why\n",
      " don\n",
      "'t\n",
      " bears\n",
      " wear\n",
      " shoes\n",
      "?\n",
      "\n",
      "\n",
      "Because\n",
      " they\n",
      " have\n",
      " bear\n",
      " feet\n",
      "!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for t in chain.stream({\"topic\": \"bears\"}):\n",
    "    print(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14a3e97d-74cb-4a79-9c19-1668bd024ed8",
   "metadata": {},
   "source": [
    "- 모든 동기 메서드는 비동기 버전도 제공됩니다. 예를 들어 ainvoke, astream, abatch 메서드가 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "04484ff4-c948-4a0c-9846-32d6004c7f59",
   "metadata": {
    "height": 47
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Why don't bears like fast food? Because they can't catch it!\""
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = await chain.ainvoke({\"topic\": \"bears\"})\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39d1bd81-7031-4ecc-a093-7bd7f8e965a6",
   "metadata": {},
   "source": [
    "# OpenAI Function Calling In LangChain\n",
    "\n",
    "- OpenAI 함수와 LangChain 표현 언어의 사용법을 소개합니다.\n",
    "- Pydantic 라이브러리를 소개합니다. Pydantic은 OpenAI 함수를 쉽게 만들 수 있게 도와주는 Python 라이브러리입니다.\n",
    "\n",
    "\n",
    "- Pydantic이란?\n",
    "    - Pydantic은 Python의 데이터 검증 라이브러리로, 다양한 스키마를 정의하고 이를 JSON으로 내보내기 쉽게 합니다.\n",
    "    - Pydantic 객체를 사용하여 OpenAI 함수 설명을 작성할 수 있습니다.\n",
    "\n",
    "\n",
    "<img src='./images/fig_03_01.png' width=800>\n",
    "\n",
    "- Pydantic 클래스 정의\n",
    "    - 일반적인 Python 클래스와 유사하며, init 메서드 대신 클래스 정의 아래 속성과 타입을 나열합니다.\n",
    "    - 이 클래스들은 실제로 사용되지는 않고, OpenAI 함수의 JSON을 생성하기 위해 사용됩니다.\n",
    "\n",
    "\n",
    "<img src='./images/fig_03_02.png' width=800>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb41f5f4-df8d-4d04-9eaa-193b8c29b00b",
   "metadata": {
    "height": 115,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv()) # read local .env file\n",
    "openai.api_key = os.environ['OPENAI_API_KEY']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aa1dddf9-8e44-4454-9d44-f8372cccf5ac",
   "metadata": {
    "height": 47
   },
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from pydantic import BaseModel, Field"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cde9c60d-c7f9-4b2d-9f0c-d44fabc06334",
   "metadata": {},
   "source": [
    "## Pydantic Syntax\n",
    "\n",
    "- Pydantic 데이터 클래스는 Python의 데이터 클래스와 Pydantic의 검증 기능이 결합된 것입니다.\n",
    "- 이들은 데이터 구조를 간결하게 정의하면서 데이터가 지정된 타입과 제약 조건을 준수하도록 보장합니다.\n",
    "- 표준 Python에서는 다음과 같이 클래스를 생성합니다:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e1557226-36e2-484b-a2fb-bb7e3180342c",
   "metadata": {
    "height": 98
   },
   "outputs": [],
   "source": [
    "class User:\n",
    "    def __init__(self, name: str, age: int, email: str):\n",
    "        self.name = name\n",
    "        self.age = age\n",
    "        self.email = email"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "11b9b584-74dc-49b8-a7fe-3865368774e9",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "foo = User(name=\"Joe\",age=32, email=\"joe@gmail.com\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0f6f9e9c-b83a-4859-8e65-e6488e05a071",
   "metadata": {
    "height": 30
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Joe'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "foo.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "10a6a0de-d7dc-414d-baaf-fa43c6d1f410",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "foo = User(name=\"Joe\",age=\"bar\", email=\"joe@gmail.com\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "613b7b12-f061-44bc-989d-433cab609164",
   "metadata": {
    "height": 30
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'bar'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "foo.age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c541cb8d-fc55-4c94-a04f-a877cccf10ec",
   "metadata": {
    "height": 81
   },
   "outputs": [],
   "source": [
    "class pUser(BaseModel):\n",
    "    name: str\n",
    "    age: int\n",
    "    email: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "27394d22-73e3-4918-9bdf-18cd7c973942",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "foo_p = pUser(name=\"Jane\", age=32, email=\"jane@gmail.com\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "49f25241-ff47-454f-bac4-ba20ab937d70",
   "metadata": {
    "height": 30
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Jane'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "foo_p.name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "612f7bb1-bfbe-4a96-b45b-76b20d732c0e",
   "metadata": {},
   "source": [
    "**Note**: The next cell is expected to fail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "37030df3-ec11-4523-ac66-b88f90099d1b",
   "metadata": {
    "height": 30
   },
   "outputs": [
    {
     "ename": "ValidationError",
     "evalue": "1 validation error for pUser\nage\n  Input should be a valid integer, unable to parse string as an integer [type=int_parsing, input_value='bar', input_type=str]\n    For further information visit https://errors.pydantic.dev/2.7/v/int_parsing",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValidationError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m foo_p \u001b[38;5;241m=\u001b[39m \u001b[43mpUser\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mJane\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbar\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43memail\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mjane@gmail.com\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/trading/lib/python3.10/site-packages/pydantic/main.py:176\u001b[0m, in \u001b[0;36mBaseModel.__init__\u001b[0;34m(self, **data)\u001b[0m\n\u001b[1;32m    174\u001b[0m \u001b[38;5;66;03m# `__tracebackhide__` tells pytest and some other tools to omit this function from tracebacks\u001b[39;00m\n\u001b[1;32m    175\u001b[0m __tracebackhide__ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 176\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__pydantic_validator__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalidate_python\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mself_instance\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mValidationError\u001b[0m: 1 validation error for pUser\nage\n  Input should be a valid integer, unable to parse string as an integer [type=int_parsing, input_value='bar', input_type=str]\n    For further information visit https://errors.pydantic.dev/2.7/v/int_parsing"
     ]
    }
   ],
   "source": [
    "foo_p = pUser(name=\"Jane\", age=\"bar\", email=\"jane@gmail.com\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "911e7677-cc5d-4957-b6c3-b3ba1493de33",
   "metadata": {
    "height": 47
   },
   "outputs": [],
   "source": [
    "class Class(BaseModel):\n",
    "    students: List[pUser]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "14920e50-688e-4dd4-9207-9b59c6b018c6",
   "metadata": {
    "height": 64
   },
   "outputs": [],
   "source": [
    "obj = Class(\n",
    "    students=[pUser(name=\"Jane\", age=32, email=\"jane@gmail.com\")]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cede1035-7581-4203-bab7-b8e6363c931f",
   "metadata": {
    "height": 30
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Class(students=[pUser(name='Jane', age=32, email='jane@gmail.com')])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obj"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ab03e6e-8224-4575-b704-a413eaebc33e",
   "metadata": {},
   "source": [
    "## Pydantic to OpenAI function definition\n",
    "- 다음은 Pydantic을 사용하여 OpenAI 함수를 생성하는 코드 예제입니다.\n",
    "- WeatherSearch 클래스는 Pydantic의 BaseModel을 상속받아 정의됩니다.\n",
    "- 클래스에는 airport_code라는 문자열 속성이 있으며, 이 속성에는 \"날씨를 가져올 공항 코드\"라는 설명이 붙어 있습니다.\n",
    "- 클래스의 docstring은 \"Call this with an airport code to get the weather at that airport\"로, 이 함수가 특정 공항의 날씨 정보를 가져오는 데 사용된다는 설명을 포함합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "617ceea9-009f-4325-adae-85ab29fccd68",
   "metadata": {
    "height": 64
   },
   "outputs": [],
   "source": [
    "class WeatherSearch(BaseModel):\n",
    "    \"\"\"Call this with an airport code to get the weather at that airport\"\"\"\n",
    "    airport_code: str = Field(description=\"airport code to get weather for\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10511ee2-323d-466b-b861-bfb1286deced",
   "metadata": {},
   "source": [
    "- convert_pydantic_to_openai_function 함수를 사용하여 WeatherSearch 클래스를 OpenAI 함수로 변환합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b22a438c-6692-47f9-9e00-1a95d04c6dd3",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "from langchain_core.utils.function_calling import convert_pydantic_to_openai_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "97e152c4-8d04-4a02-b363-ee7691f60e31",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "weather_function = convert_pydantic_to_openai_function(WeatherSearch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23ec8ba8-2549-4fd8-a873-67b1899992ce",
   "metadata": {},
   "source": [
    "- weather_function 변수는 변환된 OpenAI 함수의 JSON 스키마를 포함합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f36b041e-bd28-4e25-a0c1-fbeeeee4ae53",
   "metadata": {
    "height": 30
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'WeatherSearch',\n",
       " 'description': 'Call this with an airport code to get the weather at that airport',\n",
       " 'parameters': {'properties': {'airport_code': {'description': 'airport code to get weather for',\n",
       "    'type': 'string'}},\n",
       "  'required': ['airport_code'],\n",
       "  'type': 'object'}}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weather_function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d79cb8bc-dcaf-4db9-8e12-c1b572addbe6",
   "metadata": {},
   "source": [
    "- description 이 없으면 에러가 나야 하지만"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c5d7c573-2f84-441d-ab73-a3dd263318d4",
   "metadata": {
    "height": 47
   },
   "outputs": [],
   "source": [
    "class WeatherSearch1(BaseModel):\n",
    "    airport_code: str = Field(description=\"airport code to get weather for\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f659d4bd-03e2-4a9a-be8a-bd4c449773fc",
   "metadata": {},
   "source": [
    "- 지금은 안난다 (구버전은 났음)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "bb08f095-8190-41c5-b49c-e3580cedf992",
   "metadata": {
    "height": 30
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'WeatherSearch1',\n",
       " 'description': '',\n",
       " 'parameters': {'properties': {'airport_code': {'description': 'airport code to get weather for',\n",
       "    'type': 'string'}},\n",
       "  'required': ['airport_code'],\n",
       "  'type': 'object'}}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convert_pydantic_to_openai_function(WeatherSearch1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ed22668a-e188-45a5-844e-deee62f9bf51",
   "metadata": {
    "height": 64
   },
   "outputs": [],
   "source": [
    "class WeatherSearch2(BaseModel):\n",
    "    \"\"\"Call this with an airport code to get the weather at that airport\"\"\"\n",
    "    airport_code: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "9e001e87-4338-4720-99b3-9dc4cb3e4faf",
   "metadata": {
    "height": 30
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'WeatherSearch2',\n",
       " 'description': 'Call this with an airport code to get the weather at that airport',\n",
       " 'parameters': {'properties': {'airport_code': {'type': 'string'}},\n",
       "  'required': ['airport_code'],\n",
       "  'type': 'object'}}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convert_pydantic_to_openai_function(WeatherSearch2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a35261e8-2d36-43a8-a051-a79bef35c8dd",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "93bda0a6-9206-407a-b0da-966f9442a40c",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "model = ChatOpenAI()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85862910-d18e-43d0-8746-06330c789345",
   "metadata": {},
   "source": [
    "- model.invoke(\"what is the weather in SF today?\", functions=[weather_function])를 호출하여 모델에게 \"SF의 오늘 날씨는?\"이라는 질문을 전달합니다.\n",
    "- functions 인수에 weather_function을 전달하여, 모델이 이 질문에 대해 weather_function 함수를 사용할 수 있도록 합니다.\n",
    "- weather_function은 이전에 Pydantic을 사용하여 정의한 날씨 검색 함수로, 공항 코드를 사용하여 해당 공항의 날씨 정보를 가져오는 역할을 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "afe342d4-a7ef-49cd-b760-aa9a176d64d5",
   "metadata": {
    "height": 30
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='', additional_kwargs={'function_call': {'arguments': '{\"airport_code\":\"SFO\"}', 'name': 'WeatherSearch'}}, response_metadata={'token_usage': {'completion_tokens': 17, 'prompt_tokens': 70, 'total_tokens': 87}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': None, 'finish_reason': 'function_call', 'logprobs': None}, id='run-beb9474e-b53d-4a94-90a5-102f49a6102c-0', usage_metadata={'input_tokens': 70, 'output_tokens': 17, 'total_tokens': 87})"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.invoke(\"what is the weather in SF today?\", functions=[weather_function])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7f9ab65-9ccb-47c9-a584-03dbf498fbfe",
   "metadata": {},
   "source": [
    "- model.bind(functions=[weather_function])를 호출하여 모델에 weather_function을 바인딩합니다.\n",
    "- 이렇게 하면 모델 인스턴스가 weather_function을 항상 사용할 수 있게 됩니다.\n",
    "- model_with_function 변수는 함수가 바인딩된 모델 인스턴스를 참조합니다.\n",
    "- model_with_function.invoke(\"what is the weather in sf?\")를 호출하여 모델에게 \"SF의 날씨는?\"이라는 질문을 전달합니다.\n",
    "- 바인딩된 weather_function이 이 질문에 대해 사용됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "511e12b6-bcfb-4862-b377-4251de9969ea",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "model_with_function = model.bind(functions=[weather_function])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "de6241d9-667c-4b97-a50f-95c046fa640c",
   "metadata": {
    "height": 30
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='', additional_kwargs={'function_call': {'arguments': '{\"airport_code\":\"SFO\"}', 'name': 'WeatherSearch'}}, response_metadata={'token_usage': {'completion_tokens': 17, 'prompt_tokens': 69, 'total_tokens': 86}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': None, 'finish_reason': 'function_call', 'logprobs': None}, id='run-46bc28d0-5234-4168-81d6-5517af450401-0', usage_metadata={'input_tokens': 69, 'output_tokens': 17, 'total_tokens': 86})"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_with_function.invoke(\"what is the weather in sf?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2da549f8-0197-4909-b5b3-a39d5c993af3",
   "metadata": {},
   "source": [
    "## Forcing it to use a function\n",
    "\n",
    "- `model.bind(functions=[weather_function], function_call={\"name\":\"WeatherSearch\"})`를 호출하여 모델에 `weather_function`을 바인딩하고, 특정 함수 호출을 강제하도록 설정합니다.\n",
    "  - `functions=[weather_function]`: 모델에 바인딩할 함수 리스트를 지정합니다.\n",
    "  - `function_call={\"name\":\"WeatherSearch\"}`: 모델이 항상 \"WeatherSearch\"라는 이름의 함수를 호출하도록 강제합니다.\n",
    "- `model_with_forced_function` 변수는 함수가 바인딩되고 특정 함수 호출이 강제된 모델 인스턴스를 참조합니다.\n",
    "- `model_with_forced_function.invoke(\"what is the weather in sf?\")`를 호출하여 모델에게 \"SF의 날씨는?\"이라는 질문을 전달합니다.\n",
    "- 이 설정 덕분에 모델은 항상 `WeatherSearch` 함수를 사용하여 질문에 응답하게 됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e7d2fbc0-df39-4e93-a22f-39a6285272b4",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "model_with_forced_function = model.bind(functions=[weather_function], function_call={\"name\":\"WeatherSearch\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "cd9f4063-9e15-41d7-9cf9-253548534176",
   "metadata": {
    "height": 30
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='', additional_kwargs={'function_call': {'arguments': '{\"airport_code\":\"SFO\"}', 'name': 'WeatherSearch'}}, response_metadata={'token_usage': {'completion_tokens': 7, 'prompt_tokens': 79, 'total_tokens': 86}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-fbbbfdf8-e817-4581-908f-e292dcd7ab14-0', usage_metadata={'input_tokens': 79, 'output_tokens': 7, 'total_tokens': 86})"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_with_forced_function.invoke(\"what is the weather in sf?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "314ca7e6-b77c-4b9d-9c93-da6ef3c9c6f8",
   "metadata": {
    "height": 30
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='', additional_kwargs={'function_call': {'arguments': '{\"airport_code\":\"JFK\"}', 'name': 'WeatherSearch'}}, response_metadata={'token_usage': {'completion_tokens': 7, 'prompt_tokens': 74, 'total_tokens': 81}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-1ae737a8-ea56-467e-945b-a6b8de29247a-0', usage_metadata={'input_tokens': 74, 'output_tokens': 7, 'total_tokens': 81})"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_with_forced_function.invoke(\"hi!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3d50c2a-4c93-44f5-a072-9540f23169b5",
   "metadata": {},
   "source": [
    "## Using in a chain\n",
    "\n",
    "\n",
    "다음은 프롬프트 템플릿과 바인딩된 모델을 사용하여 체인을 호출하는 코드 예제입니다.\n",
    "\n",
    "- `ChatPromptTemplate.from_messages`를 사용하여 프롬프트 템플릿을 생성합니다.\n",
    "  - 이 템플릿은 시스템 메시지와 사용자 메시지로 구성됩니다.\n",
    "  - 시스템 메시지: \"You are a helpful assistant\" (당신은 도움이 되는 어시스턴트입니다)\n",
    "  - 사용자 메시지: `{input}` (사용자 입력을 받을 자리 표시자)\n",
    "- `prompt` 변수는 생성된 프롬프트 템플릿을 참조합니다.\n",
    "- `prompt | model_with_function`을 사용하여 프롬프트 템플릿과 바인딩된 모델을 체인으로 결합합니다.\n",
    "  - `|` 연산자는 프롬프트 템플릿의 출력을 모델에 전달하는 체인을 생성합니다.\n",
    "- `chain.invoke({\"input\": \"what is the weather in sf?\"})`를 호출하여 체인에 입력을 전달하고 결과를 가져옵니다.\n",
    "  - 입력 딕셔너리에는 `input` 키가 있으며, 값으로 \"what is the weather in sf?\"를 전달합니다.\n",
    "- 이 체인은 프롬프트 템플릿을 통해 입력을 받아 모델로 전달하고, 모델은 바인딩된 `weather_function`을 사용하여 응답을 생성합니다.\n",
    "\n",
    "이 예제는 프롬프트 템플릿과 바인딩된 모델을 체인으로 결합하여 사용자 입력을 처리하고 모델의 응답을 가져오는 방법을 보여줍니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "c12c86df-d628-4176-9f4e-24fb5a953a5d",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "83f00dc6-5d22-44a5-a0a0-4fe1ab8167ac",
   "metadata": {
    "height": 81
   },
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful assistant\"),\n",
    "    (\"user\", \"{input}\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "1eee3f5f-2176-4777-8c8a-2a197acc47a7",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "chain = prompt | model_with_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "8587907e-b4c3-4acd-9e58-1137047d0fee",
   "metadata": {
    "height": 30
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='', additional_kwargs={'function_call': {'arguments': '{\"airport_code\":\"SFO\"}', 'name': 'WeatherSearch'}}, response_metadata={'token_usage': {'completion_tokens': 17, 'prompt_tokens': 75, 'total_tokens': 92}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': None, 'finish_reason': 'function_call', 'logprobs': None}, id='run-808b6bad-a7fe-44eb-b0b9-a3df9396ec09-0', usage_metadata={'input_tokens': 75, 'output_tokens': 17, 'total_tokens': 92})"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({\"input\": \"what is the weather in sf?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74f05e85-5183-4872-8b1d-cc470fa9c04b",
   "metadata": {},
   "source": [
    "## Using multiple functions\n",
    "\n",
    "다음은 여러 Pydantic 함수를 바인딩하고 호출하는 OpenAI 모델 코드 예제입니다.\n",
    "\n",
    "- `ArtistSearch` 클래스는 Pydantic의 `BaseModel`을 상속받아 정의됩니다.\n",
    "  - `artist_name` 속성은 조회할 아티스트의 이름을 나타내며, 문자열로 타입이 지정되어 있습니다.\n",
    "  - `n` 속성은 결과의 개수를 나타내며, 정수로 타입이 지정되어 있습니다.\n",
    "  - 클래스의 docstring은 \"Call this to get the names of songs by a particular artist\"로, 특정 아티스트의 노래 목록을 가져오는 데 사용된다는 설명을 포함합니다.\n",
    "- `functions` 리스트에는 `WeatherSearch`와 `ArtistSearch` 클래스에서 변환된 OpenAI 함수가 포함됩니다.\n",
    "- `model.bind(functions=functions)`를 호출하여 모델에 여러 함수를 바인딩합니다.\n",
    "- `model_with_functions.invoke(\"what is the weather in sf?\")`를 호출하여 모델에게 \"SF의 날씨는?\"이라는 질문을 전달합니다. 모델은 바인딩된 `weather_function`을 사용하여 응답합니다.\n",
    "- `model_with_functions.invoke(\"what are three songs by taylor swift?\")`를 호출하여 모델에게 \"Taylor Swift의 노래 세 곡은 무엇인가요?\"라는 질문을 전달합니다. 모델은 바인딩된 `artist_function`을 사용하여 응답합니다.\n",
    "- `model_with_functions.invoke(\"hi!\")`를 호출하여 모델에게 \"안녕하세요!\"라는 인사말을 전달합니다. 이 경우, 모델은 함수 호출 없이 일반적인 응답을 제공합니다.\n",
    "\n",
    "이 예제는 여러 Pydantic 함수를 바인딩하여 다양한 질문에 대해 적절한 함수를 선택하여 응답하는 방법을 보여줍니다. 모델은 질문의 내용에 따라 적절한 함수를 호출하거나, 필요하지 않은 경우 일반적인 응답을 제공할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "48c8e42e-f84e-4822-b1ee-a9955fa301c4",
   "metadata": {
    "height": 81
   },
   "outputs": [],
   "source": [
    "class ArtistSearch(BaseModel):\n",
    "    \"\"\"Call this to get the names of songs by a particular artist\"\"\"\n",
    "    artist_name: str = Field(description=\"name of artist to look up\")\n",
    "    n: int = Field(description=\"number of results\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "9a51599b-ee32-4f74-9925-b6264bf43242",
   "metadata": {
    "height": 81
   },
   "outputs": [],
   "source": [
    "functions = [\n",
    "    convert_pydantic_to_openai_function(WeatherSearch),\n",
    "    convert_pydantic_to_openai_function(ArtistSearch),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "e0cc9e3b-ba38-4eff-b285-d02ee5963725",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "model_with_functions = model.bind(functions=functions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "d01cd501-03b4-4207-b1be-0c33c12a0fa5",
   "metadata": {
    "height": 30
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='', additional_kwargs={'function_call': {'arguments': '{\"airport_code\":\"SFO\"}', 'name': 'WeatherSearch'}}, response_metadata={'token_usage': {'completion_tokens': 17, 'prompt_tokens': 116, 'total_tokens': 133}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': None, 'finish_reason': 'function_call', 'logprobs': None}, id='run-597a4b35-9247-467a-8fcf-56df449a87ee-0', usage_metadata={'input_tokens': 116, 'output_tokens': 17, 'total_tokens': 133})"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_with_functions.invoke(\"what is the weather in sf?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "31599518-7387-4d08-9d68-8f5ba7282e8b",
   "metadata": {
    "height": 30
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='', additional_kwargs={'function_call': {'arguments': '{\"artist_name\":\"Taylor Swift\",\"n\":3}', 'name': 'ArtistSearch'}}, response_metadata={'token_usage': {'completion_tokens': 21, 'prompt_tokens': 118, 'total_tokens': 139}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': None, 'finish_reason': 'function_call', 'logprobs': None}, id='run-dc0befe1-cd4b-4d85-b1b1-ec61ef81a792-0', usage_metadata={'input_tokens': 118, 'output_tokens': 21, 'total_tokens': 139})"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_with_functions.invoke(\"what are three songs by taylor swift?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "4e6df989-6ea3-48af-b2c0-10978e0f4142",
   "metadata": {
    "height": 30
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Hello! How can I assist you today?', response_metadata={'token_usage': {'completion_tokens': 10, 'prompt_tokens': 111, 'total_tokens': 121}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-1dc3fbf7-9525-49fc-95af-ac152c36b433-0', usage_metadata={'input_tokens': 111, 'output_tokens': 10, 'total_tokens': 121})"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_with_functions.invoke(\"hi!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef51a000-59f6-4848-b4dc-999ad31612aa",
   "metadata": {},
   "source": [
    "# Tagging and Extraction Using OpenAI functions\n",
    "\n",
    "<img src=\"./images/fig_04_01.png\" widht=800>\n",
    "\n",
    "### 태깅 (Tagging)\n",
    "1. **개요**: 비구조화된 텍스트에 구조화된 설명을 입력하여 구조화된 데이터를 생성.\n",
    "2. **예시**: 텍스트의 감정과 언어 태그를 추출.\n",
    "3. **과정**:\n",
    "   - 비구조화된 텍스트와 구조화된 설명을 입력.\n",
    "   - LLM을 사용하여 텍스트의 감정과 언어를 태그로 추출.\n",
    "   - 구조화된 설명에 따라 결과 생성."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "19349426-38db-4f40-a273-4fd61ee754c5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import os\n",
    "# import openai\n",
    "\n",
    "# from dotenv import load_dotenv, find_dotenv\n",
    "# _ = load_dotenv(find_dotenv()) # read local .env file\n",
    "# openai.api_key = os.environ['OPENAI_API_KEY']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e92e88de-2890-4793-a230-b292eab1462f",
   "metadata": {},
   "source": [
    "1. **설정**:\n",
    "   - 필요한 함수와 클래스 임포트 (typing, Pydantic 등).\n",
    "   - Pydantic 모델 생성 (태깅에 사용).\n",
    "   - 'convert_pydantic_to_openai_function' 메소드 사용."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eb79423e-9499-4353-89fa-3490e9622704",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain_core.utils.function_calling import convert_pydantic_to_openai_function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b9b5f07-95ba-47ac-afa7-67d74febc19c",
   "metadata": {},
   "source": [
    "2. **태깅 기능 구현**:\n",
    "   - 프로토타입 태깅 모델 생성.\n",
    "   - 감정과 언어 태그를 위한 구조화된 설명 추가.\n",
    "   - 모델에 프롬프트와 언어 모델 연결.\n",
    "   - 사용자 입력을 받아 태깅 함수 호출.\n",
    "   - 결과: 감정(positive), 언어(English) 등 추출."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "09630187-8532-4304-96a2-5924f202e7ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tagging(BaseModel):\n",
    "    \"\"\"Tag the piece of text with particular info.\"\"\"\n",
    "    sentiment: str = Field(description=\"sentiment of text, should be `pos`, `neg`, or `neutral`\")\n",
    "    language: str = Field(description=\"language of text (should be ISO 639-1 code)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "385b7222-807c-49a1-8b70-ae122fc89eb5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'Tagging',\n",
       " 'description': 'Tag the piece of text with particular info.',\n",
       " 'parameters': {'properties': {'sentiment': {'description': 'sentiment of text, should be `pos`, `neg`, or `neutral`',\n",
       "    'type': 'string'},\n",
       "   'language': {'description': 'language of text (should be ISO 639-1 code)',\n",
       "    'type': 'string'}},\n",
       "  'required': ['sentiment', 'language'],\n",
       "  'type': 'object'}}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convert_pydantic_to_openai_function(Tagging)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b813c7a3-86f4-45d9-a00d-30d3922e981d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1da15a27-1936-4fe2-b5b4-096278dfbde5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ChatOpenAI(temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "93ef5e15-9508-43ac-9993-089ee891f10f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tagging_functions = [convert_pydantic_to_openai_function(Tagging)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3aeae4e6-a51b-46ca-87eb-cac2225b5f80",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Think carefully, and then tag the text as instructed\"),\n",
    "    (\"user\", \"{input}\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e15f87fb-0d08-4c53-9868-89727f2c145d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_with_functions = model.bind(\n",
    "    functions=tagging_functions,\n",
    "    function_call={\"name\": \"Tagging\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "77450277-2b66-45df-b848-09273ec003ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "tagging_chain = prompt | model_with_functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3f930401-9de3-4371-aa49-68751fa9133d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='', additional_kwargs={'function_call': {'arguments': '{\"sentiment\":\"pos\",\"language\":\"en\"}', 'name': 'Tagging'}}, response_metadata={'token_usage': {'completion_tokens': 10, 'prompt_tokens': 108, 'total_tokens': 118}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-c7883300-4dfe-4628-b030-66da464d0619-0', usage_metadata={'input_tokens': 108, 'output_tokens': 10, 'total_tokens': 118})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tagging_chain.invoke({\"input\": \"I love langchain\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "90d2c6ed-4776-493c-9cd6-045368ba7803",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='', additional_kwargs={'function_call': {'arguments': '{\"sentiment\":\"neg\",\"language\":\"it\"}', 'name': 'Tagging'}}, response_metadata={'token_usage': {'completion_tokens': 10, 'prompt_tokens': 111, 'total_tokens': 121}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-6a867821-3ad7-453d-82d2-e37edcd948ec-0', usage_metadata={'input_tokens': 111, 'output_tokens': 10, 'total_tokens': 121})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tagging_chain.invoke({\"input\": \"non mi piace questo cibo\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8d31c3cf-936e-41b0-b8eb-bc5332ce9c32",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.output_parsers.openai_functions import JsonOutputFunctionsParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "40695dc7-0685-4680-9a15-627e04b54706",
   "metadata": {},
   "outputs": [],
   "source": [
    "tagging_chain = prompt | model_with_functions | JsonOutputFunctionsParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e8a5953b-e1d4-4444-8f6c-0183ead1e0ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sentiment': 'pos', 'language': 'ko'}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tagging_chain.invoke({\"input\": \"이 순대 존맛탱이네.\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3856ea11-eefe-4301-9f1e-30bc8eebcf6d",
   "metadata": {},
   "source": [
    "## Extraction\n",
    "\n",
    "Extraction is similar to tagging, but used for extracting multiple pieces of information.\n",
    "\n",
    "<img src=\"./images/fig_04_02.png\" widht=800>\n",
    "\n",
    "### 추출 (Extraction)\n",
    "1. **개요**: 텍스트에서 특정 엔티티를 추출.\n",
    "2. **차이점**: 태깅과 달리 리스트 형태로 엔티티를 추출.\n",
    "3. **예시**: 기사에서 언급된 논문 리스트 추출."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1815e85b-b913-4a66-88e5-51e3c81e13fd",
   "metadata": {},
   "source": [
    "3. **추출 기능 구현**:\n",
    "   - 다수의 정보를 추출하기 위해 클래스 정의.\n",
    "   - 예: 사람 정보(person schema) 추출.\n",
    "   - 추출한 정보를 리스트로 관리."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "eeb89d48-2897-4550-ad47-a708953a5b90",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "class Person(BaseModel):\n",
    "    \"\"\"Information about a person.\"\"\"\n",
    "    name: str = Field(description=\"person's name\")\n",
    "    age: Optional[int] = Field(description=\"person's age\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3f1ce49b-8efd-4977-afc2-adfe237121dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Information(BaseModel):\n",
    "    \"\"\"Information to extract.\"\"\"\n",
    "    people: List[Person] = Field(description=\"List of info about people\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "de910542-5a50-4b29-8042-b8898b4fb981",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'Information',\n",
       " 'description': 'Information to extract.',\n",
       " 'parameters': {'$defs': {'Person': {'description': 'Information about a person.',\n",
       "    'properties': {'name': {'description': \"person's name\", 'type': 'string'},\n",
       "     'age': {'anyOf': [{'type': 'integer'}, {'type': 'null'}],\n",
       "      'description': \"person's age\"}},\n",
       "    'required': ['name', 'age'],\n",
       "    'type': 'object'}},\n",
       "  'properties': {'people': {'description': 'List of info about people',\n",
       "    'items': {'description': 'Information about a person.',\n",
       "     'properties': {'name': {'description': \"person's name\", 'type': 'string'},\n",
       "      'age': {'anyOf': [{'type': 'integer'}, {'type': 'null'}],\n",
       "       'description': \"person's age\"}},\n",
       "     'required': ['name', 'age'],\n",
       "     'type': 'object'},\n",
       "    'type': 'array'}},\n",
       "  'required': ['people'],\n",
       "  'type': 'object'}}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convert_pydantic_to_openai_function(Information)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fb869447-7b4a-4a6f-8a5a-dad83b647ef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "extraction_functions = [convert_pydantic_to_openai_function(Information)]\n",
    "extraction_model = model.bind(functions=extraction_functions, function_call={\"name\": \"Information\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0ea15346-dce8-4dd7-addd-f8dd8b30c5d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='', additional_kwargs={'function_call': {'arguments': '{\"people\":[{\"name\":\"Joe\",\"age\":30},{\"name\":\"Martha\",\"age\":null}]}', 'name': 'Information'}}, response_metadata={'token_usage': {'completion_tokens': 21, 'prompt_tokens': 95, 'total_tokens': 116}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-a0763e4d-88cc-49b7-9fce-4f30971f659d-0', usage_metadata={'input_tokens': 95, 'output_tokens': 21, 'total_tokens': 116})"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extraction_model.invoke(\"Joe is 30, his mom is Martha\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "fe413b83-570b-48fc-8fc5-e913d45b3d3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Extract the relevant information, if not explicitly provided do not guess. Extract partial info\"),\n",
    "    (\"human\", \"{input}\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "49f637e2-4e2b-4276-a108-fb144e73e51b",
   "metadata": {},
   "outputs": [],
   "source": [
    "extraction_chain = prompt | extraction_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "08c8d00c-a248-460f-868c-f85822a4582e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='', additional_kwargs={'function_call': {'arguments': '{\"people\":[{\"name\":\"Joe\",\"age\":30},{\"name\":\"Martha\",\"age\":null}]}', 'name': 'Information'}}, response_metadata={'token_usage': {'completion_tokens': 21, 'prompt_tokens': 112, 'total_tokens': 133}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-f941acd5-4354-4965-8ef4-413f2cbd3072-0', usage_metadata={'input_tokens': 112, 'output_tokens': 21, 'total_tokens': 133})"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extraction_chain.invoke({\"input\": \"Joe is 30, his mom is Martha\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6634ce07-60f6-49f8-aac0-757ea7aa7fa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "extraction_chain = prompt | extraction_model | JsonOutputFunctionsParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5a9430c6-d142-4277-adb2-353db15dbbfa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'people': [{'name': 'Joe', 'age': 30}, {'name': 'Martha', 'age': None}]}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extraction_chain.invoke({\"input\": \"Joe is 30, his mom is Martha\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c75779d8-3f6d-4661-8c3a-6c179029817f",
   "metadata": {},
   "source": [
    "### 출력 파서 (Output Parser)\n",
    "1. **JSON 출력 파서**: JSON 형식으로 출력된 데이터를 파싱하여 활용.\n",
    "2. **LangChain**: JSON 출력 파서 사용으로 편리한 데이터 활용.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "77db083a-5cf7-40c3-a460-278ac5a74467",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.output_parsers.openai_functions import JsonKeyOutputFunctionsParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c44a89cc-020b-4396-b310-4e179133468d",
   "metadata": {},
   "outputs": [],
   "source": [
    "extraction_chain = prompt | extraction_model | JsonKeyOutputFunctionsParser(key_name=\"people\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "fa3d0f10-8381-43ce-aaf3-cbced8ad4ba2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'name': 'Joe', 'age': 30}, {'name': 'Martha', 'age': None}]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extraction_chain.invoke({\"input\": \"Joe is 30, his mom is Martha\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5790b2a-9e5a-42bf-a035-af170fc3d5b2",
   "metadata": {},
   "source": [
    "## 실제 적용 예시\n",
    "\n",
    "우리는 태깅을 더 큰 텍스트에 적용할 수 있습니다.\n",
    "\n",
    "예를 들어, 이 블로그 포스트를 로드하고 텍스트의 일부에서 태그 정보를 추출해 봅시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "30874116-5689-4a64-ae9e-bcf0000cef04",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    }
   ],
   "source": [
    "from langchain.document_loaders import WebBaseLoader\n",
    "loader = WebBaseLoader(\"https://lilianweng.github.io/posts/2023-06-23-agent/\")\n",
    "documents = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "84874112-27f5-4dcf-9674-be7b0472e1dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = documents[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "861c09cb-8146-4667-90f6-f127df835037",
   "metadata": {},
   "outputs": [],
   "source": [
    "page_content = doc.page_content[:10000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "fb7bd741-9d0e-4dfa-b6ec-1ff58e535868",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "LLM Powered Autonomous Agents | Lil'Log\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Lil'Log\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Posts\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Archive\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Search\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Tags\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "FAQ\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "emojisearch.app\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "      LLM Powered Autonomous Agents\n",
      "    \n",
      "Date: June 23, 2023  |  Estimated Reading Time: 31 min  |  Author: Lilian Weng\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "Table of Contents\n",
      "\n",
      "\n",
      "\n",
      "Agent System Overview\n",
      "\n",
      "Component One: Planning\n",
      "\n",
      "Task Decomposition\n",
      "\n",
      "Self-Reflection\n",
      "\n",
      "\n",
      "Component Two: Memory\n",
      "\n",
      "Types of Memory\n",
      "\n",
      "Maximum Inner Product Search (MIPS)\n",
      "\n",
      "\n",
      "Component Three: Tool Use\n",
      "\n",
      "Case Studies\n",
      "\n",
      "Scientific Discovery Agent\n",
      "\n",
      "Generative Agents Simulation\n",
      "\n",
      "Proof-of-Concept Examples\n",
      "\n",
      "\n",
      "Challenges\n",
      "\n",
      "Citation\n",
      "\n",
      "References\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Building agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general\n"
     ]
    }
   ],
   "source": [
    "print(page_content[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "d4d9c5f6-fbf8-48ae-8ea8-26d8c2a3c27a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Overview(BaseModel):\n",
    "    \"\"\"Overview of a section of text.\"\"\"\n",
    "    summary: str = Field(description=\"Provide a concise summary of the content.\")\n",
    "    language: str = Field(description=\"Provide the language that the content is written in.\")\n",
    "    keywords: str = Field(description=\"Provide keywords related to the content.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "c44ce43b-1f45-46af-8af0-70fbc728d183",
   "metadata": {},
   "outputs": [],
   "source": [
    "overview_tagging_function = [\n",
    "    convert_pydantic_to_openai_function(Overview)\n",
    "]\n",
    "tagging_model = model.bind(\n",
    "    functions=overview_tagging_function,\n",
    "    function_call={\"name\":\"Overview\"}\n",
    ")\n",
    "tagging_chain = prompt | tagging_model | JsonOutputFunctionsParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "bb1a6090-1460-4ec2-ad2a-66430815fd27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'summary': 'Building autonomous agents powered by LLM (large language model) with components like planning, memory, and tool use. Includes task decomposition, self-reflection, and challenges.',\n",
       " 'language': 'English',\n",
       " 'keywords': 'LLM, autonomous agents, planning, memory, tool use, task decomposition, self-reflection, challenges'}"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tagging_chain.invoke({\"input\": page_content})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "4dcb22e7-c163-41c7-8af0-f36e6cc3b80c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Paper(BaseModel):\n",
    "    \"\"\"Information about papers mentioned.\"\"\"\n",
    "    title: str\n",
    "    author: Optional[str]\n",
    "\n",
    "\n",
    "class Info(BaseModel):\n",
    "    \"\"\"Information to extract\"\"\"\n",
    "    papers: List[Paper]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "6feff080-696b-4b37-bc21-3a0408b4244f",
   "metadata": {},
   "outputs": [],
   "source": [
    "paper_extraction_function = [\n",
    "    convert_pydantic_to_openai_function(Info)\n",
    "]\n",
    "extraction_model = model.bind(\n",
    "    functions=paper_extraction_function, \n",
    "    function_call={\"name\":\"Info\"}\n",
    ")\n",
    "extraction_chain = prompt | extraction_model | JsonKeyOutputFunctionsParser(key_name=\"papers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "e5fcb680-ade5-423f-9e81-1efc8bb9fdaa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'title': 'LLM Powered Autonomous Agents', 'author': 'Lilian Weng'}]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extraction_chain.invoke({\"input\": page_content})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "01eda25e-64b3-4367-8935-47ae5260101b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# template = \"\"\"A article will be passed to you. Extract from it all papers that are mentioned by this article. \n",
    "\n",
    "# Do not extract the name of the article itself. If no papers are mentioned that's fine - you don't need to extract any! Just return an empty list.\n",
    "\n",
    "# Do not make up or guess ANY extra information. Only extract what exactly is in the text.\"\"\"\n",
    "\n",
    "template = '''\n",
    "기사가 제공됩니다. 이 기사에서 언급된 모든 논문을 추출하세요.\n",
    "\n",
    "기사의 제목은 추출하지 마세요. 논문이 언급되지 않았다면, 빈 목록을 반환하세요.\n",
    "\n",
    "추가적인 정보를 추측하거나 만들어내지 말고, 텍스트에 정확히 있는 내용만 추출하세요.\n",
    "'''\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", template),\n",
    "    (\"human\", \"{input}\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "093a18b4-ac8b-4bfe-bbdc-e288de222ee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "extraction_chain = prompt | extraction_model | JsonKeyOutputFunctionsParser(key_name=\"papers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "840fec07-f824-4382-aa9c-75e5c7e847f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'title': 'Chain of thought (CoT; Wei et al. 2022)', 'author': None},\n",
       " {'title': 'Tree of Thoughts (Yao et al. 2023)', 'author': None},\n",
       " {'title': 'LLM+P (Liu et al. 2023)', 'author': None},\n",
       " {'title': 'ReAct (Yao et al. 2023)', 'author': None},\n",
       " {'title': 'Reflexion (Shinn & Labash 2023)', 'author': None},\n",
       " {'title': 'Chain of Hindsight (CoH; Liu et al. 2023)', 'author': None},\n",
       " {'title': 'Algorithm Distillation (AD; Laskin et al. 2023)', 'author': None}]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extraction_chain.invoke({\"input\": page_content})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "89952b39-b62d-4581-83c5-9c4a00475319",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extraction_chain.invoke({\"input\": \"hi\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "659a99c7-086d-45a9-9d9e-05a95d41af34",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_overlap=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "78b8f6aa-7b8a-4a5c-a832-7c269bb7b5e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "splits = text_splitter.split_text(doc.page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "3609c483-629f-4528-9b39-cab054459773",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "62a6d54b-d051-43bd-bada-5b54ab6d2df4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten(matrix):\n",
    "    flat_list = []\n",
    "    for row in matrix:\n",
    "        flat_list += row\n",
    "    return flat_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "18165e51-063e-483b-b0f3-4a706a2461af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flatten([[1, 2], [3, 4]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "061f1e5f-a261-4737-a578-894afbd13020",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM Powered Autonomous Agents | Lil'Log\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Lil'Log\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Posts\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Archive\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Search\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Tags\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "FAQ\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "emojisearch.app\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "      LLM Powered Autonomous Agents\n",
      "    \n",
      "Date: June 23, 2023  |  Estimated Reading Time: 31 min  |  Author: Lilian Weng\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "Table of Contents\n",
      "\n",
      "\n",
      "\n",
      "Agent System Overview\n",
      "\n",
      "Component One: Planning\n",
      "\n",
      "Task Decomposition\n",
      "\n",
      "Self-Reflection\n",
      "\n",
      "\n",
      "Component Two: Memory\n",
      "\n",
      "Types of Memory\n",
      "\n",
      "Maximum Inner Product Search (MIPS)\n",
      "\n",
      "\n",
      "Component Three: Tool Use\n",
      "\n",
      "Case Studies\n",
      "\n",
      "Scientific Discovery Agent\n",
      "\n",
      "Generative Agents Simulation\n",
      "\n",
      "Proof-of-Concept Examples\n",
      "\n",
      "\n",
      "Challenges\n",
      "\n",
      "Citation\n",
      "\n",
      "References\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Building agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\n",
      "Agent System Overview#\n",
      "In a LLM-powered autonomous agent system, LLM functions as the agent’s brain, complemented by several key components:\n",
      "\n",
      "Planning\n",
      "\n",
      "Subgoal and decomposition: The agent breaks down large tasks into smaller, manageable subgoals, enabling efficient handling of complex tasks.\n",
      "Reflection and refinement: The agent can do self-criticism and self-reflection over past actions, learn from mistakes and refine them for future steps, thereby improving the quality of final results.\n",
      "\n",
      "\n",
      "Memory\n",
      "\n",
      "Short-term memory: I would consider all the in-context learning (See Prompt Engineering) as utilizing short-term memory of the model to learn.\n",
      "Long-term memory: This provides the agent with the capability to retain and recall (infinite) information over extended periods, often by leveraging an external vector store and fast retrieval.\n",
      "\n",
      "\n",
      "Tool use\n",
      "\n",
      "The agent learns to call external APIs for extra information that is missing from the model weights (often hard to change after pre-training), including current information, code execution capability, access to proprietary information sources and more.\n"
     ]
    }
   ],
   "source": [
    "print(splits[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "fc710e25-2578-41d6-9f61-b0b86eb1379b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema.runnable import RunnableLambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "00cd92bf-2e88-40e8-b1c8-a7e8a7c29804",
   "metadata": {},
   "outputs": [],
   "source": [
    "prep = RunnableLambda(\n",
    "    lambda x: [{\"input\": doc} for doc in text_splitter.split_text(x)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "6c5bfcac-cb1f-4de9-9a79-aa652ed63e5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'input': 'hi'}]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prep.invoke(\"hi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "81ffcd5f-57f6-47d9-95bd-5f9dc6f2bda9",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = prep | extraction_chain.map() | flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "7a05b998-2880-4fc5-bbf2-d8bc662a2867",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = chain.invoke(doc.page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "77c8e463-571b-47a6-b094-f23209777591",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'title': 'LLM Powered Autonomous Agents', 'author': 'Lilian Weng'},\n",
       " {'title': 'Chain of thought', 'author': 'Wei et al. 2022'},\n",
       " {'title': 'Tree of Thoughts', 'author': 'Yao et al. 2023'},\n",
       " {'title': 'LLM+P', 'author': 'Liu et al. 2023'},\n",
       " {'title': 'ReAct', 'author': 'Yao et al. 2023'},\n",
       " {'title': 'Reflexion', 'author': 'Shinn & Labash 2023'},\n",
       " {'title': 'Chain of Hindsight (CoH)', 'author': 'Liu et al. 2023'},\n",
       " {'title': 'Algorithm Distillation (AD)', 'author': 'Laskin et al. 2023'},\n",
       " {'title': 'Algorithm Distillation for In-Context Reinforcement Learning',\n",
       "  'author': 'Laskin et al. 2023'},\n",
       " {'title': 'LSH Forest: Self-Tuning Indexes for Similarity Search',\n",
       "  'author': 'Marius Muja and David G. Lowe'},\n",
       " {'title': 'ANNOY: Approximate Nearest Neighbors Oh Yeah',\n",
       "  'author': 'Erik Bernhardsson'},\n",
       " {'title': 'Efficient and Robust Approximate Nearest Neighbor Search using Hierarchical Navigable Small World Graphs',\n",
       "  'author': 'Yu. A. Malkov and D. A. Yashunin'},\n",
       " {'title': 'FAISS: A library for efficient similarity search',\n",
       "  'author': 'Jeff Johnson, Matthijs Douze, and Hervé Jégou'},\n",
       " {'title': 'ScaNN: Efficient Vector Similarity Search and ANN',\n",
       "  'author': 'Daniel V. Le and Kurt Keutzer'},\n",
       " {'title': 'MRKL: Modular Reasoning, Knowledge and Language',\n",
       "  'author': 'Karpas et al. 2022'},\n",
       " {'title': 'TALM: Tool Augmented Language Models',\n",
       "  'author': 'Parisi et al. 2022'},\n",
       " {'title': 'Toolformer', 'author': 'Schick et al. 2023'},\n",
       " {'title': 'HuggingGPT', 'author': 'Shen et al. 2023'},\n",
       " {'title': 'API-Bank (Li et al. 2023)', 'author': 'Li et al.'},\n",
       " {'title': 'ChemCrow (Bran et al. 2023)', 'author': 'Bran et al.'},\n",
       " {'title': 'Boiko et al. (2023)', 'author': 'Boiko et al.'},\n",
       " {'title': 'Generative Agents Simulation', 'author': 'Park, et al. 2023'},\n",
       " {'title': 'Park et al. 2023', 'author': 'Unknown'},\n",
       " {'title': 'GPT-Engineer: A Project for Code Generation Using Natural Language Instructions',\n",
       "  'author': 'John Smith'},\n",
       " {'title': 'A Novel Approach to Machine Learning', 'author': 'John Smith'},\n",
       " {'title': 'Deep Learning Techniques for Image Recognition',\n",
       "  'author': 'Emily Brown'},\n",
       " {'title': 'A Systematic Mapping Study on Game Development with Python',\n",
       "  'author': 'Rafael Gomes de Oliveira'},\n",
       " {'title': 'Chain of thought prompting elicits reasoning in large language models',\n",
       "  'author': 'Wei et al.'},\n",
       " {'title': 'Tree of Thoughts: Deliberate Problem Solving with Large Language Models',\n",
       "  'author': 'Yao et al.'},\n",
       " {'title': 'Chain of Hindsight Aligns Language Models with Feedback',\n",
       "  'author': 'Liu et al.'},\n",
       " {'title': 'LLM+P: Empowering Large Language Models with Optimal Planning Proficiency',\n",
       "  'author': 'Liu et al.'},\n",
       " {'title': 'ReAct: Synergizing reasoning and acting in language models',\n",
       "  'author': 'Yao et al.'},\n",
       " {'title': 'Reflexion: an autonomous agent with dynamic memory and self-reflection',\n",
       "  'author': 'Shinn & Labash'},\n",
       " {'title': 'In-context Reinforcement Learning with Algorithm Distillation',\n",
       "  'author': 'Laskin et al.'},\n",
       " {'title': 'MRKL Systems A modular, neuro-symbolic architecture that combines large language models, external knowledge sources and discrete reasoning',\n",
       "  'author': 'Karpas et al.'},\n",
       " {'title': 'Webgpt: Browser-assisted question-answering with human feedback',\n",
       "  'author': 'Nakano et al.'},\n",
       " {'title': 'Toolformer: Language Models Can Teach Themselves to Use Tools',\n",
       "  'author': 'Schick et al.'},\n",
       " {'title': 'API-Bank: A Benchmark for Tool-Augmented LLMs',\n",
       "  'author': 'Li et al.'},\n",
       " {'title': 'HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in HuggingFace',\n",
       "  'author': 'Shen et al.'},\n",
       " {'title': 'ChemCrow: Augmenting large-language models with chemistry tools',\n",
       "  'author': 'Bran et al.'},\n",
       " {'title': 'Emergent autonomous scientific research capabilities of large language models',\n",
       "  'author': 'Boiko et al.'},\n",
       " {'title': 'Generative Agents: Interactive Simulacra of Human Behavior',\n",
       "  'author': 'Joon Sung Park, et al.'}]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (trading)",
   "language": "python",
   "name": "trading"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
