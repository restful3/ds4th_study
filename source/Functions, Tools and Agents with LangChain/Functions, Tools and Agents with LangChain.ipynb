{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cff11160-195a-4349-8b24-355079568d94",
   "metadata": {},
   "source": [
    "# OpenAI Function Calling\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5a26f23-b068-431b-8ab5-e92cb6fa3c8e",
   "metadata": {},
   "source": [
    "<img src='./images/fig_01_01.png' width=800>\n",
    "\n",
    "- OpenAI는 함수 호출을 위한 추가 매개변수를 수락하도록 최신 모델 중 일부를 미세 조정했습니다.\n",
    "- 이 모델들은 함수 호출이 관련이 있는지 여부를 결정하기 위해 미세 조정되었습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "44f1cc34-64d2-4869-a1e6-3bf42e5b6f5c",
   "metadata": {
    "height": 115,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bfe5563-6266-438f-915c-894e2c13c0ab",
   "metadata": {},
   "source": [
    "- 이 강의에서는 OpenAI SDK를 직접 사용할 것입니다.\n",
    "- 이 예제를 진행하기 위해 언어 모델에 제공하기에 흥미로운 함수가 있다고 가정해 보겠습니다.\n",
    "- 나중에 흥미로운 것이 무엇을 의미하는지 설명하겠지만, 이 새로운 매개변수에는 다양한 사용 사례가 있습니다.\n",
    "- 여기에서 getCurrentWeather 함수를 정의할 것입니다.\n",
    "- 이는 OpenAI가 이 기능을 처음 출시할 때 제공한 예제입니다.\n",
    "- 현재 날씨를 얻는 것은 언어 모델이 자체적으로 할 수 없는 작업이기 때문에 사용하기 좋은 예제입니다.\n",
    "- 따라서 언어 모델을 함수에 연결하고 싶을 때가 많습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cd972358-0ee6-4650-b1db-cc05398c1d5c",
   "metadata": {
    "height": 234
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# 예시 더미 함수는 항상 동일한 날씨 정보를 반환하도록 하드코딩\n",
    "# 실제 환경에서는 백엔드 API 또는 외부 API를 호출할 수 있습니다.\n",
    "def get_current_weather(location, unit=\"fahrenheit\"):\n",
    "    \"\"\"Get the current weather in a given location\"\"\"\n",
    "    weather_info = {\n",
    "        \"location\": location,\n",
    "        \"temperature\": \"72\",\n",
    "        \"unit\": unit,\n",
    "        \"forecast\": [\"sunny\", \"windy\"],\n",
    "    }\n",
    "    return json.dumps(weather_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6499e20a-2424-44f5-aef8-d9264e26203a",
   "metadata": {},
   "source": [
    "- 이 예제에서는 반환되는 정보를 하드 코딩하지만, 실제 환경에서는 날씨 API 또는 외부 지식 소스를 사용할 수 있습니다.\n",
    "- OpenAI는 함수 정의 목록을 전달할 수 있는 함수라는 새로운 매개변수를 노출했습니다.\n",
    "- 위의 전체 함수 정의는 바로 여기에 있습니다. 보시다시피 목록이고 목록에 있는 요소는 하나뿐이며, 여기에는 몇 가지 다른 매개변수를 가진 JSON 객체가 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4513f985-9cf9-4d0b-a142-ef8198d6cfaa",
   "metadata": {
    "height": 319
   },
   "outputs": [],
   "source": [
    "# define a function\n",
    "functions = [\n",
    "    {\n",
    "        \"name\": \"get_current_weather\",\n",
    "        \"description\": \"Get the current weather in a given location\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"location\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"The city and state, e.g. San Francisco, CA\",\n",
    "                },\n",
    "                \"unit\": {\"type\": \"string\", \"enum\": [\"celsius\", \"fahrenheit\"]},\n",
    "            },\n",
    "            \"required\": [\"location\"],\n",
    "        },\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bf67af17-c4d3-4d5a-be3c-771fff2813ff",
   "metadata": {
    "height": 115
   },
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"What's the weather like in Boston?\"\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "32219afd-b6d4-4692-990c-087f845e0124",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fa91c86-f67f-4da0-836b-743587b74cb2",
   "metadata": {},
   "source": [
    "- OpenAI SDK를 가져오고 채팅 완료 엔드포인트를 호출합니다.\n",
    "- 먼저 모델을 지정할 것입니다. 이 기능이 있는 최신 모델을 지정해야 합니다.\n",
    "- 그런 다음 위에서 정의한 메시지를 전달할 것입니다. 실행해보고 결과를 확인해봅시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "13afb042-a9b6-4cde-8808-64ee356518d4",
   "metadata": {
    "height": 98
   },
   "outputs": [],
   "source": [
    "client = OpenAI()\n",
    "response = client.chat.completions.create(\n",
    "      model=\"gpt-3.5-turbo\",\n",
    "    messages=messages,\n",
    "    functions=functions\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e00603f-9f3b-4c16-88fc-6aec4af26747",
   "metadata": {},
   "source": [
    "- 전체 응답을 봅시다. 반환된 메시지에는 assistant 역할이 있고, content는 null이며 대신 name 및 arguments라는 두 개의 객체가 있는 함수 호출 매개변수가 있습니다.\n",
    "- name은 getCurrentWeather입니다. 이는 우리가 전달한 함수의 이름과 동일하며, arguments는 이 JSON blob입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9829e25a-5301-4606-b377-5a7d8655f540",
   "metadata": {
    "height": 30
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatCompletion(id='chatcmpl-9fH2HgVYPmuUsZk6FBm9yfa19AtLD', choices=[Choice(finish_reason='function_call', index=0, logprobs=None, message=ChatCompletionMessage(content=None, role='assistant', function_call=FunctionCall(arguments='{\"location\":\"Boston\",\"unit\":\"celsius\"}', name='get_current_weather'), tool_calls=None))], created=1719623961, model='gpt-3.5-turbo-0125', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=20, prompt_tokens=82, total_tokens=102))\n"
     ]
    }
   ],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f9f94e09-8871-445d-ae23-9a41a7f10d95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatCompletionMessage(content=None, role='assistant', function_call=FunctionCall(arguments='{\"location\":\"Boston\",\"unit\":\"celsius\"}', name='get_current_weather'), tool_calls=None)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.choices[0].message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cfae511c-4975-4bc2-81b6-404870cca467",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "response_message =response.choices[0].message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5f0cce7b-31b4-4e9d-b14d-477711393197",
   "metadata": {
    "height": 30
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatCompletionMessage(content=None, role='assistant', function_call=FunctionCall(arguments='{\"location\":\"Boston\",\"unit\":\"celsius\"}', name='get_current_weather'), tool_calls=None)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response_message"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a995ef2-6359-4325-b4d0-dafa95b6dbd8",
   "metadata": {},
   "source": [
    "- 응답 메시지를 자세히 살펴보겠습니다. 다시 content는 비어 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1a495f0d-a234-4d77-b193-656e67d4f3b2",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "response_message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88a4762d-aaef-43dd-b759-ddd4e9324865",
   "metadata": {},
   "source": [
    "- function_call은 dictionary 입니다.. function_call의 arguments 매개변수 자체는 JSON 사전이므로 JSON.loads를 사용하여 이를 Python 사전으로 로드할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ce2b8624-bc1e-4b34-96a5-b448c3eeddf6",
   "metadata": {
    "height": 30
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FunctionCall(arguments='{\"location\":\"Boston\",\"unit\":\"celsius\"}', name='get_current_weather')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response_message.function_call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b49ced4e-59ee-4c54-b190-b54cdded17d8",
   "metadata": {
    "height": 30
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'location': 'Boston', 'unit': 'celsius'}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json.loads(response_message.function_call.arguments)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "826d0e48-800a-4b18-b61a-7b9b09e9da4e",
   "metadata": {},
   "source": [
    "- 반환된 arguments를 직접 전달할 수 있습니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "112e8359-6b3d-4099-a0b3-9dc945d41966",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "args = json.loads(response_message.function_call.arguments)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc034d3d-80e4-4fd0-a764-ecf20e29f01c",
   "metadata": {},
   "source": [
    "- OpenAI는 함수를 직접 호출하지 않습니다.\n",
    "- 우리가 직접 해야 합니다. 대신 호출할 함수의 이름과 해당 함수에 전달할 인수를 알려줍니다.\n",
    "- 또한 이것이 JSON blob을 반환하도록 학습되었지만 엄격히 적용되지는 않는다는 점도 주목할 가치가 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e0947c57-6074-49a4-864a-eb19b0764da3",
   "metadata": {
    "height": 30
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"location\": {\"location\": \"Boston\", \"unit\": \"celsius\"}, \"temperature\": \"72\", \"unit\": \"fahrenheit\", \"forecast\": [\"sunny\", \"windy\"]}'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_current_weather(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d88ee9ee-62e3-4b30-ae0e-0977ad80c5a2",
   "metadata": {},
   "source": [
    "- 전달한 메시지가 함수와 관련이 없는 경우에는 어떻게 될까요? 한번 시도해보고 어떤 일이 일어나는지 봅시다.\n",
    "- 이번에는 반환된 메시지에 일반적인 내용이 있고 함수 호출 매개변수가 없는 새 메시지 집합을 만들어 보겠습니다.\n",
    "- 내부적으로 모델이 함수를 사용할지 여부를 결정하고 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5aba763a-a364-410f-8882-980c2d5ac5d3",
   "metadata": {
    "height": 115
   },
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"hi!\",\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5322f212-dccb-44e5-a5fa-c9773d6b2911",
   "metadata": {
    "height": 98
   },
   "outputs": [],
   "source": [
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-3.5-turbo-0613\",\n",
    "    messages=messages,\n",
    "    functions=functions,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "aa8c200d-8ced-4067-bc10-5d45a9b0c6f1",
   "metadata": {
    "height": 30
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello! How can I assist you today?\n"
     ]
    }
   ],
   "source": [
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "507abf6d-1279-4503-a8bb-91e1f55db8e7",
   "metadata": {},
   "source": [
    "- 함수를 사용하거나 사용하지 않도록 모델을 강제할 수 있는 추가 매개변수가 있습니다.\n",
    "- 이를 살펴보겠습니다. 추가 매개변수는 함수 호출 매개변수입니다. 기본적으로 auto로 설정되어 있습니다. 이는 언어 모델이 선택함을 의미합니다. 우리가 지금까지 해온 일입니다.\n",
    "- 다른 두 가지 모드가 있습니다. 첫 번째 모드에서는 함수를 호출하도록 강제할 수 있습니다. 항상 함수를 반환하고 싶을 때 유용합니다. 나중에 이 수업에서 그 사용 사례를 살펴보겠습니다.\n",
    "- auto를 사용하고 언어 모델이 무엇을 할지 선택하도록 하므로 여기서 함수 호출이 필요하지 않음을 인식하고 이전과 같이 역할과 내용만 응답하고 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3ba7d504-2c89-4451-954c-7dd86d2c3f81",
   "metadata": {
    "height": 234
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello! How can I assist you today?\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"hi!\",\n",
    "    }\n",
    "]\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-3.5-turbo-0613\",\n",
    "    messages=messages,\n",
    "    functions=functions,\n",
    "    function_call=\"auto\",\n",
    ")\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df966ac4-d538-4f1b-b141-118d00420790",
   "metadata": {},
   "source": [
    "- 함수 호출에 사용할 수 있는 또 다른 모드는 none입니다.\n",
    "- 제공된 함수 중 아무 것도 사용하지 않도록 언어 모델을 강제합니다.\n",
    "- 이 예제에서는 내용이 함수 호출을 필요로 하지 않기 때문에 상관없습니다.\n",
    "- 그러나 메시지가 여기서 함수를 호출해야 할 때는 어떻게 될까요? getCurrentWeather 함수를 호출해야 합니다.\n",
    "- 그러나 아래를 보면 여전히 일반적인 역할과 내용이 보입니다. 이는 함수를 호출하지 않도록 강제하고 있기 때문입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "499bb6b1-2530-47ce-b4fc-f04c0c0f697c",
   "metadata": {
    "height": 234
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello! How can I assist you today?\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"hi!\",\n",
    "    }\n",
    "]\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-3.5-turbo-0613\",\n",
    "    messages=messages,\n",
    "    functions=functions,\n",
    "    function_call=\"none\",\n",
    ")\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "776e3d8c-d592-4994-bc23-b64d3627077b",
   "metadata": {
    "height": 234
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The current weather in Boston is currently unavailable. Please try again later.\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"What's the weather in Boston?\",\n",
    "    }\n",
    "]\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-3.5-turbo-0613\",\n",
    "    messages=messages,\n",
    "    functions=functions,\n",
    "    function_call=\"none\",\n",
    ")\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "577aa1b4-cd54-4224-ab76-3e14d154d937",
   "metadata": {},
   "source": [
    "- 함수 호출 매개변수의 마지막 옵션은 함수를 호출하도록 강제하는 것입니다.\n",
    "- 이를 수행하는 방법을 살펴보겠습니다. 응답을 보면 실제로 함수 호출 객체가 반환되었으며 이름이 getCurrentWeather이고 몇 가지 인수가 포함되어 있습니다.\n",
    "- 재미로 함수 호출이 필요 없는 메시지를 전달하지만 강제로 호출하도록 하면 어떻게 되는지 봅시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0b75905b-49be-4811-8d2c-7a365134124a",
   "metadata": {
    "height": 234
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatCompletion(id='chatcmpl-9fH6kjk3a5lQ1hMyDUZ75erpqDeyc', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content=None, role='assistant', function_call=FunctionCall(arguments='{\\n  \"location\": \"San Francisco, CA\"\\n}', name='get_current_weather'), tool_calls=None))], created=1719624238, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=12, prompt_tokens=83, total_tokens=95))\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"hi!\",\n",
    "    }\n",
    "]\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-3.5-turbo-0613\",\n",
    "    messages=messages,\n",
    "    functions=functions,\n",
    "    function_call={\"name\": \"get_current_weather\"},\n",
    ")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "82092993-51e7-47b8-b24c-d72ecb87d469",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b84aed6f-1f80-41d2-bffa-dc3bd3bd8a4f",
   "metadata": {},
   "source": [
    "- 여기서 San Francisco, California를 생성하고 있습니다. 다시 실행하면 계속해서 San Francisco, California를 호출합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "93ab043a-4b16-43b7-9742-350c83172e6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"location\": \"San Francisco, CA\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(response.choices[0].message.function_call.arguments)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "459513d3-0ba3-4192-9dbd-35a9f526058d",
   "metadata": {},
   "source": [
    "- 마지막으로 몇 가지 사항을 정리하겠습니다.\n",
    "- 첫째, 함수 자체와 설명이 OpenAI에 전달하는 토큰 사용 한도에 포함됩니다.\n",
    "- 함수와 함수 호출을 주석 처리하면 프롬프트 토큰 수가 15로 감소하는 것을 볼 수 있습니다.\n",
    "- 이 특정 예제에서는 함수 정의와 함수 설명이 많은 토큰을 차지하고 있습니다.\n",
    "- 이는 OpenAI 모델에 토큰 한도가 있기 때문에 중요합니다.\n",
    "- 따라서 OpenAI에 전달할 메시지를 구성할 때 이를 염두에 두어야 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4d26452c-3db2-452a-aa1b-e1a7ceaec8f5",
   "metadata": {
    "height": 234
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatCompletion(id='chatcmpl-9dBKcaRgxv6AbtK1dEvVns2WMK7O0', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content=None, role='assistant', function_call=FunctionCall(arguments='{\\n\"location\": \"Boston, MA\"\\n}', name='get_current_weather'), tool_calls=None))], created=1719125378, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=10, prompt_tokens=89, total_tokens=99))\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"What's the weather like in Boston!\",\n",
    "    }\n",
    "]\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-3.5-turbo-0613\",\n",
    "    messages=messages,\n",
    "    functions=functions,\n",
    "    function_call={\"name\": \"get_current_weather\"},\n",
    ")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ed86945f-6230-466a-9a1b-f0e751ffb4e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "\"location\": \"Boston, MA\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(response.choices[0].message.function_call.arguments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "96761cf0-fce7-4491-9b16-42efd0700503",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "messages.append(response.choices[0].message)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30f46337-0ad5-41cb-9cd1-6bc4371f47b9",
   "metadata": {},
   "source": [
    "- 마지막으로 이러한 함수 호출 중 일부와 실제로 함수 호출을 수행한 결과를 언어 모델에 다시 전달하는 방법을 살펴보겠습니다.\n",
    "- 이는 종종 언어 모델을 사용하여 호출할 함수를 결정한 다음 해당 함수를 실행하고 그 결과를 언어 모델에 다시 전달하여 최종 응답을 얻고 싶을 때 중요합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "81352f48-5962-4b05-8665-c09a372f3211",
   "metadata": {
    "height": 47
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"location\": {\"location\": \"Boston, MA\"}, \"temperature\": \"72\", \"unit\": \"fahrenheit\", \"forecast\": [\"sunny\", \"windy\"]}\n"
     ]
    }
   ],
   "source": [
    "args = json.loads(response.choices[0].message.function_call.arguments)\n",
    "observation = get_current_weather(args)\n",
    "print(observation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d226e41a-12aa-4c34-af82-27d8d2344e25",
   "metadata": {},
   "source": [
    "- 이 메시지를 가져와 메시지 목록에 추가할 것입니다.\n",
    "- 언어 모델이 제공하는 인수로 getCurrentWeather 함수를 호출하는 것을 시뮬레이션할 수 있습니다.\n",
    "- 이를 변수에 저장한 다음 방금 호출한 함수의 응답을 나타내는 새 메시지를 목록에 추가할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "419608a4-7ccf-4e45-937a-ca8cf771d047",
   "metadata": {
    "height": 132
   },
   "outputs": [],
   "source": [
    "messages.append(\n",
    "        {\n",
    "            \"role\": \"function\",\n",
    "            \"name\": \"get_current_weather\",\n",
    "            \"content\": observation,\n",
    "        }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6088c47-d91e-47d2-9a8f-4d77b20f6fa9",
   "metadata": {},
   "source": [
    "- 새로운 유형의 메시지로 이를 수행합니다. 역할이 함수와 동일합니다.\n",
    "- 이는 함수 호출의 응답임을 언어 모델에 전달하는 데 사용됩니다. 그런 다음 이름도 전달하는데, 이는 함수의 이름입니다.\n",
    "- 그리고 위에서 계산한 observation과 동일하게 설정할 수 있는 content 변수를 전달합니다.\n",
    "- 이 메시지 목록으로 언어 모델을 호출하면 언어 모델이 observation의 응답이 생성됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "44cdd955-e341-44e9-9959-629e6836cc05",
   "metadata": {
    "height": 98
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The current weather in Boston, MA is sunny and windy with a temperature of 72°F.\n"
     ]
    }
   ],
   "source": [
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-3.5-turbo-0613\",\n",
    "    messages=messages,\n",
    ")\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e2b7c78-f4c8-4e2e-b678-8cb409ffeb9a",
   "metadata": {},
   "source": [
    "# LangChain Expression Language (LCEL)\n",
    "\n",
    "<img src='./images/fig_02_01.png' width=800>\n",
    "\n",
    "<img src='./images/fig_02_03.png' width=800>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d471af55-8555-4126-9e98-500c440e6ee0",
   "metadata": {
    "height": 115,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "# import openai\n",
    "\n",
    "# from dotenv import load_dotenv, find_dotenv\n",
    "# _ = load_dotenv(find_dotenv()) # read local .env file\n",
    "# openai.api_key = os.environ['OPENAI_API_KEY']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "975b0204-4e8f-44a0-bc8f-54e6d7c51eea",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "#!pip install pydantic==1.10.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2cf21f0d-aa45-46f4-95f0-e87b11a05d6d",
   "metadata": {
    "height": 64
   },
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.schema.output_parser import StrOutputParser"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fffd74b-406d-4a1e-b7f0-4c863087fda6",
   "metadata": {},
   "source": [
    "## Simple Chain\n",
    "\n",
    "- 단순 체인을 설정합니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d38fa769-2ba3-4977-8565-bcbbd7da0c01",
   "metadata": {
    "height": 98
   },
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"{topic}에 대한 짧은 농담을 말해줘.\"\n",
    ")\n",
    "model = ChatOpenAI()\n",
    "output_parser = StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e56ff396-583b-4441-b1e8-d9356f257bf6",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "chain = prompt | model | output_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "8742ae79-c67b-4090-8bf8-cd3f42243394",
   "metadata": {
    "height": 30
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "왜 곰들이 항상 떨고 있는가? \n",
      "\n",
      "- 왜냐하면, 그들이 항상 뚜루루루루~하는거야!\n"
     ]
    }
   ],
   "source": [
    "response = chain.invoke({\"topic\": \"bears\"})\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86672128-055c-4f1f-9fdc-23d38a0fd608",
   "metadata": {},
   "source": [
    "## More complex chain\n",
    "\n",
    "- And Runnable Map to supply user-provided inputs to the prompt.\n",
    "- 이제 더 복잡한 체인을 만들어 보겠습니다. 여기서는 검색 기능이 추가된 생성을 구현합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "199978db-1704-4167-8ea6-0fc0d2c8862f",
   "metadata": {
    "height": 47
   },
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain.vectorstores import DocArrayInMemorySearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "5c1819a5-4724-47ac-ac5e-407e2761d935",
   "metadata": {
    "height": 98
   },
   "outputs": [],
   "source": [
    "vectorstore = DocArrayInMemorySearch.from_texts(\n",
    "    [\"harrison worked at kensho\", \"bears like to eat honey\"],\n",
    "    embedding=OpenAIEmbeddings()\n",
    ")\n",
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a88dcadb-7989-46f1-aa54-15dda2f1dfc4",
   "metadata": {
    "height": 30
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(page_content='harrison worked at kensho'), Document(page_content='bears like to eat honey')]\n"
     ]
    }
   ],
   "source": [
    "results = retriever.invoke( \"where did harrison work?\")\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "6d21c3bc-909d-45ee-8fe7-d8bc4969c532",
   "metadata": {
    "height": 30
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(page_content='bears like to eat honey'), Document(page_content='harrison worked at kensho')]\n"
     ]
    }
   ],
   "source": [
    "results = retriever.invoke(\"what do bears like to eat?\")\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "0afbde46-b034-4f1e-9da2-6cd9d593bb1b",
   "metadata": {
    "height": 115
   },
   "outputs": [],
   "source": [
    "template = \"\"\"Answer the question based only on the following context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "af73d0a8-d78e-4177-b70e-fd6fc63212f2",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "from langchain.schema.runnable import RunnableMap"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15c828dd-afd4-47ef-baca-5355ee5f3acf",
   "metadata": {},
   "source": [
    "- RunnableMap을 사용하여 검색된 문맥과 질문을 매핑합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "f378fbe0-027d-4b3a-aed6-333361bb5bf6",
   "metadata": {
    "height": 81
   },
   "outputs": [],
   "source": [
    "chain = RunnableMap({\n",
    "    \"context\": lambda x: retriever.invoke(x[\"question\"]),\n",
    "    \"question\": lambda x: x[\"question\"]\n",
    "}) | prompt | model | output_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "b7b6f2cc-1357-4a06-a2e3-c779c03c97b9",
   "metadata": {
    "height": 30
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Harrison worked at Kensho.'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({\"question\": \"where did harrison work?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "32b2c53b-38d8-48c3-986f-831c3aecabfe",
   "metadata": {
    "height": 81
   },
   "outputs": [],
   "source": [
    "inputs = RunnableMap({\n",
    "    \"context\": lambda x: retriever.invoke(x[\"question\"]),\n",
    "    \"question\": lambda x: x[\"question\"]\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "86b99448-17b5-4b09-a21f-815e95eb1de6",
   "metadata": {
    "height": 30
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'context': [Document(page_content='harrison worked at kensho'),\n",
       "  Document(page_content='bears like to eat honey')],\n",
       " 'question': 'where did harrison work?'}"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs.invoke({\"question\": \"where did harrison work?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "278d82cc-35de-4829-83f0-791dc459199a",
   "metadata": {},
   "source": [
    "## Bind\n",
    "\n",
    "and OpenAI Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "f5c67401-5430-4382-9561-1a39b8d15d78",
   "metadata": {
    "height": 285
   },
   "outputs": [],
   "source": [
    "functions = [\n",
    "    {\n",
    "      \"name\": \"weather_search\",\n",
    "      \"description\": \"Search for weather given an airport code\",\n",
    "      \"parameters\": {\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "          \"airport_code\": {\n",
    "            \"type\": \"string\",\n",
    "            \"description\": \"The airport code to get the weather for\"\n",
    "          },\n",
    "        },\n",
    "        \"required\": [\"airport_code\"]\n",
    "      }\n",
    "    }\n",
    "  ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "03daf6ef-d3f9-44ca-b3a0-59c4bce8fc45",
   "metadata": {
    "height": 115
   },
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"human\", \"{input}\")\n",
    "    ]\n",
    ")\n",
    "model = ChatOpenAI(temperature=0).bind(functions=functions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "81305f2a-442c-44c5-98ad-16ad2438f0fa",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "runnable = prompt | model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "29acdd06-e928-4dfa-93ff-489e6d4fa9ba",
   "metadata": {
    "height": 30
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='', additional_kwargs={'function_call': {'arguments': '{\"airport_code\":\"SFO\"}', 'name': 'weather_search'}}, response_metadata={'token_usage': {'completion_tokens': 16, 'prompt_tokens': 64, 'total_tokens': 80}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': None, 'finish_reason': 'function_call', 'logprobs': None}, id='run-499d115c-93fc-400f-8259-ed87b088c51c-0', usage_metadata={'input_tokens': 64, 'output_tokens': 16, 'total_tokens': 80})"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "runnable.invoke({\"input\": \"what is the weather in sf\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "32608694-3b58-482d-b1ba-0991630471d2",
   "metadata": {
    "height": 523
   },
   "outputs": [],
   "source": [
    "functions = [\n",
    "    {\n",
    "      \"name\": \"weather_search\",\n",
    "      \"description\": \"Search for weather given an airport code\",\n",
    "      \"parameters\": {\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "          \"airport_code\": {\n",
    "            \"type\": \"string\",\n",
    "            \"description\": \"The airport code to get the weather for\"\n",
    "          },\n",
    "        },\n",
    "        \"required\": [\"airport_code\"]\n",
    "      }\n",
    "    },\n",
    "        {\n",
    "      \"name\": \"sports_search\",\n",
    "      \"description\": \"Search for news of recent sport events\",\n",
    "      \"parameters\": {\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "          \"team_name\": {\n",
    "            \"type\": \"string\",\n",
    "            \"description\": \"The sports team to search for\"\n",
    "          },\n",
    "        },\n",
    "        \"required\": [\"team_name\"]\n",
    "      }\n",
    "    }\n",
    "  ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "f00dd814-1dd8-4a65-81ba-be845dd9ff9e",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "model = model.bind(functions=functions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "58cbaf33-74d3-496b-955c-db6fa058ca0a",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "runnable = prompt | model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "e080c5f4-03ae-48be-8d70-87625a20fe7a",
   "metadata": {
    "height": 30
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='', additional_kwargs={'function_call': {'arguments': '{\"team_name\":\"patriots\"}', 'name': 'sports_search'}}, response_metadata={'token_usage': {'completion_tokens': 18, 'prompt_tokens': 99, 'total_tokens': 117}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': None, 'finish_reason': 'function_call', 'logprobs': None}, id='run-20a90b10-f8e7-4ed6-ab8b-309225e69304-0', usage_metadata={'input_tokens': 99, 'output_tokens': 18, 'total_tokens': 117})"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "runnable.invoke({\"input\": \"how did the patriots do yesterday?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daa8571b-9823-426f-a828-d6a074e7b0bb",
   "metadata": {},
   "source": [
    "- 패트리어츠(Patriots)는 미국의 프로 미식축구 팀인 뉴잉글랜드 패트리어츠(New England Patriots)를 줄여서 부르는 말입니다. 이 팀은 NFL(National Football League)에 속해 있으며, 매사추세츠주 폭스버러에 기반을 두고 있습니다. 패트리어츠는 NFL에서 매우 성공적인 팀 중 하나로, 여러 차례 슈퍼볼 우승을 차지한 경력이 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7993cbe-b7ac-48e5-9113-a9652e83b5ec",
   "metadata": {},
   "source": [
    "## Fallbacks\n",
    "- 이제 에러 처리를 위한 폴백 체인을 설정하겠습니다.\n",
    "- 첫 번째 모델이 실패할 경우 폴백 모델을 사용합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "f9c8219b-96d7-41f3-b5c4-d42610eca331",
   "metadata": {
    "height": 47
   },
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI # 옛날 모델 \n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f884812-7dee-4f05-a61c-968582aa4010",
   "metadata": {},
   "source": [
    "**Note**: Due to the deprecation of OpenAI's model `text-davinci-001` on 4 January 2024, you'll be using OpenAI's recommended replacement model `gpt-3.5-turbo-instruct` instead."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "425709c5-df93-4364-ac93-bbcc20257d1f",
   "metadata": {},
   "source": [
    "- 단순 체인을 설정합니다. 이 체인은 언어 모델의 출력을 JSON으로 디코딩합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "c850a126-1c23-46e3-8858-12bd626c8e13",
   "metadata": {
    "height": 115
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/restful3/anaconda3/envs/trading/lib/python3.10/site-packages/langchain_core/_api/deprecation.py:119: LangChainDeprecationWarning: The class `OpenAI` was deprecated in LangChain 0.0.10 and will be removed in 0.3.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import OpenAI`.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "simple_model = OpenAI(\n",
    "    temperature=0, \n",
    "    max_tokens=1000, \n",
    "    model=\"gpt-3.5-turbo-instruct\"\n",
    ")\n",
    "\n",
    "simple_chain = simple_model | json.loads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "db988a48-2ed1-4e30-bf27-8d3053d08d66",
   "metadata": {},
   "outputs": [],
   "source": [
    "challenge = \"세 개의 시를 JSON 블롭으로 작성하세요. 각 시는 제목, 작가, 첫 줄로 구성된 JSON 블롭입니다.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "9dbd157f-0cf9-4bee-98d8-5418f7249919",
   "metadata": {
    "height": 30
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "{\n",
      "  \"title\": \"봄\",\n",
      "  \"author\": \"김소월\",\n",
      "  \"first_line\": \"봄이 오면 산들도 꽃피고\"\n",
      "}\n",
      "\n",
      "{\n",
      "  \"title\": \"가을\",\n",
      "  \"author\": \"윤동주\",\n",
      "  \"first_line\": \"가을이 오면 나는 노래한다\"\n",
      "}\n",
      "\n",
      "{\n",
      "  \"title\": \"겨울\",\n",
      "  \"author\": \"정지용\",\n",
      "  \"first_line\": \"겨울이 오면 나는 눈을 바라본다\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "response = simple_model.invoke(challenge)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aa40bee-7c0a-4c5b-8ca2-6d797b6090c3",
   "metadata": {},
   "source": [
    "- json 형식 생성에 실패"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "f28d1dfd-67c0-4388-b433-50618c7cac2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c43fcaf7-2cbb-4c7c-90c4-752397fe607c",
   "metadata": {},
   "source": [
    "**Note**: The next line is expected to fail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "8ec2dea7-e730-43b1-82d5-dd9f56b67b3a",
   "metadata": {
    "height": 47
   },
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI # 최근 모델\n",
    "\n",
    "model = ChatOpenAI(temperature=0)\n",
    "chain = model | StrOutputParser() | json.loads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "e7a4ee27-d748-4c4d-ae1c-5fcd4bca2b7c",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "response = chain.invoke(challenge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "43f86d3b-8f83-4be3-85fd-22b2c87c470e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'title': '봄', 'author': '윤동주', 'first_line': '봄이 오나 봄이 오나 봄이 오나'}, {'title': '가을', 'author': '김소월', 'first_line': '하늘 높이 구름 없이 맑은 가을하늘'}, {'title': '겨울', 'author': '이상', 'first_line': '눈 내리는 밤이면 눈 내리는 밤이면'}]\n"
     ]
    }
   ],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83835cf9-da14-4c5c-9dc4-38ed7c0118e9",
   "metadata": {},
   "source": [
    "- 최근 모델 성공"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "85a92c08-57e3-4776-a0af-039671d31e1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(response[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e503a356-1059-4ca4-882a-7282421fa850",
   "metadata": {},
   "source": [
    "- fallbacks 를 쓰면 에러가 나도 원래 chain로 돌아가서 실행을 한다.\n",
    "- 첫 번째 모델이 실패하면 폴백 모델이 호출됨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "36a9b9b1-c4f8-4f2d-94f2-b409f5ae6a42",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "final_chain = simple_chain.with_fallbacks([chain])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "a01ed29a-bcf6-4d00-86ef-8bbb63cc8eb3",
   "metadata": {
    "height": 30
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'title': '봄', 'author': '윤동주', 'first_line': '봄날의 햇살이 부드럽게 내린다.'}, {'title': '가을', 'author': '김소월', 'first_line': '단풍잎이 바람에 스치며 날아간다.'}, {'title': '겨울', 'author': '이상', 'first_line': '하얀 눈이 하늘에서 내리고 있다.'}]\n"
     ]
    }
   ],
   "source": [
    "response = final_chain.invoke(challenge)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "4c3826bc-ebbb-4210-8855-13f35afb7755",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(response[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b5dad7a-45e8-4eaf-a125-fb57e1a9240e",
   "metadata": {},
   "source": [
    "## Interface\n",
    "- LCEL을 사용하면 다양한 방식으로 런너블을 호출할 수 있습니다. 여기에는 invoke, stream, batch 메서드가 포함됩니다.\n",
    "  \n",
    "<img src='./images/fig_02_02.png' width=800>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "27cfb0a6-b832-4fe6-9840-c4935e178ce9",
   "metadata": {
    "height": 132
   },
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"Tell me a short joke about {topic}\"\n",
    ")\n",
    "model = ChatOpenAI()\n",
    "output_parser = StrOutputParser()\n",
    "\n",
    "chain = prompt | model | output_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "546b1d66-7ce4-4746-ab95-6f476c34b810",
   "metadata": {
    "height": 30
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Why don't bears wear shoes? \\n\\nBecause they have bear feet!\""
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({\"topic\": \"bears\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3053313-7983-4b0f-94db-50f9a18ff703",
   "metadata": {},
   "source": [
    "- batch 메서드는 여러 입력에 대해 런너블을 호출합니다. 이는 병렬 처리를 통해 성능을 향상시킬 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "781506fa-4e8e-4f57-b929-61265eb9769b",
   "metadata": {
    "height": 30
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"Why did the bear break up with his girlfriend?\\nBecause he couldn't bear the relationship anymore!\",\n",
       " 'Why are frogs so happy? Because they eat whatever bugs them!']"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.batch([{\"topic\": \"bears\"}, {\"topic\": \"frogs\"}])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16e790e5-ad74-4277-bdcc-29c557dc0de9",
   "metadata": {},
   "source": [
    "- stream 메서드는 스트리밍 방식으로 응답을 반환합니다. 이 방법은 응답을 점진적으로 처리할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "4de381d9-c630-475d-9e89-fef67ce7e101",
   "metadata": {
    "height": 47
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Why\n",
      " do\n",
      " bears\n",
      " have\n",
      " hairy\n",
      " coats\n",
      "?\n",
      "\n",
      "\n",
      "F\n",
      "ur\n",
      " protection\n",
      "!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for t in chain.stream({\"topic\": \"bears\"}):\n",
    "    print(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14a3e97d-74cb-4a79-9c19-1668bd024ed8",
   "metadata": {},
   "source": [
    "- 모든 동기 메서드는 비동기 버전도 제공됩니다. 예를 들어 ainvoke, astream, abatch 메서드가 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "04484ff4-c948-4a0c-9846-32d6004c7f59",
   "metadata": {
    "height": 47
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "What do you call a bear with no teeth?\n",
      "\n",
      "A gummy bear!\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "print(1)\n",
    "response = await chain.ainvoke({\"topic\": \"bears\"})\n",
    "print(response)\n",
    "print(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39d1bd81-7031-4ecc-a093-7bd7f8e965a6",
   "metadata": {},
   "source": [
    "# OpenAI Function Calling In LangChain\n",
    "\n",
    "- OpenAI 함수와 LangChain 표현 언어의 사용법을 소개합니다.\n",
    "- Pydantic 라이브러리를 소개합니다. Pydantic은 OpenAI 함수를 쉽게 만들 수 있게 도와주는 Python 라이브러리입니다.\n",
    "\n",
    "\n",
    "- Pydantic이란?\n",
    "    - Pydantic은 Python의 데이터 검증 라이브러리로, 다양한 스키마를 정의하고 이를 JSON으로 내보내기 쉽게 합니다.\n",
    "    - Pydantic 객체를 사용하여 OpenAI 함수 설명을 작성할 수 있습니다.\n",
    "\n",
    "\n",
    "<img src='./images/fig_03_01.png' width=800>\n",
    "\n",
    "- Pydantic 클래스 정의\n",
    "    - 일반적인 Python 클래스와 유사하며, init 메서드 대신 클래스 정의 아래 속성과 타입을 나열합니다.\n",
    "    - 이 클래스들은 실제로 사용되지는 않고, OpenAI 함수의 JSON을 생성하기 위해 사용됩니다.\n",
    "\n",
    "\n",
    "<img src='./images/fig_03_02.png' width=800>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb41f5f4-df8d-4d04-9eaa-193b8c29b00b",
   "metadata": {
    "height": 115,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import os\n",
    "# import openai\n",
    "\n",
    "# from dotenv import load_dotenv, find_dotenv\n",
    "# _ = load_dotenv(find_dotenv()) # read local .env file\n",
    "# openai.api_key = os.environ['OPENAI_API_KEY']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "aa1dddf9-8e44-4454-9d44-f8372cccf5ac",
   "metadata": {
    "height": 47
   },
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from pydantic import BaseModel, Field"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cde9c60d-c7f9-4b2d-9f0c-d44fabc06334",
   "metadata": {},
   "source": [
    "## Pydantic Syntax\n",
    "\n",
    "- Pydantic 데이터 클래스는 Python의 데이터 클래스와 Pydantic의 검증 기능이 결합된 것입니다.\n",
    "- 이들은 데이터 구조를 간결하게 정의하면서 데이터가 지정된 타입과 제약 조건을 준수하도록 보장합니다.\n",
    "- 표준 Python에서는 다음과 같이 클래스를 생성합니다:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "e1557226-36e2-484b-a2fb-bb7e3180342c",
   "metadata": {
    "height": 98
   },
   "outputs": [],
   "source": [
    "class User:\n",
    "    def __init__(self, name: str, age: int, email: str):\n",
    "        self.name = name\n",
    "        self.age = age\n",
    "        self.email = email"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "11b9b584-74dc-49b8-a7fe-3865368774e9",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "foo = User(name=\"Joe\",age=32, email=\"joe@gmail.com\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "0f6f9e9c-b83a-4859-8e65-e6488e05a071",
   "metadata": {
    "height": 30
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Joe'"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "foo.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "10a6a0de-d7dc-414d-baaf-fa43c6d1f410",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "foo = User(name=\"Joe\",age=\"bar\", email=\"joe@gmail.com\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "613b7b12-f061-44bc-989d-433cab609164",
   "metadata": {
    "height": 30
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'bar'"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "foo.age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "c541cb8d-fc55-4c94-a04f-a877cccf10ec",
   "metadata": {
    "height": 81
   },
   "outputs": [],
   "source": [
    "class pUser(BaseModel):\n",
    "    name: str\n",
    "    age: int\n",
    "    email: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "27394d22-73e3-4918-9bdf-18cd7c973942",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "foo_p = pUser(name=\"Jane\", age=32, email=\"jane@gmail.com\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "49f25241-ff47-454f-bac4-ba20ab937d70",
   "metadata": {
    "height": 30
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Jane'"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "foo_p.name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "612f7bb1-bfbe-4a96-b45b-76b20d732c0e",
   "metadata": {},
   "source": [
    "**Note**: The next cell is expected to fail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "37030df3-ec11-4523-ac66-b88f90099d1b",
   "metadata": {
    "height": 30
   },
   "outputs": [
    {
     "ename": "ValidationError",
     "evalue": "1 validation error for pUser\nage\n  Input should be a valid integer, unable to parse string as an integer [type=int_parsing, input_value='bar', input_type=str]\n    For further information visit https://errors.pydantic.dev/2.7/v/int_parsing",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValidationError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[92], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m foo_p \u001b[38;5;241m=\u001b[39m \u001b[43mpUser\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mJane\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbar\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43memail\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mjane@gmail.com\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/trading/lib/python3.10/site-packages/pydantic/main.py:176\u001b[0m, in \u001b[0;36mBaseModel.__init__\u001b[0;34m(self, **data)\u001b[0m\n\u001b[1;32m    174\u001b[0m \u001b[38;5;66;03m# `__tracebackhide__` tells pytest and some other tools to omit this function from tracebacks\u001b[39;00m\n\u001b[1;32m    175\u001b[0m __tracebackhide__ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 176\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__pydantic_validator__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalidate_python\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mself_instance\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mValidationError\u001b[0m: 1 validation error for pUser\nage\n  Input should be a valid integer, unable to parse string as an integer [type=int_parsing, input_value='bar', input_type=str]\n    For further information visit https://errors.pydantic.dev/2.7/v/int_parsing"
     ]
    }
   ],
   "source": [
    "foo_p = pUser(name=\"Jane\", age=\"bar\", email=\"jane@gmail.com\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "911e7677-cc5d-4957-b6c3-b3ba1493de33",
   "metadata": {
    "height": 47
   },
   "outputs": [],
   "source": [
    "class Class(BaseModel):\n",
    "    students: List[pUser]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "14920e50-688e-4dd4-9207-9b59c6b018c6",
   "metadata": {
    "height": 64
   },
   "outputs": [],
   "source": [
    "obj = Class(\n",
    "    students=[pUser(name=\"Jane\", age=32, email=\"jane@gmail.com\")]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "cede1035-7581-4203-bab7-b8e6363c931f",
   "metadata": {
    "height": 30
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Class(students=[pUser(name='Jane', age=32, email='jane@gmail.com')])"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obj"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ab03e6e-8224-4575-b704-a413eaebc33e",
   "metadata": {},
   "source": [
    "## Pydantic to OpenAI function definition\n",
    "- 다음은 Pydantic을 사용하여 OpenAI 함수를 생성하는 코드 예제입니다.\n",
    "- WeatherSearch 클래스는 Pydantic의 BaseModel을 상속받아 정의됩니다.\n",
    "- 클래스에는 airport_code라는 문자열 속성이 있으며, 이 속성에는 \"날씨를 가져올 공항 코드\"라는 설명이 붙어 있습니다.\n",
    "- 클래스의 docstring은 \"Call this with an airport code to get the weather at that airport\"로, 이 함수가 특정 공항의 날씨 정보를 가져오는 데 사용된다는 설명을 포함합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "617ceea9-009f-4325-adae-85ab29fccd68",
   "metadata": {
    "height": 64
   },
   "outputs": [],
   "source": [
    "class WeatherSearch(BaseModel):\n",
    "    \"\"\"Call this with an airport code to get the weather at that airport\"\"\"\n",
    "    airport_code: str = Field(description=\"airport code to get weather for\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10511ee2-323d-466b-b861-bfb1286deced",
   "metadata": {},
   "source": [
    "- convert_pydantic_to_openai_function 함수를 사용하여 WeatherSearch 클래스를 OpenAI 함수로 변환합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "b22a438c-6692-47f9-9e00-1a95d04c6dd3",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "from langchain_core.utils.function_calling import convert_pydantic_to_openai_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "97e152c4-8d04-4a02-b363-ee7691f60e31",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "weather_function = convert_pydantic_to_openai_function(WeatherSearch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23ec8ba8-2549-4fd8-a873-67b1899992ce",
   "metadata": {},
   "source": [
    "- weather_function 변수는 변환된 OpenAI 함수의 JSON 스키마를 포함합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "f36b041e-bd28-4e25-a0c1-fbeeeee4ae53",
   "metadata": {
    "height": 30
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'WeatherSearch',\n",
       " 'description': 'Call this with an airport code to get the weather at that airport',\n",
       " 'parameters': {'properties': {'airport_code': {'description': 'airport code to get weather for',\n",
       "    'type': 'string'}},\n",
       "  'required': ['airport_code'],\n",
       "  'type': 'object'}}"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weather_function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d79cb8bc-dcaf-4db9-8e12-c1b572addbe6",
   "metadata": {},
   "source": [
    "- description 이 없으면 에러가 나야 하지만"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c5d7c573-2f84-441d-ab73-a3dd263318d4",
   "metadata": {
    "height": 47
   },
   "outputs": [],
   "source": [
    "class WeatherSearch1(BaseModel):\n",
    "    airport_code: str = Field(description=\"airport code to get weather for\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f659d4bd-03e2-4a9a-be8a-bd4c449773fc",
   "metadata": {},
   "source": [
    "- 지금은 안난다 (구버전은 났음)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "bb08f095-8190-41c5-b49c-e3580cedf992",
   "metadata": {
    "height": 30
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'WeatherSearch1',\n",
       " 'description': '',\n",
       " 'parameters': {'properties': {'airport_code': {'description': 'airport code to get weather for',\n",
       "    'type': 'string'}},\n",
       "  'required': ['airport_code'],\n",
       "  'type': 'object'}}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convert_pydantic_to_openai_function(WeatherSearch1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "ed22668a-e188-45a5-844e-deee62f9bf51",
   "metadata": {
    "height": 64
   },
   "outputs": [],
   "source": [
    "class WeatherSearch2(BaseModel):\n",
    "    \"\"\"Call this with an airport code to get the weather at that airport\"\"\"\n",
    "    airport_code: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "9e001e87-4338-4720-99b3-9dc4cb3e4faf",
   "metadata": {
    "height": 30
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'WeatherSearch2',\n",
       " 'description': 'Call this with an airport code to get the weather at that airport',\n",
       " 'parameters': {'properties': {'airport_code': {'type': 'string'}},\n",
       "  'required': ['airport_code'],\n",
       "  'type': 'object'}}"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convert_pydantic_to_openai_function(WeatherSearch2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "a35261e8-2d36-43a8-a051-a79bef35c8dd",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "93bda0a6-9206-407a-b0da-966f9442a40c",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "model = ChatOpenAI()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85862910-d18e-43d0-8746-06330c789345",
   "metadata": {},
   "source": [
    "- model.invoke(\"what is the weather in SF today?\", functions=[weather_function])를 호출하여 모델에게 \"SF의 오늘 날씨는?\"이라는 질문을 전달합니다.\n",
    "- functions 인수에 weather_function을 전달하여, 모델이 이 질문에 대해 weather_function 함수를 사용할 수 있도록 합니다.\n",
    "- weather_function은 이전에 Pydantic을 사용하여 정의한 날씨 검색 함수로, 공항 코드를 사용하여 해당 공항의 날씨 정보를 가져오는 역할을 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "afe342d4-a7ef-49cd-b760-aa9a176d64d5",
   "metadata": {
    "height": 30
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='', additional_kwargs={'function_call': {'arguments': '{\"airport_code\":\"SFO\"}', 'name': 'WeatherSearch'}}, response_metadata={'token_usage': {'completion_tokens': 17, 'prompt_tokens': 70, 'total_tokens': 87}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': None, 'finish_reason': 'function_call', 'logprobs': None}, id='run-d45d5909-4458-47f5-8a18-0a6c4abae3f7-0', usage_metadata={'input_tokens': 70, 'output_tokens': 17, 'total_tokens': 87})"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.invoke(\"what is the weather in SF today?\", functions=[weather_function])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7f9ab65-9ccb-47c9-a584-03dbf498fbfe",
   "metadata": {},
   "source": [
    "- model.bind(functions=[weather_function])를 호출하여 모델에 weather_function을 바인딩합니다.\n",
    "- 이렇게 하면 모델 인스턴스가 weather_function을 항상 사용할 수 있게 됩니다.\n",
    "- model_with_function 변수는 함수가 바인딩된 모델 인스턴스를 참조합니다.\n",
    "- model_with_function.invoke(\"what is the weather in sf?\")를 호출하여 모델에게 \"SF의 날씨는?\"이라는 질문을 전달합니다.\n",
    "- 바인딩된 weather_function이 이 질문에 대해 사용됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "511e12b6-bcfb-4862-b377-4251de9969ea",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "model_with_function = model.bind(functions=[weather_function])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "de6241d9-667c-4b97-a50f-95c046fa640c",
   "metadata": {
    "height": 30
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='', additional_kwargs={'function_call': {'arguments': '{\"airport_code\":\"SFO\"}', 'name': 'WeatherSearch'}}, response_metadata={'token_usage': {'completion_tokens': 17, 'prompt_tokens': 69, 'total_tokens': 86}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': None, 'finish_reason': 'function_call', 'logprobs': None}, id='run-87083edf-578d-49c4-a634-a714ca284d10-0', usage_metadata={'input_tokens': 69, 'output_tokens': 17, 'total_tokens': 86})"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_with_function.invoke(\"what is the weather in sf?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2da549f8-0197-4909-b5b3-a39d5c993af3",
   "metadata": {},
   "source": [
    "## Forcing it to use a function\n",
    "\n",
    "- `model.bind(functions=[weather_function], function_call={\"name\":\"WeatherSearch\"})`를 호출하여 모델에 `weather_function`을 바인딩하고, 특정 함수 호출을 강제하도록 설정합니다.\n",
    "  - `functions=[weather_function]`: 모델에 바인딩할 함수 리스트를 지정합니다.\n",
    "  - `function_call={\"name\":\"WeatherSearch\"}`: 모델이 항상 \"WeatherSearch\"라는 이름의 함수를 호출하도록 강제합니다.\n",
    "- `model_with_forced_function` 변수는 함수가 바인딩되고 특정 함수 호출이 강제된 모델 인스턴스를 참조합니다.\n",
    "- `model_with_forced_function.invoke(\"what is the weather in sf?\")`를 호출하여 모델에게 \"SF의 날씨는?\"이라는 질문을 전달합니다.\n",
    "- 이 설정 덕분에 모델은 항상 `WeatherSearch` 함수를 사용하여 질문에 응답하게 됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "e7d2fbc0-df39-4e93-a22f-39a6285272b4",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "model_with_forced_function = model.bind(functions=[weather_function], function_call={\"name\":\"WeatherSearch\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "cd9f4063-9e15-41d7-9cf9-253548534176",
   "metadata": {
    "height": 30
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='', additional_kwargs={'function_call': {'arguments': '{\"airport_code\":\"SFO\"}', 'name': 'WeatherSearch'}}, response_metadata={'token_usage': {'completion_tokens': 7, 'prompt_tokens': 79, 'total_tokens': 86}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-92431049-a84b-4075-8bd1-7e25a01a3a60-0', usage_metadata={'input_tokens': 79, 'output_tokens': 7, 'total_tokens': 86})"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_with_forced_function.invoke(\"what is the weather in sf?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "314ca7e6-b77c-4b9d-9c93-da6ef3c9c6f8",
   "metadata": {
    "height": 30
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='', additional_kwargs={'function_call': {'arguments': '{\"airport_code\":\"SFO\"}', 'name': 'WeatherSearch'}}, response_metadata={'token_usage': {'completion_tokens': 7, 'prompt_tokens': 74, 'total_tokens': 81}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-c8499c59-398e-4884-9a27-87b622983042-0', usage_metadata={'input_tokens': 74, 'output_tokens': 7, 'total_tokens': 81})"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_with_forced_function.invoke(\"hi!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3d50c2a-4c93-44f5-a072-9540f23169b5",
   "metadata": {},
   "source": [
    "## Using in a chain\n",
    "\n",
    "\n",
    "다음은 프롬프트 템플릿과 바인딩된 모델을 사용하여 체인을 호출하는 코드 예제입니다.\n",
    "\n",
    "- `ChatPromptTemplate.from_messages`를 사용하여 프롬프트 템플릿을 생성합니다.\n",
    "  - 이 템플릿은 시스템 메시지와 사용자 메시지로 구성됩니다.\n",
    "  - 시스템 메시지: \"You are a helpful assistant\" (당신은 도움이 되는 어시스턴트입니다)\n",
    "  - 사용자 메시지: `{input}` (사용자 입력을 받을 자리 표시자)\n",
    "- `prompt` 변수는 생성된 프롬프트 템플릿을 참조합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "c12c86df-d628-4176-9f4e-24fb5a953a5d",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "83f00dc6-5d22-44a5-a0a0-4fe1ab8167ac",
   "metadata": {
    "height": 81
   },
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful assistant\"),\n",
    "    (\"user\", \"{input}\")\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfaa98f6-4e2f-4fbd-87e6-b4d9b1c9eb60",
   "metadata": {},
   "source": [
    "- `prompt | model_with_function`을 사용하여 프롬프트 템플릿과 바인딩된 모델을 체인으로 결합합니다.\n",
    "  - `|` 연산자는 프롬프트 템플릿의 출력을 모델에 전달하는 체인을 생성합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "1eee3f5f-2176-4777-8c8a-2a197acc47a7",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "chain = prompt | model_with_function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc69bdf8-08b5-4741-9491-3bd8f0645243",
   "metadata": {},
   "source": [
    "- `chain.invoke({\"input\": \"what is the weather in sf?\"})`를 호출하여 체인에 입력을 전달하고 결과를 가져옵니다.\n",
    "  - 입력 딕셔너리에는 `input` 키가 있으며, 값으로 \"what is the weather in sf?\"를 전달합니다.\n",
    "- 이 체인은 프롬프트 템플릿을 통해 입력을 받아 모델로 전달하고, 모델은 바인딩된 `weather_function`을 사용하여 응답을 생성합니다.\n",
    "\n",
    "이 예제는 프롬프트 템플릿과 바인딩된 모델을 체인으로 결합하여 사용자 입력을 처리하고 모델의 응답을 가져오는 방법을 보여줍니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "8587907e-b4c3-4acd-9e58-1137047d0fee",
   "metadata": {
    "height": 30
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='', additional_kwargs={'function_call': {'arguments': '{\"airport_code\":\"SFO\"}', 'name': 'WeatherSearch'}}, response_metadata={'token_usage': {'completion_tokens': 17, 'prompt_tokens': 75, 'total_tokens': 92}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': None, 'finish_reason': 'function_call', 'logprobs': None}, id='run-efb2c098-ca88-4f53-9b07-5d46c4e065f5-0', usage_metadata={'input_tokens': 75, 'output_tokens': 17, 'total_tokens': 92})"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({\"input\": \"what is the weather in sf?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74f05e85-5183-4872-8b1d-cc470fa9c04b",
   "metadata": {},
   "source": [
    "## Using multiple functions\n",
    "\n",
    "다음은 여러 Pydantic 함수를 바인딩하고 호출하는 OpenAI 모델 코드 예제입니다.\n",
    "\n",
    "- `ArtistSearch` 클래스는 Pydantic의 `BaseModel`을 상속받아 정의됩니다.\n",
    "  - `artist_name` 속성은 조회할 아티스트의 이름을 나타내며, 문자열로 타입이 지정되어 있습니다.\n",
    "  - `n` 속성은 결과의 개수를 나타내며, 정수로 타입이 지정되어 있습니다.\n",
    "  - 클래스의 docstring은 \"Call this to get the names of songs by a particular artist\"로, 특정 아티스트의 노래 목록을 가져오는 데 사용된다는 설명을 포함합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "48c8e42e-f84e-4822-b1ee-a9955fa301c4",
   "metadata": {
    "height": 81
   },
   "outputs": [],
   "source": [
    "class ArtistSearch(BaseModel):\n",
    "    \"\"\"Call this to get the names of songs by a particular artist\"\"\"\n",
    "    artist_name: str = Field(description=\"name of artist to look up\")\n",
    "    n: int = Field(description=\"number of results\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63117634-fb61-49ea-b980-5e0eef6be267",
   "metadata": {},
   "source": [
    "- `functions` 리스트에는 `WeatherSearch`와 `ArtistSearch` 클래스에서 변환된 OpenAI 함수가 포함됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "9a51599b-ee32-4f74-9925-b6264bf43242",
   "metadata": {
    "height": 81
   },
   "outputs": [],
   "source": [
    "functions = [\n",
    "    convert_pydantic_to_openai_function(WeatherSearch),\n",
    "    convert_pydantic_to_openai_function(ArtistSearch),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38f4a663-3fd1-4955-926c-ceb5641ad635",
   "metadata": {},
   "source": [
    "- `model.bind(functions=functions)`를 호출하여 모델에 여러 함수를 바인딩합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "e0cc9e3b-ba38-4eff-b285-d02ee5963725",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "model_with_functions = model.bind(functions=functions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "480ac624-f2f2-41e6-af70-3a463bddd472",
   "metadata": {},
   "source": [
    "- `model_with_functions.invoke(\"what is the weather in sf?\")`를 호출하여 모델에게 \"SF의 날씨는?\"이라는 질문을 전달합니다. 모델은 바인딩된 `weather_function`을 사용하여 응답합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "d01cd501-03b4-4207-b1be-0c33c12a0fa5",
   "metadata": {
    "height": 30
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='', additional_kwargs={'function_call': {'arguments': '{\"airport_code\":\"SFO\"}', 'name': 'WeatherSearch'}}, response_metadata={'token_usage': {'completion_tokens': 17, 'prompt_tokens': 116, 'total_tokens': 133}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': None, 'finish_reason': 'function_call', 'logprobs': None}, id='run-597a4b35-9247-467a-8fcf-56df449a87ee-0', usage_metadata={'input_tokens': 116, 'output_tokens': 17, 'total_tokens': 133})"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_with_functions.invoke(\"what is the weather in sf?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c169c8e5-999d-4372-add6-9929de21d779",
   "metadata": {},
   "source": [
    "- `model_with_functions.invoke(\"what are three songs by taylor swift?\")`를 호출하여 모델에게 \"Taylor Swift의 노래 세 곡은 무엇인가요?\"라는 질문을 전달합니다. 모델은 바인딩된 `artist_function`을 사용하여 응답합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "31599518-7387-4d08-9d68-8f5ba7282e8b",
   "metadata": {
    "height": 30
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='', additional_kwargs={'function_call': {'arguments': '{\"artist_name\":\"Taylor Swift\",\"n\":3}', 'name': 'ArtistSearch'}}, response_metadata={'token_usage': {'completion_tokens': 21, 'prompt_tokens': 118, 'total_tokens': 139}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': None, 'finish_reason': 'function_call', 'logprobs': None}, id='run-dc0befe1-cd4b-4d85-b1b1-ec61ef81a792-0', usage_metadata={'input_tokens': 118, 'output_tokens': 21, 'total_tokens': 139})"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_with_functions.invoke(\"what are three songs by taylor swift?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "439b0649-a386-4087-a9d9-e280bdc1a730",
   "metadata": {},
   "source": [
    "- `model_with_functions.invoke(\"hi!\")`를 호출하여 모델에게 \"안녕하세요!\"라는 인사말을 전달합니다. 이 경우, 모델은 함수 호출 없이 일반적인 응답을 제공합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "4e6df989-6ea3-48af-b2c0-10978e0f4142",
   "metadata": {
    "height": 30
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Hello! How can I assist you today?', response_metadata={'token_usage': {'completion_tokens': 10, 'prompt_tokens': 111, 'total_tokens': 121}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-1dc3fbf7-9525-49fc-95af-ac152c36b433-0', usage_metadata={'input_tokens': 111, 'output_tokens': 10, 'total_tokens': 121})"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_with_functions.invoke(\"hi!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21a72984-e85c-4778-8734-6b67168fa081",
   "metadata": {},
   "source": [
    "- 이 예제는 여러 Pydantic 함수를 바인딩하여 다양한 질문에 대해 적절한 함수를 선택하여 응답하는 방법을 보여줍니다. 모델은 질문의 내용에 따라 적절한 함수를 호출하거나, 필요하지 않은 경우 일반적인 응답을 제공할 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef51a000-59f6-4848-b4dc-999ad31612aa",
   "metadata": {},
   "source": [
    "# Tagging and Extraction Using OpenAI functions\n",
    "\n",
    "\n",
    "### 태깅 (Tagging)\n",
    "1. **개요**: 비구조화된 텍스트에 구조화된 설명을 입력하여 구조화된 데이터를 생성.\n",
    "2. **예시**: 텍스트의 감정과 언어 태그를 추출.\n",
    "3. **과정**:\n",
    "   - 비구조화된 텍스트와 구조화된 설명을 입력.\n",
    "   - LLM을 사용하여 텍스트의 감정과 언어를 태그로 추출.\n",
    "   - 구조화된 설명에 따라 결과 생성.\n",
    "\n",
    "<img src=\"./images/fig_04_01.png\" width=800>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "19349426-38db-4f40-a273-4fd61ee754c5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import os\n",
    "# import openai\n",
    "\n",
    "# from dotenv import load_dotenv, find_dotenv\n",
    "# _ = load_dotenv(find_dotenv()) # read local .env file\n",
    "# openai.api_key = os.environ['OPENAI_API_KEY']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e92e88de-2890-4793-a230-b292eab1462f",
   "metadata": {},
   "source": [
    "1. **설정**:\n",
    "   - 필요한 함수와 클래스 임포트 (typing, Pydantic 등).\n",
    "   - Pydantic 모델 생성 (태깅에 사용).\n",
    "   - 'convert_pydantic_to_openai_function' 메소드 사용."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "eb79423e-9499-4353-89fa-3490e9622704",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain_core.utils.function_calling import convert_pydantic_to_openai_function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b9b5f07-95ba-47ac-afa7-67d74febc19c",
   "metadata": {},
   "source": [
    "2. **태깅 기능 구현**:\n",
    "   - 프로토타입 태깅 모델 생성.\n",
    "   - 감정과 언어 태그를 위한 구조화된 설명 추가.\n",
    "   - 모델에 프롬프트와 언어 모델 연결.\n",
    "   - 사용자 입력을 받아 태깅 함수 호출.\n",
    "   - 결과: 감정(positive), 언어(English) 등 추출."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "09630187-8532-4304-96a2-5924f202e7ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tagging(BaseModel):\n",
    "    \"\"\"Tag the piece of text with particular info.\"\"\"\n",
    "    sentiment: str = Field(description=\"sentiment of text, should be `pos`, `neg`, or `neutral`\")\n",
    "    language: str = Field(description=\"language of text (should be ISO 639-1 code)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "385b7222-807c-49a1-8b70-ae122fc89eb5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'Tagging',\n",
       " 'description': 'Tag the piece of text with particular info.',\n",
       " 'parameters': {'properties': {'sentiment': {'description': 'sentiment of text, should be `pos`, `neg`, or `neutral`',\n",
       "    'type': 'string'},\n",
       "   'language': {'description': 'language of text (should be ISO 639-1 code)',\n",
       "    'type': 'string'}},\n",
       "  'required': ['sentiment', 'language'],\n",
       "  'type': 'object'}}"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convert_pydantic_to_openai_function(Tagging)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "b813c7a3-86f4-45d9-a00d-30d3922e981d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "1da15a27-1936-4fe2-b5b4-096278dfbde5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ChatOpenAI(temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "93ef5e15-9508-43ac-9993-089ee891f10f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tagging_functions = [convert_pydantic_to_openai_function(Tagging)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "3aeae4e6-a51b-46ca-87eb-cac2225b5f80",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Think carefully, and then tag the text as instructed\"),\n",
    "    (\"user\", \"{input}\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "e15f87fb-0d08-4c53-9868-89727f2c145d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_with_functions = model.bind(\n",
    "    functions=tagging_functions,\n",
    "    function_call={\"name\": \"Tagging\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "77450277-2b66-45df-b848-09273ec003ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "tagging_chain = prompt | model_with_functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "3f930401-9de3-4371-aa49-68751fa9133d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='', additional_kwargs={'function_call': {'arguments': '{\"sentiment\":\"pos\",\"language\":\"en\"}', 'name': 'Tagging'}}, response_metadata={'token_usage': {'completion_tokens': 10, 'prompt_tokens': 108, 'total_tokens': 118}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-48791a96-c6d7-4fc9-a729-b44af84cc2c5-0', usage_metadata={'input_tokens': 108, 'output_tokens': 10, 'total_tokens': 118})"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tagging_chain.invoke({\"input\": \"I love langchain\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "90d2c6ed-4776-493c-9cd6-045368ba7803",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='', additional_kwargs={'function_call': {'arguments': '{\"sentiment\":\"neg\",\"language\":\"it\"}', 'name': 'Tagging'}}, response_metadata={'token_usage': {'completion_tokens': 10, 'prompt_tokens': 111, 'total_tokens': 121}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-fe62103a-1b3e-4db0-b059-a89a7705631a-0', usage_metadata={'input_tokens': 111, 'output_tokens': 10, 'total_tokens': 121})"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tagging_chain.invoke({\"input\": \"non mi piace questo cibo\"})  # \"이 음식이 마음에 들지 않습니다\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "8d31c3cf-936e-41b0-b8eb-bc5332ce9c32",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.output_parsers.openai_functions import JsonOutputFunctionsParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "40695dc7-0685-4680-9a15-627e04b54706",
   "metadata": {},
   "outputs": [],
   "source": [
    "tagging_chain = prompt | model_with_functions | JsonOutputFunctionsParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "e8a5953b-e1d4-4444-8f6c-0183ead1e0ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sentiment': 'pos', 'language': 'ko'}"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tagging_chain.invoke({\"input\": \"이 순대 존맛탱이네.\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3856ea11-eefe-4301-9f1e-30bc8eebcf6d",
   "metadata": {},
   "source": [
    "## Extraction\n",
    "\n",
    "Extraction is similar to tagging, but used for extracting multiple pieces of information.\n",
    "\n",
    "### 추출 (Extraction)\n",
    "1. **개요**: 텍스트에서 특정 엔티티를 추출.\n",
    "2. **차이점**: 태깅과 달리 리스트 형태로 엔티티를 추출.\n",
    "3. **예시**: 기사에서 언급된 논문 리스트 추출.\n",
    "\n",
    "<img src=\"./images/fig_04_02.png\" width=800>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1815e85b-b913-4a66-88e5-51e3c81e13fd",
   "metadata": {},
   "source": [
    "3. **추출 기능 구현**:\n",
    "   - 다수의 정보를 추출하기 위해 클래스 정의.\n",
    "   - 예: 사람 정보(person schema) 추출.\n",
    "   - 추출한 정보를 리스트로 관리."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "eeb89d48-2897-4550-ad47-a708953a5b90",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "class Person(BaseModel):\n",
    "    \"\"\"Information about a person.\"\"\"\n",
    "    name: str = Field(description=\"person's name\")\n",
    "    age: Optional[int] = Field(description=\"person's age\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "3f1ce49b-8efd-4977-afc2-adfe237121dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Information(BaseModel):\n",
    "    \"\"\"Information to extract.\"\"\"\n",
    "    people: List[Person] = Field(description=\"List of info about people\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "de910542-5a50-4b29-8042-b8898b4fb981",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'Information',\n",
       " 'description': 'Information to extract.',\n",
       " 'parameters': {'$defs': {'Person': {'description': 'Information about a person.',\n",
       "    'properties': {'name': {'description': \"person's name\", 'type': 'string'},\n",
       "     'age': {'anyOf': [{'type': 'integer'}, {'type': 'null'}],\n",
       "      'description': \"person's age\"}},\n",
       "    'required': ['name', 'age'],\n",
       "    'type': 'object'}},\n",
       "  'properties': {'people': {'description': 'List of info about people',\n",
       "    'items': {'description': 'Information about a person.',\n",
       "     'properties': {'name': {'description': \"person's name\", 'type': 'string'},\n",
       "      'age': {'anyOf': [{'type': 'integer'}, {'type': 'null'}],\n",
       "       'description': \"person's age\"}},\n",
       "     'required': ['name', 'age'],\n",
       "     'type': 'object'},\n",
       "    'type': 'array'}},\n",
       "  'required': ['people'],\n",
       "  'type': 'object'}}"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convert_pydantic_to_openai_function(Information)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "fb869447-7b4a-4a6f-8a5a-dad83b647ef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "extraction_functions = [convert_pydantic_to_openai_function(Information)]\n",
    "extraction_model = model.bind(functions=extraction_functions, function_call={\"name\": \"Information\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "0ea15346-dce8-4dd7-addd-f8dd8b30c5d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='', additional_kwargs={'function_call': {'arguments': '{\"people\":[{\"name\":\"Joe\",\"age\":30},{\"name\":\"Martha\",\"age\":null}]}', 'name': 'Information'}}, response_metadata={'token_usage': {'completion_tokens': 21, 'prompt_tokens': 95, 'total_tokens': 116}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-a1a26392-aa04-4346-afc9-f9db11d8ee5f-0', usage_metadata={'input_tokens': 95, 'output_tokens': 21, 'total_tokens': 116})"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extraction_model.invoke(\"Joe is 30, his mom is Martha\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "fe413b83-570b-48fc-8fc5-e913d45b3d3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Extract the relevant information, if not explicitly provided do not guess. Extract partial info\"),\n",
    "    (\"human\", \"{input}\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "49f637e2-4e2b-4276-a108-fb144e73e51b",
   "metadata": {},
   "outputs": [],
   "source": [
    "extraction_chain = prompt | extraction_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "08c8d00c-a248-460f-868c-f85822a4582e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='', additional_kwargs={'function_call': {'arguments': '{\"people\":[{\"name\":\"Joe\",\"age\":30},{\"name\":\"Martha\",\"age\":null}]}', 'name': 'Information'}}, response_metadata={'token_usage': {'completion_tokens': 21, 'prompt_tokens': 112, 'total_tokens': 133}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-08f07a57-24a7-425e-a34e-b56ad70801d5-0', usage_metadata={'input_tokens': 112, 'output_tokens': 21, 'total_tokens': 133})"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extraction_chain.invoke({\"input\": \"Joe is 30, his mom is Martha\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0ca68c1-3e66-46c5-b7e6-86ed0303c338",
   "metadata": {},
   "source": [
    "### 출력 파서 (Output Parser)\n",
    "1. **JSON 출력 파서**: JSON 형식으로 출력된 데이터를 파싱하여 활용.\n",
    "2. **LangChain**: JSON 출력 파서 사용으로 편리한 데이터 활용.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "77db083a-5cf7-40c3-a460-278ac5a74467",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.output_parsers.openai_functions import JsonKeyOutputFunctionsParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "6634ce07-60f6-49f8-aac0-757ea7aa7fa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "extraction_chain = prompt | extraction_model | JsonOutputFunctionsParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "5a9430c6-d142-4277-adb2-353db15dbbfa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'people': [{'name': 'Joe', 'age': 30}, {'name': 'Martha', 'age': None}]}"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extraction_chain.invoke({\"input\": \"Joe is 30, his mom is Martha\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "c44a89cc-020b-4396-b310-4e179133468d",
   "metadata": {},
   "outputs": [],
   "source": [
    "extraction_chain = prompt | extraction_model | JsonKeyOutputFunctionsParser(key_name=\"people\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "fa3d0f10-8381-43ce-aaf3-cbced8ad4ba2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'name': 'Joe', 'age': 30}, {'name': 'Martha', 'age': None}]"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extraction_chain.invoke({\"input\": \"Joe is 30, his mom is Martha\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5790b2a-9e5a-42bf-a035-af170fc3d5b2",
   "metadata": {},
   "source": [
    "## 실제 적용 예시\n",
    "\n",
    "우리는 태깅을 더 큰 텍스트에 적용할 수 있습니다.\n",
    "\n",
    "예를 들어, 이 블로그 포스트를 로드하고 텍스트의 일부에서 태그 정보를 추출해 봅시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "30874116-5689-4a64-ae9e-bcf0000cef04",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    }
   ],
   "source": [
    "from langchain.document_loaders import WebBaseLoader\n",
    "loader = WebBaseLoader(\"https://lilianweng.github.io/posts/2023-06-23-agent/\")\n",
    "documents = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "84874112-27f5-4dcf-9674-be7b0472e1dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = documents[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "861c09cb-8146-4667-90f6-f127df835037",
   "metadata": {},
   "outputs": [],
   "source": [
    "page_content = doc.page_content[:10000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "fb7bd741-9d0e-4dfa-b6ec-1ff58e535868",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "LLM Powered Autonomous Agents | Lil'Log\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Lil'Log\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Posts\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Archive\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Search\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Tags\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "FAQ\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "emojisearch.app\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "      LLM Powered Autonomous Agents\n",
      "    \n",
      "Date: June 23, 2023  |  Estimated Reading Time: 31 min  |  Author: Lilian Weng\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "Table of Contents\n",
      "\n",
      "\n",
      "\n",
      "Agent System Overview\n",
      "\n",
      "Component One: Planning\n",
      "\n",
      "Task Decomposition\n",
      "\n",
      "Self-Reflection\n",
      "\n",
      "\n",
      "Component Two: Memory\n",
      "\n",
      "Types of Memory\n",
      "\n",
      "Maximum Inner Product Search (MIPS)\n",
      "\n",
      "\n",
      "Component Three: Tool Use\n",
      "\n",
      "Case Studies\n",
      "\n",
      "Scientific Discovery Agent\n",
      "\n",
      "Generative Agents Simulation\n",
      "\n",
      "Proof-of-Concept Examples\n",
      "\n",
      "\n",
      "Challenges\n",
      "\n",
      "Citation\n",
      "\n",
      "References\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Building agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general\n"
     ]
    }
   ],
   "source": [
    "print(page_content[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "d4d9c5f6-fbf8-48ae-8ea8-26d8c2a3c27a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Overview(BaseModel):\n",
    "    \"\"\"Overview of a section of text.\"\"\"\n",
    "    summary: str = Field(description=\"Provide a concise summary of the content.\")\n",
    "    language: str = Field(description=\"Provide the language that the content is written in.\")\n",
    "    keywords: str = Field(description=\"Provide keywords related to the content.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "c44ce43b-1f45-46af-8af0-70fbc728d183",
   "metadata": {},
   "outputs": [],
   "source": [
    "overview_tagging_function = [\n",
    "    convert_pydantic_to_openai_function(Overview)\n",
    "]\n",
    "tagging_model = model.bind(\n",
    "    functions=overview_tagging_function,\n",
    "    function_call={\"name\":\"Overview\"}\n",
    ")\n",
    "tagging_chain = prompt | tagging_model | JsonOutputFunctionsParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "bb1a6090-1460-4ec2-ad2a-66430815fd27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'summary': 'The article discusses building autonomous agents powered by LLM (large language model) as the core controller, with components like planning, memory, and tool use. It explores task decomposition, self-reflection, and challenges in implementing LLM-powered agents.',\n",
       " 'language': 'English',\n",
       " 'keywords': 'LLM, autonomous agents, planning, memory, tool use, task decomposition, self-reflection, challenges'}"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tagging_chain.invoke({\"input\": page_content})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "4dcb22e7-c163-41c7-8af0-f36e6cc3b80c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Paper(BaseModel):\n",
    "    \"\"\"Information about papers mentioned.\"\"\"\n",
    "    title: str\n",
    "    author: Optional[str]\n",
    "\n",
    "\n",
    "class Info(BaseModel):\n",
    "    \"\"\"Information to extract\"\"\"\n",
    "    papers: List[Paper]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "6feff080-696b-4b37-bc21-3a0408b4244f",
   "metadata": {},
   "outputs": [],
   "source": [
    "paper_extraction_function = [\n",
    "    convert_pydantic_to_openai_function(Info)\n",
    "]\n",
    "extraction_model = model.bind(\n",
    "    functions=paper_extraction_function, \n",
    "    function_call={\"name\":\"Info\"}\n",
    ")\n",
    "extraction_chain = prompt | extraction_model | JsonKeyOutputFunctionsParser(key_name=\"papers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "e5fcb680-ade5-423f-9e81-1efc8bb9fdaa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'title': 'LLM Powered Autonomous Agents', 'author': 'Lilian Weng'}]"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extraction_chain.invoke({\"input\": page_content})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "01eda25e-64b3-4367-8935-47ae5260101b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# template = \"\"\"A article will be passed to you. Extract from it all papers that are mentioned by this article. \n",
    "\n",
    "# Do not extract the name of the article itself. If no papers are mentioned that's fine - you don't need to extract any! Just return an empty list.\n",
    "\n",
    "# Do not make up or guess ANY extra information. Only extract what exactly is in the text.\"\"\"\n",
    "\n",
    "template = '''\n",
    "기사가 제공됩니다. 이 기사에서 언급된 모든 논문을 추출하세요.\n",
    "\n",
    "기사의 제목은 추출하지 마세요. 논문이 언급되지 않았다면, 빈 목록을 반환하세요.\n",
    "\n",
    "추가적인 정보를 추측하거나 만들어내지 말고, 텍스트에 정확히 있는 내용만 추출하세요.\n",
    "'''\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", template),\n",
    "    (\"human\", \"{input}\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "093a18b4-ac8b-4bfe-bbdc-e288de222ee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "extraction_chain = prompt | extraction_model | JsonKeyOutputFunctionsParser(key_name=\"papers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "840fec07-f824-4382-aa9c-75e5c7e847f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'title': 'Chain of thought (CoT; Wei et al. 2022)', 'author': None},\n",
       " {'title': 'Tree of Thoughts (Yao et al. 2023)', 'author': None},\n",
       " {'title': 'LLM+P (Liu et al. 2023)', 'author': None},\n",
       " {'title': 'ReAct (Yao et al. 2023)', 'author': None},\n",
       " {'title': 'Reflexion (Shinn & Labash 2023)', 'author': None},\n",
       " {'title': 'Chain of Hindsight (CoH; Liu et al. 2023)', 'author': None},\n",
       " {'title': 'Algorithm Distillation (AD; Laskin et al. 2023)', 'author': None}]"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extraction_chain.invoke({\"input\": page_content})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "89952b39-b62d-4581-83c5-9c4a00475319",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extraction_chain.invoke({\"input\": \"hi\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "659a99c7-086d-45a9-9d9e-05a95d41af34",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_overlap=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "78b8f6aa-7b8a-4a5c-a832-7c269bb7b5e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "splits = text_splitter.split_text(doc.page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "3609c483-629f-4528-9b39-cab054459773",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "62a6d54b-d051-43bd-bada-5b54ab6d2df4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten(matrix):\n",
    "    flat_list = []\n",
    "    for row in matrix:\n",
    "        flat_list += row\n",
    "    return flat_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "18165e51-063e-483b-b0f3-4a706a2461af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4]"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flatten([[1, 2], [3, 4]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "061f1e5f-a261-4737-a578-894afbd13020",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM Powered Autonomous Agents | Lil'Log\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Lil'Log\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Posts\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Archive\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Search\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Tags\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "FAQ\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "emojisearch.app\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "      LLM Powered Autonomous Agents\n",
      "    \n",
      "Date: June 23, 2023  |  Estimated Reading Time: 31 min  |  Author: Lilian Weng\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "Table of Contents\n",
      "\n",
      "\n",
      "\n",
      "Agent System Overview\n",
      "\n",
      "Component One: Planning\n",
      "\n",
      "Task Decomposition\n",
      "\n",
      "Self-Reflection\n",
      "\n",
      "\n",
      "Component Two: Memory\n",
      "\n",
      "Types of Memory\n",
      "\n",
      "Maximum Inner Product Search (MIPS)\n",
      "\n",
      "\n",
      "Component Three: Tool Use\n",
      "\n",
      "Case Studies\n",
      "\n",
      "Scientific Discovery Agent\n",
      "\n",
      "Generative Agents Simulation\n",
      "\n",
      "Proof-of-Concept Examples\n",
      "\n",
      "\n",
      "Challenges\n",
      "\n",
      "Citation\n",
      "\n",
      "References\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Building agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\n",
      "Agent System Overview#\n",
      "In a LLM-powered autonomous agent system, LLM functions as the agent’s brain, complemented by several key components:\n",
      "\n",
      "Planning\n",
      "\n",
      "Subgoal and decomposition: The agent breaks down large tasks into smaller, manageable subgoals, enabling efficient handling of complex tasks.\n",
      "Reflection and refinement: The agent can do self-criticism and self-reflection over past actions, learn from mistakes and refine them for future steps, thereby improving the quality of final results.\n",
      "\n",
      "\n",
      "Memory\n",
      "\n",
      "Short-term memory: I would consider all the in-context learning (See Prompt Engineering) as utilizing short-term memory of the model to learn.\n",
      "Long-term memory: This provides the agent with the capability to retain and recall (infinite) information over extended periods, often by leveraging an external vector store and fast retrieval.\n",
      "\n",
      "\n",
      "Tool use\n",
      "\n",
      "The agent learns to call external APIs for extra information that is missing from the model weights (often hard to change after pre-training), including current information, code execution capability, access to proprietary information sources and more.\n"
     ]
    }
   ],
   "source": [
    "print(splits[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "fc710e25-2578-41d6-9f61-b0b86eb1379b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema.runnable import RunnableLambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "00cd92bf-2e88-40e8-b1c8-a7e8a7c29804",
   "metadata": {},
   "outputs": [],
   "source": [
    "prep = RunnableLambda(\n",
    "    lambda x: [{\"input\": doc} for doc in text_splitter.split_text(x)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "6c5bfcac-cb1f-4de9-9a79-aa652ed63e5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'input': 'hi'}]"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prep.invoke(\"hi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "81ffcd5f-57f6-47d9-95bd-5f9dc6f2bda9",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = prep | extraction_chain.map() | flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "7a05b998-2880-4fc5-bbf2-d8bc662a2867",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = chain.invoke(doc.page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "77c8e463-571b-47a6-b094-f23209777591",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'title': 'Maximum Inner Product Search: A Survey',\n",
       "  'author': 'Yi Tay and Shuai Zhang'},\n",
       " {'title': 'Chain of thought', 'author': 'Wei et al. 2022'},\n",
       " {'title': 'Tree of Thoughts', 'author': 'Yao et al. 2023'},\n",
       " {'title': 'LLM+P', 'author': 'Liu et al. 2023'},\n",
       " {'title': 'ReAct', 'author': 'Yao et al. 2023'},\n",
       " {'title': 'Reflexion', 'author': 'Shinn & Labash 2023'},\n",
       " {'title': 'Chain of Hindsight (CoH)', 'author': 'Liu et al. 2023'},\n",
       " {'title': 'Algorithm Distillation (AD)', 'author': 'Laskin et al. 2023'},\n",
       " {'title': 'Algorithm Distillation for Learning In-Context Reinforcement Learning Algorithms',\n",
       "  'author': 'Laskin et al. 2023'},\n",
       " {'title': 'LSH Forest: Self-Tuning Indexes for Similarity Search',\n",
       "  'author': 'Marius Muja and David G. Lowe'},\n",
       " {'title': 'ANNOY: Approximate Nearest Neighbors Oh Yeah',\n",
       "  'author': 'Erik Bernhardsson'},\n",
       " {'title': 'Efficient and Robust Approximate Nearest Neighbor Search using Hierarchical Navigable Small World Graphs',\n",
       "  'author': 'Yu. A. Malkov and D. A. Yashunin'},\n",
       " {'title': 'FAISS: A library for efficient similarity search',\n",
       "  'author': 'Jeff Johnson, Matthijs Douze, and Hervé Jégou'},\n",
       " {'title': 'ScaNN: Efficient Vector Similarity Search and ANN',\n",
       "  'author': 'Daniel V. Le and Kurt Keutzer'},\n",
       " {'title': 'MRKL: Modular Reasoning, Knowledge and Language',\n",
       "  'author': 'Karpas et al. 2022'},\n",
       " {'title': 'TALM: Tool Augmented Language Models',\n",
       "  'author': 'Parisi et al. 2022'},\n",
       " {'title': 'Toolformer', 'author': 'Schick et al. 2023'},\n",
       " {'title': 'HuggingGPT', 'author': 'Shen et al. 2023'},\n",
       " {'title': 'API-Bank', 'author': 'Li et al. 2023'},\n",
       " {'title': 'ChemCrow', 'author': 'Bran et al. 2023'},\n",
       " {'title': 'Boiko et al. (2023)', 'author': 'Boiko et al.'},\n",
       " {'title': 'Generative Agents Simulation', 'author': 'Park, et al. 2023'},\n",
       " {'title': 'Park et al. 2023', 'author': 'Unknown'},\n",
       " {'title': 'GPT-3: Language Models are Few-Shot Learners',\n",
       "  'author': 'Tom B. Brown et al.'},\n",
       " {'title': 'GPT-4: The Next Evolution in Generative Pre-trained Transformers',\n",
       "  'author': 'OpenAI'},\n",
       " {'title': 'A Novel Approach to Machine Learning', 'author': 'John Smith'},\n",
       " {'title': 'Deep Learning Techniques for Image Recognition',\n",
       "  'author': 'Emily Brown'},\n",
       " {'title': 'A Systematic Mapping Study on Software Architecture Documentation Practices',\n",
       "  'author': 'Muhammad Ali Babar, Liming Zhu, and Christian Lange'},\n",
       " {'title': 'Chain of thought prompting elicits reasoning in large language models',\n",
       "  'author': 'Wei et al.'},\n",
       " {'title': 'Tree of Thoughts: Deliberate Problem Solving with Large Language Models',\n",
       "  'author': 'Yao et al.'},\n",
       " {'title': 'Chain of Hindsight Aligns Language Models with Feedback',\n",
       "  'author': 'Liu et al.'},\n",
       " {'title': 'LLM+P: Empowering Large Language Models with Optimal Planning Proficiency',\n",
       "  'author': 'Liu et al.'},\n",
       " {'title': 'ReAct: Synergizing reasoning and acting in language models',\n",
       "  'author': 'Yao et al.'},\n",
       " {'title': 'Reflexion: an autonomous agent with dynamic memory and self-reflection',\n",
       "  'author': 'Shinn & Labash'},\n",
       " {'title': 'In-context Reinforcement Learning with Algorithm Distillation',\n",
       "  'author': 'Laskin et al.'},\n",
       " {'title': 'MRKL Systems A modular, neuro-symbolic architecture that combines large language models, external knowledge sources and discrete reasoning',\n",
       "  'author': 'Karpas et al.'},\n",
       " {'title': 'Webgpt: Browser-assisted question-answering with human feedback',\n",
       "  'author': 'Nakano et al.'},\n",
       " {'title': 'Toolformer: Language Models Can Teach Themselves to Use Tools',\n",
       "  'author': 'Schick et al.'},\n",
       " {'title': 'API-Bank: A Benchmark for Tool-Augmented LLMs',\n",
       "  'author': 'Li et al.'},\n",
       " {'title': 'HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in HuggingFace',\n",
       "  'author': 'Shen et al.'},\n",
       " {'title': 'ChemCrow: Augmenting large-language models with chemistry tools',\n",
       "  'author': 'Bran et al.'},\n",
       " {'title': 'Emergent autonomous scientific research capabilities of large language models',\n",
       "  'author': 'Boiko et al.'},\n",
       " {'title': 'Generative Agents: Interactive Simulacra of Human Behavior',\n",
       "  'author': 'Joon Sung Park, et al.'}]"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dca3c19-d544-4431-b65f-e896caecd801",
   "metadata": {},
   "source": [
    "# Tools and Routing\n",
    "\n",
    "- 이 강의에서는 LangChain의 도구 개념을 소개하고, 언어 모델이 함수와 상호작용하는 방법에 대해 자세히 설명합니다.\n",
    "\n",
    "<img src='./images/fig_05_01.png' width=800>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b460e90-174d-414b-9a2a-b94f111ac302",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import os\n",
    "# import openai\n",
    "\n",
    "# from dotenv import load_dotenv, find_dotenv\n",
    "# _ = load_dotenv(find_dotenv()) # read local .env file\n",
    "# openai.api_key = os.environ['OPENAI_API_KEY']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcfc6856-b802-48be-ab10-703f7b5be83d",
   "metadata": {},
   "source": [
    "#### 도구 데코레이터 사용\n",
    "1. **도구 데코레이터 소개**\n",
    "    - `langchain` 패키지에서 `tool` 데코레이터를 가져와서 사용합니다.\n",
    "    - 이 데코레이터는 함수를 LangChain 도구로 자동 변환합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "df33dff7-6c89-4b8b-b407-ecb02b7d391e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "42887f80-2f5c-4b33-83e7-f29a035150d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "def search(query: str) -> str:\n",
    "    \"\"\"Search for weather online\"\"\"\n",
    "    return \"42f\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "69510791-5be3-4d5d-a0ff-3bc1cbf095ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'search'"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "12573185-eba3-4321-81f2-bfcddcab55c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Search for weather online'"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search.description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "59b508fe-102f-4405-b5ac-10fa6af115c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'query': {'title': 'Query', 'type': 'string'}}"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search.args"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e83a8d6-1ed3-4f50-8b03-43ad023f010d",
   "metadata": {},
   "source": [
    "2. **입력 스키마 정의**\n",
    "    - 명확한 입력 스키마를 정의하는 것이 중요합니다.\n",
    "    - 언어 모델이 어떤 입력값을 사용해야 하는지 결정하는 데 도움을 줍니다.\n",
    "    - Pydantic 모델을 사용하여 입력 스키마를 정의할 수 있습니다.\n",
    "\n",
    "3. **도구 예시**\n",
    "    - 예시 함수: 위도와 경도를 입력 받아 현재 온도를 반환하는 도구.\n",
    "    - `openmedio` API를 사용하여 날씨 정보를 가져옵니다.\n",
    "    - Pydantic 모델을 사용하여 입력 스키마를 정의하고, 도구 데코레이터를 사용하여 함수를 도구로 변환합니다.\n",
    "\n",
    "4. **도구 설명**\n",
    "    - 도구의 이름과 설명을 확인할 수 있습니다.\n",
    "    - 함수 서명(signature) 및 docstring을 포함한 설명을 볼 수 있습니다.\n",
    "    - OpenAI 함수 정의 형식으로 변환할 수 있습니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "161fb336-f256-45cf-ba3f-24eeca9b6e64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pydantic import BaseModel, Field\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "\n",
    "class SearchInput(BaseModel):\n",
    "    query: str = Field(description=\"Thing to search for\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "4774dd9c-8ebb-4c90-b8e3-2bee355ae619",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool(args_schema=SearchInput)\n",
    "def search(query: str) -> str:\n",
    "    \"\"\"Search for the weather online.\"\"\"\n",
    "    return \"42f\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "74fb4fa0-9065-43c8-90ec-e75598c66f23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'query': {'title': 'Query',\n",
       "  'description': 'Thing to search for',\n",
       "  'type': 'string'}}"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search.args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "9cefb33f-1718-455f-bbfd-8759ce760899",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'42f'"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search.run(\"sf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "1f4c1459-4d50-4939-9fe0-b5ef5a337537",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "import datetime\n",
    "\n",
    "# Define the input schema\n",
    "class OpenMeteoInput(BaseModel):\n",
    "    latitude: float = Field(..., description=\"Latitude of the location to fetch weather data for\")\n",
    "    longitude: float = Field(..., description=\"Longitude of the location to fetch weather data for\")\n",
    "\n",
    "@tool(args_schema=OpenMeteoInput)\n",
    "def get_current_temperature(latitude: float, longitude: float) -> dict:\n",
    "    \"\"\"Fetch current temperature for given coordinates.\"\"\"\n",
    "    \n",
    "    BASE_URL = \"https://api.open-meteo.com/v1/forecast\"\n",
    "    \n",
    "    # Parameters for the request\n",
    "    params = {\n",
    "        'latitude': latitude,\n",
    "        'longitude': longitude,\n",
    "        'hourly': 'temperature_2m',\n",
    "        'forecast_days': 1,\n",
    "    }\n",
    "\n",
    "    # Make the request\n",
    "    response = requests.get(BASE_URL, params=params)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        results = response.json()\n",
    "    else:\n",
    "        raise Exception(f\"API Request failed with status code: {response.status_code}\")\n",
    "\n",
    "    current_utc_time = datetime.datetime.utcnow()\n",
    "    time_list = [datetime.datetime.fromisoformat(time_str.replace('Z', '+00:00')) for time_str in results['hourly']['time']]\n",
    "    temperature_list = results['hourly']['temperature_2m']\n",
    "    \n",
    "    closest_time_index = min(range(len(time_list)), key=lambda i: abs(time_list[i] - current_utc_time))\n",
    "    current_temperature = temperature_list[closest_time_index]\n",
    "    \n",
    "    return f'The current temperature is {current_temperature}°C'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "1c81f36f-2245-4697-a57d-7905fb4001d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'get_current_temperature'"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_current_temperature.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "5575f98c-d8f4-4dfe-9c54-b17675c830b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Fetch current temperature for given coordinates.'"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_current_temperature.description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "ed4548f6-23ac-43aa-8146-874d7f7362a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'latitude': {'title': 'Latitude',\n",
       "  'description': 'Latitude of the location to fetch weather data for',\n",
       "  'type': 'number'},\n",
       " 'longitude': {'title': 'Longitude',\n",
       "  'description': 'Longitude of the location to fetch weather data for',\n",
       "  'type': 'number'}}"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_current_temperature.args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "a79b83c8-cf9a-409c-ba67-95aff784a283",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain.tools.render import format_tool_to_openai_function\n",
    "from langchain_core.utils.function_calling import convert_to_openai_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "e8a43cb7-4aa3-4238-af8f-b15ed7750d79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'get_current_temperature',\n",
       " 'description': 'Fetch current temperature for given coordinates.',\n",
       " 'parameters': {'type': 'object',\n",
       "  'properties': {'latitude': {'description': 'Latitude of the location to fetch weather data for',\n",
       "    'type': 'number'},\n",
       "   'longitude': {'description': 'Longitude of the location to fetch weather data for',\n",
       "    'type': 'number'}},\n",
       "  'required': ['latitude', 'longitude']}}"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# format_tool_to_openai_function(get_current_temperature)\n",
    "convert_to_openai_function(get_current_temperature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "d66cafa2-b80b-4d34-bb99-8d26c57e7ea9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The current temperature is 27.9°C'"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_current_temperature.invoke({\"latitude\": 13, \"longitude\": 14})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "63d0c21f-f1e4-4c9e-8b2a-dd32a0a6ef50",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wikipedia\n",
    "\n",
    "@tool\n",
    "def search_wikipedia(query: str) -> str:\n",
    "    \"\"\"Run Wikipedia search and get page summaries.\"\"\"\n",
    "    page_titles = wikipedia.search(query)\n",
    "    summaries = []\n",
    "    for page_title in page_titles[: 3]:\n",
    "        try:\n",
    "            wiki_page =  wikipedia.page(title=page_title, auto_suggest=False)\n",
    "            summaries.append(f\"Page: {page_title}\\nSummary: {wiki_page.summary}\")\n",
    "        except (\n",
    "            self.wiki_client.exceptions.PageError,\n",
    "            self.wiki_client.exceptions.DisambiguationError,\n",
    "        ):\n",
    "            pass\n",
    "    if not summaries:\n",
    "        return \"No good Wikipedia Search Result was found\"\n",
    "    return \"\\n\\n\".join(summaries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "a9def9f9-17c2-474b-9b25-1302f2663ab0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'search_wikipedia'"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_wikipedia.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "d0d0f41c-b42b-4010-9556-747990613732",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Run Wikipedia search and get page summaries.'"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_wikipedia.description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "abb7570c-973a-4a21-ab23-41ed9d0b6bf1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'search_wikipedia',\n",
       " 'description': 'Run Wikipedia search and get page summaries.',\n",
       " 'parameters': {'type': 'object',\n",
       "  'properties': {'query': {'type': 'string'}},\n",
       "  'required': ['query']}}"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convert_to_openai_function(search_wikipedia)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "f342593b-efb9-4139-8c55-9d245baf63b6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Page: LangChain\\nSummary: LangChain is a framework designed to simplify the creation of applications using large language models (LLMs). As a language model integration framework, LangChain's use-cases largely overlap with those of language models in general, including document analysis and summarization, chatbots, and code analysis.\\n\\n\\n\\nPage: DataStax\\nSummary: DataStax, Inc. is a real-time data for AI company based in Santa Clara, California. Its product Astra DB is a cloud database-as-a-service based on Apache Cassandra. DataStax also offers DataStax Enterprise (DSE), an on-premises database built on Apache Cassandra, and Astra Streaming, a messaging and event streaming cloud service based on Apache Pulsar. As of June 2022, the company has roughly 800 customers distributed in over 50 countries.\\n\\n\\n\\nPage: Sentence embedding\\nSummary: In natural language processing, a sentence embedding refers to a numeric representation of a sentence in the form of a vector of real numbers which encodes meaningful semantic information.\\nState of the art embeddings are based on the learned hidden layer representation of dedicated sentence transformer models. BERT pioneered an approach involving the use of a dedicated [CLS] token prepended to the beginning of each sentence inputted into the model; the final hidden state vector of this token encodes information about the sentence and can be fine-tuned for use in sentence classification tasks. In practice however, BERT's sentence embedding with the [CLS] token achieves poor performance, often worse than simply averaging non-contextual word embeddings. SBERT later achieved superior sentence embedding performance by fine tuning BERT's [CLS] token embeddings through the usage of a siamese neural network architecture on the SNLI dataset. \\nOther approaches are loosely based on the idea of distributional semantics applied to sentences. Skip-Thought trains an encoder-decoder structure for the task of neighboring sentences predictions. Though this has been shown to achieve worse performance than approaches such as InferSent or SBERT. \\nAn alternative direction is to aggregate word embeddings, such as those returned by Word2vec, into sentence embeddings. The most straightforward approach is to simply compute the average of word vectors, known as continuous bag-of-words (CBOW). However, more elaborate solutions based on word vector quantization have also been proposed. One such approach is the vector of locally aggregated word embeddings (VLAWE), which demonstrated performance improvements in downstream text classification tasks.\\n\\n\""
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_wikipedia.invoke({\"query\": \"langchain\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b179a521-07b1-44fb-9c3b-584851138d6e",
   "metadata": {},
   "source": [
    "### Routing\n",
    "\n",
    "In lesson 3, we show an example of function calling deciding between two candidate functions.\n",
    "\n",
    "Given our tools above, let's format these as OpenAI functions and show this same behavior.\n",
    "\n",
    "\n",
    "#### 언어 모델을 사용한 Route 선택\n",
    "1. **언어 모델로 함수 호출 결정**\n",
    "    - 언어 모델을 사용하여 어떤 함수를 호출할지 결정할 수 있습니다.\n",
    "    - OpenAI 모델을 사용하여 함수 호출을 결정하고, 해당 함수를 호출하는 과정을 보여줍니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "30c48c78-e8d3-474a-a7a3-6e6eeb3c0e4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "fe7e080a-8917-4f6b-927c-81179d512c09",
   "metadata": {},
   "outputs": [],
   "source": [
    "functions = [\n",
    "    convert_to_openai_function(f) for f in [\n",
    "        search_wikipedia, get_current_temperature\n",
    "    ]\n",
    "]\n",
    "model = ChatOpenAI(temperature=0).bind(functions=functions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "976959ca-5420-4546-8561-ad13def963bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='', additional_kwargs={'function_call': {'arguments': '{\"latitude\":37.7749,\"longitude\":-122.4194}', 'name': 'get_current_temperature'}}, response_metadata={'token_usage': {'completion_tokens': 25, 'prompt_tokens': 105, 'total_tokens': 130}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': None, 'finish_reason': 'function_call', 'logprobs': None}, id='run-bbbddc51-7ca4-4400-860e-441e20f86187-0', usage_metadata={'input_tokens': 105, 'output_tokens': 25, 'total_tokens': 130})"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.invoke(\"what is the weather in sf right now\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "5e1374da-62a5-4c1f-8e04-09b1d96dcfc8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='', additional_kwargs={'function_call': {'arguments': '{\"query\":\"Langchain\"}', 'name': 'search_wikipedia'}}, response_metadata={'token_usage': {'completion_tokens': 16, 'prompt_tokens': 101, 'total_tokens': 117}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': None, 'finish_reason': 'function_call', 'logprobs': None}, id='run-cf5212ce-e902-4bd8-b825-3fb297b42e91-0', usage_metadata={'input_tokens': 101, 'output_tokens': 16, 'total_tokens': 117})"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.invoke(\"what is langchain\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "48dc3f59-d55f-4930-96fa-16c84cbfa42b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are helpful but sassy assistant\"),\n",
    "    (\"user\", \"{input}\"),\n",
    "])\n",
    "chain = prompt | model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "4dfa6145-7bea-4a7c-85bf-57c276424780",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='', additional_kwargs={'function_call': {'arguments': '{\"latitude\":37.7749,\"longitude\":-122.4194}', 'name': 'get_current_temperature'}}, response_metadata={'token_usage': {'completion_tokens': 25, 'prompt_tokens': 113, 'total_tokens': 138}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': None, 'finish_reason': 'function_call', 'logprobs': None}, id='run-9fd63a06-1841-498c-874c-e72704c58ea8-0', usage_metadata={'input_tokens': 113, 'output_tokens': 25, 'total_tokens': 138})"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({\"input\": \"what is the weather in sf right now\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18b7b23d-84d5-450b-a1bc-6bff8c9d15e2",
   "metadata": {},
   "source": [
    "#### 응답 파서 사용\n",
    "1. **응답 파서 소개**\n",
    "    - 응답 파서를 사용하여 언어 모델의 출력을 더 사용하기 쉽게 변환합니다.\n",
    "    - 함수 호출 시나리오와 단순 응답 시나리오를 구분하여 처리합니다.\n",
    "\n",
    "2. **출력 파서**\n",
    "    - `OpenAIFunctionsAgentOutputParser`를 사용하여 출력값을 에이전트 액션 또는 에이전트 종료로 변환합니다.\n",
    "    - 에이전트 액션: 함수 호출이 필요한 경우.\n",
    "    - 에이전트 종료: 단순 응답인 경우."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "01ce9fac-5503-44b1-a283-37588498f8c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents.output_parsers import OpenAIFunctionsAgentOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "e6389019-7ce6-4ef7-bff2-3c5ece8d92a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = prompt | model | OpenAIFunctionsAgentOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "7de78634-f784-4148-ab1d-aa0421a5c0e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = chain.invoke({\"input\": \"what is the weather in sf right now\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "c31b148a-6a98-40d7-8fba-802bb14ca0c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "langchain_core.agents.AgentActionMessageLog"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "643db197-fae8-4fb9-a3d3-3a6745e23592",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'get_current_temperature'"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "40ea5425-6a1b-485b-9433-053f2c21ac6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'latitude': 37.7749, 'longitude': -122.4194}"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.tool_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "f8427091-8ef4-4a67-9373-3b3c3e9241c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The current temperature is 19.0°C'"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_current_temperature(result.tool_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "239973ec-63db-4b6c-9b30-82255e046241",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = chain.invoke({\"input\": \"hi!\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "ac5952fd-a61f-4c8e-8a1e-769a377ca45b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "langchain_core.agents.AgentFinish"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "fd76c0f7-4887-4faa-9fe4-71806f21fc4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'output': 'Well, hello there! How can I assist you today?'}"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.return_values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f85bc138-1710-4df8-8a7e-0aac0eb8eba7",
   "metadata": {},
   "source": [
    "\n",
    "#### 라우팅 함수 추가\n",
    "1. **라우팅 함수 정의**\n",
    "    - 라우팅 함수를 정의하여 언어 모델의 결과에 따라 적절한 조치를 취합니다.\n",
    "    - 에이전트 액션과 에이전트 종료를 구분하여 처리합니다.\n",
    "    - 에이전트 액션인 경우, 적절한 도구를 찾아 실행합니다.\n",
    "    - 에이전트 종료인 경우, 단순히 응답 값을 반환합니다.\n",
    "\n",
    "2. **새로운 체인 생성**\n",
    "    - 새로운 체인을 생성하여 프롬프트와 모델, 출력 파서, 라우팅 함수를 결합합니다.\n",
    "    - 예시 입력에 따라 도구를 호출하거나 단순 응답을 반환합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "1a304433-7b4b-4876-a535-ba1f991ac511",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema.agent import AgentFinish\n",
    "def route(result):\n",
    "    if isinstance(result, AgentFinish):\n",
    "        return result.return_values['output']\n",
    "    else:\n",
    "        tools = {\n",
    "            \"search_wikipedia\": search_wikipedia, \n",
    "            \"get_current_temperature\": get_current_temperature,\n",
    "        }\n",
    "        return tools[result.tool].run(result.tool_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "681b4c48-ea9f-4083-b86f-e81bcdf25d18",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = prompt | model | OpenAIFunctionsAgentOutputParser() | route"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "851d638d-0da3-4931-8cf5-c7218afb72d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = chain.invoke({\"input\": \"What is the weather in san francisco right now?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "152841d7-ea8b-4ddb-a7c5-aebfcf6a5a5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The current temperature is 19.0°C'"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "37ba05d1-1bb5-4206-9d05-f0299c62f12a",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = chain.invoke({\"input\": \"What is langchain?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "23545412-7af3-4e09-b0b0-81178fddf208",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Page: LangChain\\nSummary: LangChain is a framework designed to simplify the creation of applications using large language models (LLMs). As a language model integration framework, LangChain's use-cases largely overlap with those of language models in general, including document analysis and summarization, chatbots, and code analysis.\\n\\nPage: DataStax\\nSummary: DataStax, Inc. is a real-time data for AI company based in Santa Clara, California. Its product Astra DB is a cloud database-as-a-service based on Apache Cassandra. DataStax also offers DataStax Enterprise (DSE), an on-premises database built on Apache Cassandra, and Astra Streaming, a messaging and event streaming cloud service based on Apache Pulsar. As of June 2022, the company has roughly 800 customers distributed in over 50 countries.\\n\\n\\n\\nPage: Sentence embedding\\nSummary: In natural language processing, a sentence embedding refers to a numeric representation of a sentence in the form of a vector of real numbers which encodes meaningful semantic information.\\nState of the art embeddings are based on the learned hidden layer representation of dedicated sentence transformer models. BERT pioneered an approach involving the use of a dedicated [CLS] token prepended to the beginning of each sentence inputted into the model; the final hidden state vector of this token encodes information about the sentence and can be fine-tuned for use in sentence classification tasks. In practice however, BERT's sentence embedding with the [CLS] token achieves poor performance, often worse than simply averaging non-contextual word embeddings. SBERT later achieved superior sentence embedding performance by fine tuning BERT's [CLS] token embeddings through the usage of a siamese neural network architecture on the SNLI dataset. \\nOther approaches are loosely based on the idea of distributional semantics applied to sentences. Skip-Thought trains an encoder-decoder structure for the task of neighboring sentences predictions. Though this has been shown to achieve worse performance than approaches such as InferSent or SBERT. \\nAn alternative direction is to aggregate word embeddings, such as those returned by Word2vec, into sentence embeddings. The most straightforward approach is to simply compute the average of word vectors, known as continuous bag-of-words (CBOW). However, more elaborate solutions based on word vector quantization have also been proposed. One such approach is the vector of locally aggregated word embeddings (VLAWE), which demonstrated performance improvements in downstream text classification tasks.\\n\\n\""
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "1257f8c9-e227-4947-b1ad-2a61a12a0946",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Well, hello there! How can I assist you today?'"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({\"input\": \"hi!\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93b30cc9-e1d0-40f8-9030-d3238851c457",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### 요약\n",
    "이 강의에서는 LangChain 도구의 개념과 사용자 정의 도구를 만드는 방법, OpenAPI 사양을 사용하는 방법, 언어 모델을 사용하여 함수 호출을 결정하는 방법 등을 다루었습니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "765b0585-78cb-4b9f-9c42-4abee5f79333",
   "metadata": {},
   "source": [
    "# Conversational agent\n",
    "\n",
    "## 강의 소개 및 개요\n",
    "- 이번 강의에서는 대화형 에이전트를 구축하는 방법을 학습합니다. 이는 도구 사용과 채팅 메모리를 결합하여 ChatGPT와 유사한 대화형 에이전트를 만드는 과정입니다.\n",
    "\n",
    "<img src='./images/fig_06_01.png' width=600>\n",
    "\n",
    "\n",
    "## 에이전트 기본 개념\n",
    "- 에이전트는 언어 모델과 코드의 조합입니다. 언어 모델은 수행할 단계와 그 입력을 결정하고, 에이전트 루프는 도구를 선택하여 호출하고 결과를 관찰합니다. 이를 반복하여 특정 조건을 충족할 때까지 진행합니다.\n",
    "- 에이전트 종료 조건: 언어 모델이 종료를 결정하거나 최대 반복 횟수 등의 규칙을 설정할 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a54f81f-0c43-4f6e-a22e-264bc6812f25",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 실습: 도구와 에이전트 루프 구현\n",
    "- 이전 강의에서 구축한 도구를 사용하여 에이전트 루프를 작성합니다. 에이전트 루프는 입력을 받아 적절한 도구를 선택하고 호출한 후 결과를 반환합니다. 이를 반복하여 종료 조건이 충족될 때까지 진행합니다.\n",
    "    - 모델 및 프롬프트 설정: 언어 모델과 프롬프트 템플릿을 설정하고, 도구 목록을 OpenAI 함수로 변환합니다.\n",
    "    - 루프 생성: 도구를 선택하고 호출한 후 결과를 반환하는 루프를 생성합니다. 이를 위해 프롬프트에 메시지 목록을 추가하여 이전 도구 호출 내역과 결과를 저장합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "1f85903a-a3e3-41b4-b128-10de7f7097a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.tools import tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "86c5244a-16c8-4476-a7e5-7492944d9d5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "import datetime\n",
    "\n",
    "# Define the input schema\n",
    "class OpenMeteoInput(BaseModel):\n",
    "    latitude: float = Field(..., description=\"Latitude of the location to fetch weather data for\")\n",
    "    longitude: float = Field(..., description=\"Longitude of the location to fetch weather data for\")\n",
    "\n",
    "@tool(args_schema=OpenMeteoInput)\n",
    "def get_current_temperature(latitude: float, longitude: float) -> dict:\n",
    "    \"\"\"Fetch current temperature for given coordinates.\"\"\"\n",
    "    \n",
    "    BASE_URL = \"https://api.open-meteo.com/v1/forecast\"\n",
    "    \n",
    "    # Parameters for the request\n",
    "    params = {\n",
    "        'latitude': latitude,\n",
    "        'longitude': longitude,\n",
    "        'hourly': 'temperature_2m',\n",
    "        'forecast_days': 1,\n",
    "    }\n",
    "\n",
    "    # Make the request\n",
    "    response = requests.get(BASE_URL, params=params)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        results = response.json()\n",
    "    else:\n",
    "        raise Exception(f\"API Request failed with status code: {response.status_code}\")\n",
    "\n",
    "    current_utc_time = datetime.datetime.utcnow()\n",
    "    time_list = [datetime.datetime.fromisoformat(time_str.replace('Z', '+00:00')) for time_str in results['hourly']['time']]\n",
    "    temperature_list = results['hourly']['temperature_2m']\n",
    "    \n",
    "    closest_time_index = min(range(len(time_list)), key=lambda i: abs(time_list[i] - current_utc_time))\n",
    "    current_temperature = temperature_list[closest_time_index]\n",
    "    \n",
    "    return f'The current temperature is {current_temperature}°C'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "0d231b2e-6dae-4ce6-9c2e-cc2b83591856",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wikipedia\n",
    "\n",
    "@tool\n",
    "def search_wikipedia(query: str) -> str:\n",
    "    \"\"\"Run Wikipedia search and get page summaries.\"\"\"\n",
    "    page_titles = wikipedia.search(query)\n",
    "    summaries = []\n",
    "    for page_title in page_titles[: 3]:\n",
    "        try:\n",
    "            wiki_page =  wikipedia.page(title=page_title, auto_suggest=False)\n",
    "            summaries.append(f\"Page: {page_title}\\nSummary: {wiki_page.summary}\")\n",
    "        except (\n",
    "            self.wiki_client.exceptions.PageError,\n",
    "            self.wiki_client.exceptions.DisambiguationError,\n",
    "        ):\n",
    "            pass\n",
    "    if not summaries:\n",
    "        return \"No good Wikipedia Search Result was found\"\n",
    "    return \"\\n\\n\".join(summaries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "8bb7cc71-91bd-4dd5-87df-89f39180225d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = [get_current_temperature, search_wikipedia]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "52728518-a0fc-428a-8087-f5d9cd325224",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_core.utils.function_calling import convert_to_openai_function\n",
    "from langchain.agents.output_parsers import OpenAIFunctionsAgentOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "4cebf0a4-47bd-4676-afaa-dd487cee5c54",
   "metadata": {},
   "outputs": [],
   "source": [
    "functions = [convert_to_openai_function(f) for f in tools]\n",
    "model = ChatOpenAI(temperature=0).bind(functions=functions)\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are helpful but sassy assistant\"),\n",
    "    (\"user\", \"{input}\"),\n",
    "])\n",
    "chain = prompt | model | OpenAIFunctionsAgentOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "dd514e2f-9f7c-4bfd-982d-f352c4bb2ad0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AgentActionMessageLog(tool='get_current_temperature', tool_input={'latitude': 37.7749, 'longitude': -122.4194}, log=\"\\nInvoking: `get_current_temperature` with `{'latitude': 37.7749, 'longitude': -122.4194}`\\n\\n\\n\", message_log=[AIMessage(content='', additional_kwargs={'function_call': {'arguments': '{\"latitude\":37.7749,\"longitude\":-122.4194}', 'name': 'get_current_temperature'}}, response_metadata={'token_usage': {'completion_tokens': 25, 'prompt_tokens': 112, 'total_tokens': 137}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': None, 'finish_reason': 'function_call', 'logprobs': None}, id='run-1cfdbb80-dc1b-44c0-ab80-2134ec90664b-0', usage_metadata={'input_tokens': 112, 'output_tokens': 25, 'total_tokens': 137})])"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = chain.invoke({\"input\": \"what is the weather is sf?\"})\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "c7f55c07-62c5-414b-b48f-8c4f077d617f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'get_current_temperature'"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "3cff445f-44a0-455e-b0ea-30642d5d6dbc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'latitude': 37.7749, 'longitude': -122.4194}"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.tool_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "288d2740-290e-44af-99ff-b48e810a9e7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import MessagesPlaceholder\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are helpful but sassy assistant\"),\n",
    "    (\"user\", \"{input}\"),\n",
    "    MessagesPlaceholder(variable_name=\"agent_scratchpad\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "378d7200-6424-40ba-8c31-103ff2abe889",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = prompt | model | OpenAIFunctionsAgentOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "20814dc4-b251-4997-b2f8-d7e886dfb4a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "result1 = chain.invoke({\n",
    "    \"input\": \"what is the weather is sf?\",\n",
    "    \"agent_scratchpad\": []\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "678d549f-2744-4c41-b673-5201ea21c7ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AgentActionMessageLog(tool='get_current_temperature', tool_input={'latitude': 37.7749, 'longitude': -122.4194}, log=\"\\nInvoking: `get_current_temperature` with `{'latitude': 37.7749, 'longitude': -122.4194}`\\n\\n\\n\", message_log=[AIMessage(content='', additional_kwargs={'function_call': {'arguments': '{\"latitude\":37.7749,\"longitude\":-122.4194}', 'name': 'get_current_temperature'}}, response_metadata={'token_usage': {'completion_tokens': 25, 'prompt_tokens': 112, 'total_tokens': 137}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': None, 'finish_reason': 'function_call', 'logprobs': None}, id='run-9a73bd95-54cb-4de4-90e3-f1ee4e5125d2-0', usage_metadata={'input_tokens': 112, 'output_tokens': 25, 'total_tokens': 137})])"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "75bd844e-192e-44fa-a54f-50f90bcde975",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'get_current_temperature'"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result1.tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "21f45b38-971d-4adc-9a00-93ccaac65fc9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'latitude': 37.7749, 'longitude': -122.4194}"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result1.tool_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "4c5c5f03-edb4-4b9a-a111-36c3c34eb151",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The current temperature is 17.0°C'"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "observation = get_current_temperature.invoke(result1.tool_input)\n",
    "observation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "01896f42-b6eb-4bf7-992e-bc50ad2daa6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "langchain_core.agents.AgentActionMessageLog"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(result1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef7a4ac9-77e3-4e64-9501-e8f2964fb5a1",
   "metadata": {},
   "source": [
    "- (AgentAction, tool output) 튜플을 FunctionMessage로 변환합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "94073dcc-931b-4436-9ea3-496bf287e782",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents.format_scratchpad import format_to_openai_functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "c3b5d4d6-5341-4b61-9463-ac73b0448eb1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[AIMessage(content='', additional_kwargs={'function_call': {'arguments': '{\"latitude\":37.7749,\"longitude\":-122.4194}', 'name': 'get_current_temperature'}}, response_metadata={'token_usage': {'completion_tokens': 25, 'prompt_tokens': 112, 'total_tokens': 137}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': None, 'finish_reason': 'function_call', 'logprobs': None}, id='run-9a73bd95-54cb-4de4-90e3-f1ee4e5125d2-0', usage_metadata={'input_tokens': 112, 'output_tokens': 25, 'total_tokens': 137})]"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result1.message_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "b139e885-d496-49fd-bb55-374ae88b9119",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[AIMessage(content='', additional_kwargs={'function_call': {'arguments': '{\"latitude\":37.7749,\"longitude\":-122.4194}', 'name': 'get_current_temperature'}}, response_metadata={'token_usage': {'completion_tokens': 25, 'prompt_tokens': 112, 'total_tokens': 137}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': None, 'finish_reason': 'function_call', 'logprobs': None}, id='run-9a73bd95-54cb-4de4-90e3-f1ee4e5125d2-0', usage_metadata={'input_tokens': 112, 'output_tokens': 25, 'total_tokens': 137}),\n",
       " FunctionMessage(content='The current temperature is 17.0°C', name='get_current_temperature')]"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "format_to_openai_functions([(result1, observation), ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "9d32c884-8ed2-47bd-8165-fd029d0b770b",
   "metadata": {},
   "outputs": [],
   "source": [
    "result2 = chain.invoke({\n",
    "    \"input\": \"what is the weather is sf?\", \n",
    "    \"agent_scratchpad\": format_to_openai_functions([(result1, observation)])\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "5bc02772-da9b-4c43-9433-34f16c7e275a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AgentFinish(return_values={'output': 'The current temperature in San Francisco is 17.0°C.'}, log='The current temperature in San Francisco is 17.0°C.')"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6540a3a-fb4b-48b2-b89d-994ee94977a0",
   "metadata": {},
   "source": [
    "## 에이전트 실행 함수 작성\n",
    "- 에이전트를 실행하는 함수 run_agent를 작성합니다. 이 함수는 사용자 입력을 받아 에이전트 루프를 실행하고, 종료 조건이 충족될 때까지 반복합니다.\n",
    "    - 에이전트 체인 생성: agent_chain을 생성하여 입력과 중간 결과를 받아 에이전트 스크래치패드를 생성하고, 이를 모델과 파서에 전달합니다.\n",
    "    - 루프 간소화: 에이전트 체인을 직접 호출하여 입력과 중간 결과를 처리하는 간단한 루프를 작성합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "fe1a4907-150a-46b6-9a47-33db65a58550",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema.agent import AgentFinish\n",
    "def run_agent(user_input):\n",
    "    intermediate_steps = []\n",
    "    while True:\n",
    "        result = chain.invoke({\n",
    "            \"input\": user_input, \n",
    "            \"agent_scratchpad\": format_to_openai_functions(intermediate_steps)\n",
    "        })\n",
    "        if isinstance(result, AgentFinish):\n",
    "            return result\n",
    "        tool = {\n",
    "            \"search_wikipedia\": search_wikipedia, \n",
    "            \"get_current_temperature\": get_current_temperature,\n",
    "        }[result.tool]\n",
    "        observation = tool.run(result.tool_input)\n",
    "        intermediate_steps.append((result, observation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "053085db-1c06-4b1f-b666-1fcf8bc635b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "agent_chain = RunnablePassthrough.assign(\n",
    "    agent_scratchpad= lambda x: format_to_openai_functions(x[\"intermediate_steps\"])\n",
    ") | chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "0cf6e4ac-0ebf-4152-98f2-d77b493b2392",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_agent(user_input):\n",
    "    intermediate_steps = []\n",
    "    while True:\n",
    "        result = agent_chain.invoke({\n",
    "            \"input\": user_input, \n",
    "            \"intermediate_steps\": intermediate_steps\n",
    "        })\n",
    "        if isinstance(result, AgentFinish):\n",
    "            return result\n",
    "        tool = {\n",
    "            \"search_wikipedia\": search_wikipedia, \n",
    "            \"get_current_temperature\": get_current_temperature,\n",
    "        }[result.tool]\n",
    "        observation = tool.run(result.tool_input)\n",
    "        intermediate_steps.append((result, observation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "68db2bfe-7602-4de8-a46e-be5a4e1664ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AgentFinish(return_values={'output': 'The current temperature in San Francisco is 17.0°C.'}, log='The current temperature in San Francisco is 17.0°C.')"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_agent(\"what is the weather is sf?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "2186bec8-e131-4639-af73-2c26750645b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AgentFinish(return_values={'output': 'LangChain is a framework designed to simplify the creation of applications using large language models (LLMs). It is a language model integration framework with use-cases including document analysis and summarization, chatbots, and code analysis.'}, log='LangChain is a framework designed to simplify the creation of applications using large language models (LLMs). It is a language model integration framework with use-cases including document analysis and summarization, chatbots, and code analysis.')"
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_agent(\"what is langchain?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "76de0d06-b3eb-4ead-a71d-5acb8d159c33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AgentFinish(return_values={'output': 'Well, hello there! How can I assist you today?'}, log='Well, hello there! How can I assist you today?')"
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_agent(\"hi!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "265f983f-2bd0-40d3-9369-9094c4c30d16",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import AgentExecutor\n",
    "agent_executor = AgentExecutor(agent=agent_chain, tools=tools, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "6801b8fb-a17e-446c-82ee-e2557715ecc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `search_wikipedia` with `{'query': 'Langchain'}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[33;1m\u001b[1;3mPage: LangChain\n",
      "Summary: LangChain is a framework designed to simplify the creation of applications using large language models (LLMs). As a language model integration framework, LangChain's use-cases largely overlap with those of language models in general, including document analysis and summarization, chatbots, and code analysis.\n",
      "\n",
      "\n",
      "\n",
      "Page: DataStax\n",
      "Summary: DataStax, Inc. is a real-time data for AI company based in Santa Clara, California. Its product Astra DB is a cloud database-as-a-service based on Apache Cassandra. DataStax also offers DataStax Enterprise (DSE), an on-premises database built on Apache Cassandra, and Astra Streaming, a messaging and event streaming cloud service based on Apache Pulsar. As of June 2022, the company has roughly 800 customers distributed in over 50 countries.\n",
      "\n",
      "\n",
      "\n",
      "Page: Sentence embedding\n",
      "Summary: In natural language processing, a sentence embedding refers to a numeric representation of a sentence in the form of a vector of real numbers which encodes meaningful semantic information.\n",
      "State of the art embeddings are based on the learned hidden layer representation of dedicated sentence transformer models. BERT pioneered an approach involving the use of a dedicated [CLS] token prepended to the beginning of each sentence inputted into the model; the final hidden state vector of this token encodes information about the sentence and can be fine-tuned for use in sentence classification tasks. In practice however, BERT's sentence embedding with the [CLS] token achieves poor performance, often worse than simply averaging non-contextual word embeddings. SBERT later achieved superior sentence embedding performance by fine tuning BERT's [CLS] token embeddings through the usage of a siamese neural network architecture on the SNLI dataset. \n",
      "Other approaches are loosely based on the idea of distributional semantics applied to sentences. Skip-Thought trains an encoder-decoder structure for the task of neighboring sentences predictions. Though this has been shown to achieve worse performance than approaches such as InferSent or SBERT. \n",
      "An alternative direction is to aggregate word embeddings, such as those returned by Word2vec, into sentence embeddings. The most straightforward approach is to simply compute the average of word vectors, known as continuous bag-of-words (CBOW). However, more elaborate solutions based on word vector quantization have also been proposed. One such approach is the vector of locally aggregated word embeddings (VLAWE), which demonstrated performance improvements in downstream text classification tasks.\n",
      "\n",
      "\u001b[0m\u001b[32;1m\u001b[1;3mLangChain is a framework designed to simplify the creation of applications using large language models (LLMs). It is a language model integration framework with use-cases including document analysis and summarization, chatbots, and code analysis.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'what is langchain?',\n",
       " 'output': 'LangChain is a framework designed to simplify the creation of applications using large language models (LLMs). It is a language model integration framework with use-cases including document analysis and summarization, chatbots, and code analysis.'}"
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent_executor.invoke({\"input\": \"what is langchain?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "fda817fc-78fa-4904-bd76-90debcf1d48a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mNice to meet you, Bob! How can I assist you today?\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'my name is bob',\n",
       " 'output': 'Nice to meet you, Bob! How can I assist you today?'}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent_executor.invoke({\"input\": \"my name is bob\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7bb594c7-199f-424b-a914-a0bcb3cc4b4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mI'm sorry, I don't have access to your personal information. How can I assist you today?\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'what is my name',\n",
       " 'output': \"I'm sorry, I don't have access to your personal information. How can I assist you today?\"}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent_executor.invoke({\"input\": \"what is my name\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd13f4b8-67be-43e9-b0ee-397111312618",
   "metadata": {},
   "source": [
    "## 메모리 기능 추가\n",
    "- 에이전트가 이전 대화를 기억할 수 있도록 메모리 기능을 추가합니다. 이를 통해 사용자와의 대화를 유지하고, 이전 메시지를 참조할 수 있습니다.\n",
    "    - 메시지 플레이스홀더 추가: 프롬프트에 채팅 기록을 위한 메시지 플레이스홀더를 추가합니다.\n",
    "    - 메모리 객체 설정: 메시지를 메모리에 저장하고 이를 에이전트 체인에 전달하여 대화 기록을 유지합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "0bb847f3-6e03-4520-9f7f-ad54c463d0ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are helpful but sassy assistant\"),\n",
    "    MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "    (\"user\", \"{input}\"),\n",
    "    MessagesPlaceholder(variable_name=\"agent_scratchpad\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "14be4668-d8ee-4dc8-9daf-748c7efdc2f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_chain = RunnablePassthrough.assign(\n",
    "    agent_scratchpad= lambda x: format_to_openai_functions(x[\"intermediate_steps\"])\n",
    ") | prompt | model | OpenAIFunctionsAgentOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "3b9a8d94-e415-4de5-9a06-da1d402403a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationBufferMemory\n",
    "memory = ConversationBufferMemory(return_messages=True,memory_key=\"chat_history\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "04d4b448-c82a-402a-9111-0a1e69634d15",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_executor = AgentExecutor(agent=agent_chain, tools=tools, verbose=True, memory=memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "afb96202-1ec9-4696-8013-76644bf60f80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mNice to meet you, Bob! How can I assist you today?\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'my name is bob',\n",
       " 'chat_history': [HumanMessage(content='my name is bob'),\n",
       "  AIMessage(content='Nice to meet you, Bob! How can I assist you today?')],\n",
       " 'output': 'Nice to meet you, Bob! How can I assist you today?'}"
      ]
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent_executor.invoke({\"input\": \"my name is bob\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "13443a46-838b-4598-a467-275e6cb5dbfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mYour name is Bob.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'whats my name',\n",
       " 'chat_history': [HumanMessage(content='my name is bob'),\n",
       "  AIMessage(content='Nice to meet you, Bob! How can I assist you today?'),\n",
       "  HumanMessage(content='whats my name'),\n",
       "  AIMessage(content='Your name is Bob.')],\n",
       " 'output': 'Your name is Bob.'}"
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent_executor.invoke({\"input\": \"whats my name\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "5a35be72-0b47-4882-a77b-631a49b5d645",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `get_current_temperature` with `{'latitude': 37.7749, 'longitude': -122.4194}`\n",
      "responded: I can help you with that. Just give me a moment to fetch the current weather in San Francisco.\n",
      "\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3mThe current temperature is 10.9°C\u001b[0m\u001b[32;1m\u001b[1;3mThe current temperature in San Francisco is 10.9°C.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'whats the weather in sf?',\n",
       " 'chat_history': [HumanMessage(content='my name is bob'),\n",
       "  AIMessage(content='Nice to meet you, Bob! How can I assist you today?'),\n",
       "  HumanMessage(content='whats my name'),\n",
       "  AIMessage(content='Your name is Bob.'),\n",
       "  HumanMessage(content='whats the weather in sf?'),\n",
       "  AIMessage(content='The current temperature in San Francisco is 10.9°C.')],\n",
       " 'output': 'The current temperature in San Francisco is 10.9°C.'}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent_executor.invoke({\"input\": \"whats the weather in sf?\"})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (trading)",
   "language": "python",
   "name": "trading"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
