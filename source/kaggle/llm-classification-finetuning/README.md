# LLM Classification Fine-tuning - Baseline

Kaggle Competition: [LLM Classification Fine-tuning](https://www.kaggle.com/competitions/llm-classification-finetuning)

## ëª©ì°¨
- [ëŒ€íšŒ ê°œìš”](#ëŒ€íšŒ-ê°œìš”)
- [ëŒ€íšŒ ë°°ê²½ ë° ëª©ì ](#ëŒ€íšŒ-ë°°ê²½-ë°-ëª©ì )
- [í‰ê°€ ì§€í‘œ](#í‰ê°€-ì§€í‘œ)
- [ë°ì´í„°ì…‹ ì„¤ëª…](#ë°ì´í„°ì…‹-ì„¤ëª…)
- [í”„ë¡œì íŠ¸ êµ¬ì¡°](#í”„ë¡œì íŠ¸-êµ¬ì¡°)
- [ë¹ ë¥¸ ì‹œì‘](#ë¹ ë¥¸-ì‹œì‘-quick-start)
- [Baseline ëª¨ë¸ êµ¬ì¡°](#baseline-ëª¨ë¸-êµ¬ì¡°)
- [Baseline ì½”ë“œ ìƒì„¸ ì„¤ëª…](#baseline-ì½”ë“œ-ìƒì„¸-ì„¤ëª…)
- [ì¤‘ìš” ì‚¬í•­](#ì¤‘ìš”-ì‚¬í•­)
- [ì„±ëŠ¥ ê°œì„  ì•„ì´ë””ì–´](#ì„±ëŠ¥-ê°œì„ -ì•„ì´ë””ì–´)
- [ë¬¸ì œ í•´ê²°](#ë¬¸ì œ-í•´ê²°)
- [FAQ](#faq-ìì£¼-ë¬»ëŠ”-ì§ˆë¬¸)
- [ì°¸ê³  ìë£Œ](#ì°¸ê³ -ìë£Œ)

## ëŒ€íšŒ ê°œìš”

**LLM Classification Fine-tuning**ì€ ì‚¬ìš©ìê°€ ë‘ ê°œì˜ LLM ì‘ë‹µ ì¤‘ ì–´ëŠ ê²ƒì„ ì„ í˜¸í• ì§€ ì˜ˆì¸¡í•˜ëŠ” ëŒ€íšŒì…ë‹ˆë‹¤.

### ë¬¸ì œ ì •ì˜
- **Task**: Head-to-Head ë°°í‹€ì—ì„œ ì‚¬ìš©ì ì„ í˜¸ë„ ì˜ˆì¸¡
- **ë°ì´í„°**: Chatbot Arenaì—ì„œ ìˆ˜ì§‘ëœ ì‹¤ì œ ëŒ€í™” ë°ì´í„°
- **ëª©í‘œ**: ì‚¬ìš©ìê°€ ì„ í˜¸í•˜ëŠ” ì‘ë‹µì„ ì˜ˆì¸¡í•˜ëŠ” ëª¨ë¸ ê°œë°œ

### ì¶œë ¥ í˜•ì‹
3ê°œ í´ë˜ìŠ¤ì— ëŒ€í•œ í™•ë¥ ê°’ (í•©ì´ 1):
- `winner_model_a`: ëª¨ë¸ Aê°€ ë” ë‚˜ì€ í™•ë¥ 
- `winner_model_b`: ëª¨ë¸ Bê°€ ë” ë‚˜ì€ í™•ë¥ 
- `winner_tie`: ë¹„ìŠ·í•œ í™•ë¥ 

### ëŒ€íšŒ ìœ í˜•
- **Getting Started Competition**: ì´ˆë³´ìë¥¼ ìœ„í•œ ë¹„ê²½ìŸ ëŒ€íšŒ
- **Rolling Leaderboard**: 2ê°œì›” ë¡¤ë§ ìœˆë„ìš° (ì˜¤ë˜ëœ ì œì¶œì€ ìë™ ì œê±°)
- **ë¬´ê¸°í•œ ìš´ì˜**: ì–¸ì œë“  ì°¸ê°€ ê°€ëŠ¥
- **ìƒê¸ˆ ì—†ìŒ**: í•™ìŠµ ë° ê²½í—˜ ëª©ì 

## ëŒ€íšŒ ë°°ê²½ ë° ëª©ì 

### ë°°ê²½: Chatbot Arena
ì´ ëŒ€íšŒëŠ” **Chatbot Arena** ë°ì´í„°ë¥¼ í™œìš©í•©ë‹ˆë‹¤:
- ì‚¬ìš©ìê°€ ë‘ ê°œì˜ ìµëª… LLMê³¼ ëŒ€í™”
- ì‚¬ìš©ìê°€ ë” ì„ í˜¸í•˜ëŠ” ì‘ë‹µ ì„ íƒ
- ì‹¤ì œ ì‚¬ìš©ì ì„ í˜¸ë„ ë°ì´í„° ìˆ˜ì§‘

### ëª©ì : RLHFì˜ Reward Model
ì´ ëŒ€íšŒëŠ” **Reinforcement Learning from Human Feedback (RLHF)** ì˜ í•µì‹¬ ìš”ì†Œì¸ **Reward Model** ë˜ëŠ” **Preference Model** ê°œë°œê³¼ ì§ì ‘ ì—°ê´€ë©ë‹ˆë‹¤.

#### ê¸°ì¡´ LLMì˜ í•œê³„
ê¸°ì¡´ LLMì„ ì§ì ‘ ì‚¬ìš©í•˜ì—¬ ì„ í˜¸ë„ë¥¼ ì˜ˆì¸¡í•  ë•Œ ë°œìƒí•˜ëŠ” í¸í–¥:
- **Position Bias**: ë¨¼ì € ì œì‹œëœ ì‘ë‹µì„ ì„ í˜¸
- **Verbosity Bias**: ë” ê¸´ ì‘ë‹µì„ ì„ í˜¸
- **Self-Enhancement Bias**: ìì‹ ì˜ ì‘ë‹µì„ ì„ í˜¸

#### ì´ ëŒ€íšŒì˜ ì˜ì˜
- í¸í–¥ì„ ê·¹ë³µí•˜ëŠ” íš¨ê³¼ì ì¸ ì„ í˜¸ë„ ì˜ˆì¸¡ ëª¨ë¸ ê°œë°œ
- ì‚¬ìš©ì ê°œì¸ì˜ ì„ í˜¸ë„ì— ë§ì¶˜ ì‘ë‹µ ìƒì„± ê°€ëŠ¥
- ë” ì‚¬ìš©ì ì¹œí™”ì ì¸ AI ëŒ€í™” ì‹œìŠ¤í…œ êµ¬ì¶•

## í‰ê°€ ì§€í‘œ

### Log Loss (Cross-Entropy Loss)
```
Log Loss = -1/N * Î£ Î£ y_ij * log(p_ij)
```

**ì œì¶œ í˜•ì‹**:
- ê° í…ŒìŠ¤íŠ¸ ìƒ˜í”Œì— ëŒ€í•´ 3ê°œ í´ë˜ìŠ¤ì˜ í™•ë¥  ì˜ˆì¸¡
- ëª¨ë“  í™•ë¥ ì˜ í•©ì€ 1ì´ì–´ì•¼ í•¨
- `eps=auto` ì ìš© (0 ë˜ëŠ” 1ì— ê°€ê¹Œìš´ í™•ë¥  ë³´ì •)

**ì˜ˆì‹œ**:
```csv
id,winner_model_a,winner_model_b,winner_tie
136060,0.33,0.33,0.33
211333,0.40,0.35,0.25
1233961,0.25,0.50,0.25
```

**Log Loss í•´ì„**:
- **ë‚®ì„ìˆ˜ë¡ ì¢‹ìŒ**: 0ì— ê°€ê¹Œìš¸ìˆ˜ë¡ ì™„ë²½í•œ ì˜ˆì¸¡
- **í™•ë¥  ê¸°ë°˜**: ì •í™•í•œ í´ë˜ìŠ¤ë¿ë§Œ ì•„ë‹ˆë¼ í™•ë¥  ë¶„í¬ì˜ ì •í™•ë„ë„ í‰ê°€
- **ë¶ˆí™•ì‹¤ì„± ë°˜ì˜**: ì—¬ëŸ¬ í´ë˜ìŠ¤ì— í™•ë¥ ì„ ë¶„ì‚°ì‹œí‚¤ë©´ ì•ˆì „í•˜ì§€ë§Œ ì ìˆ˜ëŠ” ë‚®ì•„ì§

## ë°ì´í„°ì…‹ ì„¤ëª…

### ë°ì´í„° ì¶œì²˜
**ChatBot Arena**ì˜ ì‹¤ì œ ì‚¬ìš©ì ì¸í„°ë™ì…˜ ë°ì´í„°ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤:
- ì‚¬ìš©ì(judge)ê°€ í•˜ë‚˜ ì´ìƒì˜ í”„ë¡¬í”„íŠ¸ë¥¼ ë‘ ê°œì˜ ì„œë¡œ ë‹¤ë¥¸ LLMì— ì œê³µ
- ì‚¬ìš©ìê°€ ë” ë§Œì¡±ìŠ¤ëŸ¬ìš´ ì‘ë‹µì„ ì œê³µí•œ ëª¨ë¸ì„ ì„ íƒ
- ëª©í‘œ: ì‚¬ìš©ìì˜ ì„ í˜¸ë„ë¥¼ ì˜ˆì¸¡í•˜ê³  ì£¼ì–´ì§„ prompt/response ìŒì´ ìŠ¹ìë¡œ ì„ íƒë  í™•ë¥  ê²°ì •

### ë°ì´í„° ê·œëª¨
- **í•™ìŠµ ë°ì´í„°**: 57,477í–‰ (ëŒ€íšŒ ì„¤ëª…ì—ëŠ” "ì•½ 55,000í–‰"ìœ¼ë¡œ ëª…ì‹œ)
- **í…ŒìŠ¤íŠ¸ ë°ì´í„°**: ì•½ 25,000í–‰ (ì‹¤ì œ ì œì¶œ ì‹œ)
- **ì˜ˆì œ í…ŒìŠ¤íŠ¸ ë°ì´í„°**: 3í–‰ (ì œì¶œ ì‹œ ì „ì²´ í…ŒìŠ¤íŠ¸ ì„¸íŠ¸ë¡œ ëŒ€ì²´ë¨)

âš ï¸ **ê²½ê³ **: ì´ ë°ì´í„°ì…‹ì—ëŠ” ëª¨ìš•ì ì´ê±°ë‚˜ ì €ì†í•˜ê±°ë‚˜ ê³µê²©ì ìœ¼ë¡œ ê°„ì£¼ë  ìˆ˜ ìˆëŠ” í…ìŠ¤íŠ¸ê°€ í¬í•¨ë˜ì–´ ìˆìŠµë‹ˆë‹¤.

### íŒŒì¼ êµ¬ì¡°

#### train.csv (í•™ìŠµ ë°ì´í„°)
| ì»¬ëŸ¼ëª… | íƒ€ì… | ì„¤ëª… |
|--------|------|------|
| `id` | int | í–‰ì˜ ê³ ìœ  ì‹ë³„ì |
| `model_a` | string | ëª¨ë¸ Aì˜ ì´ë¦„ (ì˜ˆ: gpt-4, claude-2) |
| `model_b` | string | ëª¨ë¸ Bì˜ ì´ë¦„ |
| `prompt` | string | ë‘ ëª¨ë¸ì— ì…ë ¥ìœ¼ë¡œ ì œê³µëœ í”„ë¡¬í”„íŠ¸ |
| `response_a` | string | ëª¨ë¸ Aê°€ í”„ë¡¬í”„íŠ¸ì— ëŒ€í•´ ìƒì„±í•œ ì‘ë‹µ |
| `response_b` | string | ëª¨ë¸ Bê°€ í”„ë¡¬í”„íŠ¸ì— ëŒ€í•´ ìƒì„±í•œ ì‘ë‹µ |
| `winner_model_a` | float | ëª¨ë¸ Aê°€ ìŠ¹ë¦¬í•œ ê²½ìš° 1, ì•„ë‹ˆë©´ 0 (ëª©í‘œ ë³€ìˆ˜) |
| `winner_model_b` | float | ëª¨ë¸ Bê°€ ìŠ¹ë¦¬í•œ ê²½ìš° 1, ì•„ë‹ˆë©´ 0 (ëª©í‘œ ë³€ìˆ˜) |
| `winner_tie` | float | ë¹„ê¸´ ê²½ìš° 1, ì•„ë‹ˆë©´ 0 (ëª©í‘œ ë³€ìˆ˜) |

**ì˜ˆì‹œ**:
```csv
id,model_a,model_b,prompt,response_a,response_b,winner_model_a,winner_model_b,winner_tie
1,gpt-4,claude-2,"What is AI?","AI is..","Artificial Intelligence...",0,1,0
```

#### test.csv (í…ŒìŠ¤íŠ¸ ë°ì´í„°)
| ì»¬ëŸ¼ëª… | íƒ€ì… | ì„¤ëª… |
|--------|------|------|
| `id` | int | í–‰ì˜ ê³ ìœ  ì‹ë³„ì |
| `prompt` | string | ë‘ ëª¨ë¸ì— ì…ë ¥ìœ¼ë¡œ ì œê³µëœ í”„ë¡¬í”„íŠ¸ |
| `response_a` | string | ëª¨ë¸ Aì˜ ì‘ë‹µ |
| `response_b` | string | ëª¨ë¸ Bì˜ ì‘ë‹µ |

**ì£¼ì˜**: `model_a`ì™€ `model_b` ì»¬ëŸ¼ì€ í…ŒìŠ¤íŠ¸ ë°ì´í„°ì— í¬í•¨ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤.

**ì˜ˆì‹œ**:
```csv
id,prompt,response_a,response_b
136060,"Explain quantum computing","Quantum computing uses...","A quantum computer..."
```

#### sample_submission.csv (ì œì¶œ í˜•ì‹ ì˜ˆì‹œ)
| ì»¬ëŸ¼ëª… | íƒ€ì… | ì„¤ëª… |
|--------|------|------|
| `id` | int | test.csvì˜ idì™€ ë§¤ì¹­ |
| `winner_model_a` | float | ëª¨ë¸ Aê°€ ìŠ¹ë¦¬í•  í™•ë¥  (0~1) |
| `winner_model_b` | float | ëª¨ë¸ Bê°€ ìŠ¹ë¦¬í•  í™•ë¥  (0~1) |
| `winner_tie` | float | ë¹„ê¸¸ í™•ë¥  (0~1) |

**ì œì•½**: ê° í–‰ì˜ ì„¸ í™•ë¥ ì˜ í•©ì€ 1ì´ì–´ì•¼ í•¨.

**ì˜ˆì‹œ**:
```csv
id,winner_model_a,winner_model_b,winner_tie
136060,0.33,0.33,0.34
211333,0.25,0.50,0.25
```

### ë°ì´í„° íŠ¹ì§•

#### 1. ë‹¤ì¤‘ í´ë˜ìŠ¤ í™•ë¥  ì˜ˆì¸¡
- ë‹¨ìˆœ ë¶„ë¥˜ê°€ ì•„ë‹Œ í™•ë¥  ë¶„í¬ ì˜ˆì¸¡
- Soft labels ê°€ëŠ¥ (ì˜ˆ: [0.4, 0.5, 0.1])

#### 2. ëª¨ë¸ ì •ë³´ ì‚¬ìš© ë¶ˆê°€ (í…ŒìŠ¤íŠ¸ ì‹œ)
- í•™ìŠµ ë°ì´í„°ì—ëŠ” `model_a`, `model_b` í¬í•¨
- í…ŒìŠ¤íŠ¸ ë°ì´í„°ì—ëŠ” ëª¨ë¸ ì •ë³´ ì—†ìŒ
- **ì „ëµ**: ëª¨ë¸ ì´ë¦„ì— ì˜ì¡´í•˜ì§€ ì•ŠëŠ” ì¼ë°˜í™”ëœ ëª¨ë¸ ê°œë°œ í•„ìš”

#### 3. í…ìŠ¤íŠ¸ ê¸¸ì´ ë³€ë™ì„±
- Prompt: ì§§ì€ ì§ˆë¬¸ë¶€í„° ê¸´ ì„¤ëª…ê¹Œì§€
- Response: í•œ ë¬¸ì¥ë¶€í„° ì—¬ëŸ¬ ë¬¸ë‹¨ê¹Œì§€
- **ê³ ë ¤ì‚¬í•­**: MAX_LENGTH ì„¤ì • ì‹œ ì£¼ì˜

#### 4. ë‹¤ì–‘í•œ ë„ë©”ì¸
- ê¸°ìˆ , ê³¼í•™, ì¼ìƒ ëŒ€í™”, ì°½ì‘ ë“±
- ë‹¤ì–‘í•œ ì§ˆë¬¸ ìœ í˜• (ì„¤ëª…, ë¹„êµ, ì¡°ì–¸, ì½”ë”© ë“±)

### ë°ì´í„° í™œìš© íŒ

#### 1. íƒìƒ‰ì  ë°ì´í„° ë¶„ì„ (EDA)
```python
import pandas as pd

train = pd.read_csv('data/train.csv')

# í´ë˜ìŠ¤ ë¶„í¬ í™•ì¸
print(train[['winner_model_a', 'winner_model_b', 'winner_tie']].sum())

# í…ìŠ¤íŠ¸ ê¸¸ì´ ë¶„í¬
train['prompt_len'] = train['prompt'].str.len()
train['response_a_len'] = train['response_a'].str.len()
train['response_b_len'] = train['response_b'].str.len()

print(train[['prompt_len', 'response_a_len', 'response_b_len']].describe())

# ëª¨ë¸ ë¶„í¬
print(train['model_a'].value_counts())
print(train['model_b'].value_counts())
```

#### 2. ë°ì´í„° ì „ì²˜ë¦¬ ê³ ë ¤ì‚¬í•­
- **ê²°ì¸¡ê°’**: í™•ì¸ ë° ì²˜ë¦¬
- **íŠ¹ìˆ˜ ë¬¸ì**: ì´ëª¨ì§€, HTML íƒœê·¸ ë“±
- **ê¸´ í…ìŠ¤íŠ¸**: í† í° ì œí•œìœ¼ë¡œ ì˜ë¦´ ìˆ˜ ìˆìŒ
- **ë¶ˆê· í˜•**: í´ë˜ìŠ¤ ë¶„í¬ í™•ì¸

#### 3. Feature Engineering ì•„ì´ë””ì–´
- ì‘ë‹µ ê¸¸ì´ ì°¨ì´
- ì–´íœ˜ ë‹¤ì–‘ì„± (unique words)
- ê°ì„± ë¶„ì„ ì ìˆ˜
- ê°€ë…ì„± ì ìˆ˜ (Flesch-Kincaid ë“±)
- ë¬¸ë²• ì˜¤ë¥˜ ìˆ˜

## í”„ë¡œì íŠ¸ êµ¬ì¡°

```
llm-classification-finetuning/
â”œâ”€â”€ README.md                # ì´ íŒŒì¼
â”œâ”€â”€ requirements.txt         # Python íŒ¨í‚¤ì§€ ì˜ì¡´ì„±
â”œâ”€â”€ setup_jupyter.sh         # Jupyter ì»¤ë„ ì„¤ì • ìŠ¤í¬ë¦½íŠ¸
â”‚
â”œâ”€â”€ baseline.py             # Kaggle ì œì¶œìš© (Kaggle ë…¸íŠ¸ë¶ì— ë³µì‚¬)
â”œâ”€â”€ baseline_local.py       # ë¡œì»¬ í…ŒìŠ¤íŠ¸ìš©
â”œâ”€â”€ download_model.py       # DistilBERT ë‹¤ìš´ë¡œë“œ ë° ì••ì¶• ìŠ¤í¬ë¦½íŠ¸
â”œâ”€â”€ exploration.ipynb       # ë°ì´í„° íƒìƒ‰ìš© Jupyter ë…¸íŠ¸ë¶
â”‚
â”œâ”€â”€ data/                   # ë°ì´í„° ë””ë ‰í† ë¦¬ (gitignore)
â”‚   â”œâ”€â”€ train.csv          # í•™ìŠµ ë°ì´í„° (~176MB, 57,477í–‰)
â”‚   â”œâ”€â”€ test.csv           # í…ŒìŠ¤íŠ¸ ë°ì´í„°
â”‚   â””â”€â”€ sample_submission.csv
â”‚
â”œâ”€â”€ models/                 # ëª¨ë¸ ë””ë ‰í† ë¦¬ (gitignore)
â”‚   â”œâ”€â”€ distilbert-base-uncased/
â”‚   â”‚   â”œâ”€â”€ config.json
â”‚   â”‚   â”œâ”€â”€ model.safetensors
â”‚   â”‚   â””â”€â”€ ...
â”‚   â””â”€â”€ distilbert-base-uncased.zip  # Kaggle ì—…ë¡œë“œìš©
â”‚
â””â”€â”€ outputs/                # ë¡œì»¬ ì‹¤í–‰ ê²°ê³¼ (gitignore)
    â”œâ”€â”€ best_model.pt      # í•™ìŠµëœ ëª¨ë¸ (ë¡œì»¬ìš©)
    â””â”€â”€ submission.csv     # ì˜ˆì¸¡ ê²°ê³¼ (ë¡œì»¬ìš©)
```

## ë¹ ë¥¸ ì‹œì‘ (Quick Start)

### ì›Œí¬í”Œë¡œìš° ì„ íƒ

#### ë¡œì»¬ì—ì„œ í…ŒìŠ¤íŠ¸ í›„ Kaggle ì œì¶œ

ë¡œì»¬ì—ì„œ ì½”ë“œë¥¼ ê²€ì¦í•œ í›„ Kaggleì—ì„œ ì‹¤í–‰í•©ë‹ˆë‹¤.

**1ë‹¨ê³„: í™˜ê²½ ì„¤ì •**
```bash
# ê°€ìƒí™˜ê²½ ìƒì„± ë° í™œì„±í™” (ì„ íƒ)
python -m venv .venv
source .venv/bin/activate  # Windows: .venv\Scripts\activate

# íŒ¨í‚¤ì§€ ì„¤ì¹˜
pip install -r requirements.txt
```

**2ë‹¨ê³„: ëª¨ë¸ ë‹¤ìš´ë¡œë“œ**
```bash
python download_model.py
```
ì´ ìŠ¤í¬ë¦½íŠ¸ëŠ” `models/distilbert-base-uncased/` í´ë”ì— ëª¨ë¸ì„ ë‹¤ìš´ë¡œë“œí•©ë‹ˆë‹¤ (~254 MB).
ìë™ìœ¼ë¡œ ëª¨ë¸ì„ `models/distilbert-base-uncased.zip` ì— ì••ì¶• í•©ë‹ˆë‹¤.

**3ë‹¨ê³„: ë°ì´í„° ì¤€ë¹„**

```bash
kaggle competitions download -c llm-classification-finetuning
```
ìœ„ ëª…ë ¹ì–´ë¡œ Kaggleì—ì„œ ëŒ€íšŒ ë°ì´í„°(llm-classification-finetuning.zip)ë¥¼ ë‹¤ìš´ë¡œë“œ í•˜ê³  ì••ì¶•ì„ í’€ì–´ `data/` í´ë”ì— ë°°ì¹˜:
- `data/train.csv`
- `data/test.csv`
- `data/sample_submission.csv`

**4ë‹¨ê³„: ë°ì´í„° íƒìƒ‰ (ì„ íƒ, ê¶Œì¥)**
```bash
# Jupyter ì»¤ë„ ì„¤ì •
bash setup_jupyter.sh

# Jupyter Notebook ì‹¤í–‰
jupyter notebook exploration.ipynb
```
- ë…¸íŠ¸ë¶ì—ì„œ ì»¤ë„ ì„ íƒ: `Kernel â†’ Change Kernel â†’ Python (LLM-Classification-FT)`
- ë°ì´í„° ë¶„í¬, í…ìŠ¤íŠ¸ ê¸¸ì´, ëª¨ë¸ ë¶„í¬ ë“± ìƒì„¸ ë¶„ì„
- Feature Engineering ì•„ì´ë””ì–´ ë„ì¶œ

**5ë‹¨ê³„: ë¡œì»¬ì—ì„œ í•™ìŠµ í…ŒìŠ¤íŠ¸ (ì„ íƒ)**
```bash
python baseline_local.py
```
- ì‘ì€ ë°ì´í„°ì…‹ìœ¼ë¡œ í…ŒìŠ¤íŠ¸í•˜ê±°ë‚˜ EPOCHSë¥¼ 1ë¡œ ì„¤ì •í•˜ì—¬ ë¹ ë¥´ê²Œ ê²€ì¦
- ê²°ê³¼ëŠ” `outputs/` í´ë”ì— ì €ì¥ë¨
- ì´ ë‹¨ê³„ëŠ” ì½”ë“œ ê²€ì¦ìš©ì´ë©°, ìƒì„±ëœ ëª¨ë¸ì€ Kaggleì— ì—…ë¡œë“œí•  ìˆ˜ ì—†ìŒ

**6ë‹¨ê³„: Kaggle ë…¸íŠ¸ë¶ ìƒì„± ë° ì‹¤í–‰**
1. [ëŒ€íšŒ í˜ì´ì§€](https://www.kaggle.com/competitions/llm-classification-finetuning)ì˜ "Code" íƒ­ìœ¼ë¡œ ì´ë™
2. "New Notebook" í´ë¦­
3. **Settings â†’ GPU T4 x2** í™œì„±í™” (í•„ìˆ˜)
4. **Add Data** í´ë¦­:
   - Competition ë°ì´í„°: `llm-classification-finetuning` ì¶”ê°€
   - Model upload: `distilbert-base-uncased.zip` ì¶”ê°€
5. ë…¸íŠ¸ë¶ì— [baseline.py](baseline.py)ì˜ ì „ì²´ ì½”ë“œë¥¼ ë³µì‚¬/ë¶™ì—¬ë„£ê¸°
6. **Run All** í´ë¦­
7. í•™ìŠµ ì™„ë£Œ í›„ ë…¸íŠ¸ë¶ì„ ëŒ€íšŒì— ì œì¶œ

## Baseline ëª¨ë¸ êµ¬ì¡°

### ê°œìš”

- **ëª¨ë¸**: DistilBERT (ê²½ëŸ‰í™”ëœ BERT, íŒŒë¼ë¯¸í„° 66M)
- **ì…ë ¥ í˜•ì‹**: `prompt [SEP] response_a [SEP] response_b`
- **ì¶œë ¥**: 3ê°œ í´ë˜ìŠ¤ì— ëŒ€í•œ í™•ë¥  ë²¡í„°
- **í•™ìŠµ ì‹œê°„**: GPU T4 ê¸°ì¤€ ì•½ 15-20ë¶„ (1 epoch)

### ìƒì„¸ ì•„í‚¤í…ì²˜

```
Input Text (Concatenated)
    â†“
[Tokenizer] - MAX_LENGTH=256
    â†“
DistilBERT Encoder (6 layers, 768 hidden)
    â†“
[CLS] Token Representation (768-dim)
    â†“
Dropout (p=0.3)
    â†“
Linear Layer (768 â†’ 3)
    â†“
Softmax
    â†“
Output Probabilities [P(A wins), P(B wins), P(tie)]
```

### ì£¼ìš” í•˜ì´í¼íŒŒë¼ë¯¸í„°

| íŒŒë¼ë¯¸í„° | ê°’ | ì„¤ëª… | ì½”ë“œ ìœ„ì¹˜ |
|---------|-----|------|----------|
| MAX_LENGTH | 256 | í† í° ìµœëŒ€ ê¸¸ì´ | [baseline.py:56](baseline.py#L56) |
| BATCH_SIZE | 16 | ë°°ì¹˜ í¬ê¸° | [baseline.py:57](baseline.py#L57) |
| EPOCHS | 1 | ì—í¬í¬ ìˆ˜ (3-5ë¡œ ì¦ê°€ ê¶Œì¥) | [baseline.py:58](baseline.py#L58) |
| LEARNING_RATE | 2e-5 | í•™ìŠµë¥  | [baseline.py:59](baseline.py#L59) |
| DROPOUT | 0.3 | Dropout ë¹„ìœ¨ | [baseline.py:164](baseline.py#L164) |

## Baseline ì½”ë“œ ìƒì„¸ ì„¤ëª…

[baseline.py](baseline.py)ëŠ” ì´ 338ì¤„ì˜ ì™„ì „í•œ í•™ìŠµ ë° ì¶”ë¡  íŒŒì´í”„ë¼ì¸ì…ë‹ˆë‹¤. ì´ ì„¹ì…˜ì—ì„œëŠ” ì½”ë“œì˜ ê° ë¶€ë¶„ì„ ë‹¨ê³„ë³„ë¡œ ìƒì„¸íˆ ì„¤ëª…í•©ë‹ˆë‹¤.

### ì½”ë“œ ì‹¤í–‰ íë¦„ (Execution Flow)

```
1. ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸ & ì‹œë“œ ì„¤ì •
   â†“
2. Config í´ë˜ìŠ¤ë¡œ í•˜ì´í¼íŒŒë¼ë¯¸í„° ì„¤ì •
   â†“
3. ë°ì´í„° ë¡œë”© (train.csv, test.csv)
   â†“
4. Train/Validation ë¶„í•  (90%/10%)
   â†“
5. Dataset & DataLoader ìƒì„±
   â†“
6. ëª¨ë¸ ì´ˆê¸°í™” (DistilBERT + Classification Head)
   â†“
7. Optimizer & Scheduler ì„¤ì •
   â†“
8. í•™ìŠµ ë£¨í”„ (Epochs)
   â”‚  â”œâ”€ Training
   â”‚  â”œâ”€ Validation
   â”‚  â””â”€ Best Model ì €ì¥
   â†“
9. Best Model ë¡œë“œ
   â†“
10. í…ŒìŠ¤íŠ¸ ë°ì´í„° ì˜ˆì¸¡
   â†“
11. Submission íŒŒì¼ ìƒì„±
```

### 1. ì„¤ì • ë° ì´ˆê¸°í™” ([baseline.py:13-38](baseline.py#L13-L38))

```python
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.metrics import log_loss
import torch
from torch.utils.data import Dataset, DataLoader
from transformers import AutoTokenizer, AutoModel, get_linear_schedule_with_warmup
import torch.nn as nn
from torch.optim import AdamW
from tqdm import tqdm
```

**ì£¼ìš” ë¼ì´ë¸ŒëŸ¬ë¦¬**:
- `transformers`: HuggingFaceì˜ DistilBERT ëª¨ë¸ ì‚¬ìš©
- `torch`: PyTorch ë”¥ëŸ¬ë‹ í”„ë ˆì„ì›Œí¬
- `sklearn`: ë°ì´í„° ë¶„í•  ë° í‰ê°€ ì§€í‘œ

**ëœë¤ ì‹œë“œ ì„¤ì •** ([baseline.py:32-37](baseline.py#L32-L37)):
```python
def set_seed(seed=42):
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
```
ì¬í˜„ ê°€ëŠ¥í•œ ê²°ê³¼ë¥¼ ìœ„í•´ ëª¨ë“  ëœë¤ ì‹œë“œë¥¼ ê³ ì •í•©ë‹ˆë‹¤.

### 2. ì„¤ì • í´ë˜ìŠ¤ ([baseline.py:43-69](baseline.py#L43-L69))

```python
class Config:
    MODEL_NAME = "/kaggle/input/distilbert-base-uncased/..."
    MAX_LENGTH = 256
    BATCH_SIZE = 16
    EPOCHS = 1
    LEARNING_RATE = 2e-5
    DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")

    # ê²½ë¡œ ì„¤ì •
    TRAIN_DATA_PATH = "/kaggle/input/.../train.csv"
    TEST_DATA_PATH = "/kaggle/input/.../test.csv"
    SUBMISSION_PATH = "/kaggle/working/submission.csv"
    MODEL_SAVE_PATH = "/kaggle/working/best_model.pt"
```

**ì—­í• **: ëª¨ë“  í•˜ì´í¼íŒŒë¼ë¯¸í„°ì™€ ê²½ë¡œë¥¼ í•œ ê³³ì—ì„œ ê´€ë¦¬í•˜ì—¬ ìˆ˜ì •ì´ ìš©ì´í•©ë‹ˆë‹¤.

**ğŸ’¡ ì´ˆë³´ì íŒ**:
- `BATCH_SIZE`ë¥¼ ì¤„ì´ë©´ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ê°ì†Œ (GPU ë©”ëª¨ë¦¬ ë¶€ì¡± ì‹œ)
- `EPOCHS`ë¥¼ ëŠ˜ë¦¬ë©´ ì„±ëŠ¥ í–¥ìƒ ê°€ëŠ¥ (ë‹¨, ê³¼ì í•© ì£¼ì˜)
- `MAX_LENGTH`ë¥¼ ëŠ˜ë¦¬ë©´ ë” ê¸´ í…ìŠ¤íŠ¸ ì²˜ë¦¬ ê°€ëŠ¥ (ë‹¨, ë©”ëª¨ë¦¬ ì‚¬ìš© ì¦ê°€)

### 3. ë°ì´í„° ë¡œë”© ë° ì „ì²˜ë¦¬ ([baseline.py:92-107](baseline.py#L92-L107))

```python
train_df = pd.read_csv(config.TRAIN_DATA_PATH)
test_df = pd.read_csv(config.TEST_DATA_PATH)

# Train/Validation split (90%/10%)
train_data, val_data = train_test_split(
    train_df,
    test_size=0.1,
    random_state=42,
    stratify=train_df['winner_model_a'].astype(str) +
             train_df['winner_model_b'].astype(str)
)
```

**Stratified Split**: í´ë˜ìŠ¤ ë¶„í¬ë¥¼ ìœ ì§€í•˜ë©´ì„œ ë°ì´í„°ë¥¼ ë¶„í• í•˜ì—¬ ê²€ì¦ ì„¸íŠ¸ì˜ ëŒ€í‘œì„±ì„ ë³´ì¥í•©ë‹ˆë‹¤.

**ğŸ¯ ì™œ Stratified Splitì¸ê°€?**:
- ì„¸ í´ë˜ìŠ¤(`winner_model_a`, `winner_model_b`, `winner_tie`)ì˜ ë¹„ìœ¨ì´ ë¶ˆê· í˜•í•  ìˆ˜ ìˆìŒ
- Stratified splitì€ train/val ì„¸íŠ¸ ëª¨ë‘ì—ì„œ ë™ì¼í•œ í´ë˜ìŠ¤ ë¹„ìœ¨ ìœ ì§€
- ì˜ˆ: Trainì— winner_tieê°€ 20%ë¼ë©´, Valì—ë„ ì•½ 20% ìœ ì§€

**ğŸ’¡ ì´ˆë³´ì íŒ**:
- `test_size=0.1`ì€ 10% ê²€ì¦, 90% í•™ìŠµì„ ì˜ë¯¸
- `random_state=42`ëŠ” ì¬í˜„ì„±ì„ ìœ„í•œ ì‹œë“œ (ë‹¤ë¥¸ ìˆ«ìë„ ê°€ëŠ¥)

### 4. Dataset í´ë˜ìŠ¤ ([baseline.py:112-149](baseline.py#L112-L149))

```python
class LLMComparisonDataset(Dataset):
    def __init__(self, df, tokenizer, max_length, is_test=False):
        self.df = df.reset_index(drop=True)
        self.tokenizer = tokenizer
        self.max_length = max_length
        self.is_test = is_test

    def __getitem__(self, idx):
        row = self.df.iloc[idx]
        # ì…ë ¥ í˜•ì‹: prompt + [SEP] + response_a + [SEP] + response_b
        text = f"{row['prompt']} [SEP] {row['response_a']} [SEP] {row['response_b']}"

        encoding = self.tokenizer.encode_plus(
            text,
            add_special_tokens=True,  # [CLS], [SEP] í† í° ì¶”ê°€
            max_length=self.max_length,
            padding='max_length',     # ìµœëŒ€ ê¸¸ì´ê¹Œì§€ íŒ¨ë”©
            truncation=True,          # ìµœëŒ€ ê¸¸ì´ ì´ˆê³¼ ì‹œ ìë¥´ê¸°
            return_attention_mask=True,
            return_tensors='pt'
        )

        item = {
            'input_ids': encoding['input_ids'].flatten(),
            'attention_mask': encoding['attention_mask'].flatten(),
        }        

        if not self.is_test:
            labels = torch.tensor([
                row['winner_model_a'],
                row['winner_model_b'],
                row['winner_tie']
            ], dtype=torch.float)
            item['labels'] = labels

        return item
```

**í•µì‹¬ ë™ì‘**:
1. **ì…ë ¥ ê²°í•©**: Promptì™€ ë‘ ì‘ë‹µì„ `[SEP]` í† í°ìœ¼ë¡œ êµ¬ë¶„í•˜ì—¬ í•˜ë‚˜ì˜ ì‹œí€€ìŠ¤ë¡œ ë§Œë“­ë‹ˆë‹¤.
2. **í† í°í™”**: DistilBERT í† í¬ë‚˜ì´ì €ë¡œ í…ìŠ¤íŠ¸ë¥¼ í† í° IDë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
3. **íŒ¨ë”©/ìë¥´ê¸°**: ëª¨ë“  ì…ë ¥ì„ 256 í† í°ìœ¼ë¡œ í†µì¼í•©ë‹ˆë‹¤.
4. **ë ˆì´ë¸”**: í•™ìŠµ/ê²€ì¦ ë°ì´í„°ì˜ ê²½ìš° 3ê°œ í´ë˜ìŠ¤ì˜ í™•ë¥ ê°’ì„ ë°˜í™˜í•©ë‹ˆë‹¤.

**ğŸ¯ ì™œ ì´ëŸ° ì…ë ¥ í˜•ì‹ì¸ê°€?**:
```
ì…ë ¥: "What is AI? [SEP] AI is artificial intelligence... [SEP] Artificial Intelligence refers to..."
       â†“
í† í°í™”: [CLS] What is AI ? [SEP] AI is artificial ... [SEP] Artificial Intelligence ... [SEP] [PAD] [PAD] ...
       â†“
BERTê°€ ì„¸ ë¶€ë¶„ì˜ ê´€ê³„ë¥¼ í•™ìŠµ: Prompt â†’ Response A â†’ Response B
```

**ì‹¤ì œ ì˜ˆì‹œ**:
- `row['prompt']` = "Explain quantum computing"
- `row['response_a']` = "Quantum computing uses qubits..."
- `row['response_b']` = "A quantum computer leverages..."
- ê²°í•©ëœ í…ìŠ¤íŠ¸ = "Explain quantum computing [SEP] Quantum computing uses qubits... [SEP] A quantum computer leverages..."

**ğŸ’¡ ì´ˆë³´ì íŒ**:
- `max_length=256`ì´ í…ìŠ¤íŠ¸ë³´ë‹¤ ì§§ìœ¼ë©´ ë’·ë¶€ë¶„ì´ ì˜ë¦¼ (`truncation=True`)
- `padding='max_length'`ë¡œ ëª¨ë“  ì…ë ¥ì„ ë™ì¼í•œ ê¸¸ì´ë¡œ ë§Œë“¦ (ë°°ì¹˜ ì²˜ë¦¬ë¥¼ ìœ„í•´ í•„ìˆ˜)
- `return_attention_mask=True`ëŠ” ì‹¤ì œ í† í°ê³¼ íŒ¨ë”©ì„ êµ¬ë¶„í•˜ëŠ” ë§ˆìŠ¤í¬ ìƒì„±

### 5. Model í´ë˜ìŠ¤ ([baseline.py:156-171](baseline.py#L156-L171))

```python
class LLMComparisonModel(nn.Module):
    def __init__(self, model_name, num_classes=3):
        super(LLMComparisonModel, self).__init__()
        self.bert = AutoModel.from_pretrained(model_name)
        self.dropout = nn.Dropout(0.3)
        self.fc = nn.Linear(self.bert.config.hidden_size, num_classes)
        self.softmax = nn.Softmax(dim=1)

    def forward(self, input_ids, attention_mask):
        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)
        pooled_output = outputs.last_hidden_state[:, 0, :]  # [CLS] í† í°
        pooled_output = self.dropout(pooled_output)
        logits = self.fc(pooled_output)
        probs = self.softmax(logits)
        return probs
```

**êµ¬ì¡° ì„¤ëª…**:
1. **DistilBERT Encoder**: Pre-trained ëª¨ë¸ì„ ë¡œë“œí•˜ì—¬ í…ìŠ¤íŠ¸ ì¸ì½”ë”©
2. **[CLS] í† í° ì¶”ì¶œ**: `last_hidden_state[:, 0, :]`ë¡œ ì²« ë²ˆì§¸ í† í°(ë¬¸ì¥ ì „ì²´ í‘œí˜„)ì„ ê°€ì ¸ì˜´
3. **Dropout**: ê³¼ì í•© ë°©ì§€ë¥¼ ìœ„í•œ ì •ê·œí™” (30% í™•ë¥ ë¡œ ë‰´ëŸ° ë¹„í™œì„±í™”)
4. **Classification Head**: 768ì°¨ì› â†’ 3ì°¨ì›ìœ¼ë¡œ ì„ í˜• ë³€í™˜
5. **Softmax**: ë¡œì§“ì„ í™•ë¥ ê°’ìœ¼ë¡œ ë³€í™˜ (í•©ì´ 1)

**ğŸ¯ [CLS] í† í°ê³¼ ìŠ¬ë¼ì´ì‹± ì´í•´í•˜ê¸°**:

`[CLS]`ëŠ” "Classification"ì˜ ì•½ìë¡œ, BERTê°€ ë¬¸ì¥ ì „ì²´ ì˜ë¯¸ë¥¼ ìš”ì•½í•˜ë„ë¡ í•™ìŠµëœ íŠ¹ìˆ˜ í† í°ì…ë‹ˆë‹¤.

```python
# DistilBERT ì¶œë ¥ í˜•íƒœ
outputs.last_hidden_state: [ë°°ì¹˜_í¬ê¸°, ì‹œí€€ìŠ¤_ê¸¸ì´, íˆë“ _ì°¨ì›]
                          [16,       256,        768]
                           â†“         â†“           â†“
                         ìƒ˜í”Œìˆ˜   í† í°ìˆ˜    ë²¡í„°ì°¨ì›

# ê° í† í°ë§ˆë‹¤ 768ì°¨ì› ë²¡í„° ìƒì„±:
# [CLS] â†’ [0.23, 0.45, ..., 0.89]  (768ê°œ) â† ë¬¸ì¥ ì „ì²´ ì •ë³´ ì••ì¶•!
# What  â†’ [0.11, 0.33, ..., 0.77]  (768ê°œ)
# is    â†’ [0.44, 0.21, ..., 0.66]  (768ê°œ)
# ...

# [:, 0, :] ìŠ¬ë¼ì´ì‹± ì˜ë¯¸
pooled_output = outputs.last_hidden_state[:, 0, :]
#                                          â†‘  â†‘  â†‘
#                                          |  |  â””â”€ ëª¨ë“  768ê°œ ì°¨ì›
#                                          |  â””â”€â”€â”€â”€ 0ë²ˆì§¸ í† í° ([CLS])
#                                          â””â”€â”€â”€â”€â”€â”€â”€ ëª¨ë“  ë°°ì¹˜ ìƒ˜í”Œ

# ê²°ê³¼: [16, 768] - ê° ìƒ˜í”Œì˜ [CLS] í† í° ë²¡í„°ë§Œ ì¶”ì¶œ
```

**ë°ì´í„° íë¦„**:
```
[16, 256, 768]  â†’  [:, 0, :]  â†’  [16, 768]  â†’  Dropout  â†’  Linear  â†’  [16, 3]  â†’  Softmax  â†’  í™•ë¥ 
ëª¨ë“  í† í° ë²¡í„°      [CLS]ë§Œ       ì¤‘ê°„ í‘œí˜„                              ë¡œì§“              ìµœì¢… í™•ë¥ 
```

**ğŸ’¡ ì´ˆë³´ì íŒ**:
- `[CLS]` í† í°ì€ ì¤‘ê°„ ë‹¨ê³„ì´ë©°, ìµœì¢… ê²°ê³¼ëŠ” 3ê°œ í´ë˜ìŠ¤ì˜ í™•ë¥ ê°’ì…ë‹ˆë‹¤
- `nn.Dropout(0.3)`: í•™ìŠµ ì‹œ 30%ì˜ ë‰´ëŸ°ì„ ë¬´ì‘ìœ„ë¡œ ë” â†’ ê³¼ì í•© ë°©ì§€
- `nn.Linear(768, 3)`: 768ì°¨ì› ì…ë ¥ì„ 3ì°¨ì› ì¶œë ¥(3ê°œ í´ë˜ìŠ¤)ìœ¼ë¡œ ë³€í™˜
- `nn.Softmax(dim=1)`: ë¡œì§“ì„ í™•ë¥ ë¡œ ë³€í™˜, `dim=1`ì€ í´ë˜ìŠ¤ ì°¨ì›ì— ëŒ€í•´ ì ìš©

**ëª¨ë¸ í¬ê¸°**:
- DistilBERT íŒŒë¼ë¯¸í„°: ~66M
- Classification Head íŒŒë¼ë¯¸í„°: 768 Ã— 3 + 3 = 2,307
- ì „ì²´: ~66.4M íŒŒë¼ë¯¸í„°

### 6. í•™ìŠµ í•¨ìˆ˜ ([baseline.py:200-221](baseline.py#L200-L221))

```python
def train_epoch(model, dataloader, optimizer, scheduler, device):
    model.train()
    total_loss = 0

    for batch in dataloader:
        optimizer.zero_grad()

        # Forward pass
        input_ids = batch['input_ids'].to(device)
        attention_mask = batch['attention_mask'].to(device)
        labels = batch['labels'].to(device)
        outputs = model(input_ids, attention_mask)

        # Loss ê³„ì‚° (Binary Cross Entropy)
        loss = nn.BCELoss()(outputs, labels)

        # Backward pass
        loss.backward()
        optimizer.step()
        scheduler.step()

        total_loss += loss.item()

    return total_loss / len(dataloader)
```

**í•™ìŠµ í”„ë¡œì„¸ìŠ¤**:
1. **Forward Pass**: ì…ë ¥ â†’ ëª¨ë¸ â†’ ì˜ˆì¸¡ í™•ë¥ 
2. **Loss ê³„ì‚°**: BCE Lossë¡œ ì˜ˆì¸¡ê³¼ ì‹¤ì œ ë ˆì´ë¸” ê°„ ì°¨ì´ ì¸¡ì •
3. **Backward Pass**: ê·¸ë˜ë””ì–¸íŠ¸ ê³„ì‚° ë° íŒŒë¼ë¯¸í„° ì—…ë°ì´íŠ¸
4. **Scheduler**: Learning rateë¥¼ ì ì§„ì ìœ¼ë¡œ ê°ì†Œ (warmup ì‚¬ìš©)

**BCE Loss ì„ íƒ ì´ìœ **:
- Multi-label classificationìœ¼ë¡œ ê°„ì£¼ (ì—¬ëŸ¬ í´ë˜ìŠ¤ê°€ ë™ì‹œì— ë¶€ë¶„ì ìœ¼ë¡œ ì°¸ì¼ ìˆ˜ ìˆìŒ)
- í™•ë¥ ê°’ì´ soft labelë¡œ ì œê³µë¨ (ì˜ˆ: [0.5, 0.3, 0.2])

**ğŸ’¡ ì´ˆë³´ì íŒ**:
- `optimizer.zero_grad()`: ì´ì „ batchì˜ ê·¸ë˜ë””ì–¸íŠ¸ë¥¼ ì´ˆê¸°í™” (í•„ìˆ˜!)
- `loss.backward()`: ì—­ì „íŒŒë¡œ ê·¸ë˜ë””ì–¸íŠ¸ ê³„ì‚°
- `optimizer.step()`: ê³„ì‚°ëœ ê·¸ë˜ë””ì–¸íŠ¸ë¡œ íŒŒë¼ë¯¸í„° ì—…ë°ì´íŠ¸
- `scheduler.step()`: Learning rate ì¡°ì • (ë§¤ stepë§ˆë‹¤ í˜¸ì¶œ)

**í•™ìŠµ ê³¼ì • ì‹œê°í™”**:
```
Batch 1: loss=0.7182
Batch 2: loss=0.7036  â† ì¡°ê¸ˆì”© ê°ì†Œ
Batch 3: loss=0.7312
...
Batch 3234: loss=0.6161
Average Training Loss: 0.6308  â† ì „ì²´ í‰ê· 
```

### 7. ê²€ì¦ í•¨ìˆ˜ ([baseline.py:223-242](baseline.py#L223-L242))

```python
def validate(model, dataloader, device):
    model.eval()
    predictions = []
    actuals = []

    with torch.no_grad():  # ê·¸ë˜ë””ì–¸íŠ¸ ê³„ì‚° ì•ˆ í•¨
        for batch in dataloader:
            input_ids = batch['input_ids'].to(device)
            attention_mask = batch['attention_mask'].to(device)
            labels = batch['labels'].to(device)

            outputs = model(input_ids, attention_mask)
            predictions.append(outputs.cpu().numpy())
            actuals.append(labels.cpu().numpy())

    predictions = np.vstack(predictions)
    actuals = np.vstack(actuals)
    loss = log_loss(actuals, predictions)

    return loss, predictions, actuals
```

**í‰ê°€ ì§€í‘œ**: Log Loss (Cross Entropy)
- Kaggle ëŒ€íšŒì˜ ê³µì‹ í‰ê°€ ì§€í‘œ
- í™•ë¥  ì˜ˆì¸¡ì˜ ì •í™•ë„ë¥¼ ì¸¡ì •

### 8. Optimizerì™€ Scheduler ([baseline.py:249-251](baseline.py#L249-L251))

```python
optimizer = AdamW(model.parameters(), lr=config.LEARNING_RATE)
scheduler = get_linear_schedule_with_warmup(
    optimizer,
    num_warmup_steps=0,
    num_training_steps=total_steps
)
```

**AdamW**: Adam optimizerì˜ ê°œì„  ë²„ì „ìœ¼ë¡œ weight decayë¥¼ ì˜¬ë°”ë¥´ê²Œ ì²˜ë¦¬í•©ë‹ˆë‹¤.

**Linear Schedule with Warmup**:
- Learning rateë¥¼ ì„ í˜•ì ìœ¼ë¡œ ê°ì†Œì‹œì¼œ ì•ˆì •ì ì¸ ìˆ˜ë ´ì„ ë„ì›€
- Warmup ì—†ì´ ì‹œì‘ (BERT fine-tuningì—ì„œ ì¼ë°˜ì )

### 9. í•™ìŠµ ë£¨í”„ ([baseline.py:265-278](baseline.py#L265-L278))

```python
best_val_loss = float('inf')

for epoch in range(config.EPOCHS):
    train_loss = train_epoch(model, train_loader, optimizer, scheduler, config.DEVICE)
    val_loss, val_preds, val_actuals = validate(model, val_loader, config.DEVICE)

    # Best model ì €ì¥
    if val_loss < best_val_loss:
        best_val_loss = val_loss
        torch.save(model.state_dict(), config.MODEL_SAVE_PATH)
```

**Early Stopping íŒ¨í„´**:
- ê²€ì¦ ì†ì‹¤ì´ ê°œì„ ë  ë•Œë§Œ ëª¨ë¸ ì €ì¥
- ê³¼ì í•© ë°©ì§€

### 10. ì˜ˆì¸¡ ë° ì œì¶œ ([baseline.py:292-313](baseline.py#L292-L313))

```python
# Best model ë¡œë“œ
model.load_state_dict(torch.load(config.MODEL_SAVE_PATH))
model.eval()

# í…ŒìŠ¤íŠ¸ ë°ì´í„° ì˜ˆì¸¡
predictions = []
with torch.no_grad():
    for batch in test_loader:
        input_ids = batch['input_ids'].to(config.DEVICE)
        attention_mask = batch['attention_mask'].to(config.DEVICE)
        outputs = model(input_ids, attention_mask)
        predictions.append(outputs.cpu().numpy())

predictions = np.vstack(predictions)

# ì œì¶œ íŒŒì¼ ìƒì„±
submission = sample_submission.copy()
submission['winner_model_a'] = predictions[:, 0]
submission['winner_model_b'] = predictions[:, 1]
submission['winner_tie'] = predictions[:, 2]

submission.to_csv(config.SUBMISSION_PATH, index=False)
```

**ìµœì¢… ë‹¨ê³„**:
1. ì €ì¥ëœ best model ë¡œë“œ
2. í…ŒìŠ¤íŠ¸ ë°ì´í„°ì— ëŒ€í•´ ì˜ˆì¸¡ ìˆ˜í–‰
3. í™•ë¥ ê°’ì„ ì œì¶œ í˜•ì‹ì— ë§ê²Œ ë³€í™˜
4. CSV íŒŒì¼ë¡œ ì €ì¥

### ì „ì²´ ì‹¤í–‰ ì˜ˆì‹œ (ë¡œì»¬ ì‹¤í–‰ ê²°ê³¼)

```
Libraries imported successfully!
PyTorch version: 2.9.0+cu128
CUDA available: True
Random seeds set to 42

Configuration:
  Model: ./models/distilbert-base-uncased
  Device: cuda
  Batch Size: 16
  Epochs: 1

Loading data...
Train data shape: (57477, 9)
Test data shape: (3, 4)
Train size: 51729, Validation size: 5748

Loading tokenizer and model...
âœ“ Model loaded on: cuda
âœ“ Model parameters: 66,365,187

============================================================
STARTING TRAINING
============================================================

Epoch 1/1
------------------------------------------------------------
Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3234/3234 [10:50<00:00, 4.97it/s, loss=0.6161]
Training loss: 0.6308
Validating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 360/360 [00:21<00:00, 16.52it/s]
Validation loss: 1.0750
âœ“ Model saved with validation loss: 1.0750

============================================================
TRAINING COMPLETED! Best validation loss: 1.0750
============================================================

Making predictions on test data...
Predicting: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 15.31it/s]
âœ“ Predictions shape: (3, 3)

============================================================
SUBMISSION FILE CREATED
============================================================
Saved to: outputs/submission.csv

First few predictions:
        id  winner_model_a  winner_model_b  winner_tie
0   136060        0.233993        0.215965    0.550042
1   211333        0.317617        0.405372    0.277011
2  1233961        0.355113        0.364986    0.279901

Probability sums check:
  Min: 1.000000
  Max: 1.000000
  Mean: 1.000000

âœ“ All probabilities sum to ~1.0. Submission is valid!
```

**ì‹¤í–‰ ì‹œê°„ ë¶„ì„**:
- í•™ìŠµ: 10ë¶„ 50ì´ˆ (3,234 batches, ~4.97 it/s)
- ê²€ì¦: 21ì´ˆ (360 batches, ~16.52 it/s)
- ì˜ˆì¸¡: 1ì´ˆ ë¯¸ë§Œ (í…ŒìŠ¤íŠ¸ ë°ì´í„° 3ê°œ)
- **ì´ ì†Œìš” ì‹œê°„**: ì•½ 11ë¶„ (GPU T4 ê¸°ì¤€, 1 epoch)

**ì„±ëŠ¥ ë¶„ì„**:
- Training Loss: 0.6308 (BCE Loss)
- Validation Loss: 1.0750 (Log Loss)
- Validation lossê°€ training lossë³´ë‹¤ ë†’ìŒ â†’ ì•½ê°„ì˜ ê³¼ì í•© ë˜ëŠ” ë°ì´í„° ë¶„í¬ ì°¨ì´

**ê°œì„  ë°©í–¥**:
- Epochsë¥¼ 3-5ë¡œ ì¦ê°€ì‹œì¼œ ì„±ëŠ¥ í–¥ìƒ
- Validation loss ëª¨ë‹ˆí„°ë§í•˜ë©° early stopping ì ìš©
- Learning rate ì¡°ì • ë˜ëŠ” warmup ì¶”ê°€

## ì¤‘ìš” ì‚¬í•­

### Code Competition ê·œì •

ì´ ëŒ€íšŒëŠ” **Code Competition**ì…ë‹ˆë‹¤. ëª¨ë“  ì œì¶œì€ Kaggle Notebookì„ í†µí•´ ì´ë£¨ì–´ì ¸ì•¼ í•©ë‹ˆë‹¤.

#### ì œì¶œ ìš”êµ¬ì‚¬í•­
- âœ… **CPU Notebook**: ìµœëŒ€ 9ì‹œê°„ ì‹¤í–‰ ì‹œê°„
- âœ… **GPU Notebook**: ìµœëŒ€ 9ì‹œê°„ ì‹¤í–‰ ì‹œê°„
- âŒ **ì¸í„°ë„· ë¹„í™œì„±í™”**: ì™¸ë¶€ ë°ì´í„° ë‹¤ìš´ë¡œë“œ ë¶ˆê°€ (ë°ì´í„°ì…‹ìœ¼ë¡œ ë¯¸ë¦¬ ì—…ë¡œë“œ í•„ìš”)
- âœ… **ì™¸ë¶€ ë°ì´í„° í—ˆìš©**: ê³µê°œì ì´ê³  í•©ë¦¬ì  ë¹„ìš©ì˜ ë°ì´í„°/ëª¨ë¸ ì‚¬ìš© ê°€ëŠ¥ (ìì„¸í•œ ë‚´ìš©ì€ í•˜ë‹¨ "ì™¸ë¶€ ë°ì´í„° ë° ë„êµ¬" ì°¸ì¡°)
- âœ… **ì œì¶œ íŒŒì¼ëª…**: ë°˜ë“œì‹œ `submission.csv`ì—¬ì•¼ í•¨
- ğŸ“ **ì‹¤í–‰ ì‹œê°„ ë‚œë…í™”**: ë™ì¼í•œ ì œì¶œë„ ìµœëŒ€ 15ë¶„ì˜ ì°¨ì´ ë°œìƒ

#### ì œì¶œ ë°©ì‹
1. âŒ **CSV íŒŒì¼ ì§ì ‘ ì œì¶œ ë¶ˆê°€**: submission.csvë¥¼ ì§ì ‘ ì—…ë¡œë“œí•  ìˆ˜ ì—†ìŒ
2. âœ… **ë…¸íŠ¸ë¶ ì œì¶œ**: Kaggle ë…¸íŠ¸ë¶ ìì²´ë¥¼ ì œì¶œí•˜ë©´ ìë™ìœ¼ë¡œ ì‹¤í–‰ë˜ì–´ í‰ê°€ë¨
3. âŒ **ë¡œì»¬ í•™ìŠµ ëª¨ë¸ ì—…ë¡œë“œ ë¶ˆê°€**: ë¡œì»¬ì—ì„œ í•™ìŠµí•œ ëª¨ë¸ì„ ì‚¬ìš©í•  ìˆ˜ ì—†ìŒ
4. âœ… **Pre-trained ëª¨ë¸ í—ˆìš©**: DistilBERT, BERT ë“± ê³µê°œ ëª¨ë¸ì€ ë°ì´í„°ì…‹ìœ¼ë¡œ ì—…ë¡œë“œ í›„ ì‚¬ìš© ê°€ëŠ¥

#### ì œì¶œ í”„ë¡œì„¸ìŠ¤
```
1. ë…¸íŠ¸ë¶ ì‘ì„± â†’ 2. ë…¸íŠ¸ë¶ ì œì¶œ â†’ 3. Kaggleì´ ìë™ ì‹¤í–‰
   â†’ 4. submission.csv ìƒì„± â†’ 5. ìë™ í‰ê°€ â†’ 6. ë¦¬ë”ë³´ë“œ ì—…ë°ì´íŠ¸
```

**ì¤‘ìš”**: `submission.csv`ì™€ `best_model.pt`ëŠ” ë…¸íŠ¸ë¶ ì‹¤í–‰ ì¤‘ì— ìë™ìœ¼ë¡œ ìƒì„±ë˜ì–´ì•¼ í•˜ë©°, Kaggleì´ ì´ë¥¼ í‰ê°€ì— ì‚¬ìš©í•©ë‹ˆë‹¤. ìš°ë¦¬ê°€ ì§ì ‘ ì—…ë¡œë“œí•˜ëŠ” ê²ƒì´ ì•„ë‹™ë‹ˆë‹¤.

### ëŒ€íšŒ ê·œì¹™ (Competition Rules)

#### ì°¸ê°€ ìê²© (Eligibility)
- **ê³„ì •**: Kaggle.comì— ë“±ë¡ëœ ê³„ì • ë³´ìœ ì
- **ì—°ë ¹**: 18ì„¸ ì´ìƒ ë˜ëŠ” ê±°ì£¼ ì§€ì—­ì˜ ì„±ë…„ ë‚˜ì´
- **ê±°ì£¼ ì§€ì—­ ì œí•œ**: ë‹¤ìŒ ì§€ì—­ ê±°ì£¼ìëŠ” ì°¸ê°€ ë¶ˆê°€
  - í¬ë¦¼ë°˜ë„, ë„ë„¤ì¸ í¬, ë£¨í•œìŠ¤í¬, ì¿ ë°”, ì´ë€, ì‹œë¦¬ì•„, ë¶í•œ
  - ë¯¸êµ­ ìˆ˜ì¶œ í†µì œ ë˜ëŠ” ì œì¬ ëŒ€ìƒì
- **ë³µìˆ˜ ê³„ì • ê¸ˆì§€**: í•œ ëª…ë‹¹ í•˜ë‚˜ì˜ Kaggle ê³„ì •ë§Œ ì‚¬ìš© ê°€ëŠ¥

#### íŒ€ ê·œì • (Team Rules)
- **ìµœëŒ€ íŒ€ í¬ê¸°**: 10ëª…
- **íŒ€ í•©ë³‘**: í—ˆìš©ë¨ (ë‹¨, ì œì¶œ íšŸìˆ˜ ì œí•œ ë‚´ì—ì„œ)
- **íŒ€ êµ¬ì„±**: ê° íŒ€ì›ì€ ê°œë³„ Kaggle ê³„ì • í•„ìš”
- **íŒ€ í•©ë³‘ ì¡°ê±´**:
  - í•©ë³‘ í›„ íŒ€ í¬ê¸°ê°€ ìµœëŒ€ ì œí•œ ì´ë‚´
  - í•©ë³‘ ì‹œì ê¹Œì§€ì˜ ì œì¶œ íšŸìˆ˜ê°€ í—ˆìš© ë²”ìœ„ ì´ë‚´
  - í•©ë³‘ ë§ˆê°ì¼ ì´ì „ì— ì™„ë£Œ

#### ì œì¶œ ì œí•œ (Submission Limits)
- **ì¼ì¼ ì œì¶œ**: ìµœëŒ€ 10íšŒ
- **ìµœì¢… ì œì¶œ**: ìµœëŒ€ 2ê°œ ì„ íƒ ê°€ëŠ¥ (ìµœì¢… í‰ê°€ìš©)

#### ë°ì´í„° ì‚¬ìš© ê·œì • (Data Usage)
- **ë¼ì´ì„ ìŠ¤**: CC BY-NC 4.0
- **ë¹„ìƒì—…ì  ìš©ë„ë§Œ í—ˆìš©**: ëŒ€íšŒ ì°¸ê°€, Kaggle í¬ëŸ¼, í•™ìˆ  ì—°êµ¬ ë° êµìœ¡ ëª©ì 
- **ê¸ˆì§€ ì‚¬í•­**:
  - Hand-labeling (ìˆ˜ë™ ë¼ë²¨ë§) ê¸ˆì§€
  - ê²€ì¦/í…ŒìŠ¤íŠ¸ ë°ì´í„°ì— ëŒ€í•œ ìˆ˜ë™ ì˜ˆì¸¡ ê¸ˆì§€
  - ëŒ€íšŒ ë°ì´í„°ë¥¼ ëŒ€íšŒ ì™¸ ì°¸ê°€ìì—ê²Œ ê³µìœ  ê¸ˆì§€
- **ë­í‚¹ í¬ì¸íŠ¸**: ê³µê°œ ë°ì´í„° íŠ¹ì„±ìƒ Kaggle ë­í‚¹ í¬ì¸íŠ¸ ë¯¸ë¶€ì—¬

#### ì™¸ë¶€ ë°ì´í„° ë° ë„êµ¬ (External Data & Tools)
**í—ˆìš©ë˜ëŠ” ì™¸ë¶€ ë°ì´í„°**:
- âœ… ëª¨ë“  ì°¸ê°€ìì—ê²Œ ê³µê°œì ì´ê³  ë™ë“±í•˜ê²Œ ì ‘ê·¼ ê°€ëŠ¥í•œ ë°ì´í„°
- âœ… ë¹„ìš©ì´ ë“¤ì§€ ì•Šê±°ë‚˜ ìµœì†Œ ë¹„ìš©ìœ¼ë¡œ ì´ìš© ê°€ëŠ¥í•œ ë°ì´í„°
- âœ… "í•©ë¦¬ì„± ê¸°ì¤€(Reasonableness Standard)" ì¶©ì¡± ì‹œ ì‚¬ìš© ê°€ëŠ¥

**ì˜ˆì‹œ**:
- âœ… **í—ˆìš©**: Gemini Advanced ê°™ì€ ì†Œì•¡ êµ¬ë…ë£Œ (í•©ë¦¬ì  ë¹„ìš©)
- âŒ **ë¶ˆí—ˆ**: ëŒ€íšŒ ìƒê¸ˆì„ ì´ˆê³¼í•˜ëŠ” ë…ì  ë°ì´í„°ì…‹ ë¼ì´ì„ ìŠ¤

**ìë™í™”ëœ ML ë„êµ¬ (AMLT)**:
- Google AutoML, H2O Driverless AI ë“± ì‚¬ìš© ê°€ëŠ¥
- ë‹¨, ì ì ˆí•œ ë¼ì´ì„ ìŠ¤ ë³´ìœ  ë° ëŒ€íšŒ ê·œì¹™ ì¤€ìˆ˜ í•„ìš”

#### ì½”ë“œ ê³µìœ  ê·œì • (Code Sharing)
**ë¹„ê³µê°œ ê³µìœ  (Private Sharing)**:
- âŒ **ê¸ˆì§€**: íŒ€ ì™¸ë¶€ë¡œ Competition Code ë¹„ê³µê°œ ê³µìœ 
- âŒ **ê¸ˆì§€**: íŒ€ ê°„ ì½”ë“œ ê³µìœ  (íŒ€ í•©ë³‘ ì œì™¸)
- ìœ„ë°˜ ì‹œ ì‹¤ê²© ì²˜ë¦¬

**ê³µê°œ ê³µìœ  (Public Sharing)**:
- âœ… **í—ˆìš©**: Competition Codeë¥¼ ê³µê°œì ìœ¼ë¡œ ê³µìœ  ê°€ëŠ¥
- âœ… **ì¡°ê±´**: Kaggle.comì˜ í•´ë‹¹ ëŒ€íšŒ Discussion ë˜ëŠ” Notebooksì— ê³µìœ 
- âœ… **ë¼ì´ì„ ìŠ¤**: Open Source Initiative ìŠ¹ì¸ ë¼ì´ì„ ìŠ¤ ì ìš©
- âœ… **ìƒì—…ì  ì‚¬ìš©**: ê³µìœ  ì½”ë“œì˜ ìƒì—…ì  ì‚¬ìš© ì œí•œ ë¶ˆê°€

**ì˜¤í”ˆ ì†ŒìŠ¤ ì‚¬ìš©**:
- Open Source Initiative ìŠ¹ì¸ ë¼ì´ì„ ìŠ¤ë§Œ ì‚¬ìš© ê°€ëŠ¥
- ìƒì—…ì  ì‚¬ìš©ì„ ì œí•œí•˜ì§€ ì•ŠëŠ” ë¼ì´ì„ ìŠ¤

#### ìš°ìŠ¹ì ê²°ì • (Winner Determination)
- **í‰ê°€ ê¸°ì¤€**: Private Leaderboard ì ìˆ˜
- **Public Leaderboard**: ê³µê°œ í…ŒìŠ¤íŠ¸ ì„¸íŠ¸ ê¸°ë°˜ (ëŒ€íšŒ ì¤‘ ê³µê°œ)
- **Private Leaderboard**: ë¹„ê³µê°œ í…ŒìŠ¤íŠ¸ ì„¸íŠ¸ ê¸°ë°˜ (ìµœì¢… ìˆœìœ„ ê²°ì •)
- **ë™ì  ì²˜ë¦¬**: ë¨¼ì € ì œì¶œí•œ íŒ€ì´ ìš°ìŠ¹

#### ì‹¤ê²© ì‚¬ìœ  (Disqualification)
ë‹¤ìŒ í–‰ìœ„ ì‹œ ì‹¤ê²© ì²˜ë¦¬ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤:
- ë¶€ì •í–‰ìœ„, ì†ì„ìˆ˜, ë¶ˆê³µì • í”Œë ˆì´
- ë‹¤ë¥¸ ì°¸ê°€ì, ì£¼ìµœì, Kaggleì„ ìœ„í˜‘í•˜ê±°ë‚˜ ê´´ë¡­í˜
- ê·œì¹™ ìœ„ë°˜ (ë³µìˆ˜ ê³„ì •, ì½”ë“œ ë¹„ê³µê°œ ê³µìœ  ë“±)
- ëŒ€íšŒì˜ í•©ë²•ì  ìš´ì˜ì„ ë°©í•´í•˜ëŠ” í–‰ìœ„

#### ì§€ì  ì¬ì‚°ê¶Œ (Intellectual Property)
- **ì œì¶œë¬¼ ì†Œìœ ê¶Œ**: ì°¸ê°€ìê°€ ì œì¶œë¬¼ì˜ ë…ì ì  ì†Œìœ ìì—¬ì•¼ í•¨
- **ê¸ˆì§€ ì‚¬í•­**:
  - ì œ3ìì˜ ì§€ì ì¬ì‚°ê¶Œ ì¹¨í•´
  - ì €ì‘ê¶Œ, ìƒí‘œê¶Œ, íŠ¹í—ˆê¶Œ, ì˜ì—…ë¹„ë°€, ê°œì¸ì •ë³´ ì¹¨í•´
  - ëª…ì˜ˆí›¼ì†
- **ë³´ìƒ**: ì¹¨í•´ ë°œìƒ ì‹œ ì°¸ê°€ìê°€ ëŒ€íšŒ ì£¼ìµœìì—ê²Œ ë°°ìƒ ì±…ì„

#### ìƒê¸ˆ ë° ì„¸ê¸ˆ (Prizes & Taxes)
- **ì´ ëŒ€íšŒ**: Getting Started Competitionìœ¼ë¡œ ìƒê¸ˆ ì—†ìŒ
- **ì¼ë°˜ ê·œì •**: ìƒê¸ˆì´ ìˆëŠ” ê²½ìš° ëª¨ë“  ì„¸ê¸ˆì€ ìš°ìŠ¹ì ì±…ì„
- **íŒ€ ìƒê¸ˆ**: ê· ë“± ë¶„ë°° (íŒ€ì› í•©ì˜ ì‹œ ë‹¤ë¥¸ ë¶„ë°° ê°€ëŠ¥)

#### ê°œì¸ì •ë³´ ë³´í˜¸ (Privacy)
- Kaggleê³¼ ëŒ€íšŒ ì£¼ìµœìê°€ ê°œì¸ì •ë³´ ìˆ˜ì§‘ ë° ì‚¬ìš©
- Kaggle Privacy Policy ì ìš©
- ëŒ€íšŒ ì£¼ìµœìì—ê²Œ ê°œì¸ì •ë³´ ì „ì†¡ (êµ­ê°€ ê°„ ì „ì†¡ í¬í•¨)

#### ë²•ë¥  ë° ê´€í• ê¶Œ (Governing Law)
- **ì¤€ê±°ë²•**: ìº˜ë¦¬í¬ë‹ˆì•„ ì£¼ë²•
- **ê´€í•  ë²•ì›**: ìº˜ë¦¬í¬ë‹ˆì•„ Santa Clara ì¹´ìš´í‹° ì—°ë°© ë˜ëŠ” ì£¼ ë²•ì›

#### ëŒ€íšŒ íƒ€ì„ë¼ì¸
- **ì‹œì‘ì¼**: 2024ë…„ 10ì›” 16ì¼
- **ì¢…ë£Œì¼**: ì—†ìŒ (ë¬´ê¸°í•œ ìš´ì˜)
- **ë¡¤ë§ ë¦¬ë”ë³´ë“œ**: 2ê°œì›” ì´ìƒ ëœ ì œì¶œì€ ìë™ ì œê±°

#### ê³ ìš© ê´€ê³„ ë¶€ì¬
- ëŒ€íšŒ ì°¸ê°€ëŠ” ê³ ìš© ì œì•ˆ ë˜ëŠ” ê³ ìš© ê³„ì•½ì„ êµ¬ì„±í•˜ì§€ ì•ŠìŒ
- ì œì¶œë¬¼ì€ ìë°œì ìœ¼ë¡œ ì œê³µë˜ë©° ì‹ ë¢° ê´€ê³„ ì•„ë‹˜

### ë¡œì»¬ vs Kaggle ì½”ë“œ ì°¨ì´

ë‘ íŒŒì¼ì€ **ê²½ë¡œë§Œ ë‹¤ë¥´ê³  ë‚˜ë¨¸ì§€ ì½”ë“œëŠ” ë™ì¼**í•©ë‹ˆë‹¤:

| êµ¬ë¶„ | baseline_local.py | baseline.py |
|------|-------------------|-------------|
| ëª¨ë¸ ê²½ë¡œ | `./models/distilbert-base-uncased` | `/kaggle/input/distilbert-base-uncased/...` |
| ë°ì´í„° ê²½ë¡œ | `./data/train.csv` | `/kaggle/input/llm-classification-finetuning/train.csv` |
| ì¶œë ¥ ê²½ë¡œ | `./outputs/` | `/kaggle/working/` |
| ìš©ë„ | ë¡œì»¬ í…ŒìŠ¤íŠ¸ ë° ê²€ì¦ | Kaggle ì œì¶œìš© |
| ê²½ë¡œ ê²€ì¦ | íŒŒì¼ ì¡´ì¬ ì—¬ë¶€ í™•ì¸ í›„ ì¢…ë£Œ | ì¡´ì¬ ì—¬ë¶€ë§Œ ì¶œë ¥ |

## ì„±ëŠ¥ ê°œì„  ì•„ì´ë””ì–´

### 1. ëª¨ë¸ ê°œì„ 
- **ë” í° ëª¨ë¸**: BERT-base (110M), RoBERTa-base (125M), DeBERTa-base (140M)
- **ì•™ìƒë¸”**: ì—¬ëŸ¬ ëª¨ë¸ì˜ ì˜ˆì¸¡ í‰ê·  ë˜ëŠ” ê°€ì¤‘ í‰ê· 
- **Multi-task Learning**: ê´€ë ¨ íƒœìŠ¤í¬ë¥¼ í•¨ê»˜ í•™ìŠµ

### 2. í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹
- **Epochs ëŠ˜ë¦¬ê¸°**: 3-5 epochs (ê³¼ì í•© ì£¼ì˜)
- **Learning rate ì¡°ì •**: 1e-5 ~ 5e-5 ë²”ìœ„ì—ì„œ ì‹¤í—˜
- **Batch size ì¡°ì •**: 8, 16, 32 (ë©”ëª¨ë¦¬ í—ˆìš© ë²”ìœ„ ë‚´)
- **Dropout ë¹„ìœ¨**: 0.1 ~ 0.5 ì‚¬ì´ì—ì„œ ì¡°ì •
- **Max Length**: 512ë¡œ ëŠ˜ë ¤ ë” ê¸´ ì»¨í…ìŠ¤íŠ¸ í™œìš©
- **Warmup steps**: ì „ì²´ stepì˜ 10% ì •ë„ ì‚¬ìš©

### 3. ì…ë ¥ í˜•ì‹ ê°œì„ 
- **ê° ì‘ë‹µ ë³„ë„ ì¸ì½”ë”©**: `[CLS] prompt [SEP] response_a [SEP]`ì™€ `[CLS] prompt [SEP] response_b [SEP]`ë¥¼ ê°ê° ì¸ì½”ë”© í›„ ê²°í•©
- **Cross-attention**: ë‘ ì‘ë‹µ ê°„ ìƒí˜¸ì‘ìš© ëª¨ë¸ë§
- **íŠ¹ìˆ˜ í† í° ì¶”ê°€**: ì‘ë‹µ ì‹œì‘/ëì„ ëª…ì‹œì ìœ¼ë¡œ í‘œì‹œ

### 4. ì¶”ê°€ íŠ¹ì„± í™œìš©
- **ì‘ë‹µ ê¸¸ì´**: ê¸¸ì´ê°€ í’ˆì§ˆê³¼ ìƒê´€ê´€ê³„ê°€ ìˆì„ ìˆ˜ ìˆìŒ
- **ëª¨ë¸ ì´ë¦„**: model_a, model_b ì •ë³´ í™œìš©
- **í…ìŠ¤íŠ¸ í†µê³„**: ë¬¸ì¥ ìˆ˜, ë‹¨ì–´ ë‹¤ì–‘ì„± ë“±

### 5. ë°ì´í„° ì¦ê°•
- **Back-translation**: ë‹¤ë¥¸ ì–¸ì–´ë¡œ ë²ˆì—­ í›„ ë‹¤ì‹œ ë²ˆì—­
- **Paraphrasing**: ë™ì¼í•œ ì˜ë¯¸ì˜ ë‹¤ë¥¸ í‘œí˜„ ìƒì„±
- **Mixup/Cutout**: í…ìŠ¤íŠ¸ ì¼ë¶€ë¥¼ ë§ˆìŠ¤í‚¹í•˜ê±°ë‚˜ ì„ê¸°

### 6. ì •ê·œí™” ê¸°ë²•
- **Label Smoothing**: Hard labelì„ ë¶€ë“œëŸ½ê²Œ ë§Œë“¤ê¸°
- **Weight Decay**: L2 ì •ê·œí™” ê°•ë„ ì¡°ì •
- **Gradient Clipping**: ê·¸ë˜ë””ì–¸íŠ¸ í­ë°œ ë°©ì§€

### 7. í•™ìŠµ ì „ëµ
- **Gradual Unfreezing**: BERT ë ˆì´ì–´ë¥¼ ì ì§„ì ìœ¼ë¡œ í•™ìŠµ
- **Discriminative Learning Rate**: ë ˆì´ì–´ë³„ë¡œ ë‹¤ë¥¸ learning rate ì ìš©
- **K-Fold Cross Validation**: ì—¬ëŸ¬ foldë¡œ í•™ìŠµ í›„ ì•™ìƒë¸”

## ë¬¸ì œ í•´ê²°

### Out of Memory ì—ëŸ¬
[baseline.py:57](baseline.py#L57) ë˜ëŠ” [baseline_local.py:48](baseline_local.py#L48)ì—ì„œ:
```python
BATCH_SIZE = 8  # 16 â†’ 8 ë˜ëŠ” 4ë¡œ ì¤„ì´ê¸°
MAX_LENGTH = 128  # 256 â†’ 128ë¡œ ì¤„ì´ê¸°
```

### ëª¨ë¸ ê²½ë¡œ ì—ëŸ¬ (Kaggle)
Kaggleì—ì„œ ë°ì´í„°ì…‹ì´ ì œëŒ€ë¡œ ì¶”ê°€ë˜ì—ˆëŠ”ì§€ í™•ì¸:
```python
import os
print(os.listdir('/kaggle/input'))

# ì˜¬ë°”ë¥¸ ê²½ë¡œ ì°¾ê¸°
for root, dirs, files in os.walk('/kaggle/input/distilbert-base-uncased'):
    if 'config.json' in files:
        print(f"Model path: {root}")
        break
```

### ë¡œì»¬ ì‹¤í–‰ ì‹œ ë°ì´í„°/ëª¨ë¸ ê²½ë¡œ ì—ëŸ¬
- `data/` í´ë”ì— train.csv, test.csv, sample_submission.csvê°€ ìˆëŠ”ì§€ í™•ì¸
- `models/distilbert-base-uncased/` í´ë”ì— ëª¨ë¸ íŒŒì¼ì´ ìˆëŠ”ì§€ í™•ì¸
- ì—†ë‹¤ë©´ ìœ„ì˜ "ë¹ ë¥¸ ì‹œì‘" ì„¹ì…˜ ì°¸ì¡°

### íŒ¨í‚¤ì§€ ì„¤ì¹˜ ì—ëŸ¬
```bash
pip install --upgrade pip
pip install -r requirements.txt

# CUDA ê´€ë ¨ ì—ëŸ¬ ì‹œ
pip install torch --index-url https://download.pytorch.org/whl/cu118
```

### í•™ìŠµì´ ë„ˆë¬´ ëŠë¦° ê²½ìš°
- GPUê°€ ì œëŒ€ë¡œ ì‚¬ìš©ë˜ê³  ìˆëŠ”ì§€ í™•ì¸: `torch.cuda.is_available()` â†’ `True`
- DataLoaderì˜ `num_workers` ì¡°ì • (2~4 ì¶”ì²œ)
- Mixed Precision Training ì‚¬ìš©: `torch.cuda.amp` í™œìš©

### Validation Lossê°€ ê°œì„ ë˜ì§€ ì•ŠëŠ” ê²½ìš°
- Learning rateë¥¼ ë‚®ì¶”ê¸° (1e-5)
- Epochsë¥¼ ëŠ˜ë¦¬ê¸° (3-5)
- ë°ì´í„°ë¥¼ ë‹¤ì‹œ í™•ì¸ (í´ë˜ìŠ¤ ë¶ˆê· í˜• ë“±)
- Dropoutì„ ë‚®ì¶”ê¸° (0.1-0.2)

## FAQ (ìì£¼ ë¬»ëŠ” ì§ˆë¬¸)

### Q1: Getting Started Competitionì´ë€?
**A**: Kaggleì´ ë¨¸ì‹ ëŸ¬ë‹ ì´ˆë³´ìë¥¼ ìœ„í•´ ë§Œë“  ë¹„ê²½ìŸ ëŒ€íšŒì…ë‹ˆë‹¤.
- **ëª©ì **: ë¨¸ì‹ ëŸ¬ë‹ ê¸°ì´ˆ ê°œë… í•™ìŠµ ë° Kaggle í”Œë«í¼ ìµíˆê¸°
- **íŠ¹ì§•**: ìƒê¸ˆ ì—†ìŒ, ë¬´ê¸°í•œ ìš´ì˜, ì»¤ë®¤ë‹ˆí‹° êµë¥˜ ì¤‘ì‹¬
- **ëŒ€ìƒ**: ë°ì´í„° ì‚¬ì´ì–¸ìŠ¤ ì…ë¬¸ì ë˜ëŠ” MOOC ìˆ˜ê°• ì™„ë£Œì

### Q2: íŒ€ì€ ì–´ë–»ê²Œ ë§Œë“œë‚˜ìš”?
**A**: ëŒ€íšŒ ê·œì¹™ì— ë™ì˜í•˜ë©´ ìë™ìœ¼ë¡œ ê°œì¸ íŒ€ì´ ìƒì„±ë©ë‹ˆë‹¤.
- **íŒ€ ê´€ë¦¬**: More > Team í˜ì´ì§€ì—ì„œ ê´€ë¦¬
- **íŒ€ ì´ˆëŒ€**: ë‹¤ë¥¸ ì‚¬ëŒì„ ì´ˆëŒ€í•˜ê±°ë‚˜ íŒ€ í•©ë³‘ ê°€ëŠ¥
- **íŒ€ ì°¾ê¸°**: Team íƒ­ì—ì„œ íŒ€ì›ì„ ì°¾ëŠ” ê¸€ ê²Œì‹œ ê°€ëŠ¥
- **íŒ€ì˜ ì¥ì **: ìƒˆë¡œìš´ ê¸°ìˆ ì„ ë°°ìš°ê³  ì¦ê²ê²Œ ê²½ìŸí•˜ëŠ” ìµœê³ ì˜ ë°©ë²•

### Q3: Kaggle Notebooksë€?
**A**: ì¬í˜„ ê°€ëŠ¥í•˜ê³  í˜‘ì—… ê°€ëŠ¥í•œ í´ë¼ìš°ë“œ ì»´í“¨íŒ… í™˜ê²½ì…ë‹ˆë‹¤.
- **ì§€ì› ì–¸ì–´**: Python, R
- **ì§€ì› í˜•ì‹**: Jupyter Notebooks, RMarkdown
- **ë¬´ë£Œ ë¦¬ì†ŒìŠ¤**: GPU ì‚¬ìš© ê°€ëŠ¥
- **ê³µìœ  ê¸°ëŠ¥**: Code íƒ­ì—ì„œ ë‹¤ë¥¸ ì°¸ê°€ìì˜ ë…¸íŠ¸ë¶ í™•ì¸ ê°€ëŠ¥
- **í•™ìŠµ ìë£Œ**: [Kaggle Courses](https://www.kaggle.com/learn) ì°¸ê³ 

### Q4: ë‚´ íŒ€ì´ ë¦¬ë”ë³´ë“œì—ì„œ ì‚¬ë¼ì¡Œì–´ìš”!
**A**: 2ê°œì›” ë¡¤ë§ ìœˆë„ìš° ë•Œë¬¸ì…ë‹ˆë‹¤.
- **ê·œì¹™**: 2ê°œì›” ì´ìƒ ì˜¤ë˜ëœ ì œì¶œì€ ìë™ìœ¼ë¡œ ë¬´íš¨í™”ë¨
- **ëª©ì **: ë¦¬ë”ë³´ë“œë¥¼ ê´€ë¦¬ ê°€ëŠ¥í•œ í¬ê¸°ë¡œ ìœ ì§€í•˜ê³  ìµœì‹  ìƒíƒœ ìœ ì§€
- **ì¬ë“±ì¥ ë°©ë²•**: ìƒˆë¡œìš´ ì œì¶œì„ í•˜ë©´ ë‹¤ì‹œ ë¦¬ë”ë³´ë“œì— ë‚˜íƒ€ë‚¨
- **ìì„¸í•œ ì„¤ëª…**: [Rolling Leaderboard ê²°ì • ì´ìœ ](https://www.kaggle.com/discussions)

### Q5: ë„ì›€ì´ í•„ìš”í•˜ë©´ ì–´ë–»ê²Œ í•˜ë‚˜ìš”?
**A**: Discussion Forumì„ í™œìš©í•˜ì„¸ìš”.
- **ë¹ ë¥¸ ë‹µë³€**: ì „ë‹´ ì§€ì›íŒ€ì´ ì—†ìœ¼ë¯€ë¡œ í¬ëŸ¼ì´ ê°€ì¥ ë¹ ë¦„
- **ìœ ìš©í•œ ì •ë³´**: ë°ì´í„°, í‰ê°€ ì§€í‘œ, ì ‘ê·¼ ë°©ë²•ì— ëŒ€í•œ ì •ë³´ ê°€ë“
- **ì§€ì‹ ê³µìœ **: ì§ˆë¬¸í•˜ê³  ë‹µë³€í•˜ë©´ì„œ í•¨ê»˜ ì„±ì¥
- **ì „ì²´ ì°¸ê°€ì ë¬¸ì œ**: ëª¨ë“  ì°¸ê°€ìì—ê²Œ ì˜í–¥ì„ ì£¼ëŠ” ë¬¸ì œë§Œ Support íŒ€ì— ë¬¸ì˜

### Q6: ë¡œì»¬ì—ì„œ í•™ìŠµí•œ ëª¨ë¸ì„ ì œì¶œí•  ìˆ˜ ìˆë‚˜ìš”?
**A**: ì•„ë‹ˆìš”, Code Competitionì´ë¯€ë¡œ ë¶ˆê°€ëŠ¥í•©ë‹ˆë‹¤.
- âŒ **ë¶ˆê°€**: ë¡œì»¬ì—ì„œ í•™ìŠµí•œ ëª¨ë¸ ì—…ë¡œë“œ í›„ inferenceë§Œ ì‹¤í–‰
- âœ… **ê°€ëŠ¥**: Pre-trained ëª¨ë¸(DistilBERT ë“±)ì€ ë°ì´í„°ì…‹ìœ¼ë¡œ ì—…ë¡œë“œ ê°€ëŠ¥
- **ì´ìœ **: ê³µì •í•œ ê²½ìŸê³¼ ì¬í˜„ì„±ì„ ìœ„í•´ ëª¨ë“  í•™ìŠµì´ Kaggleì—ì„œ ì‹¤í–‰ë˜ì–´ì•¼ í•¨

### Q7: ì œì¶œí•œ ë…¸íŠ¸ë¶ì˜ ì‹¤í–‰ ì‹œê°„ì´ ë§¤ë²ˆ ë‹¬ë¼ìš”
**A**: ì •ìƒì…ë‹ˆë‹¤. ì œì¶œ ì‹¤í–‰ ì‹œê°„ì€ ì•½ê°„ ë‚œë…í™”ë˜ì–´ ìˆìŠµë‹ˆë‹¤.
- **ë³€ë™ ë²”ìœ„**: ë™ì¼í•œ ë…¸íŠ¸ë¶ë„ ìµœëŒ€ 15ë¶„ ì°¨ì´ ë°œìƒ
- **ëª©ì **: í•˜ë“œì›¨ì–´ ìƒì„¸ ì •ë³´ ë³´í˜¸
- **ì˜í–¥**: ì„±ëŠ¥ ì¸¡ì •ì—ëŠ” ì˜í–¥ ì—†ìŒ

### Q8: ì¸í„°ë„·ì´ ë¹„í™œì„±í™”ë˜ëŠ”ë° ì–´ë–»ê²Œ ëª¨ë¸ì„ ë‹¤ìš´ë¡œë“œí•˜ë‚˜ìš”?
**A**: ë°ì´í„°ì…‹ìœ¼ë¡œ ë¯¸ë¦¬ ì—…ë¡œë“œí•´ì•¼ í•©ë‹ˆë‹¤.
1. **ë¡œì»¬ ë‹¤ìš´ë¡œë“œ**: `download_model.py`ë¡œ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ
2. **ì••ì¶•**: `zip -r model.zip models/`
3. **Kaggle ì—…ë¡œë“œ**: [Kaggle Datasets](https://www.kaggle.com/datasets)ì— ì—…ë¡œë“œ
4. **ë…¸íŠ¸ë¶ì— ì¶”ê°€**: Add Dataì—ì„œ ì—…ë¡œë“œí•œ ë°ì´í„°ì…‹ ì„ íƒ

### Q9: submission.csvë¥¼ ì–´ë””ì— ì €ì¥í•´ì•¼ í•˜ë‚˜ìš”?
**A**: `/kaggle/working/submission.csv`ì— ì €ì¥í•´ì•¼ í•©ë‹ˆë‹¤.
- **í•„ìˆ˜ ê²½ë¡œ**: Kaggleì´ ì´ ê²½ë¡œì—ì„œ ì œì¶œ íŒŒì¼ì„ ì°¾ìŒ
- **íŒŒì¼ëª…**: ë°˜ë“œì‹œ `submission.csv`ì—¬ì•¼ í•¨
- **ìë™ ê°ì§€**: ë…¸íŠ¸ë¶ ì‹¤í–‰ ì™„ë£Œ í›„ ìë™ìœ¼ë¡œ í‰ê°€

### Q10: ê°™ì€ ì‹¤ìˆ˜ë¥¼ ë°˜ë³µí•˜ê³  ìˆëŠ”ë° ë””ë²„ê¹… ë°©ë²•ì€?
**A**: [Code Debugging Guide](https://www.kaggle.com/docs/competitions#code-debugging) ì°¸ê³ í•˜ì„¸ìš”.
- **ì¼ë°˜ì ì¸ ë¬¸ì œ**: ê²½ë¡œ ì˜¤ë¥˜, ë©”ëª¨ë¦¬ ë¶€ì¡±, ì‹œê°„ ì´ˆê³¼
- **ë””ë²„ê¹… íŒ**: print ë¬¸ í™œìš©, ì‘ì€ ë°ì´í„°ì…‹ìœ¼ë¡œ í…ŒìŠ¤íŠ¸, ì—ëŸ¬ ë©”ì‹œì§€ í™•ì¸

## ì°¸ê³  ìë£Œ

### ê³µì‹ ë¬¸ì„œ
- [Hugging Face Transformers](https://huggingface.co/docs/transformers/index)
- [Kaggle Code Competitions](https://www.kaggle.com/docs/competitions#kernels-only-FAQ)
- [PyTorch Documentation](https://pytorch.org/docs/stable/index.html)

### ë…¼ë¬¸
- [DistilBERT Paper](https://arxiv.org/abs/1910.01108) - A distilled version of BERT
- [BERT Paper](https://arxiv.org/abs/1810.04805) - Pre-training of Deep Bidirectional Transformers
- [Attention Is All You Need](https://arxiv.org/abs/1706.03762) - Original Transformer paper

### íŠœí† ë¦¬ì–¼
- [BERT Explained](https://jalammar.github.io/illustrated-bert/)
- [The Illustrated Transformer](https://jalammar.github.io/illustrated-transformer/)
- [HuggingFace Course](https://huggingface.co/learn/nlp-course/chapter1/1)

## Citation

ì´ í”„ë¡œì íŠ¸ëŠ” Kaggleì˜ LLM Classification Finetuning ëŒ€íšŒë¥¼ ê¸°ë°˜ìœ¼ë¡œ í•©ë‹ˆë‹¤.

```bibtex
@misc{llm-classification-finetuning,
    author = {Wei-lin Chiang and Lianmin Zheng and Lisa Dunlap and Joseph E. Gonzalez and Ion Stoica and Paul Mooney and Sohier Dane and Addison Howard and Nate Keating},
    title = {LLM Classification Finetuning},
    year = {2024},
    howpublished = {\url{https://kaggle.com/competitions/llm-classification-finetuning}},
    note = {Kaggle}
}
```

## ë¼ì´ì„ ìŠ¤

êµìœ¡ ë° ëŒ€íšŒ ì°¸ê°€ ëª©ì ìœ¼ë¡œ ì‚¬ìš©ë©ë‹ˆë‹¤.

---

**ë§ˆì§€ë§‰ ì—…ë°ì´íŠ¸**: 2024ë…„ 10ì›”
**ëŒ€íšŒ ë§í¬**: https://www.kaggle.com/competitions/llm-classification-finetuning
**Discussion Forum**: https://www.kaggle.com/competitions/llm-classification-finetuning/discussion
