{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **제3장. 평가 방법론**\n",
    "\n",
    "AI 모델이 많아질수록 이를 평가하는 것이 더욱 중요해집니다. AI의 사용이 증가할수록 대규모 실패의 위험도 높아집니다. 우리는 이미 짧은 시간 안에 기초 모델과 관련된 여러 실패 사례를 목격했습니다. 한 남성이 챗봇의 권유를 받고 자살을 한 사건이 있었고, 변호사들은 AI가 환각적으로 생성한 거짓 증거를 제출했습니다. AI 캐나다는 AI 기반 챗봇이 승객에게 거짓 정보를 제공한 것에 대해 손해 배상 명령을 받았습니다. 우리가 AI 결과물을 품질 관리할 방법이 없다면, 많은 애플리케이션에서 AI의 이점보다 위험이 더 커질 수 있습니다.\n",
    "\n",
    "기업들이 AI를 채택하려는 속도가 빨라지면서, 많은 사람들이 AI 애플리케이션을 현실화하는 데 가장 큰 장애물이 평가라는 사실을 빠르게 깨닫고 있습니다. 일부 애플리케이션의 경우, 평가를 알아내는 데 개발 작업의 대다수를 할애할 수 있습니다. 2023년 12월, OpenAI의 공동 설립자인 Greg Brockman은 \"평가는 놀랍게도 종종 전부입니다.\"라고 트윗했습니다.\n",
    "\n",
    "평가의 중요성과 복잡성 때문에, 이 책은 평가에 대해 두 장으로 나누어 설명합니다. 이 장에서는 개방형 모델을 평가하기 위해 사용되는 다양한 평가 방법과 이러한 방법의 작동 방식, 그리고 그 한계에 대해 다룹니다. 다음 장에서는 이러한 방법을 사용하는 방법과 모델을 선택하고 애플리케이션을 위한 평가 파이프라인을 구축하는 방법에 초점을 맞춥니다.\n",
    "\n",
    "평가 방법을 다루기에 앞서, 기초 모델을 평가하는 데 따른 어려움을 인식하는 것이 중요합니다. 평가는 어렵기 때문에, 많은 사람들은 결과를 눈대중으로 판단하는 데 그칩니다. 이는 애플리케이션에 더 많은 위험을 가져오고, 반복 과정을 지연시킵니다. 대신 우리는 체계적인 평가에 투자하여 프로세스를 더 쉽고 신뢰할 수 있는 결과를 제공하도록 만들어야 합니다.\n",
    "\n",
    "많은 기초 모델에는 언어 모델 구성 요소가 포함되어 있으므로, 이 장에서는 언어 모델을 평가하는 데 사용되는 교차 엔트로피와 당혹도(perplexity)를 포함한 주요 지표를 간략히 살펴봅니다. 이러한 지표는 언어 모델의 훈련과 미세 조정을 안내하는 데 필수적이며, 많은 평가 방법에서 자주 사용됩니다.\n",
    "\n",
    "기초 모델을 평가하는 것은 특히 도전적입니다. 이는 모델이 개방형이기 때문이며, 이러한 문제를 해결하기 위한 모범 사례를 다룰 예정입니다. 많은 애플리케이션에서 인간 평가자는 여전히 필수적인 선택지로 남아 있지만, 느리고 비용이 많이 드는 사람 주석 작업을 피하는 것이 목표입니다. 이 책은 정확하고 주관적인 평가를 포함한 자동화 평가에 초점을 맞춥니다.\n",
    "\n",
    "주관적 평가의 새로운 흐름은 AI를 심판으로 사용하는 -- 즉 AI가 AI 응답을 평가하는 접근법입니다. 이 접근법은 모델과 AI 심판이 사용하는 프롬프트에 따라 점수가 달라지기 때문에 주관적입니다. 이러한 접근 방식은 업계에서 빠르게 인기를 얻고 있지만, AI가 이 중요한 작업을 수행하기에 충분히 신뢰할 수 없다고 믿는 사람들로부터 반대를 초래하기도 합니다. 저는 이 논의에 더 깊이 들어가기를 기대하고 있으며, 독자분들도 그러길 바랍니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **기초 모델 평가의 어려움**\n",
    "\n",
    "기계 학습(ML) 모델 평가가 항상 어려웠던 것처럼, 기초 모델의 도입으로 평가가 더욱 어려워졌습니다. 기초 모델 평가가 기존의 ML 모델 평가보다 더 어렵게 만드는 여러 가지 이유가 있습니다.\n",
    "\n",
    "첫째, AI 모델이 더 지능화될수록 평가가 더 어려워집니다. 대부분의 사람은 초등학생 수준의 수학 풀이가 틀렸다는 것을 쉽게 알 수 있지만, 박사 과정 수준의 수학 풀이가 틀렸다는 것을 아는 사람은 거의 없습니다. 책 요약이 엉망이라면 품질이 나쁘다는 것을 쉽게 알 수 있지만, 요약이 논리적이면 품질을 검증하기가 훨씬 더 어렵습니다. 요약의 품질을 검증하려면 책 전체를 읽어야 할 수도 있습니다. 이를 통해 도출되는 결론은, 고급 작업일수록 평가가 훨씬 더 시간 소모적이라는 점입니다. 이제는 단순히 응답의 겉모습으로 판단할 수 없으며, 사실 확인, 추론, 심지어 전문 분야의 전문 지식을 포함해야 합니다.\n",
    "\n",
    "둘째, 기초 모델의 개방형 성격은 모델을 정답과 비교하여 평가하는 기존의 접근 방식을 약화시킵니다. 기존 ML에서는 대부분의 작업이 폐쇄형입니다. 예를 들어, 분류 모델은 예상된 카테고리 내에서만 출력을 생성할 수 있습니다. 분류 모델을 평가할 때는 출력이 예상된 결과와 일치하는지 확인하면 됩니다. 그러나 개방형 작업에서는 주어진 입력에 대해 가능한 올바른 응답의 목록을 종합적으로 작성하는 것이 불가능합니다.\n",
    "\n",
    "셋째, 대부분의 기초 모델은 블랙 박스로 간주됩니다. 이는 모델 제공자가 모델 세부 정보를 공개하지 않거나 애플리케이션 개발자가 이를 이해할 전문 지식이 부족하기 때문입니다. 모델 아키텍처, 학습 데이터, 학습 과정과 같은 세부 정보는 모델의 강점과 약점에 대해 많은 것을 밝혀줄 수 있습니다. 그러나 이러한 세부 정보 없이 모델의 출력만으로 모델을 평가할 수밖에 없습니다.\n",
    "\n",
    "동시에 평가 벤치마크는 지속적으로 구식이 되고 있습니다. 이상적으로는 평가가 모델의 전체 기능 범위를 포착해야 하지만, AI가 발전함에 따라 평가도 이에 맞춰 발전해야 합니다. 벤치마크는 모델이 인간 기준에 도달하거나 이를 초과하는 순간 포화 상태가 됩니다. 기초 모델의 경우, 벤치마크가 매우 빠르게 포화 상태에 도달합니다. 예를 들어, GLUE(General Language Understanding Evaluation)는 2018년에 도입되어 단 1년 만에 포화 상태가 되어 2019년에 SuperGLUE로 대체되었습니다. 마찬가지로 NaturalInstructions(2021)는 2022년에 Super-NaturalInstructions로 대체되었습니다.\n",
    "\n",
    "마지막으로, 평가의 범위는 범용 모델로 확장되었습니다. 작업별 모델의 경우, 평가에는 모델이 학습된 작업에서 얼마나 잘 수행하는지를 측정하는 것이 포함됩니다. 반면, 범용 모델의 경우 평가는 알려진 작업의 벤치마크뿐만 아니라 모델이 수행할 수 있는 새로운 작업, 인간 능력을 초과하는 작업 등을 포함합니다. 이러한 평가에는 AI의 잠재력과 한계를 탐구하는 책임이 추가됩니다.\n",
    "\n",
    "좋은 소식은 이러한 새로운 평가의 도전 과제가 새로운 방법과 벤치마크를 촉진했다는 점입니다. **그림 3-1**은 2023년 상반기에 LLM(대규모 언어 모델) 평가 논문 수가 매월 기하급수적으로 증가했음을 보여줍니다. 2023년 초에는 매월 2편 이하였던 논문이 5월에는 거의 매월 35편에 달했습니다.\n",
    "\n",
    "<img src=\"images/fig_03_01.png\" width=\"800\">\n",
    "\n",
    "제가 GitHub의 상위 1000개 AI 관련 저장소를 별 개수로 분석한 결과(2024년 5월 12일 기준), 평가에 전념하는 저장소가 40개 발견되었습니다. 평가 저장소 수를 생성 날짜별로 플로팅한 결과, 성장 곡선이 기하급수적임을 보여주었으며, 이는 **그림 3-2**에서 확인할 수 있습니다.\n",
    "\n",
    "<img src=\"images/fig_03_02.png\" width=\"800\">\n",
    "\n",
    "나쁜 소식은 평가에 대한 관심이 증가했음에도 불구하고, AI 엔지니어링 파이프라인의 다른 부분에 비해 관심이 부족하다는 점입니다. DeepMind의 Balduzzi 등은 논문에서 \"평가를 개발하는 작업은 알고리즘을 개발하는 작업에 비해 체계적인 관심을 거의 받지 못했다\"고 언급했습니다. 논문에 따르면, 실험 결과는 거의 전적으로 알고리즘 개선에 사용되며 평가 개선에는 거의 사용되지 않는다고 합니다. 평가에 대한 투자 부족을 인식한 Anthropic은 정책 입안자들에게 새로운 평가 방법론을 개발하고 기존 평가의 강건성을 분석하기 위한 정부 자금 및 보조금 증대를 촉구했습니다.\n",
    "\n",
    "**그림 3-3**에서 볼 수 있듯이, 평가 도구의 수는 모델링, 훈련 및 AI 조정을 위한 도구 수에 비해 적습니다.\n",
    "\n",
    "<img src=\"images/fig_03_03.png\" width=\"800\">\n",
    "\n",
    "불충분한 투자는 불충분한 인프라로 이어져, 사람들이 체계적인 평가를 수행하기 어렵게 만듭니다. AI 애플리케이션을 어떻게 평가하고 있는지 물었을 때, 많은 사람들이 단순히 결과를 눈대중으로 판단한다고 말했습니다. 많은 경우, 모델을 평가하는 데 사용하는 소수의 대표적인 프롬프트 세트를 가지고 있습니다. 이러한 프롬프트를 구성하는 과정은 애플리케이션의 필요가 아닌 큐레이터의 개인 경험에 기반한 경우가 많습니다. 프로젝트를 시작할 때 이러한 임시 방식으로 어느 정도 진행할 수는 있지만, 애플리케이션 반복(iteration)에는 충분하지 않습니다. 이 책은 평가에 대한 체계적인 접근 방식에 중점을 둡니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **언어 모델링 지표 이해하기**\n",
    "\n",
    "기초 모델은 언어 모델에서 진화했습니다. 많은 기초 모델은 여전히 언어 모델을 주요 구성 요소로 가지고 있습니다. 이러한 모델의 경우, 언어 모델 구성 요소의 성능은 기초 모델이 다운스트림 애플리케이션에서 보이는 성능과 잘 연관되는 경향이 있습니다(Liu et al., 2023). 따라서, 언어 모델링 지표에 대한 대략적인 이해는 다운스트림 성능을 이해하는 데 매우 유용할 수 있습니다.\n",
    "\n",
    "1장에서 논의된 것처럼, 언어 모델링은 1951년 Claude Shannon이 발표한 논문 **Printed English의 예측과 엔트로피(Prediction and Entropy of Printed English)**에서 대중화된 이후로 수십 년 동안 존재해왔습니다. 언어 모델 개발을 안내하는 데 사용되는 지표는 그 이후로 크게 변하지 않았습니다. 대부분의 자기회귀 언어 모델은 교차 엔트로피 또는 그와 관련된 당혹도(perplexity)로 훈련됩니다. 논문이나 모델 보고서를 읽을 때, 가끔씩 문자당 비트(Bits Per Character, BPC)와 바이트당 비트(Bits Per Byte, BPB)라는 용어를 접할 수도 있습니다. 이 둘은 모두 교차 엔트로피의 변형입니다.\n",
    "\n",
    "교차 엔트로피, 당혹도, BPC, BPB의 네 가지 지표는 서로 밀접하게 관련되어 있습니다. 이 중 하나의 값을 알면 필요한 정보를 통해 나머지를 계산할 수 있습니다. 이러한 지표를 언어 모델링 지표라고 부르지만, 이는 텍스트 토큰뿐만 아니라 비텍스트 토큰을 생성하는 모든 모델에 사용할 수 있습니다.\n",
    "\n",
    "언어 모델은 언어에 대한 통계적 정보를 인코딩합니다. 예를 들어, 문맥 \"I like drinking __\"에서, 다음 단어가 \"charcoal\"보다는 \"tea\"일 가능성이 더 높습니다. 모델이 주어진 문맥에서 토큰이 나타날 확률과 같은 통계적 정보를 더 잘 포착할수록, 다음 토큰을 더 잘 예측할 수 있습니다.\n",
    "\n",
    "ML(기계 학습) 용어로, 언어 모델은 훈련 데이터의 **분포를 학습합니다**. 모델이 학습을 잘할수록, 훈련 데이터에서 보지 못한 내용을 예측하는 능력이 좋아지고, 훈련 데이터의 교차 엔트로피 값이 낮아집니다. 모든 ML 모델과 마찬가지로, 모델의 성능을 평가할 때는 훈련 데이터뿐만 아니라 관련 데이터에서의 성능도 중요합니다. 일반적으로 모델의 훈련 데이터와 사용자의 데이터가 더 가까울수록 모델의 교차 엔트로피가 사용자의 데이터와 더 잘 맞습니다.\n",
    "\n",
    "이 책의 다른 부분에 비해, 이 섹션은 수학적 내용이 많습니다. 만약 이 내용이 혼란스럽다면, 수학 부분은 건너뛰고 이러한 지표의 중요성을 논의하는 부분에 집중해도 괜찮습니다. 언어 모델을 직접 훈련하거나 미세 조정하지 않더라도, 이러한 지표를 이해하면 애플리케이션에 적합한 모델을 선택하는 데 도움이 될 수 있습니다. 또한, 이러한 지표는 책 전반에 걸쳐 논의되는 특정 평가 및 데이터 중복 제거 기술에 유용하게 사용됩니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **엔트로피**\n",
    "\n",
    "엔트로피(Entropy)는 평균적으로 하나의 토큰이 얼마나 많은 정보를 담고 있는지를 측정합니다. 엔트로피가 높을수록, 각 토큰이 더 많은 정보를 담고 있으며, 이를 나타내기 위해 더 많은 비트가 필요합니다.\n",
    "\n",
    "간단한 예를 들어 설명하겠습니다. 정사각형 안의 위치를 설명하는 언어를 만든다고 가정해 봅시다. 언어에 두 개의 토큰만 있는 경우(그림 3-4의 (a) 참조), 각 토큰은 위치가 위쪽인지 아래쪽인지 나타낼 수 있습니다. 두 개의 토큰만 있기 때문에 이를 나타내는 데 1비트로 충분합니다. 따라서 이 언어의 엔트로피는 1입니다.\n",
    "\n",
    "언어에 네 개의 토큰이 있는 경우(그림 3-4의 (b) 참조), 각 토큰은 더 구체적인 위치를 나타낼 수 있습니다: 왼쪽 위, 왼쪽 아래, 오른쪽 위, 오른쪽 아래. 그러나 이제 네 개의 토큰이 있기 때문에 이를 나타내는 데 2비트가 필요합니다. 이 언어의 엔트로피는 2입니다. 이는 각 토큰이 더 많은 정보를 담고 있지만, 각 토큰을 나타내기 위해 더 많은 비트가 필요함을 의미합니다.\n",
    "\n",
    "<img src=\"images/fig_03_04.png\" width=\"800\">\n",
    "\n",
    "직관적으로, 엔트로피는 언어에서 다음에 무엇이 올지 예측하는 것이 얼마나 어려운지를 측정합니다. 언어의 엔트로피가 낮을수록(즉, 언어의 각 토큰이 담고 있는 정보가 적을수록), 그 언어는 더 예측 가능합니다. 내가 말하는 것을 완벽히 예측할 수 있다면, 내가 말하는 내용은 당신에게 새로운 정보를 전달하지 않는 것과 마찬가지입니다.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **교차 엔트로피**\n",
    "\n",
    "데이터셋에서 언어 모델을 훈련할 때의 목표는 훈련 데이터의 분포를 모델이 학습하도록 만드는 것입니다. 다시 말해, 모델이 훈련 데이터에서 다음에 무엇이 올지를 예측하는 데 얼마나 어려운지를 측정하는 것이 목표입니다. 언어 모델의 교차 엔트로피는 데이터셋에서 다음에 올 것을 예측하는 데 얼마나 어려운지를 나타냅니다.\n",
    "\n",
    "모델의 훈련 데이터에 대한 교차 엔트로피는 두 가지 요인에 따라 달라집니다:\n",
    "\n",
    "1. **훈련 데이터의 예측 가능성**, 이는 훈련 데이터의 엔트로피로 측정됩니다.  \n",
    "2. **언어 모델이 캡처한 분포가 훈련 데이터의 실제 분포와 얼마나 다른지**, 즉 언어 모델의 분포가 훈련 데이터의 분포와 얼마나 다르게 나타나는지.  \n",
    "\n",
    "엔트로피와 교차 엔트로피는 동일한 수학적 표기 $ H $를 사용합니다. 여기서, $ P $는 훈련 데이터의 실제 분포(진실 분포)이고, $ Q $는 언어 모델이 학습한 분포입니다.\n",
    "\n",
    "- 훈련 데이터의 엔트로피는 $ H(P) $입니다.\n",
    "- $ Q $의 $ P $에 대한 발산은 Kullback-Leibler(KL) 발산을 사용하여 측정되며, 이는 수학적으로 $ D_{KL}(P || Q) $로 표현됩니다.\n",
    "- 훈련 데이터에 대한 모델의 교차 엔트로피는 다음과 같이 표현됩니다:  \n",
    "  $\n",
    "  H(P, Q) = H(P) + D_{KL}(P || Q)\n",
    "  $\n",
    "\n",
    "교차 엔트로피는 대칭적이지 않습니다. $ P $에 대한 $ Q $의 교차 엔트로피 $ H(P, Q) $는 $ Q $에 대한 $ P $의 교차 엔트로피 $ H(Q, P) $와 다릅니다.\n",
    "\n",
    "\n",
    "언어 모델은 훈련 데이터에 대한 교차 엔트로피를 최소화하도록 훈련됩니다. 언어 모델이 훈련 데이터로부터 완벽히 학습한다면, 모델의 교차 엔트로피는 훈련 데이터의 엔트로피와 정확히 동일하게 됩니다. $ Q $의 $ P $에 대한 KL 발산이 0이 되면, 모델의 교차 엔트로피는 훈련 데이터의 근사치로 생각할 수 있습니다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **문자당 비트(Bits-per-character)와 바이트당 비트(Bits-per-byte)**\n",
    "\n",
    "엔트로피와 교차 엔트로피의 기본 단위는 비트(bits)입니다. 만약 언어 모델의 교차 엔트로피가 6비트라면, 이 언어 모델은 각 토큰을 나타내는 데 6비트를 필요로 합니다.\n",
    "\n",
    "다른 모델들은 서로 다른 토큰화 방식을 사용하기 때문에(예: 어떤 모델은 단어를 토큰으로 사용하고, 다른 모델은 문자를 토큰으로 사용), 토큰당 비트 수는 모델 간에 비교할 수 없습니다. 대신 문자당 비트(Bits-per-character, BPC)를 사용하는 경우가 있습니다. 만약 토큰당 비트 수가 6이고, 평균적으로 각 토큰이 2개의 문자로 구성된다면, BPC는 6/2 = 3입니다.\n",
    "\n",
    "BPC에서 발생할 수 있는 복잡성 중 하나는 문자 인코딩 방식의 차이입니다. 예를 들어, ASCII에서는 각 문자가 7비트로 인코딩되지만, UTF-8에서는 한 문자가 8비트에서 32비트까지 어느 것이든 사용될 수 있습니다. 더 표준화된 지표는 바이트당 비트(Bits-per-byte, BPB)로, 이는 언어 모델이 원래 훈련 데이터의 1바이트를 나타내는 데 필요한 비트 수입니다. 만약 BPC가 3이고 각 문자가 7비트, 즉 바이트의 7/8에 해당한다면, BPB는 3 / (7/8) = 3.43입니다.\n",
    "\n",
    "교차 엔트로피는 언어 모델이 텍스트를 얼마나 잘 압축할 수 있는지를 보여줍니다. 만약 한 언어 모델의 BPB가 3.43이라면, 8비트(각 문자를 나타내는 비트)를 3.43비트로 표현할 수 있습니다. 즉, 이 언어 모델은 원래 훈련 데이터를 절반 이하로 압축할 수 있습니다.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **당혹도(Perplexity)**\n",
    "\n",
    "당혹도(Perplexity)는 엔트로피와 교차 엔트로피의 지수적 표현입니다. 당혹도는 종종 PPL로 줄여서 표현됩니다. 주어진 데이터셋에서 실제 분포 $P$를 기반으로 할 때, 당혹도는 다음과 같이 정의됩니다:  \n",
    "$ \\text{PPL}(P) = 2^{H(P)} $\n",
    "\n",
    "언어 모델(학습된 분포 $Q$)이 있는 경우, 해당 데이터셋에서의 당혹도는 다음과 같이 정의됩니다:  \n",
    "$ \\text{PPL}(P, Q) = 2^{H(P, Q)} $\n",
    "\n",
    "교차 엔트로피가 언어 모델이 다음 토큰을 예측하는 것이 얼마나 어려운지를 측정한다면, 당혹도는 다음 토큰을 예측할 때 모델이 느끼는 불확실성을 측정합니다. 불확실성이 높을수록 다음 토큰에 대한 가능한 옵션이 더 많아집니다.\n",
    "\n",
    "예를 들어, **그림 3-4(b)**에 표시된 4개의 위치 토큰을 완벽하게 인코딩하도록 훈련된 언어 모델을 고려해봅시다. 이 언어 모델의 교차 엔트로피는 2비트입니다. 이 언어 모델이 정사각형 내의 위치를 예측하려면 $2^2 = 4$개의 가능한 옵션 중 하나를 선택해야 합니다. 따라서 이 언어 모델의 당혹도는 4입니다.\n",
    "\n",
    "지금까지 엔트로피와 교차 엔트로피의 단위로 비트(bit)를 사용해왔습니다. 각 비트는 2개의 고유한 값을 나타낼 수 있으므로, 위의 당혹도 식에서 2를 기반(base)으로 사용합니다.\n",
    "\n",
    "TensorFlow 및 PyTorch와 같은 인기 있는 ML 프레임워크는 엔트로피와 교차 엔트로피의 단위로 자연 로그(natural log)를 사용하며, 이를 nat이라고 부릅니다. Nat은 자연 로그의 기반인 $e$를 사용합니다. 만약 nat을 단위로 사용한다면, 당혹도는 다음과 같이 계산됩니다:  \n",
    "$ \\text{PPL}(P, Q) = e^{H(P, Q)} $\n",
    "\n",
    "비트와 nat 간의 혼란으로 인해, 많은 사람들이 언어 모델의 성능을 보고할 때 교차 엔트로피 대신 당혹도를 사용합니다.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **당혹도(Perplexity) 해석 및 사용 사례**\n",
    "\n",
    "앞서 논의한 바와 같이, 교차 엔트로피, 당혹도(Perplexity), BPC(문자당 비트), BPB(바이트당 비트)는 언어 모델의 예측 정확도를 측정하는 지표의 변형입니다. 모델이 텍스트를 더 정확하게 예측할수록 이러한 지표의 값은 더 낮아집니다. 이 책에서는 당혹도를 기본 언어 모델링 지표로 사용할 것입니다. 모델이 주어진 데이터셋에서 다음에 무엇이 올지 예측하는 데 불확실성이 클수록, 당혹도는 더 높아진다는 점을 기억하세요.\n",
    "\n",
    "당혹도의 적정 값은 데이터 자체와 당혹도를 계산하는 방법(예: 모델이 액세스할 수 있는 이전 토큰의 개수)에 따라 달라집니다. 다음은 일반적인 경험 법칙입니다:\n",
    "\n",
    "**더 구조화된 데이터는 더 낮은 당혹도를 제공합니다**\n",
    "\n",
    "구조화된 데이터는 더 예측 가능성이 높습니다. 예를 들어, HTML 코드는 일상 텍스트보다 더 예측 가능합니다. `<head>`와 같은 HTML 태그를 보면 근처에 닫는 태그 `</head>`가 있을 가능성을 예측할 수 있습니다. 따라서, HTML 코드에 대한 모델의 기대 당혹도는 일상 텍스트에 대한 모델의 기대 당혹도보다 낮아야 합니다.\n",
    "\n",
    "**어휘가 클수록 당혹도는 높아집니다**\n",
    "\n",
    "직관적으로, 가능한 토큰이 많을수록 모델이 다음 토큰을 예측하기 더 어려워집니다. 예를 들어, 아동 도서에 대한 모델의 당혹도는 동일한 모델이 *전쟁과 평화*와 같은 작품에서 보이는 당혹도보다 낮을 가능성이 높습니다. 동일한 데이터셋에서, 문자 기반 당혹도(다음 문자를 예측)는 단어 기반 당혹도(다음 단어를 예측)보다 낮을 것입니다. 이는 가능한 문자의 수가 가능한 단어의 수보다 적기 때문입니다.\n",
    "\n",
    "**문맥 길이가 길수록 당혹도는 낮아집니다**\n",
    "\n",
    "모델이 참조할 수 있는 문맥이 많을수록, 다음 토큰을 예측하는 데 있어 불확실성이 줄어듭니다. 1951년 Claude Shannon은 10개의 이전 토큰 이하를 조건으로 모델의 교차 엔트로피를 평가했습니다. 현재 기준으로, 모델의 당혹도는 일반적으로 500~10,000개의 이전 토큰을 조건으로 계산할 수 있으며, 모델의 최대 문맥 길이에 따라 그 이상도 가능할 수 있습니다.\n",
    "\n",
    "참고로, 당혹도 값이 3 또는 그 이하로 나오는 경우도 드물지 않습니다. 가상의 언어에서 모든 토큰이 발생할 확률이 동일하다면, 당혹도가 3이라는 것은 모델이 다음 토큰을 정확히 예측할 확률이 1/3이라는 뜻입니다. 모델의 어휘가 10,000~100,000개의 단어로 구성된다고 가정할 때, 이러한 확률은 매우 놀랍습니다.\n",
    "\n",
    "언어 모델 훈련을 안내하는 것 외에도, 당혹도는 AI 엔지니어링 워크플로의 여러 부분에서 유용합니다. 첫째, 당혹도는 모델의 성능을 나타내는 좋은 대리 지표가 됩니다. 모델이 다음 토큰을 예측하는 데 성능이 나쁘다면, 다운스트림 작업에서도 성능이 나쁠 가능성이 큽니다. OpenAI의 GPT-2 보고서에 따르면, 더 크고 더 강력한 모델은 다양한 데이터셋에서 일관되게 더 낮은 당혹도를 보였습니다(그림 3-5 참조). 그러나 안타깝게도, 기업들이 점점 더 자사의 모델 정보를 공개하지 않으면서, 많은 기업들이 모델의 당혹도를 보고하지 않게 되었습니다.\n",
    "\n",
    "<img src=\"./images/fig_03_05.png\" alt=\"fig3-5\" width=\"800\">\n",
    "\n",
    ">**경고**\n",
    ">\n",
    ">당혹도(Perplexity)는 SFT(Supervised Fine-Tuning)와 RLHF(Reinforcement Learning with Human Feedback) 같은 기술을 사용하여 후속 훈련(Post-training)을 받은 모델을 평가하는 데 적합하지 않을 수 있습니다. 후속 훈련은 모델이 작업을 완료하는 방법을 학습하도록 만드는 과정입니다. 모델이 작업을 더 잘 수행하게 될수록, 다음 토큰을 예측하는 능력은 오히려 나빠질 수 있습니다. 언어 모델의 당혹도는 일반적으로 후속 훈련 후에 증가합니다. 일부 사람들은 후속 훈련이 엔트로피를 \"축소\"한다고 표현하기도 합니다. 마찬가지로, 모델의 수치적 정밀도를 줄이고 메모리 사용량(memory footprint)을 함께 줄이는 기술인 양자화(Quantization)도 모델의 당혹도를 예기치 못한 방식으로 변화시킬 수 있습니다.\n",
    "\n",
    "모델의 특정 텍스트에 대한 당혹도는 해당 텍스트를 모델이 예측하는 데 얼마나 어려움을 겪는지를 측정합니다. 주어진 모델에서, 당혹도는 모델이 훈련 중에 본 적이 있고 암기한 텍스트에 대해 가장 낮습니다. 따라서, 당혹도는 텍스트가 모델의 훈련 데이터에 포함되어 있었는지 여부를 감지하는 데 사용할 수 있습니다. 이는 데이터 오염을 감지하는 데 유용합니다. 예를 들어, 벤치마크 데이터가 모델의 훈련 데이터에 포함되어 있다면, 해당 벤치마크의 데이터는 모델의 성능 평가에 있어 신뢰성이 떨어질 수 있습니다. 또한, 당혹도를 통해 훈련 데이터 중복 제거 작업도 수행할 수 있습니다. 예를 들어, 새로운 데이터의 당혹도가 높을 경우에만 기존 훈련 데이터에 추가할 수 있습니다.\n",
    "\n",
    "당혹도는 예측 불가능한 텍스트에서 가장 높습니다. 예를 들어, \"내 개가 여가 시간에 양자 물리학을 가르친다(my dog teaches quantum physics in his free time)\"와 같은 독특한 아이디어를 표현한 텍스트나, \"home cat go eye\"와 같은 의미 없는 텍스트에서는 당혹도가 높습니다. 따라서, 당혹도는 독특하거나 의미 없는 텍스트를 감지하는 데 사용할 수 있습니다.\n",
    "\n",
    "--- \n",
    "\n",
    "**언어 모델을 사용하여 텍스트의 당혹도 계산하기**\n",
    "\n",
    "모델의 텍스트에 대한 당혹도(Perplexity)는 해당 텍스트를 모델이 예측하는 데 얼마나 어려움을 겪는지를 측정합니다. 언어 모델 $ X $와 토큰 시퀀스 $[x_1, x_2, \\dots, x_n]$가 주어졌을 때, 이 시퀀스에 대한 $ X $의 당혹도는 다음과 같이 정의됩니다:\n",
    "\n",
    "$\n",
    "P(x_1, x_2, \\dots, x_n)^{-\\frac{1}{n}} = \\left( \\frac{1}{P(x_1, x_2, \\dots, x_n)} \\right)^{\\frac{1}{n}} = \\left( \\prod_{i=1}^{n} \\frac{1}{P(x_i | x_1, \\dots, x_{i-1})} \\right)^{\\frac{1}{n}}\n",
    "$\n",
    "\n",
    "여기서 $ P(x_i | x_1, \\dots, x_{i-1}) $는 $ X $가 이전 토큰 $ x_1, \\dots, x_{i-1} $가 주어졌을 때 토큰 $ x_i $에 할당하는 확률을 나타냅니다.\n",
    "\n",
    "당혹도를 계산하려면, 언어 모델이 각 다음 토큰에 할당하는 확률(또는 로그 확률)에 접근할 수 있어야 합니다. 그러나, 모든 상용 모델이 이러한 로그 확률을 노출하는 것은 아니며, 이는 2장에서 논의된 바와 같습니다.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **정확한 평가(Exact Evaluation)**\n",
    "\n",
    "당혹도(Perplexity)와 관련된 지표는 기본 언어 모델의 성능을 이해하는 데 도움을 줍니다. 이는 모델의 다운스트림 작업 성능을 이해하는 대리 지표입니다. 이 섹션에서는 모델의 다운스트림 작업에 대한 성능을 직접 측정하는 방법에 대해 논의합니다.\n",
    "\n",
    "모델의 성능을 평가할 때, 정확한 평가와 주관적인 평가를 구별하는 것이 중요합니다. 정확한 평가는 명확한 판단을 내리며 모호함이 없습니다. 예를 들어, 다지선다형 문제에서 정답이 A인데, 당신이 B를 선택했다면 당신의 답은 틀린 것입니다. 여기에 모호함은 없습니다. 반면에, 에세이 채점은 주관적입니다. 에세이 점수는 채점을 누가 하느냐에 따라 달라질 수 있습니다. 동일한 사람이 같은 에세이를 일정 시간이 지난 후 다시 채점한다면, 다른 점수를 줄 수도 있습니다. 명확한 채점 기준을 제공하면 에세이 채점이 더 정확해질 수 있습니다. 다음 섹션에서 설명할 AI-as-a-judge 접근법은 주관적입니다. 평가 결과는 심사위원과 프롬프트에 따라 달라질 수 있습니다.\n",
    "\n",
    "이 섹션에서는 정확한 점수를 산출하는 두 가지 평가 방법에 대해 논의합니다: 기능적 정확성(functional correctness)과 참조 데이터(reference data)에 대한 유사성 측정입니다. 이 섹션은 폐쇄형 응답(예: 분류)보다는 개방형 응답(임의 텍스트 생성)을 평가하는 데 중점을 둡니다. 이는 기초 모델이 폐쇄형 작업에 사용되지 않기 때문이 아니라, 많은 기초 모델 시스템에 의도 분류(intent classification)나 점수 산정과 같은 분류 구성 요소가 포함되어 있기 때문입니다. 그러나 이 섹션은 폐쇄형 평가가 이미 잘 이해되고 있기 때문에 개방형 평가에 초점을 맞추고 있습니다.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **기능적 정확성(Functional Correctness)**\n",
    "\n",
    "기능적 정확성 평가는 시스템이 의도된 기능을 수행했는지 여부를 기준으로 시스템을 평가하는 것을 의미합니다. 예를 들어, 모델에게 웹사이트를 생성하도록 요청했을 때 생성된 웹사이트가 요구 사항을 충족했습니까? 모델에게 특정 레스토랑의 예약을 요청했을 때, 모델이 성공적으로 수행했습니까?\n",
    "\n",
    "기능적 정확성은 애플리케이션의 성능을 평가하기 위한 궁극적인 척도입니다. 이는 애플리케이션이 의도된 대로 작동했는지를 측정하기 때문입니다. 그러나 기능적 정확성은 항상 간단히 측정할 수 있는 것은 아니며, 자동화가 쉽지 않은 경우도 있습니다.\n",
    "\n",
    "코드 생성은 기능적 정확성을 자동으로 측정할 수 있는 작업의 한 예입니다. 코딩에서 기능적 정확성은 때로 **실행 정확성(execution accuracy)**이라고 불립니다. 예를 들어, 모델에게 Python 함수 `gcd(num1, num2)`를 작성하도록 요청했다고 가정합시다. 이 함수는 두 숫자 $ \\text{num1} $과 $ \\text{num2} $의 최대공약수를 찾아야 합니다. 생성된 코드는 Python 인터프리터에 입력되어 코드가 유효한지 확인하고, 그렇다면 주어진 입력 $ (15, 20) $에 대해 올바른 결과(5)를 출력하는지 확인할 수 있습니다. 만약 함수 `gcd(15, 20)`가 5를 반환하지 않는다면, 이 함수는 잘못된 것입니다.\n",
    "\n",
    "AI가 코드 작성을 시작하기 훨씬 전부터, 코드의 기능적 정확성을 자동으로 검증하는 것은 소프트웨어 엔지니어링의 표준 관행이었습니다. 코드는 일반적으로 **단위 테스트(unit tests)**를 통해 검증됩니다. 코드는 다양한 시나리오에서 실행되어 예상 출력을 생성하는지 확인됩니다. 기능적 정확성 평가는 LeetCode나 HackerRank와 같은 코딩 플랫폼에서 제출된 솔루션을 검증하는 방식과 동일합니다.\n",
    "\n",
    "AI의 코드 생성 기능을 평가하기 위한 인기 있는 벤치마크에는 OpenAI의 **HumanEval**과 Google의 **MBPP**가 있습니다. 이러한 벤치마크는 기능적 정확성을 주요 지표로 사용합니다. 자연어에서 SQL 쿼리를 생성하는 벤치마크(예: **Spider**(Yu et al., 2018), **BIRD-SQL**(Li et al., 2023), **WikiSQL**(Zhong et al., 2017))도 기능적 정확성을 기반으로 합니다.\n",
    "\n",
    "벤치마크 문제는 일련의 테스트 케이스로 구성됩니다. 각 테스트 케이스는 코드가 실행되어야 하는 시나리오와 해당 시나리오에 대한 예상 출력을 제공합니다.\n",
    "\n",
    "다음은 HumanEval의 한 문제 예시입니다:\n",
    "\n",
    "```python\n",
    "from typing import List\n",
    "\n",
    "def has_close_elements(numbers: List[float], threshold: float) -> bool:\n",
    "    \"\"\"\n",
    "    주어진 숫자 목록에서, 어떤 두 숫자가 주어진 임계값보다 가까운지를 확인합니다.\n",
    "    \"\"\"\n",
    "    >>> has_close_elements([1.0, 2.0, 3.0], 0.5)\n",
    "    False\n",
    "    >>> has_close_elements([1.0, 2.8, 3.0, 4.0, 5.0, 2.0], 0.3)\n",
    "    True\n",
    "```\n",
    "각 테스트 케이스(assert 문이 각각의 테스트 케이스를 나타냄)는 다음과 같습니다:\n",
    "\n",
    "```python\n",
    "def check(candidate):\n",
    "    assert candidate([1.0, 2.0, 3.9, 4.0, 5.0, 2.2], 0.3) == True\n",
    "    assert candidate([1.0, 2.0, 3.9, 4.0, 5.0, 2.2], 0.05) == False\n",
    "    assert candidate([1.0, 2.0, 5.9, 4.0, 5.0], 0.95) == True\n",
    "    assert candidate([1.0, 2.0, 5.9, 4.0, 5.0], 0.8) == False\n",
    "    assert candidate([1.0, 2.0, 3.0, 4.0, 5.0, 2.0], 0.1) == True\n",
    "    assert candidate([1.1, 2.2, 3.1, 4.1, 5.1], 1.0) == True\n",
    "    assert candidate([1.1, 2.2, 3.1, 4.1, 5.1], 0.5) == False\n",
    "```\n",
    "\n",
    "모델을 평가할 때, 각 문제마다 $ k $개의 코드 샘플이 생성됩니다. 모델이 생성한 $ k $개의 코드 샘플 중 하나라도 해당 문제의 모든 테스트 케이스를 통과하면, 모델은 그 문제를 해결한 것으로 간주됩니다. 최종 점수는 **pass@k**로 불리며, 이는 전체 문제 중 해결된 문제의 비율로 계산됩니다. 예를 들어, 10개의 문제가 주어지고 모델이 $ k=3 $일 때 5개의 문제를 해결했다면, 모델의 pass@3 점수는 50%가 됩니다. 모델이 더 많은 코드 샘플을 생성할수록 각 문제를 해결할 가능성이 높아지며, 따라서 최종 점수도 높아집니다. 이는 일반적으로 pass@1 점수가 pass@3보다 낮고, pass@10보다도 낮아야 함을 의미합니다.\n",
    "\n",
    "기능적 정확성을 자동으로 평가할 수 있는 또 다른 작업 범주는 비디오 게임 봇입니다. 예를 들어, 테트리스(Tetris)를 플레이하는 봇을 생성하면, 봇이 얻는 점수를 통해 봇의 성능을 판단할 수 있습니다. 측정 가능한 목표를 가진 작업은 기능적 정확성을 기반으로 평가할 수 있습니다. 예를 들어, AI에게 작업 부하를 최적화하여 에너지 소비를 줄이는 일정을 짜도록 요청하면, AI의 성능은 절약한 에너지량으로 측정할 수 있습니다.\n",
    "\n",
    "--- "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **참조 데이터에 대한 유사성 측정(Similarity Measurements Against Reference Data)**\n",
    "\n",
    "기능적 정확성을 사용하여 AI의 출력을 자동으로 평가할 수 없는 작업의 경우, 일반적으로 참조 데이터(reference data)와 비교하여 AI의 출력을 평가하는 접근 방식이 사용됩니다. 예를 들어, AI에게 프랑스어 문장을 영어로 번역하도록 요청한 경우, 생성된 영어 번역을 올바른 영어 번역과 비교하여 평가할 수 있습니다.\n",
    "\n",
    "참조 데이터의 각 예시는 다음과 같은 형식을 따릅니다: (입력, 참조 응답). 하나의 입력에 대해 여러 참조 응답이 있을 수 있습니다. 예를 들어, 프랑스어 문장의 여러 가능한 영어 번역이 참조 응답으로 제공될 수 있습니다. 참조 응답은 \"기준 진리(ground truths)\" 또는 \"표준 응답(canonical responses)\"이라고도 불립니다.\n",
    "\n",
    "이 평가 접근 방식은 참조 데이터에 의존하기 때문에, 얼마나 많은 참조 데이터를 얼마나 빨리 생성할 수 있는지가 병목점이 됩니다. 참조 데이터는 일반적으로 사람이 생성하며, 점점 더 AI가 이를 생성하는 경우도 많아지고 있습니다. 사람에 의해 생성된 데이터를 골드 스탠다드(gold standard)로 취급하고 AI의 성능을 인간 성능과 비교하여 측정합니다. 하지만, 사람에 의해 생성된 데이터는 비용이 많이 들고 시간이 오래 걸립니다. 이로 인해 많은 사람들이 참조 데이터를 생성하는 데 AI를 사용하고 있습니다. AI가 생성한 데이터는 여전히 사람의 검토가 필요할 수 있지만, 처음부터 참조 데이터를 생성하는 데 필요한 노동에 비하면 검토에 필요한 노동은 훨씬 적습니다.\n",
    "\n",
    "생성된 응답이 참조 응답과 더 유사할수록 더 좋은 것으로 간주됩니다. 두 개의 개방형 텍스트 간 유사성을 측정하는 네 가지 방법은 다음과 같습니다:\n",
    "\n",
    "1. 평가자가 두 텍스트가 동일한지 여부를 판단하도록 요청하기.\n",
    "2. **정확한 일치(Exact match)**: 생성된 응답이 참조 응답 중 하나와 정확히 일치하는지 여부.\n",
    "3. **어휘적 유사성(Lexical similarity)**: 생성된 응답이 참조 응답과 얼마나 유사해 보이는지.\n",
    "4. **의미적 유사성(Semantic similarity)**: 생성된 응답이 참조 응답과 의미적으로 얼마나 가까운지.\n",
    "\n",
    "두 응답을 비교하는 데 사용되는 평가자는 사람일 수도 있고 AI일 수도 있습니다. 하지만 이미 사람을 사용하여 비교 작업을 수행하고 있다면 참조 데이터가 필요하지 않을 수도 있습니다. 사람이 생성된 응답을 직접 평가할 수 있기 때문입니다. AI 평가자를 사용하는 것은 AI-as-a-judge 접근 방식의 일부로 점점 더 흔해지고 있으며, 이는 다음 섹션의 초점이 될 것입니다.\n",
    "\n",
    "이 섹션은 사람이 설계한 측정 기준에 초점을 맞춥니다: 정확한 일치, 어휘적 유사성, 의미적 유사성. 정확한 일치는 이진 값(일치 또는 불일치)으로 계산되며, 다른 두 점수는 0과 1 또는 -1과 1 사이의 연속 척도에서 계산됩니다. AI-as-a-judge 접근 방식의 용이성과 유연성에도 불구하고, 사람이 설계한 유사성 측정은 정확한 특성으로 인해 업계에서 여전히 널리 사용됩니다.\n",
    "\n",
    "---\n",
    "\n",
    ">**참고**  \n",
    ">\n",
    ">이 섹션에서 유사성 측정은 생성된 출력의 품질을 평가하는 데 사용됩니다. 유사성 측정은 다음을 포함하되 이에 국한되지 않는 많은 다른 사용 사례에도 활용됩니다:\n",
    ">\n",
    ">- **검색 및 탐색(Retrieval and search)**: 쿼리와 유사한 항목을 찾기.  \n",
    ">- **순위 매기기(Ranking)**: 항목이 쿼리와 얼마나 유사한지에 따라 순위를 매기기.  \n",
    ">- **클러스터링(Clustering)**: 항목들 간의 유사성에 따라 그룹화하기.  \n",
    ">- **이상 탐지(Anomaly detection)**: 나머지와 가장 유사하지 않은 항목 탐지하기.  \n",
    ">- **데이터 중복 제거(Data deduplication)**: 다른 항목과 너무 유사한 항목 제거하기.  \n",
    ">\n",
    ">이 섹션에서 논의된 기술은 이 책 전반에서 다시 언급될 것입니다.\n",
    "\n",
    "---\n",
    "\n",
    "**정확한 일치(Exact Match)**\n",
    "\n",
    "생성된 응답이 참조 응답 중 하나와 정확히 일치하면 정확한 일치로 간주됩니다. 정확한 일치는 간단한 수학 문제, 일반 상식 질문, 퀴즈 스타일 질문과 같이 짧고 정확한 응답을 기대하는 작업에 유용합니다. 다음은 짧고 정확한 응답이 필요한 입력의 예입니다:\n",
    "\n",
    "- “2 + 3은 무엇입니까?”\n",
    "- “노벨상을 수상한 첫 번째 여성은 누구입니까?”\n",
    "- “내 현재 계좌 잔액은 얼마입니까?”\n",
    "- “빈칸 채우기: 파리는 프랑스와 같고, ___는 영국과 같습니다.”\n",
    "\n",
    "형식 문제를 고려한 일치 방식의 변형이 존재합니다. 하나의 변형은 참조 응답을 포함하는 모든 출력을 일치로 허용하는 것입니다. 예를 들어, 질문 “2 + 3은 무엇입니까?”에 대한 참조 응답이 “5”라고 가정합니다. 이 변형은 “5”를 포함하는 모든 출력을 허용하며, 여기에는 “정답은 5입니다”와 “2+3은 5입니다”도 포함됩니다.\n",
    "\n",
    "그러나, 이러한 변형은 때때로 잘못된 해결책을 허용할 수 있습니다. 예를 들어, 질문 “Anne Frank는 몇 년도에 태어났습니까?”에 대해 Anne Frank는 1929년 6월 12일에 태어났으므로 올바른 응답은 1929년입니다. 모델이 “1929년 9월 12일”이라고 출력한다면, 출력에 올바른 연도가 포함되긴 했지만, 출력 자체는 사실상 잘못된 것입니다.\n",
    "\n",
    "단순한 작업을 넘어가면, 정확한 일치는 거의 효과적이지 않습니다. 예를 들어, 프랑스어 문장 “Comment ça va?”는 “How are you?”, “How is everything?”, “How are you doing?”과 같은 여러 가능한 영어 번역을 가질 수 있습니다. 참조 데이터가 이 세 가지 번역만 포함하고 모델이 “How is it going?”을 생성한다면, 모델의 응답은 일치하지 않은 것으로 간주됩니다. 원래 텍스트가 길고 복잡할수록 가능한 번역의 수는 많아지고, 입력에 대한 가능한 응답의 포괄적인 집합을 생성하는 것은 불가능합니다. 복잡한 작업의 경우, 어휘적 유사성(Lexical Similarity)과 의미적 유사성(Semantic Similarity)이 더 효과적입니다.\n",
    "\n",
    "---\n",
    "\n",
    "**어휘적 유사성(Lexical Similarity)**\n",
    "\n",
    "어휘적 유사성은 두 텍스트가 얼마나 겹치는지를 측정합니다. 이를 위해 각 텍스트를 더 작은 토큰으로 나누는 작업을 먼저 수행합니다. 1장에서 논의된 바와 같이, 토큰은 문자, 단어 또는 단어의 일부가 될 수 있습니다.\n",
    "\n",
    "가장 단순한 형태로, 어휘적 유사성은 두 텍스트가 얼마나 많은 토큰을 공통으로 가지는지 세어 측정할 수 있습니다. 예를 들어, 참조 응답이 “My cats scare the mice”이고, 두 개의 생성된 응답이 다음과 같다고 가정합니다:\n",
    "\n",
    "1. “My cats eat the mice”  \n",
    "2. “Cats and mice fight all the time”  \n",
    "\n",
    "각 토큰이 단어라고 가정할 때, 개별 단어의 겹침만 계산한다면, 응답 A는 참조 응답의 5개 단어 중 4개를 포함하여 유사성이 80%입니다. 반면 응답 B는 참조 응답의 5개 단어 중 3개만 포함하여 유사성이 60%입니다. 따라서 응답 A가 참조 응답에 더 유사한 것으로 간주됩니다.\n",
    "\n",
    "실제로, 어휘적 유사성은 단일 단어 대신 단어 시퀀스의 겹침을 기반으로 측정됩니다. $ n $-그램(예: 2-그램, 바이그램)은 두 개의 단어로 이루어진 세트를 의미합니다. 예를 들어, “My cats scare the mice”는 “my cats”, “cats scare”, “scare the”, “the mice”의 네 가지 바이그램으로 구성됩니다. 참조 응답의 $ n $-그램 중 몇 퍼센트가 생성된 응답에도 포함되어 있는지를 측정합니다. 따라서 어휘적 유사성은 **$ n $-그램 유사성**이라고도 불립니다.\n",
    "\n",
    "어휘적 유사성을 위한 일반적인 지표로는 BLEU, ROUGE, METEOR++, TER, CIDEr가 있습니다. 이들은 겹침을 계산하는 방식에서 약간의 차이를 보입니다. 기초 모델이 등장하기 전에는 BLEU, ROUGE, 그리고 이와 관련된 지표가 특히 번역 작업에서 흔히 사용되었습니다. 기초 모델 이후에는 어휘적 유사성을 사용하는 벤치마크가 더 적습니다. 이러한 지표를 사용하는 벤치마크의 예로는 WMT 2024, COCO Captions, GEMv2가 있습니다.\n",
    "\n",
    "이 방법의 단점 중 하나는 포괄적인 참조 응답 세트를 구성해야 한다는 점입니다. 참조 세트에 유사한 응답이 포함되지 않은 경우, 좋은 응답도 낮은 유사성 점수를 받을 수 있습니다. 일부 벤치마크에서, Adept는 Fuyu 모델이 잘못된 출력을 생성했기 때문이 아니라, 몇 가지 올바른 응답이 참조 데이터에 없었기 때문에 낮은 점수를 받은 사례를 발견했습니다. **그림 3-6**은 Fuyu가 올바른 캡션을 생성했음에도 낮은 점수를 받은 이미지 캡셔닝 작업의 예를 보여줍니다.\n",
    "\n",
    "<img src=\"./images/fig_03_06.png\" alt=\"fig3-6\" width=\"800\">\n",
    "\n",
    "또 다른 단점은 더 높은 어휘적 유사성 점수가 항상 더 나은 응답을 의미하지는 않는다는 점입니다. 예를 들어, HumanEval이라는 코드 생성 벤치마크에서, OpenAI는 BLEU 점수가 올바른 솔루션과 잘못된 솔루션 모두에서 유사하게 높은 경우가 있음을 발견했습니다. 이는 BLEU 점수 최적화가 기능적 정확성 최적화와 같지 않음을 나타냅니다(Chen et al., 2021).\n",
    "\n",
    "---\n",
    "\n",
    "**의미적 유사성(Semantic Similarity)**\n",
    "\n",
    "어휘적 유사성은 두 텍스트가 시각적으로 유사한지를 측정하며, 의미적으로 동일한지 여부를 측정하지는 않습니다. 예를 들어, “What’s up?”과 “How are you?”를 고려해봅시다. 어휘적으로는 단어와 철자의 겹침이 거의 없으므로 다릅니다. 하지만 의미적으로는 매우 가깝습니다. 반대로, 시각적으로 유사한 텍스트가 완전히 다른 의미를 가질 수도 있습니다. 예를 들어, “Let’s eat, grandma”와 “Let’s eat grandma”는 전혀 다른 의미를 가집니다.\n",
    "\n",
    "**의미적 유사성**은 텍스트의 의미적 유사성을 계산하는 것을 목표로 합니다. 이를 위해 먼저 텍스트를 임베딩(embedding)이라고 불리는 수치 표현으로 변환해야 합니다. 예를 들어, 문장 “the cat sits on a mat”는 다음과 같은 임베딩으로 표현될 수 있습니다: $[0.11, 0.02, 0.54]$. 따라서, 의미적 유사성은 **임베딩 유사성(embedding similarity)**이라고도 불립니다.\n",
    "\n",
    "임베딩의 작동 방식에 대한 자세한 내용은 \"임베딩에 대한 간단한 소개\" 섹션에서 다루겠지만, 여기서는 텍스트를 임베딩으로 변환할 수 있는 방법이 있다고 가정합니다. 두 임베딩 간의 유사성은 코사인 유사성(cosine similarity)과 같은 지표를 사용하여 계산할 수 있습니다. 두 임베딩이 정확히 동일하면 유사성 점수는 1이 되고, 정반대인 경우 점수는 -1이 됩니다.\n",
    "\n",
    "텍스트 예제를 사용하고 있지만, 의미적 유사성은 이미지, 오디오를 포함한 모든 데이터 모달리티의 임베딩에 대해 계산할 수 있습니다. 텍스트의 의미적 유사성은 때때로 **의미 텍스트 유사성(semantic textual similarity)**이라고도 합니다.\n",
    "\n",
    "---\n",
    "\n",
    ">**경고**  \n",
    ">\n",
    ">의미적 유사성을 정확한 평가 카테고리에 넣었지만, 서로 다른 임베딩 알고리즘이 서로 다른 임베딩을 생성할 수 있기 때문에 주관적일 수 있습니다. 하지만, 두 개의 임베딩이 주어졌을 때, 이들 사이의 유사성 점수는 정확히 계산됩니다.\n",
    "\n",
    "---\n",
    "\n",
    "수학적으로, 생성된 응답의 임베딩을 $ A $, 참조 응답의 임베딩을 $ B $라고 하면, $ A $와 $ B $ 간의 코사인 유사성은 다음과 같이 계산됩니다:  \n",
    "$\n",
    "\\text{Cosine Similarity} = \\frac{A \\cdot B}{\\|A\\| \\|B\\|}\n",
    "$\n",
    "여기서:  \n",
    "- $ A \\cdot B $는 $ A $와 $ B $의 내적(dot product)을 의미합니다.  \n",
    "- $ \\|A\\| $는 $ A $의 유클리드 노름(또는 $ L_2 $ 노름)입니다. 예를 들어, $ A = [0.11, 0.02, 0.54] $라면, $ \\|A\\| = \\sqrt{0.11^2 + 0.02^2 + 0.54^2} $가 됩니다.\n",
    "\n",
    "의미 텍스트 유사성 측정을 위한 지표로는 BERT 임베딩을 사용하는 **BERTScore**와 다양한 알고리즘의 혼합으로 생성된 임베딩을 사용하는 **MoverScore**가 포함됩니다.\n",
    "\n",
    "의미 텍스트 유사성은 어휘적 유사성만큼 포괄적인 참조 응답 세트를 필요로 하지 않습니다. 그러나 의미적 유사성의 신뢰성은 기본 임베딩 알고리즘의 품질에 따라 달라집니다. 동일한 의미를 가진 두 텍스트도 임베딩이 나쁘다면 의미적 유사성 점수가 낮을 수 있습니다. 또 다른 단점은 기본 임베딩 알고리즘이 상당한 계산 시간과 실행 시간을 요구할 수 있다는 점입니다.\n",
    "\n",
    "--- \n",
    "\n",
    "**임베딩에 대한 소개(Introduction to embedding)**\n",
    "\n",
    "임베딩(embedding)은 복잡한 데이터를 낮은 차원의 공간으로 표현한 것입니다. 임베딩은 일반적으로 벡터 형태로 나타나며, 벡터 크기는 보통 100에서 10,000 사이입니다. 임베딩은 원래 데이터의 관련 특성을 보존하는 것을 목표로 합니다. 만약 이것이 추상적으로 느껴진다면, 예제를 통해 구체적으로 이해할 수 있습니다.\n",
    "\n",
    "예를 들어, 문장 “the cat sits on a mat”는 다음과 같은 임베딩 벡터로 표현될 수 있습니다: \\([0.11, 0.02, 0.54]\\). **임베딩 크기(embedding size)**는 임베딩 벡터의 요소 수를 의미하며, 이는 임베딩 알고리즘에 의해 결정됩니다. 임베딩을 생성하도록 특별히 훈련된 모델로는 오픈소스 모델인 BERT, CLIP, sentence-transformers 등이 있으며, API로 제공되는 많은 독점적인 임베딩 모델도 존재합니다. 표 3-1은 몇 가지 인기 있는 모델에서 사용되는 임베딩 크기를 보여줍니다.\n",
    "\n",
    "**표 3-1. 일반적인 모델에서 사용되는 임베딩 크기**\n",
    "\n",
    "| 모델                           | 임베딩 크기                           |\n",
    "|--------------------------------|----------------------------------------|\n",
    "| Google의 BERT                  | BERT base: 768<br>BERT large: 1024     |\n",
    "| OpenAI의 CLIP                  | 이미지: 512<br>텍스트: 512            |\n",
    "| OpenAI Embeddings API          | text-embedding-3-small: 1536<br>text-embedding-3-large: 3072 |\n",
    "| Cohere’s Embed v3              | embed-english-v3.0: 1024<br>embed-english-light-3.0: 384 |\n",
    "\n",
    "많은 머신러닝 모델은 데이터를 먼저 벡터 표현으로 변환해야 합니다. GPT와 LLaMA와 같은 언어 모델을 포함한 많은 ML 모델도 임베딩을 생성하는 단계를 포함합니다. 이러한 모델에 접근할 수 있다면, 임베딩만 추출할 수도 있습니다. 그러나 이러한 임베딩의 품질은 임베딩 생성을 위해 특별히 훈련된 모델이 생성한 임베딩의 품질만큼 좋지 않을 수 있습니다.\n",
    "\n",
    "임베딩 알고리즘의 목표는 원본 데이터의 본질을 포착하는 임베딩을 생성하는 것입니다. 이를 어떻게 검증할 수 있을까요? 임베딩 벡터 \\([0.11, 0.02, 0.54]\\)는 원래 텍스트 “the cat sits on a mat”와는 전혀 비슷하지 않아 보입니다.\n",
    "\n",
    "고차원에서, 임베딩 알고리즘은 더 유사한 텍스트가 더 가까운 임베딩을 가지는 경우, 즉 코사인 유사성 또는 관련 지표로 측정되는 경우 좋은 것으로 간주됩니다. 예를 들어, “the cat sits on a mat”의 임베딩은 “AI research is super fun”의 임베딩보다 “the dog plays on the grass”의 임베딩에 더 가까워야 합니다.\n",
    "\n",
    "또한 임베딩의 품질은 해당 작업에 대한 유용성에 따라 평가할 수 있습니다. 임베딩은 분류(classification), 주제 모델링(topic modeling), 추천 시스템(recommender systems), RAG 등 많은 작업에 사용됩니다. 여러 작업에서 임베딩 품질을 측정하는 벤치마크의 예로는 **MTEB(Massive Text Embedding Benchmark)**(Muennighoff et al., 2023)이 있습니다.\n",
    "\n",
    "텍스트를 예제로 사용했지만, 어떤 데이터든 임베딩 표현을 가질 수 있습니다. 예를 들어, **Criteo**와 **Coveo**는 제품에 대한 임베딩을 가지고 있습니다. **Pinterest**는 이미지, 그래프, 쿼리, 사용자에 대한 임베딩을 보유하고 있습니다.\n",
    "\n",
    "새로운 프론티어는 서로 다른 모달리티(텍스트와 이미지와 같은 데이터 유형)의 데이터를 위한 **공동 임베딩(joint embedding)**을 만드는 것입니다. **CLIP**(Radford et al., 2021)은 텍스트와 이미지를 하나의 임베딩 공간으로 매핑할 수 있는 첫 주요 모델 중 하나였습니다. **ULIP**(Xue et al., 2022)는 텍스트, 이미지, 3D 포인트 클라우드의 통합 표현을 만드는 것을 목표로 하고 있습니다. **ImageBind**(Girdhar et al., 2023)는 텍스트, 이미지, 오디오를 포함한 여섯 가지 다른 모달리티 전반에서 공동 임베딩을 학습합니다.\n",
    "\n",
    "**그림 3-7**은 CLIP의 아키텍처를 시각화한 것입니다. 훈련 목표는 텍스트의 임베딩이 공동 임베딩 공간에서 해당 이미지의 임베딩에 가깝게 만드는 것입니다.\n",
    "\n",
    "<img src=\"./images/fig_03_07.png\" alt=\"fig3-7\" width=\"800\">\n",
    "\n",
    "서로 다른 모달리티의 데이터를 표현할 수 있는 공동 임베딩 공간은 **다중 모달 임베딩 공간(multimodal embedding space)**이라고 합니다. 텍스트-이미지 공동 임베딩 공간에서, 낚시하는 사람의 이미지 임베딩은 “fashion show”(패션쇼)라는 텍스트의 임베딩보다 “a fisherman”(어부)라는 텍스트의 임베딩에 더 가까워야 합니다. 이러한 공동 임베딩 공간은 서로 다른 모달리티의 임베딩을 비교하고 결합할 수 있게 합니다. 예를 들어, 이는 텍스트 기반 이미지 검색을 가능하게 합니다. 텍스트를 주어지면, 해당 텍스트와 가장 가까운 이미지를 찾을 수 있습니다.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **AI-as-a-Judge**\n",
    "\n",
    "개방형 응답을 평가하는 어려움은 많은 팀이 인간 평가에 의존하게 만들었습니다. 그러나 AI가 점점 더 많은 도전적인 작업을 자동화하는 데 성공하면서, 평가를 자동화하는 데 AI를 사용할 수 있을까요? AI를 사용하여 AI를 평가하는 접근 방식은 **AI-as-a-judge** 또는 **LLM-as-a-judge**라고 불립니다. 다른 AI 모델을 평가하는 데 사용되는 AI 모델은 **AI 판사(AI judge)**라고도 합니다.\n",
    "\n",
    "AI를 사용해 평가를 자동화하려는 아이디어는 오래전부터 존재했지만, 실제로 실현 가능해진 것은 AI 모델이 이를 수행할 수 있을 정도로 발전한 2020년 경, GPT-3가 출시된 시점부터입니다. 이 글을 쓰는 현재, AI-as-a-judge는 실무에서 AI 모델을 평가하는 가장 일반적인 방법 중 하나가 되었습니다. 2023년과 2024년 동안 제가 본 대부분의 AI 평가 스타트업 데모는 어떤 방식으로든 AI-as-a-judge를 활용하고 있었습니다. **LangChain’s State of AI** 보고서에 따르면, 2023년에 그들의 플랫폼에서 수행된 평가의 58%가 AI 판사에 의해 이루어졌습니다. AI-as-a-judge는 또한 활발히 연구되고 있는 분야입니다.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **AI-as-a-Judge를 사용하는 이유**\n",
    "\n",
    "AI 판사는 빠르고, 사용하기 쉽고, 인간 평가자에 비해 비교적 저렴합니다. 또한 참조 데이터가 없는 상태에서도 작동할 수 있으므로 참조 데이터가 없는 실무 환경에서도 사용할 수 있습니다.\n",
    "\n",
    "AI 모델에 정답성, 반복성, 유해성, 건전성, 환각(hallucinations) 등 어떤 기준을 기반으로도 출력에 대한 판단을 요청할 수 있습니다. 이는 사람에게 어떤 주제에 대해 의견을 묻는 것과 비슷합니다. 여러분은 “하지만 사람의 의견을 항상 신뢰할 수 없는 것처럼 AI의 판단도 항상 신뢰할 수 없잖아”라고 생각할 수도 있습니다. 맞는 말입니다. 그러나 각 AI 모델이 집단의 의견을 집계한 결과로 작동하므로, AI 모델이 집단의 대표적인 판단을 내릴 가능성은 있습니다. 적절한 프롬프트와 적합한 모델을 사용하면 다양한 주제에서 꽤 합리적인 판단을 얻을 수 있습니다.\n",
    "\n",
    "연구에 따르면, 특정 AI 판사는 인간 평가자와 강하게 상관관계를 가집니다. 2023년, **Zheng et al.**의 연구는 GPT-4와 인간 간의 평가 일치율이 85%에 달했으며, 이는 인간 간 일치율(81%)보다도 높다는 것을 발견했습니다. **AlpacaEval**의 저자들(Dubois et al., 2023) 또한 그들의 AI 판사가 인간이 평가하는 LMSYS의 Chat Arena 리더보드와 거의 완벽한 상관관계(0.98)를 가짐을 발견했습니다.\n",
    "\n",
    "AI는 응답을 평가할 뿐만 아니라, 자신의 판단을 설명할 수도 있습니다. 이는 평가 결과를 감사하려고 할 때 특히 유용합니다. **그림 3-8**은 GPT-4가 자신의 판단을 설명하는 예를 보여줍니다.\n",
    "\n",
    "<img src=\"./images/fig_03_08.png\" alt=\"fig3-8\" width=\"800\">\n",
    "\n",
    "AI-as-a-judge의 유연성은 이를 다양한 응용 프로그램에 적합하게 만들며, 일부 애플리케이션에서는 유일한 평가 옵션이 되기도 합니다. AI의 판단이 인간의 판단만큼 좋지 않더라도, 애플리케이션 개발을 안내하고 프로젝트를 시작할 자신감을 제공하기에 충분할 수 있습니다.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **AI-as-a-Judge를 사용하는 방법**\n",
    "\n",
    "AI를 사용하여 판단을 내리는 데에는 여러 가지 방법이 있습니다. 예를 들어, 응답 자체의 품질을 평가하거나, 응답을 참조 데이터와 비교하거나, 응답을 다른 응답과 비교하는 방법이 있습니다. 아래는 이러한 세 가지 접근 방식을 위한 간단한 예제 프롬프트입니다.\n",
    "\n",
    "1. **응답 자체를 평가하기**  \n",
    "   주어진 원래 질문에 대해 응답 자체가 얼마나 좋은지를 평가합니다.  \n",
    "   ```\n",
    "   “다음 질문과 답변을 고려하여, 이 답변이 질문에 대해 얼마나 좋은지를 평가하십시오. 점수는 1에서 5 사이로 매깁니다.  \n",
    "   - 1은 매우 나쁨을 의미합니다.  \n",
    "   - 5는 매우 좋음을 의미합니다.  \n",
    "   질문: [QUESTION]  \n",
    "   답변: [ANSWER]  \n",
    "   점수:”\n",
    "   ```\n",
    "\n",
    "2. **생성된 응답과 참조 응답 비교하기**  \n",
    "   생성된 응답이 참조 응답과 동일한지 평가합니다. 이는 사람이 설계한 유사성 측정 방식의 대안이 될 수 있습니다.  \n",
    "   ```\n",
    "   “다음 질문, 참조 답변 및 생성된 답변을 고려하여, 이 생성된 답변이 참조 답변과 동일한지 평가하십시오. 결과는 True 또는 False로 출력하십시오.  \n",
    "   질문: [QUESTION]  \n",
    "   참조 답변: [REFERENCE ANSWER]  \n",
    "   생성된 답변: [GENERATED ANSWER]”\n",
    "   ```\n",
    "\n",
    "3. **생성된 두 응답 비교하기**  \n",
    "   생성된 두 응답 중 어떤 것이 더 나은지 또는 사용자가 선호할 가능성이 높은지 평가합니다. 이는 후처리 정렬(post-training alignment)을 위한 선호 데이터 생성(2장에서 논의됨), 테스트 시간 샘플링(test-time sampling), 그리고 비교 평가를 통해 모델을 순위 매기는 데 유용합니다(다음 섹션에서 논의됨).  \n",
    "   ```\n",
    "   “다음 질문과 두 답변을 고려하여, 어느 답변이 더 나은지 평가하십시오. A 또는 B로 출력하십시오.  \n",
    "   질문: [QUESTION]  \n",
    "   A: [FIRST ANSWER]  \n",
    "   B: [SECOND ANSWER]  \n",
    "   더 나은 답변은:”\n",
    "   ```\n",
    "\n",
    "범용 AI 판사는 작업에 독특한 기준을 포함하여 어떤 기준에 기반해서도 응답을 평가하도록 요청할 수 있습니다. 예를 들어, 롤플레잉 챗봇을 구축하는 경우, 챗봇의 응답이 사용자가 기대하는 역할과 일치하는지 평가할 수 있습니다. 예를 들어, “이 응답이 덤블도어가 말할 것 같은가요?”를 평가하도록 요청할 수 있습니다. 프로모션용 제품 사진을 생성하는 애플리케이션을 구축하는 경우, “1에서 5까지, 이 이미지에서 제품의 신뢰성을 어떻게 평가하시겠습니까?”라고 물을 수 있습니다. **표 3-2**는 일부 AI 도구에서 제공하는 AI-as-a-judge의 일반적인 내장 기준을 보여줍니다.\n",
    "\n",
    "**표 3-2. 일부 AI 도구에서 제공하는 AI-as-a-judge 내장 기준의 예**\n",
    "\n",
    "| **AI 도구**               | **내장 기준**                                                                                                    |\n",
    "|--------------------------|------------------------------------------------------------------------------------------------------------------|\n",
    "| **Azure AI Studio**      | Groundedness, Relevance, Coherence, Fluency, GPT-Similarity                                                     |\n",
    "| **MLflow Evaluate**      | Faithfulness, Relevance                                                                                        |\n",
    "| **Langchain**            | Conciseness, Relevance, Correctness, Coherence, Harmfulness, Maliciousness, Helpfulness, Controversiality, Misogyny, Insensitivity, Criminality |\n",
    "| **RAGAS**                | Faithfulness, Answer relevance                                                                                 |\n",
    "\n",
    "AI-as-a-judge 기준은 표준화되어 있지 않다는 점을 반드시 기억해야 합니다. Azure AI Studio의 관련성 점수는 MLflow의 관련성 점수와 크게 다를 수 있습니다. 이러한 점수는 판사로 사용되는 기본 모델과 프롬프트에 따라 달라집니다.\n",
    "\n",
    "AI 판사에 프롬프트를 제공하는 방법은 일반적인 AI 애플리케이션에 프롬프트를 제공하는 방법과 유사합니다. 일반적으로, 판사에게 제공하는 프롬프트는 다음을 명확히 설명해야 합니다:\n",
    "\n",
    "1. 모델이 수행해야 하는 작업  \n",
    "   예: 생성된 응답과 질문 간의 관련성을 평가하는 작업.\n",
    "\n",
    "2. 모델이 평가 시 따라야 하는 기준  \n",
    "   예: “주요 초점은 생성된 응답이 주어진 질문에 대한 정답에 따라 충분한 정보를 포함하는지를 판단하는 데 두어야 합니다.”  \n",
    "   지침이 구체적일수록 좋습니다.\n",
    "\n",
    "3. 점수 체계는 다음과 같을 수 있습니다:  \n",
    "   a. **분류(Classification)**: 좋음/나쁨 또는 관련성 있음/없음/중립과 같은 방식.  \n",
    "   b. **이산적 수치 값(Discrete numerical values)**: 1~5 또는 1~10과 같은 값. 이산적 수치 값은 분류의 특수한 사례로 간주될 수 있으며, 이 경우 각 클래스는 의미적 해석 대신 수치적 해석을 갖습니다.  \n",
    "   c. **연속적 수치 값(Continuous numerical values)**: 0~1 사이의 값과 같이 유사성 정도를 평가하고자 할 때 사용됩니다.\n",
    "\n",
    "---\n",
    "\n",
    ">**참고(Note)**  \n",
    ">\n",
    ">언어 모델은 숫자보다 텍스트에서 더 잘 작동하는 경향이 있습니다. 보고된 바에 따르면, AI 판사는 숫자 점수 체계보다 분류 작업에서 더 잘 작동한다고 합니다.\n",
    ">\n",
    ">숫자 점수 체계를 사용하는 경우, 이산적 점수가 연속적 점수보다 더 잘 작동하는 것으로 보입니다. 경험적으로, 이산적 점수의 범위가 넓을수록 모델 성능이 저하될 가능성이 높습니다. 일반적인 이산적 점수 체계는 1에서 5 사이입니다.\n",
    "\n",
    "---\n",
    "\n",
    "예제가 포함된 프롬프트는 더 나은 성능을 보이는 것으로 나타났습니다. 1에서 5 사이의 점수 체계를 사용하는 경우, 각 점수(예: 1, 2, 3, 4, 5)에 해당하는 응답의 예제를 포함하고, 가능하다면 응답이 특정 점수를 받는 이유를 설명하십시오.\n",
    "\n",
    "다음은 **Azure AI Studio**의 \"관련성(relevance)\" 기준에 사용된 프롬프트의 일부입니다. 이 프롬프트는 작업, 기준, 점수 체계, 낮은 점수를 받은 입력의 예제, 그리고 이 입력이 낮은 점수를 받은 이유를 설명합니다. 일부 내용은 간결성을 위해 생략되었습니다.\n",
    "\n",
    "```\n",
    "당신의 작업은 생성된 응답과 질문 사이의 관련성을 평가하는 것이며, 1에서 5 사이의 범위로 점수를 매기고, 점수를 부여한 이유도 제공하는 것입니다.\n",
    "\n",
    "당신의 주요 초점은 생성된 응답이 주어진 질문에 대한 정답(ground truth answer)을 기준으로 충분한 정보를 포함하는지를 판단하는 데 두어야 합니다. …\n",
    "\n",
    "생성된 응답이 정답과 모순될 경우, 1~2점의 낮은 점수를 받게 됩니다.\n",
    "\n",
    "예를 들어, 질문 “하늘은 파랗습니까?”에 대해 정답이 “네, 하늘은 파랗습니다.”이고, 생성된 응답이 “아니요, 하늘은 파랗지 않습니다.”인 경우를 고려하십시오.\n",
    "\n",
    "이 예에서 생성된 응답은 하늘이 파랗지 않다고 말하며 정답과 모순됩니다. 실제로 하늘은 파랗기 때문입니다.\n",
    "\n",
    "이 불일치는 1~2점의 낮은 점수를 받게 하며, 낮은 점수를 받은 이유는 생성된 응답과 정답 사이의 모순을 반영합니다.\n",
    "```\n",
    "\n",
    "**그림 3-9**는 주어진 질문에 따라 응답의 품질을 평가하는 AI 판사의 예를 보여줍니다.\n",
    "\n",
    "<img src=\"./images/fig_03_09.png\" alt=\"fig3-9\" width=\"800\">\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **AI-as-a-Judge의 한계**\n",
    "\n",
    "AI-as-a-judge의 많은 장점에도 불구하고, 많은 팀이 이 접근 방식을 채택하는 것을 주저합니다. AI를 사용하여 AI를 평가하는 것은 자기모순적(tautological)으로 보일 수 있습니다. AI의 확률적 특성은 AI 판사가 평가자로서 너무 신뢰할 수 없게 만듭니다. AI 판사는 애플리케이션에 상당한 비용과 지연(latency)을 초래할 가능성도 있습니다. 이러한 한계로 인해 일부 팀은 AI-as-a-judge를 실무에서 시스템을 평가할 다른 방법이 없을 때 사용하는 대안 옵션으로 간주합니다.\n",
    "\n",
    "---\n",
    "\n",
    "**일관성**\n",
    "\n",
    "평가 방법이 신뢰할 수 있으려면, 그 결과가 일관적이어야 합니다. 그러나 AI 판사는 다른 모든 AI 애플리케이션과 마찬가지로 확률적입니다. 동일한 판사가 동일한 입력에 대해 다른 프롬프트를 받으면 다른 점수를 출력할 수 있습니다. 심지어 동일한 판사가 동일한 프롬프트로 두 번 실행되더라도 다른 점수를 출력할 수 있습니다. 이러한 비일관성은 평가 결과를 재현하거나 신뢰하기 어렵게 만듭니다.\n",
    "\n",
    "AI 판사가 더 일관성을 가지도록 만드는 것은 가능합니다. 2장에서 샘플링 변수(sampling variables)를 통해 이를 수행하는 방법을 논의했습니다. **Zheng et al. (2023)**은 프롬프트에 평가 예제를 포함시키는 것이 GPT-4의 일관성을 65%에서 77.5%로 증가시킬 수 있음을 보여주었습니다. 그러나 그들은 높은 일관성이 반드시 높은 정확성을 의미하지 않을 수 있음을 인정했습니다. 이는 판사가 일관되게 동일한 실수를 저지를 수도 있기 때문입니다. 게다가 더 많은 예제를 포함하면 프롬프트가 더 길어지고, 긴 프롬프트는 더 높은 추론 비용을 초래합니다. **Zheng et al.**의 실험에서는 프롬프트에 더 많은 예제를 포함시키는 것이 GPT-4의 비용을 네 배로 증가시켰습니다.\n",
    "\n",
    "---\n",
    "\n",
    "**기준 모호성**\n",
    "\n",
    "많은 사람이 설계한 메트릭과 달리, AI-as-a-judge 메트릭은 표준화되어 있지 않아 쉽게 오해되거나 오용될 수 있습니다. 이 글을 쓰는 시점에서, 오픈 소스 도구 **MLflow**, **RAGAS**, **LlamaIndex** 모두 **faithfulness(신뢰성)** 기준을 내장하고 있습니다. 그러나 이들의 지침과 점수 체계는 모두 다릅니다. **표 3-3**에서 보듯, MLflow는 1에서 5까지의 점수 체계를 사용하며, RAGAS는 0과 1을 사용하고, LlamaIndex는 판사에게 YES와 NO를 출력하도록 요청합니다.\n",
    "\n",
    "이 세 도구가 출력하는 신뢰성 점수는 비교할 수 없습니다. 예를 들어, 주어진 (문맥, 답변) 쌍에 대해 MLflow가 신뢰성 점수로 3을 출력하고, RAGAS는 1을, LlamaIndex는 NO를 출력한다면 어떤 점수를 사용하시겠습니까?\n",
    "\n",
    "**표 3-3. 동일한 기준에 대해 매우 다른 기본 프롬프트를 가진 도구들**\n",
    "\n",
    "| **도구**         | **프롬프트**                                                                                                                                                                                                                                                                                                                                                                           | **점수 체계** |\n",
    "|-----------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|-------------|\n",
    "| **MLflow**      | 신뢰성(faithfulness)은 제공된 출력과 제공된 문맥에서만 평가되며, 신뢰성을 점수화할 때 제공된 입력은 완전히 무시하십시오. 신뢰성은 제공된 출력이 제공된 문맥과 사실적으로 얼마나 일치하는지를 평가합니다. <br> … <br> 신뢰성: 점수에 대한 세부 정보는 아래와 같습니다: <br> - 점수 1: 출력의 주장 중 어떤 것도 제공된 문맥에서 유추할 수 없음. <br> - 점수 2: …                                                                                                                                                                | 1~5         |\n",
    "| **RAGAS**       | 주어진 문맥에 따라 일련의 진술의 신뢰성을 판단하는 것이 당신의 작업입니다. 각 진술에 대해 해당 진술이 문맥에 기반하여 검증될 수 있다면 1을 반환하고, 검증될 수 없다면 0을 반환하십시오.                                                                                                                                                                                                                                   | 0과 1       |\n",
    "| **LlamaIndex**  | 특정 정보가 문맥에 의해 지원되는지 여부를 알려주세요. \"YES\" 또는 \"NO\"로 답변해야 합니다. <br> 문맥의 대부분이 관련이 없더라도 문맥 중 일부가 정보를 지원한다면 \"YES\"로 답변하십시오. 아래는 몇 가지 예제입니다. <br> 정보: 애플 파이는 일반적으로 더블 크러스트입니다. <br> 문맥: 애플 파이는 과일 파이입니다 … 일반적으로 속 위아래에 페이스트리로 감싸져 있습니다 … <br> 답변: YES | YES와 NO    |\n",
    "\n",
    "애플리케이션은 시간이 지나면서 진화하지만, 평가 방식은 이상적으로 고정되어야 합니다. 이렇게 하면 평가 메트릭을 사용하여 애플리케이션의 변화를 모니터링할 수 있습니다. 하지만 AI 판사는 또한 AI 애플리케이션이기 때문에 시간이 지나면서 변할 수 있습니다.\n",
    "\n",
    "예를 들어, 지난달 애플리케이션의 일관성(coherence) 점수가 90%였고 이번 달에는 92%라고 가정해봅시다. 이것이 애플리케이션의 일관성이 향상되었음을 의미할까요? 이 질문에 대한 답변은 두 경우 모두 동일한 AI 판사를 사용했는지 확실히 알지 않는 한 어렵습니다. 만약 이번 달 판사의 프롬프트가 지난달과 다르다면 어떻게 될까요? 아마도 성능이 약간 더 나은 프롬프트로 변경되었거나, 동료가 지난달의 프롬프트에 있던 오타를 수정했을 수도 있습니다. 또한 이번 달 판사가 더 관대할 수도 있습니다.\n",
    "\n",
    "이 상황은 애플리케이션 팀과 AI 판사 팀이 서로 다른 팀에 의해 관리되는 경우 특히 혼란스러울 수 있습니다. AI 판사 팀이 애플리케이션 팀에게 알리지 않고 판사를 변경할 수도 있습니다. 그 결과, 애플리케이션 팀은 평가 결과의 변화를 애플리케이션의 변화로 잘못 해석할 수 있으며, 실제로는 판사의 변화 때문일 수 있습니다.\n",
    "\n",
    "---\n",
    "\n",
    ">**팁(TIP)**  \n",
    ">\n",
    ">판사로 사용된 모델과 프롬프트를 확인할 수 없는 경우, AI 판사를 신뢰하지 마십시오.\n",
    "\n",
    "---\n",
    "\n",
    "평가 방법이 표준화되는 데는 시간이 걸립니다. 이 분야가 진화하고 더 많은 가드레일이 도입됨에 따라, AI 판사가 훨씬 더 표준화되고 신뢰할 수 있게 되기를 희망합니다.\n",
    "\n",
    "---\n",
    "\n",
    "**비용 증가와 지연**\n",
    "\n",
    "AI 판사는 실험 단계와 프로덕션 단계 모두에서 애플리케이션을 평가하는 데 사용할 수 있습니다. 많은 팀이 프로덕션에서 리스크를 줄이기 위해 AI 판사를 가드레일로 사용하며, AI 판사가 좋은 것으로 판단한 응답만 사용자에게 반환합니다.\n",
    "\n",
    "응답을 평가하기 위해 강력한 모델을 사용하는 것은 비용이 많이 들 수 있습니다. 만약 GPT-4를 사용해 응답을 생성하고 평가한다면, GPT-4 호출 횟수가 두 배로 늘어나 API 비용이 대략 두 배가 됩니다. 세 가지 평가 기준(예: 응답의 전반적인 품질, 사실적 일관성, 유해성)을 평가하려는 경우, 프롬프트가 세 배로 늘어나 API 호출 횟수는 네 배로 증가할 수 있습니다.\n",
    "\n",
    "비용을 줄이기 위해 더 약한 모델이나 자체 호스팅된 모델을 판사로 사용할 수 있습니다(다음 섹션 “어떤 모델이 판사 역할을 할 수 있는가?” 참조). 또한 응답의 일부만 평가하는 **부분 점검(spot-checking)**을 통해 비용을 절감할 수 있습니다. 부분 점검은 몇 가지 실패를 포착하지 못할 수도 있다는 의미입니다. 평가하는 샘플의 비율이 클수록 애플리케이션에 대한 신뢰가 높아지지만, 비용도 더 높아집니다. 비용과 신뢰 사이의 적절한 균형을 찾는 것은 시행착오가 필요할 수 있습니다. 종합적으로 볼 때, AI 판사는 여전히 인간 평가자보다 훨씬 저렴합니다.\n",
    "\n",
    "---\n",
    "\n",
    "**AI 판사의 편향**\n",
    "\n",
    "인간 평가자에게는 편향이 있으며, AI 판사도 마찬가지입니다. 서로 다른 AI 판사들은 다양한 편향을 가지고 있으며, 다음은 그 중 일반적인 예들입니다. AI 판사의 편향을 인지하면 이들의 점수를 올바르게 통합하고 이러한 편향을 완화하는 데 도움을 받을 수 있습니다.\n",
    "\n",
    "AI 판사는 **자기 강화 편향(self-enhance bias)**을 가지는 경향이 있습니다. 이는 모델이 자신이 생성한 응답을 다른 모델이 생성한 응답보다 선호하는 경우입니다. 모델이 가장 가능성이 높은 응답을 생성하도록 돕는 동일한 메커니즘이 이 응답에도 높은 점수를 부여하게 만듭니다. Zheng et al.의 실험에서, GPT-4는 자신에게 10% 더 높은 승률을 부여했으며, Claude-v1은 자신에게 25% 더 높은 승률을 부여했습니다.\n",
    "\n",
    "많은 AI 모델들은 **첫 번째 위치 편향(first-position bias)**을 가지고 있습니다. AI 판사는 쌍 비교(pairwise comparison)에서 첫 번째 응답이나 옵션 목록의 첫 번째 항목을 선호할 수 있습니다. 이는 동일한 테스트를 여러 번 반복 실행하거나 프롬프트 순서를 신중히 설계함으로써 완화할 수 있습니다. AI의 위치 편향은 인간과 반대되는 경향이 있습니다. 인간은 **마지막에 본 응답(recency bias)**을 선호하는 경향이 있습니다.\n",
    "\n",
    "일부 AI 판사는 **장문 편향(verbosity bias)**을 가지며, 응답의 품질과 무관하게 더 긴 응답을 선호합니다. Wu와 Aji(2023)는 GPT-4와 Claude-1이 짧고 올바른 응답(~50단어)보다 사실적 오류가 포함된 긴 응답(~100단어)을 선호한다고 발견했습니다. 그러나 Zheng et al.(2023)은 GPT-4가 GPT-3.5보다 이러한 편향에 덜 민감하다는 것을 발견했으며, 이는 모델이 더 강력해지면서 이 편향이 사라질 수 있음을 시사합니다.\n",
    "\n",
    "이러한 모든 편향 외에도, AI 판사는 모든 AI 애플리케이션과 동일한 한계를 가지고 있습니다. 여기에는 개인 정보 보호와 지적 재산권 문제가 포함됩니다. AI 판사로 독점 모델을 사용하는 경우, 데이터를 해당 모델에 보내야 할 수도 있습니다. 모델 제공자가 교육 데이터를 공개하지 않는다면, 판사가 상업적으로 안전하게 사용 가능한지 확신할 수 없습니다.\n",
    "\n",
    "AI 판사 접근 방식에는 한계가 있지만, 이 방식이 가진 많은 장점 때문에 이 접근 방식의 채택이 계속 증가할 것이라고 믿습니다. 그러나 AI 판사는 정확한 평가 방법 또는 인간 평가와 함께 보완적으로 사용되어야 합니다.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **어떤 모델이 판사 역할을 할 수 있는가?**\n",
    "\n",
    "판사는 평가 대상 모델보다 강력하거나, 약하거나, 동등할 수 있습니다. 각 시나리오에는 장단점이 존재합니다.\n",
    "\n",
    "더 강력한 판사는 합리적으로 보입니다. 시험 채점자가 시험 응시자보다 더 많은 지식을 가져야 하지 않을까요? 더 강력한 모델은 더 나은 판단을 내릴 수 있을 뿐만 아니라, 약한 모델이 더 나은 응답을 생성하도록 유도하여 이를 개선할 수도 있습니다.\n",
    "\n",
    "여러분은 다음과 같은 의문을 가질 수 있습니다: 더 강력한 모델을 이미 사용할 수 있다면, 약한 모델을 사용하여 응답을 생성할 필요가 있을까요? 그 답은 비용과 지연 시간에 있습니다. 더 강력한 모델을 사용하여 모든 응답을 생성할 예산이 없을 수도 있습니다. 이 경우 응답의 일부에 대해서만 강력한 모델을 사용하여 평가하는 방법을 사용할 수 있습니다. 예를 들어, 저렴한 자체 개발 모델을 사용하여 응답의 99%를 생성하고, GPT-4를 사용하여 나머지 1%를 평가하는 것입니다.\n",
    "\n",
    "더 강력한 모델이 너무 느려서 애플리케이션에 적합하지 않을 수도 있습니다. 빠른 모델을 사용하여 응답을 생성하는 동안, 더 강력하지만 느린 모델이 백그라운드에서 평가 작업을 수행할 수도 있습니다. 강력한 모델이 약한 모델의 응답이 나쁘다고 판단하면, 강력한 모델의 응답으로 이를 업데이트하는 조치가 취해질 수 있습니다. 반대로 약한 모델이 생성한 응답을 강력한 모델이 백그라운드에서 평가하는 방식도 일반적입니다.\n",
    "\n",
    "더 강력한 모델을 판사로 사용하는 경우, 두 가지 도전 과제가 남습니다. 첫째, 가장 강력한 모델에는 적합한 판사가 없게 됩니다. 둘째, 어떤 모델이 가장 강력한지 판단할 대체 평가 방법이 필요합니다.\n",
    "\n",
    "모델이 스스로를 판단하도록 하는 것은, 특히 자기 강화 편향(self-enhance bias) 때문에 부정 행위처럼 보일 수 있습니다. 그러나 자기 평가(self-evaluation)는 정합성 점검(sanity check)에는 유용할 수 있습니다. 모델이 자신의 응답이 잘못되었다고 생각한다면, 해당 모델은 신뢰할 수 없는 것일 수도 있습니다. 정합성 점검을 넘어, 모델에게 스스로를 평가하도록 요청하는 것은 모델이 응답을 수정하고 이를 개선하도록 유도할 수 있습니다(Press et al., 2022; Gou et al., 2023; Valmeekam et al., 2023). 아래는 자기 평가가 어떻게 작동할 수 있는지를 보여주는 예시입니다:\n",
    "\n",
    "**프롬프트 [사용자]:** 10+3은 무엇인가요?  \n",
    "**첫 번째 응답 [AI]:** 30  \n",
    "**자체 비판 [AI]:** 이 응답이 맞습니까?  \n",
    "**최종 응답 [AI]:** 아닙니다. 정답은 13입니다.\n",
    "\n",
    "판사가 판단 대상 모델보다 약할 수 있는지에 대한 의문도 남습니다. 일부는 판정(judging)이 생성(generation)보다 더 쉬운 작업이라고 주장합니다. 노래를 쓸 수는 없더라도, 누군가가 노래가 좋은지 여부에 대한 의견을 가질 수 있는 것과 같습니다. 약한 모델은 더 강력한 모델의 출력을 평가할 수 있어야 합니다.\n",
    "\n",
    "Zheng et al. (2023)은 더 강력한 모델이 인간의 선호도와 더 잘 연결된다는 것을 발견했으며, 이는 사람들이 자신이 감당할 수 있는 가장 강력한 모델을 선택하게 만듭니다. 그러나 이 실험은 일반적인 목적의 심판으로 제한되었습니다. 제가 흥미를 느끼는 연구 방향 중 하나는 소형, 전문화된 심판(specialized judges)입니다. 전문화된 심판은 특정 판단을 내리도록 훈련되었으며, 특정 기준을 사용하고 특정 채점 시스템을 따릅니다. 소형, 전문화된 심판은 대형, 범용 심판보다 특정 판단에 더 신뢰할 수 있습니다.\n",
    "\n",
    "AI 심판을 사용하는 많은 방법이 있기 때문에, 다양한 전문화된 AI 심판이 존재할 수 있습니다. 여기에서는 세 가지 전문화된 심판의 예를 살펴보겠습니다: 보상 모델(reward models), 참조 기반 심판(reference-guided judges), 선호도 모델(preference models).\n",
    "\n",
    "**보상 모델**은 (프롬프트, 응답) 쌍을 입력으로 받아 프롬프트에 주어진 응답이 얼마나 적합한지 점수를 매깁니다. 보상 모델은 RLHF(강화 학습을 통한 인간 피드백)에서 수년간 성공적으로 사용되었습니다. Google(2023)에서 개발한 Cappy는 또 다른 보상 모델의 예입니다. (프롬프트, 응답) 쌍이 주어지면, Cappy는 응답이 얼마나 올바른지를 0에서 1 사이의 점수로 출력합니다. Cappy는 3억 6천만 개의 파라미터를 가진 경량 모델로, 범용 기반 모델보다 훨씬 작습니다.\n",
    "\n",
    "**참조 기반 심판**은 생성된 응답을 하나 이상의 참조 응답과 비교하여 평가합니다. 이 심판은 유사성 점수나 품질 점수(생성된 응답이 참조 응답과 비교하여 얼마나 좋은지를 나타내는)를 출력할 수 있습니다. 예를 들어, BLEURT(Sellam et al., 2020)는 (후보 응답, 참조 응답) 쌍을 입력으로 받아 후보 응답과 참조 응답 간의 유사성 점수를 출력합니다. Prometheus(Kim et al., 2023)는 (프롬프트, 생성된 응답, 참조 응답, 채점 기준)을 입력으로 받아, 참조 응답이 5점을 받는다고 가정했을 때 1에서 5 사이의 품질 점수를 출력합니다.\n",
    "\n",
    "**선호도 모델**은 (프롬프트, 응답 1, 응답 2)를 입력으로 받아 주어진 프롬프트에 대해 어떤 응답이 더 나은지(사용자가 선호하는 응답)를 출력합니다. 이는 아마도 전문화된 심판 중 가장 흥미로운 방향일 것입니다. 인간의 선호도를 예측할 수 있게 되면 많은 가능성이 열립니다. 2장에서 논의한 바와 같이, 선호도 데이터는 AI 모델을 인간 선호도와 정렬하는 데 필수적이며, 이를 얻는 것은 어렵고 비용이 많이 듭니다. 좋은 인간 선호도 예측기를 가지면 평가를 더 쉽게 하고 모델을 더 안전하게 사용할 수 있습니다. PandaLM(Wang et al., 2023)과 JudgeLM(Zhu et al., 2023)을 포함한 많은 선호도 모델 구축 이니셔티브가 있었습니다. Figure 3-10은 PandaLM의 작동 방식을 보여줍니다. 이는 어떤 응답이 더 나은지뿐만 아니라 그 이유도 설명합니다.\n",
    "\n",
    "<img src=\"images/fig_03_10.png\" width=\"800\">\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **비교 평가를 통한 모델 순위 매기기**  \n",
    "\n",
    "종종 모델을 평가할 때 점수 자체보다는 어떤 모델이 가장 적합한지를 알고 싶어 합니다. 이러한 경우 원하는 것은 모델의 순위를 매기는 것입니다. 모델의 순위를 매기는 방법으로 독립 평가 또는 비교 평가를 사용할 수 있습니다.\n",
    "\n",
    "**독립 평가**에서는 각 모델을 개별적으로 평가한 다음 점수를 기준으로 순위를 매깁니다. 예를 들어, 춤 실력을 기준으로 어떤 후보가 가장 우수한지 알고 싶다면, 각 후보를 개별적으로 평가하여 점수를 매기고, 가장 높은 점수를 받은 후보를 선택합니다.\n",
    "\n",
    "**비교 평가**에서는 모델을 서로 비교하여 순위를 매깁니다. 동일한 춤 경연 대회에서 각 후보에게 나란히 춤을 추게 하고, 심사위원에게 어떤 후보의 춤이 가장 마음에 드는지 물어 가장 많은 선호를 받은 후보를 선택하는 방식입니다.\n",
    "\n",
    "2장에서 논의된 바와 같이, 응답의 품질을 라벨링할 때 각 응답에 구체적인 점수를 부여하는 것보다 두 응답을 비교하여 더 나은 것을 선택하는 것이 훨씬 더 쉬웠다는 것이 발견되었습니다. 이 방법은 모델 평가에도 활용할 수 있습니다. 각 모델이 얼마나 좋은지를 독립적으로 평가하는 대신, 서로를 비교하여 평가합니다.\n",
    "\n",
    "AI에서는 비교 평가가 2021년에 Anthropic에 의해 처음 도입되어 다양한 모델을 순위화하는 데 사용되었습니다. 이는 LMYSYS의 **Chatbot Arena** 리더보드에서도 활용되며, 커뮤니티의 페어와이즈(pairwise) 모델 비교를 통해 계산된 점수를 바탕으로 모델 순위를 매깁니다.\n",
    "\n",
    "많은 모델 제공업체는 프로덕션 환경에서 모델을 평가하기 위해 비교 평가를 사용합니다. **그림 3-11**은 ChatGPT가 사용자에게 두 출력을 나란히 비교하도록 요청하는 예를 보여줍니다. 이러한 출력은 서로 다른 모델에서 생성되거나 동일한 모델에서 다양한 샘플링 변수로 생성될 수 있습니다.\n",
    "\n",
    "<img src=\"images/fig_03_11.png\" width=\"800\">\n",
    "\n",
    "각 요청에 대해 두 개 이상의 모델이 선택되어 응답을 생성합니다. 평가자는 인간일 수도 있고 AI일 수도 있으며, 승자를 선택합니다.\n",
    "\n",
    "---\n",
    "\n",
    ">**참고**  \n",
    ">\n",
    ">비교 평가를 수행할 때, 두 옵션이 동일하게 우수할 경우 임의로 승자가 선택되는 것을 방지하기 위해 동점을 허용하세요.\n",
    "\n",
    "---\n",
    "\n",
    "비교 평가는 A/B 테스트와 혼동되어서는 안 됩니다. A/B 테스트에서는 사용자가 한 번에 하나의 모델을 평가합니다. 반면 비교 평가에서는 사용자가 여러 모델을 동시에 평가합니다.\n",
    "\n",
    "각 비교는 **매치(match)**라고 불립니다. 이 과정은 일련의 비교 결과를 낳으며, 아래 **표 3-4**에 나타나 있습니다.\n",
    "\n",
    "**표 3-4. 페어와이즈 모델 비교 기록의 예시**\n",
    "\n",
    "| 매치 번호 | 모델 A   | 모델 B   | 승자    |\n",
    "|-----------|----------|----------|---------|\n",
    "| 1         | 모델 1   | 모델 2   | 모델 1  |\n",
    "| 2         | 모델 3   | 모델 10  | 모델 10 |\n",
    "| 3         | 모델 7   | 모델 4   | 모델 4  |\n",
    "| ...       | ...      | ...      | ...     |\n",
    "\n",
    "모델 A가 모델 B보다 선호될 확률은 **A의 승률(win rate)**로 표현됩니다. A와 B 간의 모든 매치를 살펴보고 A가 이긴 비율을 계산하여 이 승률을 구할 수 있습니다.\n",
    "\n",
    "만약 모델이 두 개뿐이라면 순위를 매기는 것은 간단합니다. 더 자주 이긴 모델이 더 높은 순위를 차지합니다. 그러나 모델의 수가 많아질수록 순위를 매기는 작업은 더 어려워집니다. 예를 들어, 다섯 개의 모델이 있고 모델 쌍 사이의 경험적 승률이 **표 3-4**에 나와 있는 것처럼 주어졌다고 가정해 봅시다. 이 데이터를 보고 이러한 다섯 개의 모델을 어떻게 순위화해야 하는지 명확하지 않을 수 있습니다.\n",
    "\n",
    "**표 3-5. 다섯 개 모델의 승률 예시 (A >> B는 A가 B보다 선호되는 경우를 나타냄)**\n",
    "\n",
    "| 모델 쌍 번호 | 모델 A   | 모델 B   | 매치 수 | A >> B (%) |\n",
    "|-------------|----------|----------|---------|------------|\n",
    "| 1           | 모델 1   | 모델 2   | 1000    | 90%        |\n",
    "| 2           | 모델 1   | 모델 3   | 1000    | 40%        |\n",
    "| 3           | 모델 1   | 모델 4   | 1000    | 15%        |\n",
    "| 4           | 모델 1   | 모델 5   | 1000    | 10%        |\n",
    "| 5           | 모델 2   | 모델 3   | 1000    | 60%        |\n",
    "| 6           | 모델 2   | 모델 4   | 1000    | 80%        |\n",
    "| 7           | 모델 2   | 모델 5   | 1000    | 80%        |\n",
    "| 8           | 모델 3   | 모델 4   | 1000    | 70%        |\n",
    "| 9           | 모델 3   | 모델 5   | 1000    | 10%        |\n",
    "| 10          | 모델 4   | 모델 5   | 1000    | 20%        |\n",
    "\n",
    "비교 신호를 바탕으로 **평가 알고리즘(rating algorithm)**이 사용되어 모델의 순위를 계산합니다. 일반적으로 이 알고리즘은 각 모델의 점수를 먼저 계산한 후, 점수를 기준으로 모델을 순위화합니다.\n",
    "\n",
    "비교 평가는 AI에서는 비교적 새로운 개념이지만, 스포츠 및 비디오 게임 등 다른 산업에서는 거의 한 세기 동안 사용되어 왔습니다. 이들 도메인을 위해 개발된 다양한 평가 알고리즘(예: Elo, Bradley-Terry, TrueSkill)을 AI 모델 평가에 적응할 수 있습니다. LMSYS의 **Chatbot Arena**는 처음에는 Elo 알고리즘을 사용하여 모델 순위를 계산했지만, 평가자와 프롬프트의 순서에 민감하다는 이유로 Bradley-Terry 알고리즘으로 전환했습니다.\n",
    "\n",
    "순위가 올바르다고 판단되려면, 어떤 모델 쌍에서든 상위 순위의 모델이 하위 순위 모델과의 매치에서 더 자주 이겨야 합니다. 만약 모델 A가 모델 B보다 높은 순위라면, 사용자들은 절반 이상의 시간 동안 모델 A를 모델 B보다 선호해야 합니다.\n",
    "\n",
    "이 관점에서 모델 순위는 예측 문제로 볼 수 있습니다. 우리는 과거 매치 결과 데이터를 바탕으로 순위를 계산하고, 이를 사용해 미래 매치 결과를 예측합니다. 다양한 평가 알고리즘은 서로 다른 순위를 생성할 수 있으며, 올바른 순위에 대한 기준은 없습니다. 순위의 품질은 미래 매치 결과를 얼마나 잘 예측할 수 있는지에 따라 결정됩니다. **Chat Arena**의 순위 시스템 분석 결과, 충분한 매치 데이터가 있을 경우 생성된 순위가 좋은 결과를 낳는 것으로 나타났습니다. 분석 내용은 부록(Appendix)에 수록되어 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **비교 평가의 도전 과제**\n",
    "\n",
    "각 모델을 독립적으로 평가할 때, 가장 중요한 과정은 적절한 지표를 수집하기 위한 벤치마크와 메트릭을 설계하는 것입니다. 모델 점수를 계산하여 순위를 매기는 것은 쉽습니다. 하지만 비교 평가에서는 신호 수집과 모델 순위 매기기 모두 도전 과제가 됩니다. 이 섹션에서는 세 가지 일반적인 문제를 다룹니다.\n",
    "\n",
    "**확장성 병목 현상**\n",
    "\n",
    "비교 평가는 데이터 집약적입니다. 모델 쌍의 수는 모델 수의 제곱에 비례하여 증가합니다. 2024년 1월 기준으로, LMSYS는 57개의 모델을 평가했으며, 이는 1,596개의 모델 쌍에 해당합니다. 이 57개의 모델은 244,000번의 비교를 통해 순위가 매겨졌으며, 모델 쌍당 평균 153번의 비교가 이루어졌습니다. 이는 우리가 기반 모델로 수행하려는 다양한 작업을 고려했을 때 비교적 적은 수치입니다.\n",
    "\n",
    "다행히도, 두 모델 간의 직접적인 비교가 항상 필요한 것은 아닙니다. 순위 알고리즘은 일반적으로 **전이성(transitivity)**을 가정합니다. 예를 들어, 모델 A가 B보다 높고, B가 C보다 높다면, A가 C보다 높다고 가정할 수 있습니다. 즉, 알고리즘이 A가 B보다 우수하고 B가 C보다 우수하다는 것을 확신한다면, A와 C를 비교하지 않아도 A가 더 우수하다는 결론을 내릴 수 있습니다.\n",
    "\n",
    "그러나 이러한 전이성 가정이 AI 모델에도 적용되는지는 불확실합니다. AI 평가를 분석한 여러 논문(Boubdir et al., Balduzzi et al., Munos et al.)은 전이성 가정을 한계로 언급합니다. 그들은 인간의 선호도가 반드시 전이적이지는 않다고 주장했습니다. 또한, 비전이성(non-transitivity)은 다른 평가자들이 다른 프롬프트에서 다양한 모델 쌍을 평가하기 때문에 발생할 수 있습니다.\n",
    "\n",
    "새로운 모델을 평가하는 문제도 있습니다. 독립적인 평가에서는 새로운 모델만 평가하면 되지만, 비교 평가에서는 새로운 모델을 기존 모델들과 비교해야 하므로 기존 모델의 순위가 변경될 수 있습니다.\n",
    "\n",
    "사적인 모델을 평가하는 것도 어렵습니다. 예를 들어, 귀하가 회사용으로 내부 데이터를 사용하여 모델을 구축했다고 가정해 봅시다. 이 모델이 공개 모델을 사용하는 것보다 더 유리한지 결정하려면, 이 모델을 공개 모델과 비교하고 싶을 것입니다. 모델에 대해 비교 평가를 사용하려면, 별도의 비교 신호를 수집하거나, 자체 순위표를 생성하거나, 개인적인 평가를 위해 공개 순위표 제공자에게 비용을 지불해야 할 가능성이 큽니다.\n",
    "\n",
    "확장성 병목 현상은 더 나은 매칭 알고리즘으로 완화할 수 있습니다. 현재까지는 모든 모델이 각 경기에서 무작위로 선택된다고 가정했으며, 따라서 모든 모델 쌍이 대략 동일한 수의 경기를 수행합니다. 하지만 모든 모델 쌍이 동등하게 비교되는 것은 아닙니다. 모델 쌍의 결과에 대한 확신이 생기면 해당 쌍을 더 이상 비교하지 않아도 됩니다. 효율적인 매칭 알고리즘은 전체 순위의 불확실성을 가장 크게 줄이는 일치 항목을 샘플링해야 합니다.\n",
    "\n",
    "**표준화 및 품질 관리 부족**\n",
    "\n",
    "비교 신호를 수집하는 한 가지 방법은 LMSYS Arena가 하는 것처럼 커뮤니티에 비교 작업을 크라우드소싱하는 것입니다. 누구나 그들의 웹사이트를 방문하여 프롬프트를 입력하고, 두 개의 익명 모델로부터 응답을 받아 더 나은 것을 선택할 수 있습니다. 투표가 완료된 후에야 모델 이름이 공개됩니다.\n",
    "\n",
    "이 접근법의 장점은 다양한 신호를 포착할 수 있고 조작하기 상대적으로 어렵다는 것입니다. 하지만 단점은 표준화와 품질 관리를 강제하기 어렵다는 것입니다.\n",
    "\n",
    "첫째, 인터넷에 접속할 수 있는 누구나 어떤 프롬프트로든 이러한 모델을 평가할 수 있으며, 더 나은 응답을 구성하는 기준이 없습니다. 이러한 자원봉사자들에게 응답의 사실 확인을 기대하는 것은 무리일 수 있으며, 따라서 그들은 사실과 다르지만 더 그럴듯하게 들리는 응답을 무의식적으로 선호할 수 있습니다.\n",
    "\n",
    "일부 사람들은 정중하고 절제된 응답을 선호하는 반면, 다른 사람들은 필터가 없는 응답을 선호할 수 있습니다. 이는 장단점이 있습니다. 장점은 야생 환경에서 인간의 선호를 포착하는 데 도움이 된다는 것입니다. 단점은 야생에서의 인간 선호가 모든 사용 사례에 적합하지 않을 수 있다는 점입니다. 일부는 심지어 악의적으로 유해한 응답을 선호하는 것으로 선택하여 순위를 오염시킬 수도 있습니다.\n",
    "\n",
    "둘째, 크라우드소싱 비교는 사용자가 실제 사용 환경 외부에서 모델을 평가하도록 요구합니다. 실제 환경과 연결되지 않은 상태에서, 테스트 프롬프트는 이러한 모델이 실제 세계에서 어떻게 사용되고 있는지를 반영하지 않을 수 있습니다. 사람들은 단순히 떠오르는 첫 번째 프롬프트를 사용할 수도 있으며, 정교한 프롬프트 기법을 사용할 가능성이 낮습니다.\n",
    "\n",
    "LMSYS Arena가 2023년에 공개한 33,000개의 프롬프트 중, 180개는 \"hello\"와 \"hi\"였으며 이는 데이터의 0.55%를 차지합니다. 여기에는 \"hello!\", \"hello.\", \"hola\", \"hey\" 등의 변형이 포함되지 않습니다. 많은 두뇌 게임 문제도 있습니다. 예를 들어, \"X에게 세 명의 자매가 있고, 각 자매에게 형제가 있습니다. X에게는 몇 명의 형제가 있습니까?\"라는 질문은 44번이나 제기되었습니다.\n",
    "\n",
    "단순한 프롬프트는 응답하기 쉬워 모델 성능을 차별화하기 어렵게 만듭니다. 너무 많은 단순 프롬프트는 순위를 오염시킬 수 있습니다.\n",
    "\n",
    "공개 순위표가 내부 데이터베이스에서 가져온 관련 문서로 맥락을 증강하는 것과 같은 정교한 맥락 구성을 지원하지 않으면, 해당 순위는 RAG 시스템에서 모델이 실제로 작동하는 방식을 반영하지 못할 것입니다. 관련 문서를 검색하는 능력은 가장 관련성 높은 문서를 검색하는 모델의 성능과 다릅니다.\n",
    "\n",
    "**표준화를 위한 잠재적 방법**\n",
    "\n",
    "표준화를 강화하는 한 가지 방법은 사용자를 미리 정해진 프롬프트 세트로 제한하는 것입니다. 그러나 이는 리더보드가 다양한 사용 사례를 캡처하는 능력에 영향을 미칠 수 있습니다. LMSYS는 사용자가 임의의 프롬프트를 사용할 수 있도록 허용하되, 내부 모델을 사용해 \"하드 프롬프트\"만 필터링하고, 이러한 하드 프롬프트를 기준으로 모델을 순위화하는 방식을 채택합니다.\n",
    "\n",
    "다른 방법은 신뢰할 수 있는 평가자만 사용하는 것입니다. 평가자를 훈련시켜 두 응답을 비교하는 기준을 학습하게 하거나, 실용적인 프롬프트와 정교한 프롬프트 기술을 사용하도록 훈련할 수 있습니다. Scale은 이 접근 방식을 사용해 **개인 비교 리더보드**를 운영합니다. 하지만 이 접근법의 단점은 비용이 많이 들고 비교 횟수가 크게 줄어들 수 있다는 점입니다.\n",
    "\n",
    "또 다른 선택지는 비교 평가를 제품 워크플로에 통합하고, 사용자가 작업 중에 모델을 평가하도록 하는 것입니다. 예를 들어 코드 생성 작업에서는 사용자가 두 개의 코드 스니펫을 선택하여 더 나은 것을 고르도록 할 수 있습니다. 이 섹션의 시작 부분에서 언급했듯이, ChatGPT와 Gemini가 이미 이러한 방식을 사용하고 있습니다.\n",
    "\n",
    "생산 환경에서 이 기술을 배포한 일부 사람들은 대부분의 사용자가 두 가지 옵션을 읽지 않고 무작위로 클릭한다고 지적했습니다. 이는 결과에 많은 노이즈를 도입할 수 있습니다. 하지만 정확히 투표하는 소수의 사용자로부터 나오는 신호가 때로는 어떤 모델이 더 나은지 판단하는 데 충분할 수 있습니다.\n",
    "\n",
    "일부 팀은 인간 평가자보다 AI 평가자를 선호한다고 말했습니다. AI는 훈련된 인간 전문가만큼 뛰어나지 않을 수 있지만, 무작위 인터넷 사용자보다 더 신뢰할 수 있을 가능성이 있습니다.\n",
    "\n",
    "---\n",
    "\n",
    "**비교 성능에서 절대 성능으로**\n",
    "\n",
    "많은 응용 사례에서는 반드시 최고의 모델이 필요하지 않습니다. 충분히 좋은 모델이면 됩니다. 비교 평가는 어떤 모델이 더 나은지를 알려주지만, 모델이 얼마나 좋은지 또는 우리의 사용 사례에 충분히 적합한지는 알려주지 않습니다. 예를 들어, 모델 B가 모델 A보다 낫다는 순위를 얻었다고 가정해 봅시다. 다음의 시나리오 중 어느 것이든 유효할 수 있습니다:\n",
    "\n",
    "1. 모델 B는 좋지만, 모델 A는 나쁘다.  \n",
    "2. 모델 A와 모델 B 모두 나쁘다.  \n",
    "3. 모델 A와 모델 B 모두 좋다.\n",
    "\n",
    "다른 형태의 평가가 필요하며, 이를 통해 어떤 시나리오가 사실인지 판단할 수 있습니다.\n",
    "\n",
    "\n",
    "모델 A를 모델 B로 대체하는 경우를 상상해봅시다. 모델 A는 고객 지원에 사용되며, 전체 티켓의 70%를 해결할 수 있습니다. 모델 B는 A를 상대로 51%의 승률을 가지고 있다고 가정합니다. 이 51%의 승률이 다운스트림 고객 지원 애플리케이션에서 어떻게 전환될지는 불분명합니다. 여러 사람들이 말하기를, 일부 애플리케이션에서는 승률 변화 1%가 성능에 큰 향상을 가져올 수 있지만, 다른 애플리케이션에서는 그 효과가 미미하다고 합니다.\n",
    "\n",
    "A를 B로 교체할 때, 인간의 선호도만으로는 충분하지 않습니다. 우리는 비용 같은 다른 요인도 중요하게 고려합니다. 예상되는 성능 향상을 알지 못하면 비용-편익 분석을 수행하기 어렵습니다. 만약 모델 B의 비용이 A의 두 배라면, 비교 평가는 B에서 얻을 성능 향상이 추가 비용을 감수할 가치가 있는지 판단하는 데 충분하지 않습니다.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **비교 평가의 미래**\n",
    "\n",
    "비교 평가의 많은 한계를 고려할 때, 이것이 미래에도 유효할지 궁금할 수 있습니다. 비교 평가에는 많은 이점이 있습니다.\n",
    "\n",
    "첫째, 2장 \"정렬(Alignment)\" 섹션에서 논의된 것처럼, 사람들은 각 출력에 구체적인 점수를 부여하는 것보다 두 출력을 비교하는 것이 더 쉽다는 것을 발견했습니다. 모델이 점점 더 강력해지고 인간의 성능을 능가하면서, 인간 평가자가 모델 응답에 구체적인 점수를 부여하는 것이 불가능해질 수 있습니다. 이 경우 비교 평가가 유일한 옵션으로 남을 수 있습니다.\n",
    "\n",
    "둘째, 비교 평가는 우리가 중요하게 생각하는 품질, 즉 인간 선호도를 포착하는 것을 목표로 합니다. 이는 AI의 지속적으로 확장되는 능력에 맞추기 위해 더 많은 벤치마크를 생성해야 하는 부담을 줄여줍니다. AI 모델의 성능이 완벽한 점수에 도달하면 벤치마크가 무용지물이 될 수 있지만, 비교 평가는 새롭고 더 강력한 모델이 도입되는 한 포화되지 않을 것입니다.\n",
    "\n",
    "비교 평가는 데이터를 조작하기 비교적 어렵습니다. 모델을 기준 데이터로 훈련시키는 것과 같은 속임수가 쉽지 않기 때문입니다. 이러한 이유로, 많은 사람들이 Chatbot Arena와 같은 공개 비교 리더보드의 결과를 신뢰합니다.\n",
    "\n",
    "저는 비교 평가가 달리 얻을 수 없는 모델에 대한 차별화된 신호를 제공할 수 있다고 믿습니다. 오프라인 평가의 경우, 내재적 평가나 평가 벤치마크에 훌륭한 추가 요소가 될 수 있습니다. 온라인 평가의 경우, A/B 테스트를 보완하는 데 사용될 수 있습니다.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **요약**\n",
    "\n",
    "AI 모델이 강력해질수록 치명적인 오류의 가능성이 커지며, 이는 평가의 중요성을 더욱 부각시킵니다. 동시에 개방형이고 강력한 모델을 평가하는 것은 어려운 과제입니다. 이러한 도전 과제는 많은 팀이 인간 평가에 의존하도록 만듭니다. 인간을 포함시키는 것은 일관성을 유지하는 데 유용하며, 많은 경우 인간 평가가 필수적입니다. 그러나 이 장에서는 자동 평가의 다양한 접근 방식에 중점을 두었습니다.\n",
    "\n",
    "이 장은 왜 기반 모델(foundation models)이 전통적인 기계 학습 모델보다 평가하기 어려운지에 대한 논의로 시작합니다. 새로운 평가 기술들이 개발되고 있지만, 평가에 대한 투자는 여전히 모델 및 애플리케이션 개발에 대한 투자보다 뒤처져 있습니다.\n",
    "\n",
    "많은 기반 모델이 언어 모델 구성 요소를 포함하고 있기 때문에, 복잡도(perplexity)와 교차 엔트로피(cross entropy)와 같은 언어 모델링 지표에 대해 집중적으로 다루었습니다. 많은 사람들이 이러한 지표를 혼란스럽게 여겼다고 말해서, 이 지표를 평가 및 특정 데이터 처리 기술에 어떻게 활용할 수 있는지에 대한 섹션을 포함시켰습니다.\n",
    "\n",
    "이 장은 이후 기능적 정확성, 유사성 점수, 그리고 심판으로서의 AI(AI-as-a-judge)를 포함한 개방형 응답을 평가하는 다양한 접근법으로 초점을 옮겼습니다. 처음 두 가지 평가 접근법은 정확한 반면, AI-as-a-judge 평가 방식은 주관적입니다.\n",
    "\n",
    "정확한 평가와 달리, 주관적 지표는 심판에 크게 의존합니다. 이들의 점수는 사용된 심판의 맥락에서 해석되어야 합니다. 동일한 품질을 측정하기 위한 점수도 AI 심판마다 다를 수 있어 비교가 어려울 수 있습니다. AI 심판은 모든 AI 애플리케이션과 마찬가지로 반복적으로 평가되어야 하며, 이는 시간이 지남에 따라 그 판단이 변할 수 있음을 의미합니다. 이러한 특성은 AI 심판을 애플리케이션의 변화 추적을 위한 신뢰할 수 없는 벤치마크로 만듭니다. 유망하지만, AI 심판은 정확한 평가 방법이나 인간 평가 또는 이 둘을 보완해야 합니다.\n",
    "\n",
    "모델을 평가할 때, 각 모델을 독립적으로 평가한 뒤 점수에 따라 순위를 매길 수 있습니다. 또는 비교 신호를 사용하여 순위를 매길 수도 있습니다: \"두 모델 중 어느 것이 더 나은가?\" 비교 평가는 스포츠, 특히 체스에서 일반적이며, AI 평가에서도 주목받고 있습니다. 비교 평가와 사후 훈련 정렬 프로세스는 선호 신호를 필요로 하며, 이를 수집하는 데는 비용이 많이 듭니다. 이는 사용자가 선호하는 응답을 예측하는 전문화된 AI 심판 개발을 촉진했습니다.\n",
    "\n",
    "언어 모델링 지표와 수작업으로 설계된 유사성 측정은 오래전부터 존재했지만, 비교 평가와 AI-as-a-judge는 기반 모델의 등장과 함께 널리 채택되었습니다. 많은 팀이 이러한 방법을 평가 파이프라인에 통합하는 방법을 찾고 있습니다. 신뢰할 수 있는 평가 파이프라인을 구축하는 방법을 찾는 것이 다음 장의 주제입니다.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "1. 나는 \"LLM\", \"GPT\", \"generative\", \"transformer\"라는 키워드를 사용해 최소 500개 이상의 스타를 가진 모든 저장소를 검색했습니다. 또한, 내 웹사이트 [huyenchip.com](https://huyenchip.com)을 통해 누락된 저장소를 크라우드소싱으로 수집했습니다. 그런 다음 저장소를 스타 수에 따라 정렬하여 상위 1000개의 저장소를 분석했습니다. 2024년 5월 12일 기준, 이 목록의 모든 저장소는 530개 이상의 스타를 가지고 있습니다.\n",
    "\n",
    "2. 일부 사람들은 임시 접근법을 \"분위기 체크(vibe checks)\"라고 부릅니다.\n",
    "\n",
    "3. 언어 모델링 성능과 다운스트림 성능 간에 강한 상관관계가 있지만, 이는 다운스트림 성능을 완전히 설명하지는 못합니다. 이는 활발한 연구 영역입니다.\n",
    "\n",
    "4. 1장에서 논의했듯이, 토큰(token)은 문자, 단어 또는 단어의 일부가 될 수 있습니다. 클로드 섀넌(Claude Shannon)이 1951년에 엔트로피를 소개했을 때, 그가 다룬 토큰은 문자였습니다. 여기 그의 [직접적인 설명](https://en.wikipedia.org/wiki/Entropy_in_information_theory)에서: \"엔트로피는 통계적 매개변수로, 텍스트의 각 문자가 생성하는 정보량을 특정 방식으로 측정합니다. 언어가 이진 숫자(0 또는 1)로 가장 효율적으로 번역될 경우, 엔트로피는 원래 언어의 각 문자를 나타내는 데 필요한 평균 이진 숫자 수를 나타냅니다.\"\n",
    "\n",
    "5. 많은 사람들이 자연로그(natural log)를 밑이 2인 로그보다 선호하는 이유 중 하나는 자연로그가 수학적으로 더 다루기 쉽기 때문입니다. 예를 들어, 자연로그 ln(x)의 도함수는 1/x입니다.\n",
    "\n",
    "6. SFT(지도 학습된 미세 조정)와 RLHF(인간 피드백을 활용한 강화 학습)의 의미가 확실하지 않다면, 2장을 다시 참조하세요.\n",
    "\n",
    "7. 양자화(Quantization)는 8장에서 논의될 것입니다.\n",
    "\n",
    "8. 문제는 복잡한 작업에는 측정 가능한 목표가 있는 경우가 많지만, AI는 복잡한 작업을 처음부터 끝까지 완전히 수행할 만큼 충분히 강력하지 않다는 점입니다. 따라서 AI는 솔루션의 일부를 수행하는 데 사용될 수 있습니다. 때로는 솔루션의 일부를 평가하는 것이 더 어렵습니다. 예를 들어, 체스 능력을 평가한다고 가정해 봅시다. 한 번의 움직임을 평가하는 것보다 최종 게임 결과(승/패/무)를 평가하는 것이 더 쉽습니다.\n",
    "\n",
    "9. \"cats\"와 \"cat\", 또는 \"will not\"과 \"won’t\"을 사용하길 원하는지에 따라 일부 처리를 수행해야 할 수도 있습니다. 이들은 두 개의 별도 토큰으로 간주될 수 있습니다.\n",
    "\n",
    "10. 10,000개의 요소를 가진 벡터 공간은 고차원적으로 보일 수 있지만, 원시 데이터의 차원성보다는 훨씬 낮습니다.\n",
    "\n",
    "11. 문서 임베딩과는 달리, 단어 임베딩을 생성하는 모델도 있습니다. 예를 들어 [word2vec](https://en.wikipedia.org/wiki/Word2vec)(Mikolov et al., 2013)과 [GloVe](https://en.wikipedia.org/wiki/GloVe)(Pennington et al., 2014)가 있습니다.\n",
    "\n",
    "12. \"AI 심판\"이라는 용어는 법정에서 심판으로 사용되는 AI의 경우와 혼동되지 않아야 합니다.\n",
    "\n",
    "13. 2017년, 나는 NeurIPS 워크숍 MEWR에서 발표했습니다. 이는 강력한 언어 모델을 활용하여 기계 번역을 자동으로 평가하는 방법입니다. 안타깝게도, 삶의 우선순위로 인해 이 연구 방향을 계속 추구하지 못했습니다.\n",
    "\n",
    "14. 일부 경우에는 평가가 예산의 대부분을 차지할 수 있으며, 응답 생성보다 더 많은 비용이 들기도 합니다.\n",
    "\n",
    "15. 이 기법은 때때로 **자기 비판(self-critique)** 또는 **자문(self-ask)**이라고 불립니다.\n",
    "\n",
    "16. BLEURT 점수 범위는 혼란스러울 수 있습니다. 대략적으로 **-2.5에서 1.0 사이**입니다. 이는 AI 심판에서 기준의 모호성을 강조합니다. 점수 범위는 임의적일 수 있습니다.\n",
    "\n",
    "17. 예를 들어 **리커트 척도(Likert scale)**를 사용하는 경우가 있습니다.\n",
    "\n",
    "18. 점수 스케일링은 꽤 복잡합니다. 각 점수는 400(엘로(Elo) 스케일에서 사용된 값)을 곱한 뒤, 초기 엘로 점수인 1000을 더합니다. 그런 다음 이 점수는 Llama-13b 모델이 800점을 갖도록 다시 스케일링됩니다.\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
