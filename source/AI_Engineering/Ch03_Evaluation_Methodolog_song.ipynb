{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **3장. 평가 방법론**\n",
    "\n",
    "AI의 사용이 늘어날수록 치명적인 실패의 가능성도 커집니다. 우리는 이미 짧은 기간 동안 생성형 모델에서 많은 실패를 목격했습니다. 한 남성이 챗봇에 의해 격려받은 후 자살했고, 변호사들이 AI가 생성한 허위 증거를 제출했으며, Air Canada는 AI 챗봇이 승객에게 잘못된 정보를 제공한 것에 대해 손해배상 판결을 받았습니다. AI 결과물을 품질 관리할 방법이 없다면, AI의 위험성이 많은 응용 프로그램에서 이점보다 클 수 있습니다.\n",
    "\n",
    "AI를 채택하려는 팀들이 서두르는 가운데, 많은 경우 AI 응용 프로그램을 현실화하는 데 가장 큰 걸림돌은 **평가**라는 점을 빠르게 깨닫습니다. 일부 응용 프로그램에서는 평가를 정의하는 데 개발 작업의 대부분이 소요되기도 합니다.\n",
    "\n",
    "평가의 중요성과 복잡성 때문에, 이 책에서는 평가에 대해 두 개의 장을 할애했습니다. 이 장에서는 개방형 모델을 평가하는 데 사용되는 다양한 평가 방법, 이러한 방법의 작동 방식, 그리고 그 한계에 대해 다룹니다. 다음 장에서는 이러한 방법을 응용 프로그램에 맞는 모델을 선택하고 평가 파이프라인을 구축하는 데 사용하는 방법에 초점을 맞춥니다.\n",
    "\n",
    "평가를 별도의 장으로 다루기는 하지만, 평가를 전체 시스템의 맥락에서 고려해야 하며, 고립된 방식으로는 안 됩니다. 평가는 위험을 완화하고 새로운 기회를 발견하는 것을 목표로 합니다. 위험을 완화하려면, 먼저 시스템이 실패할 가능성이 높은 지점을 파악하고 이를 기준으로 평가를 설계해야 합니다. 종종 시스템의 실패를 더 잘 이해하기 위해 시스템을 다시 설계해야 할 수도 있습니다. 시스템이 실패하는 지점을 명확히 이해하지 못한다면, 어떤 평가 지표나 도구도 시스템을 견고하게 만들 수 없습니다.\n",
    "\n",
    "평가 방법으로 들어가기 전에, 생성형 모델을 평가하는 데 따르는 어려움을 인정하는 것이 중요합니다. 평가가 어렵기 때문에, 많은 사람들은 입소문(예: 누군가가 모델 X가 좋다고 말함)이나 결과를 단순히 육안으로 확인하는 데 그치곤 합니다. 이는 더 큰 위험을 초래하고 애플리케이션 반복을 느리게 만듭니다. 대신, 평가 결과를 더 신뢰할 수 있도록 체계적인 평가에 투자해야 합니다.\n",
    "\n",
    "많은 생성형 모델에 언어 모델 구성 요소가 포함되어 있기 때문에, 이 장에서는 언어 모델을 평가하는 데 사용되는 교차 엔트로피(cross entropy)와 당혹도(perplexity) 같은 지표를 간략히 다룰 것입니다. 이러한 지표는 언어 모델의 학습 및 미세 조정을 안내하는 데 필수적이며, 많은 평가 방법에서 자주 사용됩니다.\n",
    "\n",
    "생성형 모델을 평가하는 것은 특히 어렵습니다. 그 이유는 이 모델들이 개방형이기 때문이며, 이를 해결하기 위한 최선의 방법도 다룰 것입니다. 많은 응용 프로그램에서 인간 평가자는 여전히 필수적입니다. 그러나 인간 주석 작업이 느리고 비용이 많이 들 수 있기 때문에, 목표는 프로세스를 자동화하는 것입니다. 이 책은 자동 평가에 초점을 맞추며, 여기에는 정확한 평가와 주관적인 평가가 포함됩니다.\n",
    "\n",
    "최근에는 AI가 AI 응답을 평가하는 심판 역할을 맡는 주관적 평가 방식이 주목받고 있습니다. 이는 어떤 모델과 프롬프트를 AI 심판이 사용하는지에 따라 점수가 달라지기 때문에 주관적입니다. 이 접근법은 업계에서 빠르게 확산되고 있지만, 중요한 작업에서 AI를 신뢰할 수 없다는 입장을 가진 사람들의 강한 반대도 초래합니다. 저는 이 논의를 더 깊이 탐구하게 되어 매우 흥미롭습니다. 독자 여러분도 그러시길 바랍니다.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **생성형 모델 평가의 어려움**\n",
    "\n",
    "기계 학습(ML) 모델 평가는 항상 어려운 과제였습니다. 생성형 모델의 도입으로 평가 작업은 더욱 어려워졌습니다. 생성형 모델 평가가 전통적인 ML 모델 평가보다 더 도전적인 이유는 여러 가지가 있습니다.\n",
    "\n",
    "**첫째, AI 모델이 지능적일수록 평가가 더 어려워집니다.** 대부분의 사람들은 초등학생 수준의 수학 풀이가 틀렸는지 알 수 있습니다. 하지만 박사 수준의 수학 풀이에 대해 동일한 판단을 할 수 있는 사람은 거의 없습니다. 책 요약이 엉터리라면 쉽게 알 수 있지만, 그 요약이 그럴듯할 경우에는 평가가 훨씬 더 어려워집니다. 요약의 품질을 검증하려면, 책 전체를 먼저 읽어야 할 수도 있습니다. 이는 평가가 복잡한 작업일수록 훨씬 더 많은 시간이 소모된다는 점으로 이어집니다. 더 이상 응답의 외형적 품질만으로 평가할 수 없으며, 사실 확인, 추론, 그리고 도메인 전문 지식까지 필요할 수 있습니다.\n",
    "\n",
    "**둘째, 생성형 모델의 개방형 특성은 기존의 평가 방식을 약화시킵니다.** 전통적인 ML에서는 대부분의 작업이 폐쇄형(closed-ended)입니다. 예를 들어, 분류 모델은 예상 범주 내에서만 결과를 출력합니다. 분류 모델의 평가는 출력 결과를 예상 출력과 비교하면 됩니다. 예를 들어, 예상 출력이 범주 X인데 모델의 출력이 범주 Y라면, 그 모델은 틀린 것입니다. 그러나 개방형 작업에서는 동일한 입력에 대해 너무 많은 정답이 가능하기 때문에, 비교 기준이 될 포괄적인 정답 목록을 만드는 것은 불가능합니다.\n",
    "\n",
    "**셋째, 대부분의 생성형 모델은 블랙박스로 취급됩니다.** 모델 제공자가 모델의 세부 정보를 공개하지 않거나, 애플리케이션 개발자가 이를 이해할 전문 지식을 갖추지 못한 경우가 많습니다. 모델 아키텍처, 학습 데이터, 학습 과정과 같은 세부 정보는 모델의 강점과 약점을 이해하는 데 큰 도움이 됩니다. 하지만 이러한 세부 정보가 없다면, 모델의 출력만을 관찰하여 평가할 수밖에 없습니다.\n",
    "\n",
    "**동시에, 공개적으로 이용 가능한 평가 벤치마크는 생성형 모델을 평가하기에 불충분하다는 것이 입증되었습니다.** 이상적으로, 평가 벤치마크는 모델의 모든 역량을 포괄할 수 있어야 합니다. AI가 발전함에 따라 벤치마크도 그에 맞게 진화해야 합니다. 모델이 완벽한 점수를 달성하면 해당 벤치마크는 모델에 대해 포화 상태가 됩니다. 생성형 모델의 경우, 벤치마크가 매우 빠르게 포화되고 있습니다. 예를 들어, **GLUE**(General Language Understanding Evaluation)는 2018년에 출시되었고, 단 1년 만에 포화 상태가 되어 2019년에 **SuperGLUE**가 도입되었습니다. 마찬가지로, **NaturalInstructions**(2021)는 **Super-NaturalInstructions**(2022)로 대체되었습니다. 또한, 초기 생성형 모델들이 많이 의존했던 강력한 벤치마크인 **MMLU**(2020)는 2024년에 **MMLU-Pro**로 대체되었습니다.\n",
    "\n",
    "**마지막으로, 범용 모델의 경우 평가의 범위가 확장되었습니다.** 작업에 특화된 모델의 경우, 평가는 모델이 학습한 작업에서의 성능을 측정하는 데 초점이 맞춰져 있습니다. 그러나 범용 모델의 경우, 평가는 모델이 알려진 작업에서의 성능을 평가하는 것뿐만 아니라, 모델이 수행할 수 있는 새로운 작업을 발견하는 데도 초점이 맞춰져 있습니다. 이러한 작업은 인간의 능력을 넘어서는 작업을 포함할 수도 있습니다. 따라서 평가는 AI의 잠재력과 한계를 탐구하는 추가적인 책임을 지게 됩니다.\n",
    "\n",
    "긍정적인 점은 평가의 새로운 도전 과제가 많은 새로운 방법과 벤치마크의 도입을 촉진했다는 것입니다. 그림 3-1에서 보여주듯, 2023년 상반기 동안 대규모 언어 모델(LLM) 평가에 관한 논문 수는 매달 기하급수적으로 증가했습니다. 월평균 2편의 논문에서 시작해, 한 달에 거의 35편의 논문이 발표되기에 이르렀습니다.\n",
    "\n",
    "<img src=\"./images/fig_03_01.png\" width=\"800\">\n",
    "\n",
    "제가 GitHub에서 별(star) 개수로 순위가 매겨진 AI 관련 상위 1,000개 저장소를 분석한 결과, 2024년 5월 기준으로 평가에 전념하는 저장소가 50개 이상 발견되었습니다. 그림 3-2에 나타난 바와 같이, 생성일별 평가 저장소의 수를 그래프로 표시하면 성장 곡선이 기하급수적으로 증가하는 것을 확인할 수 있습니다.\n",
    "\n",
    "<img src=\"./images/fig_03_02.png\" width=\"800\">\n",
    "\n",
    "하지만 부정적인 점은, 평가에 대한 관심이 증가했음에도 불구하고, AI 엔지니어링 파이프라인의 다른 부분에 비해 여전히 관심이 뒤처지고 있다는 것입니다. DeepMind의 Balduzzi 등은 논문에서 “알고리즘 개발에 비해 평가 개발은 체계적인 주목을 거의 받지 못했다”고 지적했습니다. 논문에 따르면, 실험 결과는 거의 전적으로 알고리즘 개선에 사용되고, 평가를 개선하는 데는 거의 사용되지 않는다고 합니다. 평가에 대한 투자 부족을 인식한 Anthropic은 정책 입안자들에게 새로운 평가 방법론 개발과 기존 평가의 견고성을 분석하기 위한 정부 자금과 보조금을 늘릴 것을 촉구했습니다.\n",
    "\n",
    "AI 분야에서 평가에 대한 투자가 다른 영역에 비해 얼마나 뒤처져 있는지를 더 잘 보여주기 위해, 그림 3-3에 나타난 것처럼 평가 도구의 수는 모델링, 학습, AI 오케스트레이션을 위한 도구의 수에 비해 매우 적다는 점을 들 수 있습니다.\n",
    "\n",
    "불충분한 투자는 불충분한 인프라로 이어지며, 이는 사람들이 체계적인 평가를 수행하기 어렵게 만듭니다. AI 응용 프로그램을 어떻게 평가하고 있는지 물어보면, 많은 사람들이 단순히 결과를 육안으로 확인한다고 말했습니다. 다수는 모델을 평가하는 데 사용하는 소수의 신뢰할 만한 프롬프트 세트를 가지고 있을 뿐입니다. 이러한 프롬프트를 구성하는 과정은 애플리케이션의 필요에 기반하지 않고, 대개 큐레이터의 개인 경험에 따라 즉흥적으로 이루어집니다. 프로젝트를 처음 시작할 때는 이런 즉흥적인 접근 방식으로도 넘어갈 수 있을지 모르지만, 애플리케이션을 반복적으로 개선하는 데는 충분하지 않을 것입니다. 이 책은 평가를 위한 체계적인 접근 방식에 중점을 두고 있습니다.\n",
    "\n",
    "<img src=\"./images/fig_03_03.png\" width=\"800\">\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **언어 모델링 지표 이해하기**\n",
    "\n",
    "생성형 모델은 언어 모델에서 발전한 것입니다. 많은 생성형 모델은 여전히 언어 모델을 주요 구성 요소로 포함하고 있습니다. 이러한 모델에서는 언어 모델 구성 요소의 성능이 생성형 모델이 다운스트림 응용 프로그램에서 보여주는 성능과 밀접하게 연관되는 경향이 있습니다(Liu et al., 2023). 따라서 언어 모델링 지표에 대한 대략적인 이해는 다운스트림 성능을 이해하는 데 매우 유용할 수 있습니다.  \n",
    "\n",
    "1장에서 논의한 것처럼, 언어 모델링은 몇십 년 동안 존재해왔으며, 1951년 Claude Shannon이 발표한 논문 \"Prediction and Entropy of Printed English\"로 인해 대중화되었습니다. 언어 모델 개발을 안내하는 데 사용되는 지표는 그 이후로 크게 변하지 않았습니다. 대부분의 자동회귀 언어 모델은 교차 엔트로피(cross entropy) 또는 그 변형인 당혹도(perplexity)를 사용해 학습됩니다. 논문이나 모델 보고서를 읽다 보면, 문자당 비트(bits-per-character, BPC) 및 바이트당 비트(bits-per-byte, BPB)라는 용어를 접할 수도 있습니다. 이 두 가지는 모두 교차 엔트로피의 변형입니다.  \n",
    "\n",
    "이 네 가지 지표(교차 엔트로피, 당혹도, BPC, BPB)는 서로 밀접하게 연관되어 있습니다. 하나의 값을 알면 필요한 정보만 있다면 나머지 세 가지를 계산할 수 있습니다. 저는 이를 언어 모델링 지표라고 부르지만, 이 지표는 텍스트가 아닌 토큰을 포함한 모든 토큰 시퀀스를 생성하는 모델에 사용할 수 있습니다.  \n",
    "\n",
    "언어 모델은 언어에 대한 통계적 정보를(예: 특정 문맥에서 토큰이 나타날 확률) 인코딩합니다. 예를 들어, \"I like drinking __\"라는 문맥에서, 다음 단어는 \"charcoal\"보다 \"tea\"일 가능성이 더 높습니다. 모델이 캡처할 수 있는 통계적 정보가 많을수록, 다음 토큰을 예측하는 능력이 향상됩니다.  \n",
    "\n",
    "기계 학습 용어로, 언어 모델은 학습 데이터의 분포를 학습합니다. 모델이 학습 데이터를 잘 학습할수록, 학습 데이터에서 다음에 올 항목을 더 잘 예측할 수 있습니다. 교차 엔트로피 값이 낮을수록 학습 품질이 더 뛰어납니다. 모든 기계 학습 모델과 마찬가지로, 학습 데이터에서뿐만 아니라 실제 데이터에서도 성능이 중요합니다. 일반적으로, 실제 데이터가 학습 데이터와 유사할수록 모델이 더 좋은 성능을 발휘할 수 있습니다.  \n",
    "\n",
    "책의 다른 부분과 비교했을 때, 이 섹션은 수학적인 내용이 많습니다. 내용이 어렵다면 수학적인 부분을 건너뛰고 지표 해석에 관한 논의에 집중해도 괜찮습니다. 언어 모델을 학습하거나 미세 조정하지 않더라도, 이러한 지표를 이해하는 것은 응용 프로그램에 사용할 모델을 평가하는 데 도움이 될 수 있습니다. 이 지표는 데이터 중복 제거와 같은 특정 평가와 데이터 정제 기술에도 가끔 사용될 수 있습니다.  \n",
    "\n",
    "--- "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 엔트로피(Entropy)\n",
    "\n",
    "엔트로피는 평균적으로 하나의 토큰이 얼마나 많은 정보를 포함하고 있는지를 측정합니다. 엔트로피가 높을수록 각 토큰이 더 많은 정보를 포함하며, 해당 토큰을 표현하기 위해 더 많은 비트가 필요합니다.  \n",
    "\n",
    "간단한 예를 들어보겠습니다. 그림 3-4에 나타난 것처럼, 정사각형 내의 위치를 설명하는 언어를 만든다고 가정해 보겠습니다. 언어에 두 개의 토큰만 있다면(그림 3-4의 (a) 참조), 각 토큰은 해당 위치가 위쪽인지 아래쪽인지를 나타낼 수 있습니다. 두 개의 토큰만 있다면 이를 표현하기 위해 1비트면 충분합니다. 따라서 이 언어의 엔트로피는 1입니다.\n",
    "\n",
    "<img src=\"./images/fig_03_04.png\" width=\"800\">\n",
    "\n",
    "만약 언어에 네 개의 토큰이 있다면(그림 3-4의 (b) 참조), 각 토큰은 보다 구체적인 위치(왼쪽 위, 오른쪽 위, 왼쪽 아래, 오른쪽 아래)를 나타낼 수 있습니다. 그러나 토큰이 네 개가 되었으므로 이를 표현하려면 2비트가 필요합니다. 이 언어의 엔트로피는 2입니다. 네 개의 토큰이 포함된 언어는 더 높은 엔트로피를 가지며, 각 토큰이 더 많은 정보를 포함하지만 이를 표현하기 위해 더 많은 비트가 필요합니다.\n",
    "\n",
    "직관적으로, 엔트로피는 언어에서 다음에 나올 내용을 예측하는 데 얼마나 어려운지를 측정합니다. 언어의 엔트로피가 낮을수록(즉, 언어에서 한 토큰이 전달하는 정보가 적을수록) 해당 언어는 더 예측하기 쉽습니다. 이전의 예에서, 두 개의 토큰만 있는 언어는 네 개의 토큰이 있는 언어보다 예측하기 쉽습니다(두 가지 경우만 예측하면 되는 것에 비해 네 가지 경우를 예측해야 하기 때문). 이는 마치 여러분이 제가 다음에 무슨 말을 할지 완벽히 예측할 수 있다면, 제가 다음에 무슨 말을 하는지가 새로운 정보를 전혀 제공하지 않는 것과 같습니다.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 교차 엔트로피(Cross Entropy)\n",
    "\n",
    "언어 모델을 학습 데이터셋으로 학습할 때, 목표는 모델이 학습 데이터의 분포를 학습하도록 하는 것입니다. 즉, 학습 데이터에서 무엇이 다음에 나올지 예측하는 모델을 만드는 것이 목표입니다. 언어 모델의 교차 엔트로피는 학습 데이터에서 다음에 나올 내용을 예측하는 것이 얼마나 어려운지를 측정합니다.  \n",
    "\n",
    "모델의 학습 데이터에 대한 교차 엔트로피는 두 가지 품질에 따라 달라집니다:\n",
    "1. 학습 데이터의 예측 가능성, 이는 학습 데이터의 엔트로피로 측정됩니다.\n",
    "2. 모델이 학습한 분포와 학습 데이터의 실제 분포 사이의 차이.\n",
    "\n",
    "엔트로피와 교차 엔트로피는 동일한 수학적 표기법 $H$를 공유합니다. 학습 데이터의 실제 분포를 $P$, 모델이 학습한 분포를 $Q$라고 할 때, 다음이 성립합니다:\n",
    "\n",
    "따라서 다음이 성립합니다:\n",
    "\n",
    "- 훈련 데이터의 엔트로피는 $ H(P) $입니다.\n",
    "\n",
    "- $ Q $가 $ P $에 대해 얼마나 다른지를 나타내는 발산(Divergence)은 Kullback-Leibler(KL) 발산을 사용하여 측정할 수 있으며, 이는 수학적으로 $ D_{KL}(P || Q) $로 표현됩니다.\n",
    "\n",
    "- 모델의 훈련 데이터에 대한 **교차 엔트로피(Cross Entropy)** 는 다음과 같습니다:\n",
    "  $\n",
    "  H(P, Q) = H(P) + D_{KL}(P || Q).\n",
    "  $\n",
    "\n",
    "교차 엔트로피는 대칭적이지 않습니다. $ P $에 대한 $ Q $의 교차 엔트로피 $ H(P, Q) $는 $ Q $에 대한 $ P $의 교차 엔트로피 $ H(Q, P) $와 다릅니다.\n",
    "\n",
    "언어 모델의 목표는 학습 데이터에 대한 교차 엔트로피를 최소화하는 것입니다. 만약 언어 모델이 학습 데이터를 완벽히 학습했다면, 모델의 교차 엔트로피는 학습 데이터의 엔트로피와 정확히 동일하게 될 것입니다. $Q$의 $P$에 대한 KL 발산은 0이 됩니다. 따라서 모델의 교차 엔트로피는 학습 데이터의 엔트로피에 대한 근사치로 간주될 수 있습니다.\n",
    "\n",
    "--- "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 문자당 비트 수와 바이트당 비트 수\n",
    "\n",
    "엔트로피와 교차 엔트로피의 단위는 비트(bits)입니다. 만약 언어 모델의 교차 엔트로피가 6비트라면, 이 언어 모델은 각 토큰을 표현하기 위해 6비트를 필요로 합니다.\n",
    "\n",
    "다른 모델들이 서로 다른 토크나이제이션 방식을 사용하기 때문에—예를 들어, 어떤 모델은 단어를 토큰으로 사용하고 다른 모델은 문자를 토큰으로 사용함—토큰당 비트 수는 모델 간에 비교할 수 없습니다. 대신, 일부 모델은 **문자당 비트 수(Bits-Per-Character, BPC)** 를 사용합니다. 만약 토큰당 비트 수가 6이고, 평균적으로 각 토큰이 두 개의 문자로 구성되어 있다면, BPC는 $6 / 2 = 3$입니다.\n",
    "\n",
    "BPC와 관련된 한 가지 복잡성은 서로 다른 문자 인코딩 방식에서 발생합니다. 예를 들어, ASCII에서는 각 문자가 7비트로 인코딩되지만, UTF-8에서는 하나의 문자가 8비트에서 32비트 사이의 값으로 인코딩될 수 있습니다. 보다 표준화된 측정치는 **바이트당 비트 수(Bits-Per-Byte, BPB)** 로, 언어 모델이 원래 학습 데이터의 1바이트를 나타내는 데 필요한 비트 수를 나타냅니다. 만약 BPC가 3이고, 각 문자가 7비트(또는 1바이트의 $7/8$)라면, BPB는 $3 / (7/8) = 3.43$이 됩니다.\n",
    "\n",
    "교차 엔트로피는 언어 모델이 텍스트를 압축하는 효율성을 나타냅니다. 만약 언어 모델의 BPB가 3.43이라면, 이는 해당 언어 모델이 각 원래 바이트(8비트)를 3.43비트로 나타낼 수 있음을 의미하며, 이 모델이 원래 학습 텍스트를 원래 크기의 절반 이하로 압축할 수 있음을 보여줍니다.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 당혹도(Perplexity)\n",
    "\n",
    "당혹도는 엔트로피와 교차 엔트로피의 지수(exponential)입니다. 당혹도는 종종 PPL로 줄여서 사용됩니다. 실제 분포 $P$를 가진 데이터셋에서 당혹도는 다음과 같이 정의됩니다:\n",
    "\n",
    "$\n",
    "PPL(P) = 2^{H(P)}\n",
    "$\n",
    "\n",
    "학습된 분포 $Q$를 사용하는 언어 모델의 데이터셋에서의 당혹도는 다음과 같이 정의됩니다:\n",
    "\n",
    "$\n",
    "PPL(P, Q) = 2^{H(P, Q)}\n",
    "$\n",
    "\n",
    "교차 엔트로피가 모델이 다음 토큰을 예측하는 데 얼마나 어려운지를 측정한다면, 당혹도는 다음 토큰을 예측할 때 모델이 가지고 있는 불확실성의 정도를 측정합니다. 불확실성이 높을수록 다음 토큰에 대한 더 많은 선택지가 있음을 의미합니다.\n",
    "\n",
    "예를 들어, 그림 3-4(b)에서처럼 네 개의 위치 토큰을 완벽하게 인코딩하도록 학습된 언어 모델을 고려해 보겠습니다. 이 언어 모델의 교차 엔트로피는 2비트입니다. 이 언어 모델이 정사각형 내에서 위치를 예측하려고 한다면, 2$^2$ = 4개의 가능한 선택지 중에서 선택해야 합니다. 따라서 이 언어 모델의 당혹도는 4입니다.\n",
    "\n",
    "지금까지 엔트로피와 교차 엔트로피의 단위로 비트(bit)를 사용했습니다. 각 비트는 2개의 고유 값을 나타낼 수 있으므로, 당혹도 공식에서 밑(base)은 2가 됩니다.\n",
    "\n",
    "TensorFlow와 PyTorch와 같은 주요 기계 학습 프레임워크는 엔트로피와 교차 엔트로피의 단위로 **nat**(자연 로그, natural log)를 사용합니다. nat는 자연 로그의 밑인 $e$를 사용합니다. 만약 nat를 단위로 사용한다면, 당혹도는 다음과 같이 정의됩니다:\n",
    "\n",
    "$\n",
    "PPL(P, Q) = e^{H(P, Q)}\n",
    "$\n",
    "\n",
    "비트(bit)와 nat 간의 혼란으로 인해, 많은 사람들이 언어 모델의 성능을 보고할 때 교차 엔트로피 대신 당혹도를 사용하는 경우가 많습니다.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 당혹도의 해석과 사용 사례\n",
    "\n",
    "논의된 바와 같이, 교차 엔트로피, 당혹도, BPC, BPB는 언어 모델의 예측 정확도를 측정하는 변형된 지표입니다. 모델이 텍스트를 더 정확히 예측할수록 이러한 지표 값은 낮아집니다. 이 책에서는 기본 언어 모델링 지표로 당혹도를 사용할 것입니다. 데이터셋에서 모델이 다음에 무엇이 나올지 예측하는 데 불확실성이 클수록 당혹도 값은 더 높아집니다.\n",
    "\n",
    "당혹도에서 적절한 값이 무엇인지 여부는 데이터 자체와 당혹도가 어떻게 계산되는지(예: 모델이 접근할 수 있는 이전 토큰 수 등)에 따라 달라집니다. 다음은 일반적인 규칙들입니다:\n",
    "\n",
    "**더 구조화된 데이터는 더 낮은 당혹도를 생성함**\n",
    "더 구조화된 데이터는 더 예측 가능성이 높습니다. 예를 들어, HTML 코드는 일상 텍스트보다 더 예측 가능성이 높습니다. HTML 태그 `<head>`를 보면, 근처에 닫는 태그 `</head>`가 있어야 한다고 예측할 수 있습니다. 따라서 HTML 코드에서 모델의 예상 당혹도는 일상 텍스트에서 모델의 예상 당혹도보다 낮아야 합니다.\n",
    "\n",
    "**어휘가 클수록 당혹도가 높아짐**\n",
    "직관적으로, 가능한 토큰 수가 많을수록 모델이 다음 토큰을 예측하는 데 더 어려움을 겪습니다. 예를 들어, 어린이 책에서의 모델 당혹도는 동일한 모델이 **전쟁과 평화**에서 보이는 당혹도보다 낮을 가능성이 큽니다. 동일한 데이터셋에서, 영어 기준으로 문자를 기준으로 한 당혹도(다음 문자를 예측하는 경우)는 단어를 기준으로 한 당혹도(다음 단어를 예측하는 경우)보다 낮습니다. 이는 가능한 문자 수가 가능한 단어 수보다 적기 때문입니다.\n",
    "\n",
    "**문맥 길이가 길수록 당혹도가 낮아짐**\n",
    "모델이 더 긴 문맥을 활용할 수 있다면, 다음 토큰을 예측할 때 불확실성이 줄어듭니다. 1951년에 Claude Shannon은 최대 10개의 이전 토큰을 사용하여 다음 토큰을 예측하는 방식으로 모델의 교차 엔트로피를 평가했습니다. 현재 작성 시점에서, 모델의 당혹도는 일반적으로 500개에서 10,000개의 이전 토큰을 기반으로 계산할 수 있으며, 모델의 최대 문맥 길이에 따라 그 이상으로 증가할 수도 있습니다.\n",
    "\n",
    "참고로, 당혹도 값이 3 또는 그 이하로 나타나는 경우도 드뭅니다. 가상의 언어에서 모든 토큰이 동등한 발생 확률을 가진다고 가정했을 때, 당혹도 값이 3이라면 이 모델은 다음 토큰을 올바르게 예측할 확률이 1/3이라는 뜻입니다. 모델의 어휘가 10,000에서 100,000에 이르는 것을 고려하면, 이는 매우 놀라운 결과입니다.\n",
    "\n",
    "당혹도는 언어 모델 학습을 안내하는 것 외에도 AI 엔지니어링 워크플로우의 여러 부분에서 유용한 지표로 사용됩니다. 우선, 당혹도는 모델의 성능을 평가하는 좋은 기준이 됩니다. 모델이 다음 토큰을 예측하는 데 약하다면, 다운스트림 작업에서의 성능도 낮을 가능성이 높습니다. OpenAI의 GPT-2 보고서에 따르면, 더 크고 강력한 모델은 다양한 데이터셋에서 더 낮은 당혹도를 일관되게 보여줍니다. 불행히도, 회사들이 점점 더 비밀스럽게 모델을 운영하면서, 많은 회사들이 모델의 당혹도를 공개하지 않는 경향이 있습니다.\n",
    "\n",
    "**표 3-1. 더 큰 GPT-2 모델은 다양한 데이터셋에서 일관되게 낮은 당혹도를 제공합니다. 출처: OpenAI, 2018.**\n",
    "\n",
    "| 모델 크기        | LAMBADA (PPL) | LAMBADA (정확도) | CBT-CN (정확도) | CBT-NE (정확도) | WikiText2 (PPL) | PTB (PPL) | enwiki8 (BPB) | text8 (BPB) | WikiText103 (PPL) | IBW (PPL) |\n",
    "|------------------|---------------|------------------|-----------------|-----------------|----------------|-----------|--------------|-------------|-------------------|-----------|\n",
    "| SOTA             | 99.8          | 59.23            | 85.7            | 82.3            | 39.14         | 46.54     | 0.99         | 1.08        | 18.3              | 21.8      |\n",
    "| 117M             | 35.13         | 45.99            | 87.65           | 83.4            | 29.41         | 65.85     | 1.16         | 1.17        | 37.50             | 75.20     |\n",
    "| 345M             | 15.60         | 55.48            | 92.35           | 87.1            | 22.76         | 47.33     | 1.01         | 1.06        | 26.37             | 55.72     |\n",
    "| 762M             | 10.87         | 60.12            | 93.45           | 88.0            | 19.93         | 40.31     | 0.97         | 1.02        | 22.05             | 44.575    |\n",
    "| 1542M            | 8.63          | 63.24            | 93.30           | 89.05           | 18.34         | 35.76     | 0.93         | 0.98        | 17.48             | 42.16     |\n",
    "\n",
    "**설명**:\n",
    "- **LAMBADA (PPL)**: LAMBADA 데이터셋에서의 당혹도(PPL, Perplexity).\n",
    "- **LAMBADA (정확도)**: LAMBADA 데이터셋에서의 정확도.\n",
    "- **CBT-CN (정확도)**, **CBT-NE (정확도)**: CBT 데이터셋(주어진 문맥에서 이름(Name Entity)과 공통 명사(Common Noun)를 예측)의 정확도.\n",
    "- **WikiText2 (PPL)**, **WikiText103 (PPL)**: WikiText 데이터셋에서의 당혹도.\n",
    "- **PTB (PPL)**: Penn Treebank(Penn Treebank Corpus) 데이터셋에서의 당혹도.\n",
    "- **enwiki8 (BPB)**, **text8 (BPB)**: enwiki8 및 text8 데이터셋에서 바이트당 비트(BPB).\n",
    "- **IBW (PPL)**: IBW 데이터셋에서의 당혹도.\n",
    "\n",
    ">경고\n",
    ">\n",
    ">당혹도는 SFT(지도형 미세 조정)와 RLHF(강화 학습 기반 인간 피드백) 같은 기법을 사용하여 후속 학습(post-training)을 수행한 모델을 평가하기 위한 적합한 지표가 아닐 수 있습니다. 후속 학습은 모델이 작업을 수행하는 방법을 학습하는 데 중점을 두는데, 이 과정에서 모델이 다음 토큰을 예측하는 능력은 떨어질 수 있습니다. 언어 모델의 당혹도는 보통 후속 학습 이후 증가합니다. 일부 사람들은 후속 학습이 엔트로피를 \"축소\"시킨다고 말합니다. 마찬가지로, 양자화(모델의 수치적 정밀도를 줄이고 메모리 사용량을 감소시키는 기술)는 모델의 당혹도를 예기치 않은 방식으로 변경시킬 수 있습니다.\n",
    "\n",
    "언어 모델의 특정 텍스트에 대한 당혹도는 해당 모델이 이 텍스트를 예측하기 얼마나 어려운지를 측정합니다. 주어진 모델에서, 당혹도는 모델이 학습 중에 보았고 암기한 텍스트에 대해 가장 낮은 값을 가집니다. 따라서 당혹도는 텍스트가 모델의 학습 데이터에 포함되었는지 감지하는 데 사용할 수 있습니다. 만약 벤치마크 데이터의 당혹도가 낮다면, 해당 벤치마크가 모델의 학습 데이터에 포함되었을 가능성이 높아 이 벤치마크에서의 모델 성능은 신뢰할 수 없게 됩니다. 이는 학습 데이터 중복 제거에도 사용할 수 있습니다. 예를 들어, 새로운 데이터의 당혹도가 높을 때만 기존 학습 데이터셋에 새로운 데이터를 추가하는 방식입니다.\n",
    "\n",
    "당혹도는 비정상적인 텍스트, 예를 들어 \"내 강아지가 여가 시간에 양자 물리학을 가르친다\"와 같은 비정상적인 아이디어를 표현하거나, \"home cat go eye\" 같은 무의미한 텍스트에서 가장 높은 값을 가집니다. 따라서 당혹도는 이상 텍스트를 감지하는 데 사용할 수 있습니다.\n",
    "\n",
    "당혹도와 관련된 지표는 언어 모델의 기본 성능을 이해하는 데 도움을 주며, 이는 다운스트림 작업에서 모델 성능을 이해하는 데 중요한 지표로 작용합니다. 이 장의 나머지 부분에서는 모델의 다운스트림 작업 성능을 직접적으로 측정하는 방법을 논의합니다.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**언어 모델을 사용하여 텍스트의 당혹도를 계산하는 방법**\n",
    "\n",
    "특정 텍스트에 대한 모델의 당혹도는 모델이 해당 텍스트를 예측하는 데 얼마나 어려움을 겪는지를 측정합니다. 언어 모델 $X$와 토큰 시퀀스 $[x_1, x_2, \\dots, x_n]$이 주어졌을 때, 이 시퀀스에 대한 $X$의 당혹도는 다음과 같이 정의됩니다:\n",
    "\n",
    "$\n",
    "P(x_1, x_2, \\dots, x_n)^{-\\frac{1}{n}} = \\left(\\frac{1}{P(x_1, x_2, \\dots, x_n)}\\right)^{\\frac{1}{n}} = \\left(\\prod_{i=1}^n \\frac{1}{P(x_i | x_1, \\dots, x_{i-1})}\\right)^{\\frac{1}{n}}\n",
    "$\n",
    "\n",
    "여기서 $P(x_i | x_1, \\dots, x_{i-1})$는 $X$가 이전 토큰 $x_1, \\dots, x_{i-1}$을 기반으로 $x_i$에 할당하는 확률을 나타냅니다.\n",
    "\n",
    "당혹도를 계산하려면, 언어 모델이 각 다음 토큰에 할당하는 확률(또는 로그 확률)에 접근할 수 있어야 합니다. 하지만, 상용 모델 중 일부는 그들의 로그 확률(logprobs)을 공개하지 않습니다. 이에 대해서는 2장에서 논의한 바 있습니다.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **정확 평가(Exact Evaluation)**  \n",
    "\n",
    "모델의 성능을 평가할 때, 정확 평가와 주관 평가를 구분하는 것이 중요합니다. 정확 평가는 모호함 없이 판단을 내립니다. 예를 들어, 다지선다형 질문의 답이 A이고, 사용자가 B를 선택했다면, 그 답은 틀린 것입니다. 여기에는 모호함이 없습니다. 반면, 에세이 평가(essay grading)는 주관적입니다. 에세이의 점수는 에세이를 채점하는 사람에 따라 달라질 수 있습니다. 같은 사람이라도, 시간 간격을 두고 같은 에세이를 평가하면 다른 점수를 줄 수 있습니다. 에세이 평가에서 명확한 채점 기준이 있다면 더 정확해질 수 있지만, 이 장에서 논의할 AI를 심판(judge)으로 사용하는 경우는 주관적입니다. 평가 결과는 심판 역할을 하는 모델과 프롬프트에 따라 달라질 수 있습니다.\n",
    "\n",
    "저는 정확한 점수를 생성하는 두 가지 평가 접근법, 즉 **기능적 올바름(functional correctness)**과 **유사성 측정(similarity measurement)**을 다룰 것입니다. 이 장은 개방형 응답(임의의 텍스트 생성)을 평가하는 데 초점을 맞추고 있으며, 폐쇄형 응답(예: 분류)에는 초점을 두지 않습니다. 이는 생성형 모델이 폐쇄형 작업에 사용되지 않는다는 뜻이 아닙니다. 사실, 많은 생성형 모델 시스템은 의도 분류(intent classification)나 점수 산정(scoring)과 같은 분류 구성 요소를 포함하고 있습니다. 그러나 폐쇄형 평가 방법은 이미 잘 알려져 있기 때문에 이 장에서는 개방형 평가에 중점을 둡니다.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 기능적 올바름(Functional Correctness)\n",
    "\n",
    "기능적 올바름 평가는 시스템이 의도된 기능을 수행했는지 여부를 기반으로 평가합니다. 예를 들어, 모델에 웹사이트를 생성하도록 요청했을 때, 생성된 웹사이트가 요구 사항을 충족합니까? 또는 모델에 특정 레스토랑의 예약을 요청했을 때, 모델이 성공적으로 예약을 수행합니까?\n",
    "\n",
    "기능적 올바름은 애플리케이션의 성능을 평가하는 데 궁극적인 기준입니다. 이는 애플리케이션이 의도한 작업을 수행하는지 여부를 측정하기 때문입니다. 그러나 기능적 올바름은 항상 측정이 간단하지 않으며, 자동화하기 어려운 경우도 있습니다.\n",
    "\n",
    "코드 생성은 기능적 올바름을 자동으로 측정할 수 있는 작업의 예입니다. 코드에서 기능적 올바름은 종종 **실행 정확도(execution accuracy)**로 측정됩니다. 예를 들어, 모델에 Python 함수 `gcd(num1, num2)`를 작성하여 두 숫자 `num1`과 `num2`의 최대공약수(gcd)를 찾도록 요청한다고 가정해 보겠습니다. 생성된 코드는 Python 인터프리터에 입력되어 코드가 유효한지, 그리고 특정 입력 쌍에 대한 올바른 결과를 출력하는지 확인할 수 있습니다. 예를 들어, 입력 값이 `(num1=15, num2=20)`인 경우, 함수 `gcd(15, 20)`가 5를 반환하지 않는다면, 해당 함수는 틀린 것입니다.\n",
    "\n",
    "AI가 코드를 작성하기 전에도, 소프트웨어 엔지니어링에서는 코드의 기능적 올바름을 자동으로 검증하는 것이 표준 관행이었습니다. 코드는 일반적으로 단위 테스트(unit test)를 통해 다양한 시나리오에서 실행되어 예상 출력이 생성되는지 확인됩니다. 기능적 올바름 평가는 LeetCode와 HackerRank와 같은 코딩 플랫폼이 제출된 솔루션을 검증하는 방식과 유사합니다.\n",
    "\n",
    "AI의 코드 생성 기능을 평가하기 위한 인기 있는 벤치마크로는 OpenAI의 **HumanEval**과 Google의 **MBPP**(Mostly Basic Python Problems Dataset)가 있습니다. 텍스트에서 SQL로 변환하는 벤치마크(예: **Spider**(Yu et al., 2018), **BIRD-SQL**, **WikiSQL**(Zhong et al., 2017))도 기능적 올바름을 활용합니다.\n",
    "\n",
    "벤치마크 문제는 일련의 테스트 케이스와 함께 제공됩니다. 각 테스트 케이스는 코드가 실행해야 할 시나리오와 해당 시나리오에 대한 예상 출력을 포함합니다. 아래는 HumanEval에서의 문제와 그 테스트 케이스의 예시입니다:\n",
    "\n",
    "**문제**\n",
    "\n",
    "```python\n",
    "from typing import List\n",
    "\n",
    "def has_close_elements(numbers: List[float], threshold: float) -> bool:\n",
    "    \"\"\"\n",
    "    주어진 숫자 리스트에서 두 숫자가 지정된 임계값보다 가까운지 확인합니다.\n",
    "    \"\"\"\n",
    "    >>> has_close_elements([1.0, 2.0, 3.0], 0.5)  # False\n",
    "    >>> has_close_elements([1.0, 2.8, 3.0, 4.0, 5.0, 2.0], 0.3)  # True\n",
    "```\n",
    "\n",
    "**테스트 케이스 (각 assert 문이 하나의 테스트 케이스를 나타냅니다)**\n",
    "\n",
    "```python\n",
    "def check(candidate):\n",
    "    assert candidate([1.0, 2.0, 3.9, 4.0, 5.0, 2.2], 0.3) == True\n",
    "    assert candidate([1.0, 2.0, 3.9, 4.0, 5.0, 2.2], 0.05) == False\n",
    "    assert candidate([1.0, 2.0, 5.9, 4.0, 5.0], 0.95) == True\n",
    "    assert candidate([1.0, 2.0, 5.9, 4.0, 5.0], 0.8) == False\n",
    "    assert candidate([1.0, 2.0, 3.0, 4.0, 5.0, 2.0], 0.1) == True\n",
    "    assert candidate([1.1, 2.2, 3.1, 4.1, 5.1], 1.0) == True\n",
    "    assert candidate([1.1, 2.2, 3.1, 4.1, 5.1], 0.5) == False\n",
    "```\n",
    "\n",
    "모델을 평가할 때, 각 문제에 대해 여러 개의 코드 샘플(총 $k$개)이 생성됩니다. 모델이 생성한 코드 중 $k$개의 샘플이 해당 문제의 모든 테스트 케이스를 통과하면, 해당 문제를 해결했다고 간주합니다. 최종 점수는 **pass@k**로 불리며, 이는 모든 문제 중 모델이 해결한 문제의 비율을 나타냅니다.\n",
    "\n",
    "예를 들어, 10개의 문제가 주어지고, 모델이 $k = 3$로 설정했을 때 5개의 문제를 해결했다면, 해당 모델의 **pass@3** 점수는 50%입니다. 모델이 생성하는 코드 샘플의 수가 많을수록, 각 문제를 해결할 확률이 높아지며, 이는 더 높은 최종 점수로 이어집니다. 따라서 일반적으로 **pass@1** 점수는 **pass@3** 점수보다 낮으며, **pass@3** 점수는 **pass@10** 점수보다 낮아야 합니다.\n",
    "\n",
    "기능적 올바름을 자동으로 평가할 수 있는 또 다른 작업 범주는 게임 봇입니다. 예를 들어, 테트리스를 플레이하는 봇을 만든다면, 그 봇이 얼마나 뛰어난지는 얻은 점수로 평가할 수 있습니다. 측정 가능한 목표를 가진 작업은 일반적으로 기능적 올바름을 사용해 평가할 수 있습니다. 예를 들어, AI에게 에너지 소비를 최적화하도록 작업 스케줄을 조정하라고 요청하면, AI의 성능은 절약된 에너지 양으로 측정할 수 있습니다.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 참조 데이터와의 유사성 측정\n",
    "\n",
    "관심 있는 작업이 기능적 올바름을 사용해 자동으로 평가될 수 없는 경우, 일반적인 접근법은 AI의 출력을 참조 데이터(reference data)와 비교 평가하는 것입니다. 예를 들어, 모델에게 프랑스어 문장을 영어로 번역하도록 요청한 경우, 생성된 영어 번역을 올바른 영어 번역과 비교하여 평가할 수 있습니다.\n",
    "\n",
    "참조 데이터의 각 예시는 (입력, 참조 응답) 형식을 따릅니다. 하나의 입력은 여러 참조 응답(예: 프랑스어 문장의 여러 가능한 영어 번역)을 가질 수 있습니다. 참조 응답은 **정답(ground truth)** 또는 **표준 응답(canonical response)**이라고도 합니다. 참조가 필요한 지표는 **참조 기반(reference-based)** 지표라고 하며, 참조가 필요 없는 지표는 **참조 비기반(reference-free)** 지표라고 합니다.\n",
    "\n",
    "이 평가 접근법은 참조 데이터가 얼마나 빠르게, 얼마나 많이 생성될 수 있는지에 따라 병목 현상이 발생합니다. 참조 데이터는 일반적으로 사람이 생성하지만, 점점 더 많은 경우 AI가 이를 생성하고 있습니다. 사람 생성 데이터를 참조로 사용하는 것은 인간의 성능을 기준(gold standard)으로 간주하며, AI의 성능을 인간의 성능과 비교하여 측정한다는 것을 의미합니다. 그러나 사람 생성 데이터는 생성 비용이 높고 시간이 많이 소요됩니다. 이러한 이유로 많은 사람들이 AI를 사용하여 참조 데이터를 생성하려 합니다. AI로 생성된 데이터는 여전히 인간 검토가 필요할 수 있지만, 처음부터 참조 데이터를 생성하는 데 필요한 노동량보다는 훨씬 적습니다.\n",
    "\n",
    "참조 응답과 더 유사한 생성 응답이 더 나은 것으로 간주됩니다. 개방형 텍스트 간의 유사성을 측정하는 네 가지 방법은 다음과 같습니다:\n",
    "\n",
    "1. 평가자에게 두 텍스트가 동일한지 판단하도록 요청하기\n",
    "2. **정확 일치(Exact match):** 생성된 응답이 참조 응답 중 하나와 정확히 일치하는지 확인\n",
    "3. **어휘적 유사성(Lexical similarity):** 생성된 응답이 참조 응답과 어휘적으로 얼마나 유사한지 측정\n",
    "4. **의미적 유사성(Semantic similarity):** 생성된 응답이 의미(semantics) 측면에서 참조 응답에 얼마나 가까운지 측정\n",
    "\n",
    "두 개의 응답은 인간 평가자 또는 AI 평가자에 의해 비교될 수 있습니다. AI 평가자는 점점 더 흔해지고 있으며, 이는 다음 섹션의 초점이 됩니다.\n",
    "\n",
    "이 섹션은 인간이 설계한 지표(정확 일치, 어휘적 유사성, 의미적 유사성)에 초점을 맞춥니다. 정확 일치 점수는 이진적(일치하거나 일치하지 않음)으로 매겨지는 반면, 다른 두 점수는 0에서 1 또는 -1에서 1 사이의 연속적인 척도에서 평가됩니다. AI를 심판으로 사용하는 접근법의 사용 편의성과 유연성에도 불구하고, 인간이 설계한 유사성 측정 방식은 그 정확성 때문에 여전히 업계에서 널리 사용되고 있습니다.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">참고 (NOTE)\n",
    ">\n",
    ">이 섹션에서는 생성된 출력물의 품질을 평가하기 위해 유사성 측정 방법을 사용하는 방법에 대해 논의합니다. 그러나 유사성 측정은 다음을 포함하되 이에 국한되지 않는 여러 사용 사례에도 활용할 수 있습니다:\n",
    ">\n",
    ">- **검색 및 검색(Retrieval and search)**  \n",
    ">  쿼리와 유사한 항목 찾기  \n",
    ">\n",
    ">- **랭킹(Ranking)**  \n",
    ">  쿼리와 얼마나 유사한지에 따라 항목 순위 매기기  \n",
    ">\n",
    ">- **클러스터링(Clustering)**  \n",
    ">  항목 간 유사성을 기준으로 클러스터 생성  \n",
    ">\n",
    ">- **이상 감지(Anomaly detection)**  \n",
    ">  나머지 항목과 가장 유사하지 않은 항목 감지  \n",
    ">\n",
    ">- **데이터 중복 제거(Data deduplication)**  \n",
    ">  다른 항목과 너무 유사한 항목 제거  \n",
    ">\n",
    ">이 섹션에서 논의된 기술은 책 전반에 걸쳐 다시 언급될 것입니다.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**정확 일치(Exact match)**\n",
    "\n",
    "생성된 응답이 참조 응답 중 하나와 정확히 일치하면 이를 **정확 일치(Exact Match)**로 간주합니다. 정확 일치는 간단한 수학 문제, 일반적인 지식 질문, 퀴즈 스타일 질문과 같이 짧고 정확한 응답이 예상되는 작업에 적합합니다. 짧고 정확한 응답을 가진 입력의 예시는 다음과 같습니다:\n",
    "\n",
    "- \"2 + 3은 무엇인가요?\"\n",
    "- \"노벨상을 수상한 첫 번째 여성은 누구인가요?\"\n",
    "- \"현재 계좌 잔액은 얼마인가요?\"\n",
    "- \"빈칸 채우기: Paris는 France와 같고 ___는 England와 같다.\"\n",
    "\n",
    "정확 일치에는 포맷 문제를 고려한 변형이 있을 수 있습니다. 하나의 변형은 참조 응답을 포함하는 모든 출력을 일치로 간주하는 것입니다. 예를 들어, 질문이 \"2 + 3은 무엇인가요?\"이고, 참조 응답이 \"5\"라면, \"정답은 5입니다\" 또는 \"2 + 3은 5입니다\"와 같은 모든 출력을 수용할 수 있습니다.\n",
    "\n",
    "그러나 이러한 변형은 잘못된 해결책을 받아들이게 되는 문제를 일으킬 수 있습니다. 예를 들어, 질문이 \"Anne Frank가 태어난 해는 언제인가요?\"일 때, 정답은 1929년입니다. 모델이 \"1929년 9월 12일\"을 출력하면 정답인 연도는 포함되어 있지만, 출력 자체는 사실적으로 틀릴 수 있습니다.\n",
    "\n",
    "단순한 작업을 넘어가면, 정확 일치는 거의 잘 작동하지 않습니다. 예를 들어, 원래 프랑스어 문장 \"Comment ça va?\"에는 \"How are you?\", \"How is everything?\", \"How are you doing?\"과 같은 여러 가능한 영어 번역이 있습니다. 참조 데이터에 이러한 세 번역만 포함되어 있고, 모델이 \"How is it going?\"을 생성한다면, 모델의 응답은 틀린 것으로 표시됩니다. 입력이 길어지고 원본 텍스트가 복잡할수록 가능한 번역의 수가 많아지고, 모든 가능한 응답 세트를 포괄적으로 생성하는 것은 불가능해집니다. 복잡한 작업의 경우, 어휘적 유사성(Lexical similarity) 및 의미적 유사성(Semantic similarity)이 더 적합합니다.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**어휘적 유사성(Lexical similarity)**\n",
    "\n",
    "어휘적 유사성은 두 텍스트가 얼마나 겹치는지를 측정합니다. 이를 위해 먼저 각 텍스트를 더 작은 토큰으로 나누어야 합니다.\n",
    "\n",
    "가장 간단한 형태로, 어휘적 유사성은 두 텍스트가 공통으로 가지는 토큰의 수를 세는 방식으로 측정할 수 있습니다. 예를 들어, 참조 응답 \"My cats scare the mice\"와 두 개의 생성된 응답을 고려해 보겠습니다.\n",
    "\n",
    "이미지를 확인했으니, 텍스트를 추출한 뒤 번역을 시작하겠습니다. 잠시만 기다려 주세요.\n",
    "\n",
    "- “My cats eat the mice”  \n",
    "- “Cats and mice fight all the time”  \n",
    "\n",
    "각 토큰이 단어라고 가정합니다. 개별 단어의 중복만을 계산할 경우, 응답 A는 참조 응답의 5개 단어 중 4개를 포함하며, 유사도 점수는 80%입니다. 반면 응답 B는 5개 중 3개만 포함하며, 유사도 점수는 60%입니다. 따라서 응답 A가 참조 응답에 더 유사하다고 간주됩니다.\n",
    "\n",
    "어휘적 유사성을 측정하는 한 가지 방법은 **근사 문자열 매칭(approximate string matching)** 으로, 일반적으로 **퍼지 매칭(fuzzy matching)** 이라고 합니다. 이는 한 텍스트를 다른 텍스트로 변환하기 위해 필요한 수정 횟수를 계산하며, 이를 **수정 거리(edit distance)** 라고 합니다. 일반적인 세 가지 수정 작업은 다음과 같습니다:\n",
    "\n",
    "1. 삭제: “brad” -> “bad”  \n",
    "2. 삽입: “bad” -> “bard”  \n",
    "3. 대체: “bad” -> “bed”  \n",
    "\n",
    "일부 퍼지 매칭 도구는 두 글자를 바꾸는 전환(예: “mats” -> “mast”)을 하나의 수정 작업으로 간주하기도 합니다. 그러나 일부 퍼지 매칭 도구는 전환을 삭제와 삽입 두 개의 작업으로 취급합니다.\n",
    "\n",
    "예를 들어, “bad”를 “bard”로 변환하려면 한 번의 수정이 필요하지만, “cash”로 변환하려면 세 번의 수정이 필요합니다. 따라서 “bad”는 “cash”보다 “bard”와 더 유사하다고 간주됩니다.\n",
    "\n",
    "어휘적 유사성을 측정하는 또 다른 방법은 **n-그램 유사성(n-gram similarity)** 입니다. 이는 단일 토큰 대신 토큰의 시퀀스, 즉 n-그램의 중복을 기반으로 측정됩니다. 1-그램(유니그램)은 하나의 토큰이며, 2-그램(바이그램)은 두 개의 토큰으로 이루어진 집합입니다. “My cats scare the mice”는 네 개의 바이그램으로 구성됩니다: “my cats”, “cats scare”, “scare the”, “the mice”. 참조 응답의 n-그램 중 생성된 응답에 포함된 비율을 측정합니다.\n",
    "\n",
    "어휘적 유사성을 위한 일반적인 평가지표에는 BLEU, ROUGE, METEOR++, TER, CIDEr가 있습니다. 이들은 중복 계산 방식에서 정확히 다릅니다. 대규모 사전학습 모델(foundation model) 이전에는 BLEU, ROUGE와 같은 지표가 특히 번역 작업에서 일반적으로 사용되었습니다. 대규모 사전학습 모델이 등장한 이후, 어휘적 유사성을 사용하는 벤치마크는 감소했습니다. 이러한 지표를 사용하는 벤치마크 예시로는 WMT, COCO Captions, GEMv2가 있습니다.\n",
    "\n",
    "이 방법의 단점 중 하나는 포괄적인 참조 응답 세트를 준비해야 한다는 점입니다. 참조 세트에 비슷한 응답이 없으면 좋은 응답도 낮은 유사도 점수를 받을 수 있습니다. 일부 벤치마크 예시에서, Adept는 Fuyu 모델의 출력이 잘못된 것이 아니라 참조 데이터에서 일부 정답이 누락되어 성능이 저조했다고 보고했습니다. 그림 3-5는 이미지 캡션 생성 작업에서 Fuyu가 올바른 캡션을 생성했음에도 낮은 점수를 받은 예시를 보여줍니다.\n",
    "\n",
    "<img src=\"./images/fig_03_05.png\" width=\"800\">\n",
    "\n",
    "뿐만 아니라 참조 데이터 자체가 잘못된 경우도 있습니다. 예를 들어, WMT 2023 Metrics Shared Task의 주최자들은 데이터에서 잘못된 참조 번역을 많이 발견했다고 보고했습니다. 낮은 품질의 참조 데이터는 기준 기반(reference-based) 지표가 인간 판단과의 상관관계에서 기준 비기반(reference-free) 지표에 뒤처지는 이유 중 하나로 지적되었습니다(Freitag et al., 2023).\n",
    "\n",
    "이 측정 방법의 또 다른 단점은 더 높은 어휘적 유사성 점수가 항상 더 나은 응답을 의미하지는 않는다는 점입니다. 예를 들어, 코드 생성 벤치마크인 HumanEval에서 OpenAI는 잘못된 솔루션과 올바른 솔루션의 BLEU 점수가 유사하다는 것을 발견했습니다. 이는 BLEU 점수를 최적화하는 것이 기능적 정확성을 최적화하는 것과 동일하지 않음을 나타냅니다(Chen et al., 2021).\n",
    "\n",
    "--- \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**의미적 유사성(Semantic similarity)**\n",
    "\n",
    "어휘적 유사성은 두 텍스트가 비슷하게 보이는지를 측정하지, 동일한 의미를 가지는지를 측정하지는 않습니다. 두 문장 “What’s up?”과 “How are you?”를 생각해 보세요. 어휘적으로는 사용된 단어와 철자가 거의 겹치지 않아 다릅니다. 그러나 의미적으로는 유사합니다. 반대로, 비슷하게 보이는 텍스트는 매우 다른 의미를 가질 수 있습니다. 예를 들어, “Let’s eat, grandma”와 “Let’s eat grandma”는 완전히 다른 뜻을 가집니다.\n",
    "\n",
    "**의미적 유사성**은 의미 측면에서의 유사성을 계산하는 것을 목표로 합니다. 이를 위해 먼저 텍스트를 **임베딩(embedding)** 이라고 하는 수치적 표현으로 변환해야 합니다. 예를 들어, “the cat sits on a mat”이라는 문장은 [0.11, 0.02, 0.54]와 같은 임베딩으로 표현될 수 있습니다. 따라서 의미적 유사성은 **임베딩 유사성(embedding similarity)** 이라고도 불립니다.\n",
    "\n",
    "“**임베딩 소개(Introduction to Embedding)**”에서는 임베딩이 어떻게 작동하는지를 설명합니다. 여기서는 텍스트를 임베딩으로 변환할 수 있는 방법이 있다고 가정하겠습니다. 두 임베딩 간 유사성은 코사인 유사성(cosine similarity)과 같은 지표를 사용해 계산할 수 있습니다. 완전히 동일한 두 임베딩은 유사도 점수가 1이고, 정반대의 임베딩은 유사도 점수가 -1입니다.\n",
    "\n",
    "텍스트 예제를 사용했지만, 의미적 유사성은 이미지와 오디오를 포함한 모든 데이터 모달리티의 임베딩에 대해 계산할 수 있습니다. 텍스트의 의미적 유사성은 가끔 **의미적 텍스트 유사성(semantic textual similarity)** 이라고도 불립니다.\n",
    "\n",
    ">경고(WARNING)\n",
    ">\n",
    ">제가 의미적 유사성을 정확한 평가 범주에 포함시켰지만, 이는 주관적으로 간주될 수 있습니다. 다른 임베딩 알고리즘은 서로 다른 임베딩을 생성할 수 있기 때문입니다. 그러나 두 임베딩이 주어지면, 그들 간의 유사도 점수는 정확하게 계산됩니다.\n",
    "\n",
    "수학적으로, A를 생성된 응답의 임베딩, B를 참조 응답의 임베딩이라고 합시다. A와 B 사이의 코사인 유사성은 다음과 같이 계산됩니다:\n",
    "\n",
    "$\n",
    "\\frac{A \\cdot B}{||A|| ||B||}\n",
    "$\n",
    "\n",
    "여기서:\n",
    "- $ A \\cdot B $: A와 B의 내적(dot product)\n",
    "- $ ||A|| $: A의 **유클리드 놈(Euclidean norm)**(또는 $ L^2 $ 놈)  \n",
    "  만약 $ A = [0.11, 0.02, 0.54] $라면,  \n",
    "  $\n",
    "  ||A|| = \\sqrt{0.11^2 + 0.02^2 + 0.54^2}\n",
    "  $\n",
    "\n",
    "**의미적 텍스트 유사성(Semantic textual similarity)** 을 위한 평가지표로는 **BERTScore**(임베딩이 BERT에 의해 생성됨)와 **MoverScore**(임베딩이 여러 알고리즘의 조합으로 생성됨)이 있습니다.\n",
    "\n",
    "의미적 텍스트 유사성은 어휘적 유사성만큼 포괄적인 참조 응답 세트를 필요로 하지 않습니다. 그러나 의미적 유사성의 신뢰성은 기본 임베딩 알고리즘의 품질에 따라 달라집니다. 같은 의미를 가진 두 텍스트라도 임베딩이 나쁘다면 낮은 의미적 유사도 점수를 받을 수 있습니다. 이 측정 방법의 또 다른 단점은 기본 임베딩 알고리즘이 상당한 계산 자원과 시간이 필요할 수 있다는 점입니다.\n",
    "\n",
    "AI를 평가자로 논의하기 전에, 임베딩에 대한 간단한 소개를 살펴보겠습니다. 임베딩의 개념은 의미적 유사성의 핵심에 있으며, 이 책에서 다루는 많은 주제의 근간을 이루고 있습니다. 여기에는 6장에서 벡터 검색, 8장에서 데이터 중복 제거가 포함됩니다.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 임베딩 소개 (Introduction to Embedding)\n",
    "\n",
    "컴퓨터는 숫자로 작업하기 때문에, 모델은 입력 데이터를 컴퓨터가 처리할 수 있는 수치적 표현으로 변환해야 합니다. **임베딩(Embedding)**은 원본 데이터의 의미를 포착하기 위한 수치적 표현입니다.\n",
    "\n",
    "임베딩은 벡터입니다. 예를 들어, 문장 \"the cat sits on a mat\"은 [0.11, 0.02, 0.54]와 같은 임베딩 벡터로 표현될 수 있습니다. 여기서는 작은 벡터를 예로 사용했습니다. 실제로 임베딩 벡터의 크기(임베딩 벡터의 요소 수)는 일반적으로 100에서 10,000 사이입니다.\n",
    "\n",
    "특히 임베딩을 생성하기 위해 훈련된 모델로는 오픈소스 모델인 **BERT**, **CLIP**(대조 언어-이미지 사전 학습), **Sentence Transformers**가 포함됩니다. 또한 API로 제공되는 독점 임베딩 모델도 있습니다. **표 3-2**는 몇 가지 인기 있는 모델의 임베딩 크기를 보여줍니다.\n",
    "\n",
    "**표 3-2. 일반적으로 사용되는 모델의 임베딩 크기**\n",
    "\n",
    "| **모델(Model)**         | **임베딩 크기(Embedding size)**               |\n",
    "|--------------------------|---------------------------------------------|\n",
    "| **Google’s BERT**        | BERT base: 768                              |\n",
    "|                          | BERT large: 1024                            |\n",
    "| **OpenAI’s CLIP**        | 이미지(Image): 512                          |\n",
    "|                          | 텍스트(Text): 512                           |\n",
    "| **OpenAI Embeddings API**| text-embedding-3-small: 1536                |\n",
    "|                          | text-embedding-3-large: 3072                |\n",
    "| **Cohere’s Embed v3**    | embed-english-v3.0: 1024                    |\n",
    "|                          | embed-english-light-3.0: 384                |\n",
    "\n",
    "모델은 일반적으로 입력을 먼저 벡터 표현으로 변환해야 하므로, GPT와 Llama를 포함한 많은 머신러닝 모델은 임베딩을 생성하는 단계를 포함합니다. **'Transformer 아키텍처'**는 트랜스포머 모델에서 임베딩 레이어를 시각화합니다. 이러한 모델의 중간 레이어에 접근할 수 있다면 이를 활용해 임베딩을 추출할 수 있습니다. 그러나 이러한 임베딩의 품질은 임베딩 전용 모델이 생성하는 임베딩만큼 좋지 않을 수 있습니다.\n",
    "\n",
    "임베딩 알고리즘의 목표는 원본 데이터의 본질을 포착하는 임베딩을 생성하는 것입니다. 이를 어떻게 검증할 수 있을까요? 임베딩 벡터 [0.11, 0.02, 0.54]는 원문 \"the cat sits on a mat\"과는 전혀 닮지 않았습니다.\n",
    "\n",
    "상위 수준에서, 임베딩 알고리즘은 유사한 텍스트가 코사인 유사도 또는 관련 지표로 측정했을 때 더 가까운 임베딩을 가지면 좋은 알고리즘으로 간주됩니다. 예를 들어, 문장 \"the cat sits on a mat\"의 임베딩은 \"the dog plays on the grass\"의 임베딩에 더 가까워야 하며, \"AI research is super fun\"의 임베딩보다는 멀어야 합니다.\n",
    "\n",
    "또한, 특정 작업에 대한 임베딩의 유용성에 기반하여 품질을 평가할 수도 있습니다. 임베딩은 분류(classification), 주제 모델링(topic modeling), 추천 시스템(recommender systems), RAG(Relevance-Augmented Generation) 등 다양한 작업에 사용됩니다. 임베딩 품질을 여러 작업에 걸쳐 측정하는 벤치마크의 예로 **MTEB(Massive Text Embedding Benchmark)**(Muennighoff et al., 2023)이 있습니다.\n",
    "\n",
    "텍스트를 예로 들었지만, 모든 데이터는 임베딩 표현을 가질 수 있습니다. 예를 들어, **Criteo**와 **Coveo**와 같은 전자상거래 솔루션은 제품에 대한 임베딩을 가집니다. **Pinterest**는 이미지, 그래프, 검색어, 심지어 사용자에 대한 임베딩도 가지고 있습니다.\n",
    "\n",
    "새로운 프론티어는 서로 다른 모달리티의 데이터를 위한 공동 임베딩(joint embedding)을 생성하는 것입니다. **CLIP**(Radford et al., 2021)은 텍스트와 이미지를 포함한 서로 다른 모달리티 데이터를 공동 임베딩 공간으로 매핑할 수 있는 최초의 주요 모델 중 하나였습니다. **ULIP**(Xue et al., 2022)는 텍스트, 이미지, 3D 포인트 클라우드의 통합 표현을 생성하는 것을 목표로 하고 있으며, **ImageBind**(Girdhar et al., 2023)는 텍스트, 이미지, 오디오를 포함한 여섯 가지 다른 모달리티에 걸쳐 공동 임베딩을 학습합니다.\n",
    "\n",
    "**그림 3-6**은 CLIP의 아키텍처를 시각화한 것입니다. CLIP은 (이미지, 텍스트) 쌍을 사용하여 학습됩니다. 이미지에 해당하는 텍스트는 이미지에 관련된 캡션 또는 코멘트가 될 수 있습니다. 각 (이미지, 텍스트) 쌍에 대해 CLIP은 텍스트를 텍스트 임베딩으로 변환하는 텍스트 인코더와 이미지를 이미지 임베딩으로 변환하는 이미지 인코더를 사용합니다. 그런 다음 이 두 임베딩을 공동 임베딩 공간으로 투영합니다. 학습 목표는 이미지의 임베딩이 해당 텍스트의 임베딩과 공동 공간에서 가깝게 되는 것입니다.\n",
    "\n",
    "<img src=\"./images/fig_03_06.png\" width=\"800\">\n",
    "\n",
    "서로 다른 모달리티의 데이터를 표현할 수 있는 공동 임베딩 공간은 **멀티모달 임베딩 공간(multimodal embedding space)** 입니다. 텍스트-이미지 공동 임베딩 공간에서는 낚시하는 남성의 이미지를 나타내는 임베딩이 텍스트 \"a fisherman(어부)\"의 임베딩에 더 가까워야 하며, 텍스트 \"fashion show(패션쇼)\"의 임베딩보다는 멀어야 합니다. 이 공동 임베딩 공간은 서로 다른 모달리티의 임베딩을 비교하고 결합할 수 있게 합니다. 예를 들어, 이를 통해 텍스트 기반 이미지 검색이 가능합니다. 특정 텍스트가 주어지면, 이 텍스트와 가장 가까운 이미지를 찾는 데 도움을 줍니다.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **AI를 평가자로**\n",
    "\n",
    "개방형 응답을 평가하는 것의 어려움으로 인해 많은 팀들이 인간 평가에 의존하게 되었습니다. AI가 많은 도전적인 작업을 자동화하는 데 성공적으로 사용되어 왔는데, AI가 평가도 자동화할 수 있을까요? AI를 사용하여 AI를 평가하는 접근 방식을 'AI 평가자' 또는 'LLM 평가자'라고 합니다. 다른 AI 모델을 평가하는 데 사용되는 AI 모델을 AI 평가자라고 합니다.\n",
    "\n",
    "AI를 사용하여 평가를 자동화하는 아이디어는 오랫동안 존재해 왔지만, AI 모델이 이를 수행할 수 있는 능력을 갖추게 된 2020년 GPT-3의 출시 즈음에야 실용화되었습니다. 이 글을 쓰는 시점에서, AI 평가자는 실제 운영 환경에서 AI 모델을 평가하는 가장 일반적인 방법 중 하나가 되었습니다. 2023년과 2024년에 제가 본 대부분의 AI 평가 스타트업 데모는 어떤 식으로든 AI 평가자를 활용했습니다. LangChain의 2023년 AI 현황 보고서에 따르면 그들의 플랫폼에서 이루어진 평가의 58%가 AI 평가자에 의해 수행되었습니다. AI 평가자는 또한 활발한 연구 분야이기도 합니다.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 왜 AI를 판사로 사용해야 하는가?\n",
    "\n",
    "AI 판사는 인간 평가자에 비해 빠르고, 사용하기 쉬우며, 비교적 저렴합니다. 또한 참조 데이터 없이도 작동할 수 있어 참조 데이터가 없는 생산 환경에서도 사용할 수 있습니다.\n",
    "\n",
    "AI 모델에게 출력물을 판단하도록 요청할 수 있으며, 그 기준은 정확성, 반복성, 유해성, 건전성, 환각 여부 등 다양합니다. 이는 사람이 의견을 묻는 것과 비슷합니다. 여러분은 “사람들의 의견을 항상 신뢰할 수 없는 것처럼 AI의 판단도 신뢰할 수 없는 것 아니야?”라고 생각할 수도 있습니다. 맞는 말입니다. 하지만 AI 모델은 대중의 의견을 집약한 것이기 때문에 대중을 대표하는 판단을 내릴 가능성이 있습니다. 적절한 프롬프트와 모델만 있다면 다양한 주제에 대해 합리적인 판단을 얻을 수 있습니다.\n",
    "\n",
    "연구에 따르면 특정 AI 판사는 인간 평가자와 강한 상관관계를 보입니다. 2023년 Zheng 등은 그들의 평가 벤치마크인 MT-Bench에서 GPT-4와 인간 평가자 간의 일치율이 85%에 달하며, 이는 인간 간의 일치율(81%)보다도 높다는 것을 발견했습니다. 또한 AlpacaEval 작성자들(Dubois 등, 2023)은 그들의 AI 판사가 LMSYS의 Chat Arena 리더보드(인간 평가로 측정됨)와 거의 완벽한 상관관계(0.98)를 보였다고 밝혔습니다.\n",
    "\n",
    "AI는 단순히 응답을 평가할 수 있을 뿐 아니라, 그 판단을 설명할 수도 있습니다. 이는 평가 결과를 검토하고자 할 때 특히 유용합니다. **Figure 3-7**은 GPT-4가 설명하는 예를 보여줍니다.\n",
    "\n",
    "<img src=\"./images/fig_03_07.png\" width=\"800\">\n",
    "\n",
    "AI를 판사로 사용하는 것은 다양한 애플리케이션에서 유용하며, 경우에 따라 유일한 자동 평가 옵션이 될 수 있습니다. AI 판단이 인간의 판단만큼 뛰어나지 않더라도, 애플리케이션 개발을 안내하고 프로젝트를 시작하는 데 충분한 신뢰를 제공할 수 있습니다.\n",
    "\n",
    "--- "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AI를 판사로 사용하는 방법\n",
    "\n",
    "AI를 활용해 판단을 내리는 방법은 여러 가지가 있습니다. 예를 들어, AI는 응답 자체의 품질을 평가하거나, 그 응답을 참조 데이터와 비교하거나, 다른 응답과 비교할 수 있습니다. 아래는 이러한 세 가지 접근 방식에 대한 간단한 예제 프롬프트입니다:\n",
    "\n",
    "1. **주어진 원래 질문에 대해 응답 자체의 품질 평가하기:**\n",
    "\n",
    "   ```\n",
    "   다음의 질문과 응답을 바탕으로, 해당 응답이 질문에 대해 얼마나 좋은지 평가하세요.\n",
    "   1점에서 5점까지 점수를 사용하세요.\n",
    "   - 1점은 매우 나쁨을 의미합니다.\n",
    "   - 5점은 매우 좋음을 의미합니다.\n",
    "   질문: [QUESTION]\n",
    "   응답: [ANSWER]\n",
    "   점수:\n",
    "   ```\n",
    "\n",
    "2. **생성된 응답을 참조 응답과 비교하여 생성된 응답이 참조 응답과 동일한지 평가하기:**\n",
    "\n",
    "   ```\n",
    "   다음의 질문, 참조 응답, 그리고 생성된 응답을 바탕으로,\n",
    "   이 생성된 응답이 참조 응답과 동일한지 평가하세요.\n",
    "   True 또는 False를 출력하세요.\n",
    "   질문: [QUESTION]\n",
    "   참조 응답: [REFERENCE ANSWER]\n",
    "   생성된 응답: [GENERATED ANSWER]\n",
    "   ```\n",
    "\n",
    "3. **두 개의 생성된 응답을 비교하여 어떤 것이 더 나은지 또는 사용자가 더 선호할 가능성이 높은 것을 평가하기:**\n",
    "\n",
    "   이는 사후 학습 정렬(Chapter 2에서 논의됨), 테스트 시 계산(Chapter 2에서 논의됨), 그리고 비교 평가를 사용한 모델 순위 지정(다음 섹션에서 논의됨)을 위한 선호 데이터를 생성하는 데 유용합니다.\n",
    "\n",
    "   ```\n",
    "   다음의 질문과 두 개의 응답을 바탕으로, 어떤 응답이 더 나은지 평가하세요. A 또는 B를 출력하세요.\n",
    "   질문: [QUESTION]\n",
    "   A: [FIRST ANSWER]\n",
    "   B: [SECOND ANSWER]\n",
    "   더 나은 응답은:\n",
    "   ```\n",
    "\n",
    "일반적인 AI 판사는 어떤 기준이든 응답을 평가하도록 요청할 수 있습니다. 예를 들어, 롤플레잉 챗봇을 구축 중이라면, 챗봇의 응답이 사용자가 원하는 역할에 일관성이 있는지 평가하고 싶을 수 있습니다. 예를 들어, “이 응답이 간달프가 말할 것 같은 소리인가요?”라고 묻는 식입니다. 또는 프로모션 제품 사진을 평가하는 애플리케이션을 구축 중이라면, “1에서 5까지 점수를 매긴다면, 이 이미지 속 제품의 신뢰도를 어떻게 평가하겠습니까?”라고 묻고 싶을 수 있습니다. **Table 3-3**은 몇 가지 AI 도구에서 제공하는 AI 판사 기준의 일반적인 예를 보여줍니다.\n",
    "\n",
    "**Table 3-3. 2024년 9월 기준 일부 AI 도구에서 제공하는 판사 기준의 예**\n",
    "\n",
    "| **AI 도구**              | **내장 기준**                                                                                   |\n",
    "|---------------------------|----------------------------------------------------------------------------------------------|\n",
    "| **Azure AI Studio**       | 근거, 관련성, 일관성, 유창성, 유사성                                                          |\n",
    "| **MLflow.metrics**        | 신뢰도, 관련성                                                                               |\n",
    "| **LangChain Criteria Evaluation** | 간결성, 관련성, 정확성, 일관성, 유해성, 악의성, 도움 여부, 논란성, 여성혐오, 민감성 부족, 범죄성 |\n",
    "| **Ragas**                 | 신뢰도, 응답 관련성                                                                          |\n",
    "\n",
    "AI를 판사 기준으로 사용하는 것이 표준화되어 있지 않다는 점을 기억하는 것이 중요합니다. Azure AI Studio의 관련성 점수는 MLflow의 관련성 점수와 크게 다를 수 있습니다. 이러한 점수는 판사의 기본 모델과 프롬프트에 따라 달라집니다.\n",
    "\n",
    "AI 판사를 프롬프트하는 방법은 다른 AI 애플리케이션을 프롬프트하는 방법과 유사합니다. 일반적으로, 판사를 위한 프롬프트는 다음 내용을 명확히 설명해야 합니다:\n",
    "\n",
    "1. **모델이 수행할 작업**  \n",
    "   예: 생성된 응답과 질문 간의 관련성을 평가하는 것.\n",
    "\n",
    "2. **모델이 평가 시 따라야 할 기준**  \n",
    "   예: “귀하의 주된 초점은 생성된 응답이 주어진 질문에 대해 충분한 정보를 포함하고 있는지 확인하는 것입니다.” 지침이 구체적일수록 더 좋습니다.\n",
    "\n",
    "3. **평가 시스템**  \n",
    "   평가 시스템은 다음 중 하나일 수 있습니다:  \n",
    "   \n",
    "   - **분류**  \n",
    "     예: 좋음/나쁨 또는 관련 있음/관련 없음/중립.  \n",
    "     \n",
    "   - **이산적인 숫자 값**  \n",
    "     예: 1에서 5까지. 이산적인 숫자 값은 각 클래스가 의미적 해석 대신 수치적 해석을 가지는 분류의 특별한 경우로 간주될 수 있습니다.  \n",
    "     \n",
    "   - **연속적인 숫자 값**  \n",
    "     예: 0과 1 사이. 이는 유사성 정도를 평가하고자 할 때 사용됩니다.  \n",
    "\n",
    ">**TIP**  \n",
    ">\n",
    ">언어 모델은 일반적으로 숫자보다 텍스트에 더 능숙합니다. AI 판사가 숫자 기반 평가 시스템보다 분류에서 더 잘 작동한다고 보고되었습니다.  \n",
    ">\n",
    ">숫자 기반 평가 시스템의 경우, 이산적 점수 시스템이 연속적 점수 시스템보다 더 잘 작동하는 것으로 보입니다. 경험적으로, 이산적 점수 범위가 넓을수록 모델의 성능은 저하되는 경향이 있습니다. 일반적으로 사용되는 이산적 점수 시스템은 1에서 5 사이입니다.\n",
    "\n",
    "프롬프트에 예제를 포함하면 더 나은 결과를 얻을 수 있음이 입증되었습니다. 1점에서 5점 사이의 점수 체계를 사용하는 경우, 각각의 점수(예: 1점, 2점, 3점, 4점, 5점)가 어떤 응답에 해당하는지에 대한 예제와, 가능하다면 왜 특정 응답이 해당 점수를 받는지에 대한 이유를 포함하세요. 프롬프트 작성에 대한 최선의 방법은 Chapter 5에서 논의됩니다.\n",
    "\n",
    "다음은 Azure AI Studio에서 사용된 *relevance* 기준에 대한 프롬프트 일부입니다. 이 프롬프트는 작업, 기준, 점수 체계, 낮은 점수를 받는 입력의 예제와, 해당 입력이 낮은 점수를 받는 이유에 대한 정당화를 설명합니다. 일부 프롬프트는 간결성을 위해 제거되었습니다.\n",
    "\n",
    ">\"당신의 임무는 생성된 응답과 질문 사이의 연관성을 기준으로, 생성된 응답이 제공된 정답에 기반하여 질문을 해결하는 데 충분한 정보를 포함하고 있는지 판단하여 1점에서 5점 사이로 점수를 매기고, 점수를 매긴 이유를 제공하는 것입니다.\n",
    ">\n",
    ">당신의 주요 초점은 생성된 응답이 주어진 질문에 대해 제공된 정답(ground truth answer)을 기준으로 충분한 정보를 포함하고 있는지 판단하는 데 있어야 합니다. …\n",
    ">\n",
    ">생성된 응답이 제공된 정답(ground truth answer)과 모순되는 경우, 1-2점의 낮은 점수를 받게 됩니다.\n",
    ">\n",
    ">예를 들어, 질문 '하늘은 파란가요?'에 대한 정답은 '네, 하늘은 파랗습니다.'이고, 생성된 응답이 '아니요, 하늘은 파랗지 않습니다.'인 경우를 보겠습니다.\n",
    ">\n",
    ">이 예에서 생성된 응답은 하늘이 파랗지 않다고 주장하여 제공된 정답(ground truth answer)과 모순됩니다.\n",
    ">\n",
    ">이 불일치는 1-2점의 낮은 점수를 초래하며, 낮은 점수는 생성된 응답과 제공된 정답 사이의 모순을 반영합니다.\"\n",
    "\n",
    "Figure 3-8은 주어진 질문에 대해 응답의 품질을 평가하는 AI 판정자의 예를 보여줍니다.\n",
    "\n",
    "<img src=\"./images/fig_03_08.png\" width=\"800\">\n",
    "\n",
    "**AI 판정자는 단순히 모델만이 아닙니다**—모델과 프롬프트를 모두 포함하는 시스템입니다. 모델, 프롬프트 또는 모델의 샘플링 매개변수를 변경하면 서로 다른 판정자가 생성됩니다.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **AI 판정자로서의 한계**\n",
    "\n",
    "AI 판정자의 많은 장점에도 불구하고, 많은 팀이 이 접근 방식을 채택하는 것을 주저합니다. AI를 평가하기 위해 AI를 사용하는 것은 동어반복적으로 보일 수 있습니다. AI의 확률적 특성은 이를 평가자로 사용하기에는 너무 신뢰할 수 없게 보이게 만듭니다. AI 판정자는 애플리케이션에 상당한 비용과 지연을 초래할 가능성이 있습니다. 이러한 한계를 고려할 때, 일부 팀은 특히 프로덕션 환경에서 다른 방식으로 시스템을 평가할 방법이 없을 때 AI 판정자를 대안적인 선택지로 간주합니다.\n",
    "\n",
    "---\n",
    "\n",
    "**불일치**\n",
    "\n",
    "신뢰할 수 있는 평가 방법이 되려면 결과가 일관적이어야 합니다. 그러나 AI 판정자는 모든 AI 애플리케이션과 마찬가지로 확률적입니다. 동일한 판정자가 동일한 입력에 대해 다른 방식으로 프롬프트를 받으면 다른 점수를 출력할 수 있습니다. 동일한 지침으로 프롬프트된 동일한 판정자라도 두 번 실행할 경우 다른 점수를 출력할 수 있습니다. 이러한 불일치는 평가 결과를 재현하거나 신뢰하기 어렵게 만듭니다.\n",
    "\n",
    "AI 판정자의 일관성을 높이는 것이 가능합니다. **Chapter 2**에서는 샘플링 변수를 사용하여 이를 달성하는 방법을 논의합니다. Zheng et al. (2023)의 연구에 따르면, 평가 예제를 프롬프트에 포함시키면 GPT-4의 일관성이 65%에서 77.5%로 증가할 수 있다고 밝혔습니다. 그러나 그들은 높은 일관성이 반드시 높은 정확성을 의미하지는 않을 수 있음을 인정했습니다—판정자가 일관되게 동일한 실수를 반복할 수도 있기 때문입니다. 게다가, 더 많은 예제를 포함시키면 프롬프트가 길어지며, 긴 프롬프트는 더 높은 추론 비용을 초래합니다. Zheng et al.의 실험에서, 프롬프트에 더 많은 예제를 포함시킨 결과 GPT-4의 사용 비용이 네 배로 증가했습니다.\n",
    "\n",
    "---\n",
    "\n",
    "**기준의 모호성**\n",
    "\n",
    "많은 인간이 설계한 지표와 달리, AI 판정 기준 지표는 표준화되어 있지 않아 이를 오해하거나 오용하기 쉽습니다. 이 글을 작성하는 시점에서, 오픈 소스 도구인 **MLflow**, **Ragas**, 그리고 **LlamaIndex**는 모두 생성된 출력물이 주어진 문맥에 얼마나 충실한지를 측정하기 위한 내장 기준 **faithfulness**를 포함하고 있지만, 이들의 지침과 점수 체계는 모두 다릅니다. **Table 3-4**에서 볼 수 있듯이, **MLflow**는 1에서 5까지의 점수 체계를 사용하며, **Ragas**는 0과 1을 사용하고, **LlamaIndex**의 프롬프트는 판정자가 YES 또는 NO를 출력하도록 요청합니다.\n",
    "\n",
    "**Table 3-4. 동일한 기준에 대해 각기 다른 기본 프롬프트를 가진 도구들**\n",
    "\n",
    "| **도구**      | **프롬프트** *(간결성을 위해 일부 생략됨)*                                                                                   | **점수 체계**      |\n",
    "|---------------|-----------------------------------------------------------------------------------------------------------------------------|--------------------|\n",
    "| **MLflow**    | Faithfulness는 제공된 출력물과 제공된 문맥으로만 평가됩니다. 점수를 매길 때 제공된 입력은 완전히 무시해 주세요. Faithfulness는 제공된 출력물이 제공된 문맥과 사실적으로 얼마나 일치하는지를 평가합니다.…<br><br>Faithfulness: 아래는 점수별 세부 사항입니다:<br>- **점수 1**: 출력물의 주장 중 어떤 것도 제공된 문맥에서 유추할 수 없는 경우<br>- **점수 2**: …                                                                                      | **1–5**           |\n",
    "| **Ragas**     | 주어진 문맥을 기반으로 일련의 진술의 Faithfulness를 판단하는 것이 당신의 임무입니다. 각 진술에 대해, 문맥에 근거해 진술이 검증 가능하면 **1**을 반환하고, 문맥에 근거해 검증할 수 없으면 **0**을 반환하세요.                                                                                         | **0과 1**         |\n",
    "| **LlamaIndex**| 주어진 정보가 문맥에 의해 지지되는지 판단해 주세요.<br>**YES** 또는 **NO**로 대답해야 합니다.<br><br>문맥의 일부라도 정보가 지지된다면 **YES**라고 대답하세요. 문맥의 대부분이 관련이 없더라도 적용됩니다. 아래에 몇 가지 예가 제공됩니다:<br><br>**정보**: 애플 파이는 일반적으로 이중 껍질입니다.<br>**문맥**: 애플 파이는 과일 파이입니다… 애플 파이는 일반적으로 이중 껍질이며, 위와 아래 모두 반죽으로 덮여 있습니다…<br>**대답**: YES | **YES와 NO**      |\n",
    "\n",
    "**이 세 도구가 출력하는 Faithfulness 점수는 비교할 수 없습니다.** 만약 주어진 (문맥, 답변) 쌍에서 **MLflow**가 Faithfulness 점수로 3을 주고, **Ragas**가 1을 출력하며, **LlamaIndex**가 **NO**를 출력한다면, 어떤 점수를 사용할지 결정하시겠습니까?\n",
    "\n",
    "애플리케이션은 시간이 지나면서 발전하지만, 평가 방식은 이상적으로 고정되어야 합니다. 이러한 방식으로 평가 지표는 애플리케이션의 변화를 모니터링하는 데 사용될 수 있습니다. 하지만 AI 판정자도 AI 애플리케이션이므로, 시간이 지나면서 변화할 수 있습니다.\n",
    "\n",
    "가령, 지난달 당신의 애플리케이션의 일관성(coherence) 점수가 90%였고, 이번 달에는 92%라고 가정해 봅시다. 이것이 애플리케이션의 일관성이 향상되었음을 의미할까요? 두 경우 모두 동일한 AI 판정자를 사용했는지 확신하지 못한다면 이 질문에 답하기 어렵습니다. 이번 달 판정자의 프롬프트가 지난달의 것과 다르다면 어떨까요? 아마 약간 더 성능이 좋은 프롬프트로 교체했거나, 동료가 지난달 프롬프트에 있는 오타를 수정했을 수도 있으며, 이번 달 판정자가 더 관대할 수도 있습니다.\n",
    "\n",
    "애플리케이션과 AI 판정자가 서로 다른 팀에 의해 관리된다면, 이 상황은 특히 혼란스러워질 수 있습니다. AI 판정자 팀이 애플리케이션 팀에 알리지 않고 판정자를 변경할 수도 있습니다. 그 결과, 애플리케이션 팀은 평가 결과의 변화를 애플리케이션의 변화로 잘못 해석할 수 있으며, 실제로는 판정자의 변화 때문일 수 있습니다.\n",
    "\n",
    ">**TIP**  \n",
    ">\n",
    ">**판정에 사용된 모델과 프롬프트를 확인할 수 없다면, 어떤 AI 판정자도 신뢰하지 마세요.**\n",
    "\n",
    "평가 방법이 표준화되는 데는 시간이 걸립니다. 이 분야가 발전하고 더 많은 안전 장치(guardrail)가 도입됨에 따라, 미래의 AI 판정자는 훨씬 더 표준화되고 신뢰할 수 있게 되기를 바랍니다.\n",
    "\n",
    "---\n",
    "\n",
    "**비용 증가와 지연**\n",
    "\n",
    "AI 판정자는 실험 단계와 프로덕션 단계 모두에서 애플리케이션을 평가하는 데 사용할 수 있습니다. 많은 팀은 프로덕션에서 AI 판정자를 안전 장치로 활용하여 위험을 줄이고, AI 판정자가 양호하다고 판단한 생성 응답만 사용자에게 표시합니다.\n",
    "\n",
    "강력한 모델을 사용하여 응답을 평가하면 비용이 많이 들 수 있습니다. 예를 들어, GPT-4를 사용하여 응답을 생성하고 평가한다면, 두 배의 GPT-4 호출이 필요하며 이는 API 비용을 약 두 배로 늘립니다. 세 가지 기준(예: 전반적인 응답 품질, 사실적 일관성, 유해성)을 평가하기 위해 세 개의 평가 프롬프트를 사용하는 경우, API 호출 수가 네 배로 증가합니다.  \n",
    "\n",
    "비용을 줄이기 위해 더 약한 모델을 판정자로 사용할 수 있습니다(참고: **“What Models Can Act as Judges?”**). 또한 **Spot-checking**(일부 응답만 평가)을 통해 비용을 줄일 수도 있습니다. Spot-checking은 몇 가지 실패를 놓칠 위험이 있지만, 평가하는 샘플의 비율이 클수록 평가 결과에 대한 신뢰도가 높아집니다. 하지만 그만큼 비용도 높아집니다. 비용과 신뢰성 사이의 적절한 균형을 찾으려면 시행착오가 필요할 수도 있습니다. 이 과정은 **Chapter 4**에서 더 자세히 논의됩니다.  \n",
    "\n",
    "종합적으로 볼 때, AI 판정자는 인간 평가자보다 훨씬 저렴합니다.\n",
    "\n",
    "프로덕션 파이프라인에 AI 판정자를 구현하면 지연(latency)이 추가됩니다. 사용자가 응답을 받기 전에 평가를 수행하는 경우, 위험 감소와 지연 증가 간의 트레이드오프가 발생합니다. 이 추가된 지연은 지연 요구사항이 엄격한 애플리케이션의 경우 이 옵션을 사용할 수 없게 만들 수 있습니다.\n",
    "\n",
    "---\n",
    "\n",
    "**AI 판정자로서의 편향**\n",
    "\n",
    "인간 평가자는 편향을 가지며, AI 판정자도 마찬가지입니다. 서로 다른 AI 판정자는 서로 다른 편향을 가질 수 있습니다. 이 섹션에서는 몇 가지 일반적인 편향에 대해 논의합니다. AI 판정자의 편향을 이해하면 점수를 올바르게 해석하고 이러한 편향을 완화할 수 있습니다.\n",
    "\n",
    "AI 판정자는 **자기 편향(self-bias)**을 가지는 경향이 있습니다. 이는 모델이 다른 모델이 생성한 응답보다 자신이 생성한 응답을 선호하는 현상입니다. 모델이 가장 가능성 높은 응답을 계산하는 데 사용하는 동일한 메커니즘이 이 응답에 높은 점수를 부여하기 때문입니다. **Zheng et al.의 2023년 실험**에 따르면, GPT-4는 자기 응답에 대해 10% 더 높은 승률을 가지며, Claude-v1은 25% 더 높은 승률을 보였습니다.\n",
    "\n",
    "많은 AI 모델은 **첫 번째 위치 편향(first-position bias)**을 가집니다. AI 판정자는 쌍 비교(pairwise comparison)에서 첫 번째 응답이나 옵션 목록의 첫 번째 항목을 선호할 수 있습니다. 이를 완화하기 위해 동일한 테스트를 여러 번 반복하거나, 순서를 다르게 설정하거나, 신중히 설계된 프롬프트를 사용할 수 있습니다. AI의 첫 번째 위치 편향은 인간의 편향과 반대입니다. 인간은 마지막으로 본 응답을 선호하는 경향이 있으며, 이를 **최신성 편향(recency bias)**이라고 합니다.\n",
    "\n",
    "일부 AI 판정자는 **장문 편향(verbosity bias)**을 가지며, 응답의 품질과 관계없이 더 긴 응답을 선호합니다. **Wu와 Aji (2023)**는 GPT-4와 Claude-1 모두 짧고 정확한 응답(~50단어)보다 사실적 오류를 포함한 더 긴 응답(~100단어)을 선호한다는 사실을 발견했습니다. **Saito et al. (2023)**는 창의적 과제를 연구한 결과, 응답 길이 차이가 충분히 클 경우(예: 한 응답이 다른 응답보다 두 배 더 긴 경우) 판정자는 항상 더 긴 응답을 선호한다고 밝혔습니다. 그러나 **Zheng et al. (2023)**과 **Saito et al. (2023)**은 GPT-4가 GPT-3.5보다 이 편향에 덜 영향을 받는다는 사실을 발견했으며, 이 편향이 모델이 강력해짐에 따라 사라질 수 있음을 시사합니다.\n",
    "\n",
    "이 모든 편향 외에도, AI 판정자는 모든 AI 애플리케이션과 동일한 한계를 가지고 있습니다. 여기에는 개인 정보 보호 및 지적 재산(IP)이 포함됩니다. 판정자로 독점 모델을 사용하는 경우, 데이터를 해당 모델로 전송해야 합니다. 모델 제공자가 훈련 데이터를 공개하지 않으면 해당 판정자가 상업적으로 안전한지 확신할 수 없습니다.\n",
    "\n",
    "AI를 판정자로 사용하는 접근 방식에는 한계가 있지만, 많은 장점으로 인해 이 접근 방식이 계속 성장할 것이라고 믿습니다. 그러나 AI 판정자는 정확한 평가 방법 및/또는 인간 평가와 함께 사용해야 합니다.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 어떤 모델이 판정자 역할을 할 수 있을까요?\n",
    "\n",
    "판정자는 평가되는 모델보다 더 강력할 수도, 더 약할 수도, 혹은 동일할 수도 있습니다. 각각의 경우에는 장단점이 있습니다.\n",
    "\n",
    "처음에는 더 강력한 판정자가 더 타당해 보입니다. 시험 채점자가 시험 응시자보다 더 많은 지식을 가지고 있어야 하지 않을까요? 강력한 모델은 더 나은 판단을 내릴 수 있을 뿐 아니라, 약한 모델이 더 나은 응답을 생성하도록 유도함으로써 개선에 도움을 줄 수도 있습니다.\n",
    "\n",
    "하지만 이렇게 생각할 수도 있습니다: 이미 강력한 모델을 사용할 수 있다면, 왜 약한 모델을 사용하여 응답을 생성합니까? 답은 비용과 지연(latency)입니다. 모든 응답을 생성하기 위해 강력한 모델을 사용할 예산이 없을 수도 있습니다. 따라서 강력한 모델을 사용해 일부 응답만 평가할 수 있습니다. 예를 들어, 비용 효율적인 내부 모델을 사용하여 응답을 생성하고, GPT-4를 사용하여 전체 응답 중 1%를 평가할 수 있습니다.\n",
    "\n",
    "강력한 모델은 애플리케이션에서 너무 느릴 수도 있습니다. 빠른 모델을 사용해 응답을 생성하고, 강력하지만 느린 모델이 백그라운드에서 평가를 수행하도록 할 수 있습니다. 만약 강력한 모델이 약한 모델의 응답이 나쁘다고 판단하면, 이를 수정하는 조치를 취할 수도 있습니다. 예를 들어 강력한 모델의 응답으로 업데이트하는 것처럼 말입니다. 반대의 패턴도 흔히 나타납니다. 강력한 모델을 사용하여 응답을 생성하고, 약한 모델이 백그라운드에서 평가를 수행합니다.\n",
    "\n",
    "강력한 모델을 판정자로 사용하면 두 가지 문제가 발생합니다. 첫째, 가장 강력한 모델은 적합한 판정자를 찾지 못할 수 있습니다. 둘째, 어떤 모델이 가장 강력한지 판단하기 위한 대체 평가 방법이 필요합니다.\n",
    "\n",
    "모델이 스스로를 평가하게 하는 것, 즉 **자체 평가(self-evaluation)** 또는 **자체 비판(self-critique)**은 특히 자기 편향(self-bias) 때문에 속임수처럼 보일 수 있습니다. 그러나 자체 평가는 간단한 확인(sanity check)에는 유용할 수 있습니다. 만약 모델이 자신의 응답이 틀렸다고 생각한다면, 그 모델은 신뢰할 만하지 않을 가능성이 있습니다. 간단한 확인 외에도, 모델이 스스로를 평가하도록 요청하면 응답을 수정하고 개선하도록 모델을 유도할 수 있습니다(**Press et al., 2022; Gou et al., 2023; Valmeekam et al., 2023**). 아래는 자체 평가가 어떻게 이루어지는지 보여주는 예입니다:\n",
    "\n",
    "```\n",
    "**프롬프트 [사용자로부터]:** 10+3은 무엇입니까?  \n",
    "**첫 번째 응답 [AI로부터]:** 30  \n",
    "**자체 비판 [AI로부터]:** 이 답변이 맞습니까?  \n",
    "**최종 응답 [AI로부터]:** 아닙니다. 정답은 13입니다.\n",
    "```\n",
    "\n",
    "한 가지 열려 있는 질문은 판정자가 평가되는 모델보다 약할 수 있는가입니다. 일부는 평가가 생성보다 더 쉬운 작업이라고 주장합니다. 예를 들어, 누군가 노래가 좋은지에 대한 의견은 가질 수 있지만, 모든 사람이 노래를 쓸 수 있는 것은 아닙니다. 약한 모델도 강력한 모델의 출력을 평가할 수 있어야 합니다.\n",
    "\n",
    "**Zheng et al. (2023)**은 더 강력한 모델이 인간의 선호도와 더 잘 상관된다는 사실을 발견했으며, 이는 사람들이 자신이 감당할 수 있는 가장 강력한 모델을 선택하도록 만듭니다. 그러나 이 실험은 범용 판정자(general-purpose judges)에만 국한되었습니다. 제가 특히 흥미를 느끼는 연구 방향 중 하나는 **작고 전문화된 판정자(small, specialized judges)**입니다. 전문화된 판정자는 특정 기준과 특정 점수 체계를 따르며, 특정 판단을 내리도록 훈련됩니다. 특정 판단에 있어, 작고 전문화된 판정자는 더 크고 범용적인 판정자보다 더 신뢰할 수 있습니다.\n",
    "\n",
    "AI 판정자를 사용하는 데에는 다양한 방법이 있기 때문에, 다양한 전문화된 AI 판정자가 존재합니다. 여기에서는 세 가지 전문화된 판정자의 예를 설명하겠습니다: **보상 모델(Reward model)**, **참조 기반 판정자(Reference-based judge)**, 그리고 **선호도 모델(Preference model)**.\n",
    "\n",
    "- **보상 모델 (Reward model)**  \n",
    "    보상 모델은 (프롬프트, 응답) 쌍을 입력으로 받아 응답이 프롬프트에 대해 얼마나 좋은지 점수를 매깁니다. 보상 모델은 RLHF(강화 학습을 통한 인간 피드백)에서 여러 해 동안 성공적으로 사용되었습니다. **Cappy**는 Google(2023)이 개발한 보상 모델의 예입니다. (프롬프트, 응답) 쌍이 주어지면, **Cappy**는 0에서 1 사이의 점수를 생성하여 응답이 얼마나 정확한지 나타냅니다. Cappy는 3억 6천만 개의 매개변수를 가진 경량 채점기로, 범용 기반 모델(general-purpose foundation models)보다 훨씬 작습니다.\n",
    "\n",
    "- **참조 기반 판정자 (Reference-based judge)**  \n",
    "    참조 기반 판정자는 하나 이상의 참조 응답(reference response)을 기준으로 생성된 응답을 평가합니다. 이 판정자는 생성된 응답이 참조 응답에 비해 얼마나 유사한지 점수(similarity score)나 품질 점수(quality score)를 출력할 수 있습니다. 예를 들어, **BLEURT**(Sellam et al., 2020)는 (후보 응답, 참조 응답) 쌍을 입력으로 받아 후보 응답과 참조 응답 간의 유사성 점수를 출력합니다. 또 다른 예로, **Prometheus**(Kim et al., 2023)는 (프롬프트, 생성된 응답, 참조 응답, 채점 기준) 쌍을 입력으로 받아 1에서 5 사이의 품질 점수를 출력하며, 여기서 참조 응답의 점수가 5라고 가정합니다.\n",
    "\n",
    "- **선호도 모델 (Preference model)**  \n",
    "    선호도 모델은 (프롬프트, 응답 1, 응답 2)를 입력으로 받아, 주어진 프롬프트에 대해 두 응답 중 어떤 것이 더 나은지(사용자가 선호하는지)를 출력합니다. 이는 전문화된 판정자를 위한 가장 흥미로운 방향 중 하나일 수 있습니다. 인간의 선호도를 예측할 수 있는 능력은 많은 가능성을 열어줍니다. **Chapter 2**에서 논의된 바와 같이, 선호도 데이터는 AI 모델을 인간의 선호도에 맞추는 데 필수적이며, 이를 확보하는 것은 어렵고 비용이 많이 듭니다. 좋은 인간 선호도 예측기를 가지는 것은 일반적으로 평가를 더 쉽게 하고, 모델을 더 안전하게 사용할 수 있도록 만듭니다. PandaLM(Wang et al., 2023)과 JudgeLM(Zhu et al., 2023)을 포함하여 선호도 모델을 구축하기 위한 많은 시도가 있었습니다. **Figure 3-9**는 PandaLM이 작동하는 방식을 보여줍니다. 이 모델은 어떤 응답이 더 나은지를 출력할 뿐 아니라 그 이유도 설명합니다.\n",
    "\n",
    "    <img src=\"./images/fig_03_09.png\" width=\"800\">\n",
    "\n",
    "그 한계에도 불구하고, AI를 판정자로 사용하는 접근법은 다재다능하고 강력합니다. 더 저렴한 모델을 판정자로 사용하면 이 접근법이 더욱 유용해집니다. 초기에는 회의적이었던 제 동료들 중 다수가 이제는 프로덕션에서 이를 더 신뢰하게 되었습니다.\n",
    "\n",
    "AI를 판정자로 사용하는 것은 흥미로우며, 우리가 다음에 논의할 접근법 또한 마찬가지로 매력적입니다. 이 접근법은 매혹적인 분야인 게임 디자인에서 영감을 받았습니다.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **비교 평가를 통한 모델 순위 매기기**\n",
    "\n",
    "종종 모델을 평가하는 이유는 점수 자체에 관심이 있어서가 아니라, 어떤 모델이 자신에게 가장 적합한지를 알고 싶기 때문입니다. 당신이 원하는 것은 이러한 모델들의 순위입니다. 모델들은 **점수 기반 평가(pointwise evaluation)** 또는 **비교 평가(comparative evaluation)**를 사용하여 순위를 매길 수 있습니다.\n",
    "\n",
    "**점수 기반 평가(Pointwise evaluation)**에서는 각 모델을 독립적으로 평가한 후, 해당 점수를 기준으로 순위를 매깁니다. 예를 들어, 어떤 무용수가 가장 뛰어난지 알고 싶다면, 각 무용수를 개별적으로 평가하여 점수를 부여한 후, 가장 높은 점수를 받은 무용수를 선택하는 방식입니다.\n",
    "\n",
    "**비교 평가(comparative evaluation)**에서는 모델들을 서로 비교하여 평가한 후, 비교 결과로부터 순위를 산출합니다. 같은 댄스 경연 대회에서 모든 참가자들에게 동시에 춤을 추게 하고, 판정자들에게 어떤 참가자의 춤이 가장 마음에 드는지 묻는 방식을 사용할 수 있습니다. 가장 많은 판정자들이 선호한 무용수를 선택하는 것이죠.\n",
    "\n",
    "응답의 품질이 주관적인 경우, 비교 평가는 점수 기반 평가(pointwise evaluation)보다 일반적으로 더 쉽게 수행할 수 있습니다. 예를 들어, 두 곡 중 어느 곡이 더 나은지 판단하는 것이 각 곡에 구체적인 점수를 부여하는 것보다 더 쉽습니다.\n",
    "\n",
    "AI 분야에서 비교 평가는 2021년 **Anthropic**에 의해 처음으로 모델 간 순위를 매기기 위해 사용되었습니다. 또한 이 방식은 커뮤니티에서 모델 간 쌍 비교(pairwise comparisons) 점수를 사용하여 모델 순위를 매기는 인기 있는 **LMSYS의 Chatbot Arena 리더보드**를 지원합니다.\n",
    "\n",
    "많은 모델 제공업체들이 프로덕션 환경에서 모델을 평가하기 위해 비교 평가를 사용합니다. **Figure 3-10**은 ChatGPT가 사용자들에게 두 개의 출력을 나란히 비교하도록 요청하는 예를 보여줍니다. 이 출력들은 서로 다른 모델에 의해 생성되었거나, 동일한 모델이 서로 다른 샘플링 변수로 생성한 것일 수 있습니다.\n",
    "\n",
    "<img src=\"./images/fig_03_10.png\" width=\"800\">\n",
    "\n",
    "각 요청에 대해, 두 개 이상의 모델이 선택되어 응답을 생성합니다. 평가자는 인간일 수도 있고 AI일 수도 있으며, 승자를 선택합니다. 많은 개발자들은 초안이 모두 비슷하게 좋거나 나쁠 경우, 임의로 승자가 선택되는 것을 방지하기 위해 동점을 허용합니다.\n",
    "\n",
    "매우 중요한 점은 모든 질문이 선호도(preference)만으로 답변되어서는 안 된다는 것입니다. 많은 질문은 정답(correctness)으로 답변되어야 합니다. 예를 들어, \"휴대폰 방사선과 뇌종양 사이에 연관이 있습니까?\"라는 질문을 모델에 했다고 상상해 보세요. 모델이 두 가지 옵션, \"있다\"와 \"없다\"를 제시하고 선택하게 한다면, 선호도 기반 투표는 잘못된 신호를 초래할 수 있습니다. 이러한 신호가 모델을 훈련하는 데 사용된다면, 잘못된 행동을 유발할 수 있습니다.\n",
    "\n",
    "사용자에게 선택을 요구하는 것은 사용자에게 좌절감을 줄 수 있습니다. 예를 들어, 모델에 수학 문제를 물어보았는데, 사용자가 정답을 알지 못하기 때문에 질문했음에도 불구하고, 모델이 두 가지 다른 답변을 제공하며 어느 쪽을 선호하는지 선택하라고 한다고 상상해 보세요. 만약 정답을 알았다면 처음부터 모델에 질문하지 않았을 것입니다.\n",
    "\n",
    "사용자로부터 비교 피드백을 수집할 때의 한 가지 과제는 어떤 질문이 선호도 기반 투표(preference voting)로 결정될 수 있고, 어떤 질문이 그렇지 않은지를 구별하는 것입니다. 선호도 기반 투표는 투표자가 해당 주제에 대해 잘 알고 있을 때에만 효과적입니다. 이 접근법은 일반적으로 AI가 인턴이나 보조 역할을 하며, 사용자가 이미 알고 있는 작업을 더 빠르게 수행하도록 돕는 애플리케이션에서 작동합니다. 반대로 사용자가 AI에게 자신이 모르는 작업을 수행하도록 요청하는 경우에는 적합하지 않습니다.\n",
    "\n",
    "비교 평가는 **A/B 테스트**와 혼동해서는 안 됩니다. A/B 테스트에서는 사용자에게 한 번에 하나의 후보 모델의 출력만 보입니다. 반면, 비교 평가에서는 사용자에게 여러 모델의 출력을 동시에 보여줍니다.\n",
    "\n",
    "각 비교를 **매치(match)**라고 합니다. 이 과정은 일련의 비교로 이어지며, 이는 **Table 3-5**에 나와 있는 예시와 같습니다.\n",
    "\n",
    "**Table 3-5. 쌍 비교 모델 비교 기록의 예시**\n",
    "\n",
    "| **매치 #** | **모델 A** | **모델 B** | **승자**  |\n",
    "|------------|------------|------------|-----------|\n",
    "| 1          | Model 1    | Model 2    | Model 1   |\n",
    "| 2          | Model 3    | Model 10   | Model 10  |\n",
    "| 3          | Model 7    | Model 4    | Model 4   |\n",
    "| …          | …          | …          | …         |\n",
    "\n",
    "모델 A가 모델 B보다 선호될 확률은 A가 B를 이기는 **승률(win rate)**입니다. 우리는 A와 B 간의 모든 매치를 살펴보고, A가 이긴 비율을 계산하여 이 승률을 구할 수 있습니다.\n",
    "\n",
    "모델이 두 개뿐이라면 순위를 매기는 것은 간단합니다. 승률이 더 높은 모델이 더 높은 순위를 차지합니다. 그러나 모델이 많아질수록 순위를 매기는 것은 더 어려워집니다. 예를 들어, 다섯 개의 모델이 있고, 이들 간 쌍 비교에서의 경험적 승률이 **Table 3-6**에 나와 있다고 가정해 봅시다. 데이터를 보면 이 다섯 개 모델이 어떻게 순위가 매겨져야 하는지는 명확하지 않습니다.\n",
    "\n",
    "**Table 3-6. 다섯 개 모델의 승률 예제. A >> B 열은 A가 B보다 선호되는 사건을 나타냅니다.**\n",
    "\n",
    "| **Model pair #** | **Model A** | **Model B** | **# matches** | **A >> B** |\n",
    "|-------------------|-------------|-------------|---------------|------------|\n",
    "| 1                 | Model 1     | Model 2     | 1000          | 90%        |\n",
    "| 2                 | Model 1     | Model 3     | 1000          | 40%        |\n",
    "| 3                 | Model 1     | Model 4     | 1000          | 15%        |\n",
    "| 4                 | Model 1     | Model 5     | 1000          | 10%        |\n",
    "| 5                 | Model 2     | Model 3     | 1000          | 60%        |\n",
    "| 6                 | Model 2     | Model 4     | 1000          | 80%        |\n",
    "| 7                 | Model 2     | Model 5     | 1000          | 80%        |\n",
    "| 8                 | Model 3     | Model 4     | 1000          | 70%        |\n",
    "| 9                 | Model 3     | Model 5     | 1000          | 10%        |\n",
    "| 10                | Model 4     | Model 5     | 1000          | 20%        |\n",
    "\n",
    "비교 신호를 기반으로, **평가 알고리즘(rating algorithm)**을 사용하여 모델의 순위를 계산합니다. 일반적으로 이 알고리즘은 비교 신호로부터 각 모델의 점수를 계산한 다음, 점수를 기준으로 모델을 순위화합니다.\n",
    "\n",
    "AI에서 비교 평가는 새로운 개념이지만, 다른 산업에서는 거의 한 세기 동안 사용되어 왔습니다. 스포츠 및 비디오 게임 분야에서 특히 인기가 많습니다. 이러한 분야에서 개발된 많은 평가 알고리즘은 AI 모델을 평가하는 데도 적응될 수 있습니다. 대표적으로 **Elo**, **Bradley–Terry**, **TrueSkill**이 있습니다. LMSYS의 **Chatbot Arena**는 처음에는 Elo 알고리즘을 사용하여 모델 순위를 계산했으나, 평가자와 프롬프트 순서에 민감한 Elo 알고리즘을 대신하여 **Bradley–Terry 알고리즘**으로 전환했습니다.  \n",
    "\n",
    "순위는 다음과 같은 경우 올바른 것으로 간주됩니다: **모델 쌍 중, 더 높은 순위의 모델이 낮은 순위의 모델을 상대로 더 자주 이기는 경우**. 예를 들어, 모델 A가 모델 B보다 높은 순위에 있다면, 사용자는 모델 A가 모델 B보다 절반 이상의 경우에서 우세하다고 간주해야 합니다.\n",
    "\n",
    "이 관점에서 보면, 모델 순위 매기기는 예측 문제입니다. 우리는 과거 매치 결과를 바탕으로 순위를 계산하고 이를 사용하여 미래 매치 결과를 예측합니다. 서로 다른 순위 매기기 알고리즘은 서로 다른 순위를 생성할 수 있으며, 올바른 순위에 대한 정답(ground truth)은 존재하지 않습니다. 순위의 품질은 미래 매치 결과를 얼마나 잘 예측할 수 있는지에 따라 결정됩니다.  \n",
    "\n",
    "제가 **Chatbot Arena**의 순위를 분석한 결과, 충분한 매치 수를 가진 모델 쌍에 대해서는 생성된 순위가 적어도 예측력이 좋다는 점을 보여주었습니다. 이 분석은 책의 GitHub 저장소에서 확인할 수 있습니다.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **비교 평가의 도전 과제**\n",
    "\n",
    "점수 기반 평가(pointwise evaluation)에서는 올바른 신호를 수집하기 위해 벤치마크와 지표를 설계하는 것이 과정의 가장 중요한 부분입니다. 모델의 점수를 계산해 순위를 매기는 것은 비교적 쉽습니다. 하지만 비교 평가에서는 신호 수집과 모델 순위 매기기가 모두 어려운 과제입니다. 이 섹션에서는 비교 평가에서 일반적으로 발생하는 세 가지 도전 과제를 설명합니다.\n",
    "\n",
    "---\n",
    "\n",
    "**확장성의 병목 현상**\n",
    "\n",
    "비교 평가는 데이터 집약적입니다. 비교해야 할 모델 쌍의 수는 모델 수에 따라 제곱 비율로 증가합니다. 2024년 1월에 LMSYS는 57개의 모델을 비교하며 244,000개의 비교를 수행했습니다. 이것은 많은 비교처럼 보이지만, 모델 쌍당 평균 153개의 비교만 수행된 셈입니다(57개의 모델은 1,596개의 모델 쌍에 해당). 우리가 원하는 다양한 작업 범위를 고려하면, 이 수치는 매우 적은 편입니다.\n",
    "\n",
    "다행히도 두 모델 간 직접적인 비교가 항상 필요한 것은 아닙니다. 순위 알고리즘은 일반적으로 **전이성(transitivity)**을 가정합니다. 예를 들어, 모델 A가 B보다 높고 B가 C보다 높다면, 전이성에 따라 A가 C보다 높다는 결론을 내릴 수 있습니다. 즉, 알고리즘이 A가 B보다 우수하고 B가 C보다 우수하다는 것을 확신한다면, A와 C를 직접 비교하지 않아도 A가 더 우수하다는 것을 알 수 있습니다.\n",
    "\n",
    "그러나 이러한 전이성 가정이 AI 모델에 적용되는지는 불분명합니다. AI 평가에서 Elo 분석을 수행한 여러 논문(Boubdir 외, Balduzzi 외, Munos 외)은 전이성 가정을 한계점으로 지적합니다. 그들은 인간의 선호가 반드시 전이적이지 않다고 주장했습니다. 또한, 비전이성(non-transitivity)은 서로 다른 평가자들이 서로 다른 프롬프트를 사용해 모델 쌍을 평가할 때 발생할 수 있습니다.\n",
    "\n",
    "새로운 모델을 평가할 때, 독립적인 평가에서는 새로운 모델만 평가하면 됩니다. 하지만 비교 평가에서는 새로운 모델을 기존 모델과 비교해야 하며, 이는 기존 모델의 순위를 변경할 수 있습니다.\n",
    "\n",
    "비공개 모델을 평가하는 것도 어려운 과제입니다. 예를 들어, 회사 내부 데이터를 사용해 모델을 개발했다고 가정해봅시다. 이 모델을 공개 모델과 비교해 어떤 것이 더 유용한지 결정하고 싶다면, 비교 평가를 사용해야 합니다. 이 경우, 자체적으로 비교 신호를 수집하고 자체 리더보드를 생성하거나, 공개된 리더보드 중 하나를 이용해 평가를 진행해야 할 가능성이 높습니다.\n",
    "\n",
    "확장성 병목 현상은 더 나은 매칭 알고리즘을 통해 완화할 수 있습니다. 지금까지는 각 매치에서 모델이 무작위로 선택된다고 가정했으며, 모든 모델 쌍이 대략 동일한 횟수만큼 비교된다고 보았습니다. 그러나 모든 모델 쌍을 동일하게 비교할 필요는 없습니다. 특정 모델 쌍의 결과에 대해 확신이 생기면, 그 쌍을 더 이상 서로 비교하지 않아도 됩니다. 효율적인 매칭 알고리즘은 전체 순위에서 가장 큰 불확실성을 줄이는 매치를 샘플링해야 합니다.\n",
    "\n",
    "---\n",
    "\n",
    "**표준화 및 품질 관리 부족**\n",
    "\n",
    "모델 간 비교 신호를 수집하는 한 가지 방법은 LMSYS Chatbot Arena에서 사용하는 것처럼 커뮤니티에 비교 요청을 크라우드소싱하는 것입니다. 누구나 해당 [웹사이트](https://lmsys.org)에 접속해 프롬프트를 입력하고 두 개의 익명 모델에서 반환된 응답을 비교한 뒤 더 나은 응답에 투표할 수 있습니다. 투표가 완료된 후에야 모델 이름이 공개됩니다.\n",
    "\n",
    "이 접근 방식의 장점은 다양한 신호를 캡처할 수 있고 상대적으로 조작하기 어렵다는 점입니다. 그러나 단점은 표준화를 강제하고 품질 관리를 보장하기 어렵다는 점입니다.\n",
    "\n",
    "첫째, 인터넷 접속이 가능한 누구나 이러한 모델을 평가하기 위해 어떤 프롬프트라도 사용할 수 있습니다. 그리고 더 나은 응답이 무엇인지에 대한 표준이 없습니다. 응답을 사실 확인하는 자원봉사자들에게 과도한 부담이 될 수 있으며, 자원봉사자들은 알지 못한 채로 더 듣기 좋은 응답이지만 사실과 다를 수 있는 응답을 선호할 수도 있습니다.\n",
    "\n",
    "어떤 사람들은 공손하고 온건한 응답을 선호할 수도 있는 반면, 다른 사람들은 필터가 없는 응답을 선호할 수도 있습니다. 이것은 장점과 단점이 모두 있습니다. 장점은 사람들이 가진 다양한 선호도를 포착할 수 있다는 점입니다. 단점은 현실 세계에서 인간의 선호도가 모든 사용 사례에 적합하지 않을 수 있다는 점입니다. 예를 들어, 사용자가 모델에 부적절한 농담을 요청하고 모델이 이를 거부한 경우, 사용자는 모델의 응답을 다운보트(downvote)할 수 있습니다. 그러나 애플리케이션 개발자의 입장에서는 모델이 해당 요청을 거부하는 것이 더 바람직할 수 있습니다. 일부 사용자는 악의적으로 독성이 높은 응답을 선호하는 응답으로 선택해 랭킹을 오염시킬 수도 있습니다.\n",
    "\n",
    "둘째, 비교를 크라우드소싱하는 것은 사용자들이 실제 사용 환경이 아닌 환경에서 모델을 평가하게 합니다. 현실 세계와의 연계가 없는 상태에서 테스트 프롬프트는 모델들이 실제로 사용되는 방식을 반영하지 않을 수 있습니다. 사람들은 떠오르는 대로 단순한 프롬프트만 사용할 수 있으며, 정교한 프롬프트 기술을 사용할 가능성은 낮습니다.\n",
    "\n",
    "LMSYS Chatbot Arena에서 2023년에 공개된 33,000개의 프롬프트 중 180개는 “hello” 및 “hi”와 같은 간단한 인사말이며, 이는 전체 데이터의 0.55%를 차지합니다. 이 숫자는 “hello!” “hello.” “hola” “hey” 등 다양한 변형을 포함하지 않습니다. 또한 “X에게 세 명의 자매가 있고, 한 명의 형제가 있습니다. X에게 몇 명의 형제가 있습니까?”와 같은 많은 두뇌 자극 문제도 포함되어 있으며, 이 질문은 총 44회나 제시되었습니다.\n",
    "\n",
    "단순한 프롬프트는 응답하기 쉽기 때문에 모델의 성능 차이를 구별하기 어렵게 만듭니다. 너무 단순한 프롬프트를 사용해 모델을 평가하는 것은 랭킹을 왜곡할 수 있습니다.\n",
    "\n",
    "공개 리더보드가 내부 데이터베이스에서 검색된 관련 문서로 컨텍스트를 보강하는 것과 같은 정교한 컨텍스트 구성을 지원하지 않는다면, 해당 리더보드의 순위는 모델이 RAG 시스템에서 얼마나 잘 작동할 수 있는지를 반영하지 못할 것입니다. 좋은 응답을 생성하는 능력과 가장 관련성이 높은 문서를 검색하는 능력은 다릅니다.\n",
    "\n",
    "표준화를 적용하는 한 가지 잠재적인 방법은 사용자가 미리 정해진 프롬프트 세트에만 접근하도록 제한하는 것입니다. 그러나 이러한 방식은 리더보드가 다양한 사용 사례를 포착할 수 있는 능력에 영향을 미칠 수 있습니다. LMSYS는 대신 사용자가 모든 프롬프트를 사용할 수 있도록 허용하지만, 내부 모델을 사용해 **어려운 프롬프트(hard prompts)**를 필터링하고, 이 어려운 프롬프트만 사용해 모델을 평가하고 순위를 매기는 방식을 채택합니다.\n",
    "\n",
    "또 다른 방법은 신뢰할 수 있는 평가자를 사용하는 것입니다. 우리는 평가자들에게 두 가지 응답을 비교할 기준을 교육하거나 실질적인 프롬프트와 정교한 프롬프트 기술을 사용하는 방법을 훈련시킬 수 있습니다. 이는 Scale이 **비공개 비교 리더보드**에서 사용하는 접근 방식입니다. 그러나 이 접근 방식의 단점은 비용이 많이 들며, 비교 가능한 횟수를 크게 줄일 수 있다는 점입니다.\n",
    "\n",
    "다른 선택지는 제품 내에 비교 평가를 통합하여 사용자가 작업 흐름 중에 모델을 평가할 수 있도록 하는 것입니다. 예를 들어, 코드 생성 작업의 경우 사용자 코드 편집기에 두 가지 코드 조각을 제안하고 더 나은 것을 선택하도록 할 수 있습니다. 이미 많은 채팅 애플리케이션이 이러한 방식을 채택하고 있습니다. 하지만 앞서 언급했듯이, 사용자가 전문가가 아니라면 어떤 코드 조각이 더 나은지 알지 못할 수 있습니다.\n",
    "\n",
    "게다가, 사용자가 두 옵션을 모두 읽지 않고 단순히 무작위로 클릭할 수도 있습니다. 이는 결과에 많은 잡음을 유발할 수 있습니다. 하지만 올바르게 투표한 소수의 사용자로부터 얻은 신호는 어느 모델이 더 나은지 판단하는 데 충분할 수 있습니다.\n",
    "\n",
    "일부 팀은 인간 평가자보다 AI를 선호합니다. AI는 훈련된 인간 전문가만큼 뛰어나지는 않을 수 있지만, 무작위 인터넷 사용자보다는 더 신뢰할 수 있을 것입니다.\n",
    "\n",
    "---\n",
    "\n",
    "**비교 성능에서 절대 성능으로**\n",
    "\n",
    "많은 응용 프로그램에서는 최고의 모델이 반드시 필요하지 않습니다. 충분히 좋은 모델만 있으면 됩니다. 비교 평가는 어떤 모델이 더 나은지를 알려주지만, 모델이 얼마나 좋은지 또는 특정 사용 사례에 충분히 적합한지에 대해서는 알려주지 않습니다. 예를 들어, 모델 B가 모델 A보다 낫다는 순위를 얻었다고 가정해봅시다. 다음 시나리오 중 어느 것이든 유효할 수 있습니다:\n",
    "\n",
    "1. 모델 B는 좋고, 모델 A는 나쁘다.\n",
    "2. 모델 A와 모델 B 모두 나쁘다.\n",
    "3. 모델 A와 모델 B 모두 좋다.\n",
    "\n",
    "어느 시나리오가 사실인지 결정하려면 다른 형태의 평가가 필요합니다.\n",
    "\n",
    "고객 지원을 위해 모델 A를 사용한다고 가정해봅시다. 모델 A는 전체 요청의 70%를 해결할 수 있습니다. 모델 B는 모델 A에 대해 51%의 승률을 가지고 있습니다. 그러나 이 51%의 승률이 모델 B가 해결할 수 있는 요청 수로 얼마나 전환될지는 불분명합니다. 몇몇 사람들은 특정 애플리케이션에서는 1%의 승률 변화가 큰 성능 향상을 가져올 수 있지만, 다른 애플리케이션에서는 최소한의 향상만 가져온다고 경험적으로 이야기합니다.\n",
    "\n",
    "모델 A를 B로 교체할지를 결정할 때, 인간의 선호가 전부는 아닙니다. 비용과 같은 다른 요소들도 고려해야 합니다. 예상되는 성능 향상을 모르는 상태에서는 비용-편익 분석을 수행하기 어렵습니다. 만약 모델 B가 모델 A보다 비용이 두 배가 든다면, 비교 평가는 B의 성능 향상이 추가 비용을 감당할 만큼의 가치가 있는지 판단하는 데 충분하지 않을 수 있습니다.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **비교 평가의 미래**\n",
    "\n",
    "비교 평가의 많은 한계점을 고려했을 때, 이 평가 방식의 미래가 있을지 궁금할 수 있습니다. 그러나 비교 평가는 여전히 많은 이점을 제공합니다. \n",
    "\n",
    "첫째, **\"후 훈련(Post-Training)\"**에서 논의된 것처럼, 사람들은 각 출력에 구체적인 점수를 주는 것보다 두 출력을 비교하는 것이 더 쉽다는 점을 발견했습니다. 모델 성능이 더 강력해지고 인간 성능을 넘어섬에 따라, 인간 평가자들이 모델 응답의 구체적인 점수를 제공하는 것은 점점 어려워질 수 있습니다. 하지만 평가자들은 여전히 차이를 감지할 수 있을 수 있으며, 비교 평가가 유일한 옵션으로 남을 수 있습니다. 예를 들어, Llama 2 논문에서는 모델이 최고 수준의 인간 주석자들의 능력을 넘어서는 유형의 작문에 진입할 때에도, 두 응답을 비교할 때 유용한 피드백을 제공할 수 있음을 공유했습니다(Touvron 외, 2023).\n",
    "\n",
    "둘째, 비교 평가는 우리가 중요하게 생각하는 품질, 즉 인간의 선호를 포착하는 것을 목표로 합니다. 이는 AI의 지속적으로 확장되는 기능에 맞추기 위해 지속적으로 더 많은 벤치마크를 만들어야 한다는 압박을 줄여줍니다. 벤치마크는 더 강력한 모델이 도입되면 쓸모없어질 수 있지만, 비교 평가는 결코 포화 상태에 이르지 않을 것입니다.\n",
    "\n",
    "비교 평가는 속이기가 상대적으로 어렵습니다. 참조 데이터를 기반으로 모델을 훈련시키는 것처럼 쉽게 조작할 방법이 없습니다. 이러한 이유로, 많은 사람들은 다른 공개 리더보드보다 공개 비교 리더보드의 결과를 더 신뢰합니다.\n",
    "\n",
    "비교 평가는 다른 방법으로 얻을 수 없는 신호를 모델에 대해 제공할 수 있습니다. 오프라인 평가에서는 평가 벤치마크에 큰 추가 요소가 될 수 있으며, 온라인 평가에서는 A/B 테스트를 보완하는 역할을 할 수 있습니다.\n",
    "\n",
    "--- \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **요약**  \n",
    "\n",
    "AI 모델이 강력해질수록 치명적인 실패 가능성이 높아지며, 이는 평가의 중요성을 더욱 증가시킵니다. 동시에, 개방형이며 강력한 모델을 평가하는 것은 도전 과제가 됩니다. 이러한 어려움으로 인해 많은 팀들이 인간 평가로 눈을 돌리게 됩니다. 인간을 평가 과정에 포함시켜 검토하는 것은 항상 유용하며, 많은 경우 인간 평가가 필수적입니다. 하지만 이 장에서는 자동 평가의 다양한 접근 방식에 초점을 맞춥니다.\n",
    "\n",
    "이 장은 전통적인 기계 학습 모델보다 기본 모델(foundation models)을 평가하는 것이 왜 더 어려운지에 대한 논의로 시작합니다. 많은 새로운 평가 기술이 개발되고 있지만, 평가에 대한 투자는 모델 및 애플리케이션 개발에 대한 투자에 비해 여전히 부족합니다.\n",
    "\n",
    "많은 기본 모델이 언어 모델 구성 요소를 가지고 있기 때문에, 우리는 언어 모델링 지표(perplexity와 cross-entropy를 포함)에 초점을 맞췄습니다. 많은 사람들이 이 지표들을 혼란스럽게 여기기 때문에, 이러한 지표들을 어떻게 해석하고 이를 평가와 데이터 처리에 활용할 수 있는지에 대한 섹션을 추가했습니다.\n",
    "\n",
    "이 장은 또한 개방형 응답을 평가하기 위한 다양한 접근 방식으로 초점을 이동했습니다. 여기에는 기능적 정확성, 유사성 점수, AI를 심판으로 사용하는 평가 방식이 포함됩니다. 처음 두 가지 평가 접근 방식은 정확한 평가(exact evaluation)이며, AI를 심판으로 사용하는 평가는 주관적입니다.\n",
    "\n",
    "정확한 평가와 달리, 주관적 지표는 심판에 크게 의존합니다. 이들의 점수는 심판이 사용되는 맥락에서 해석되어야 합니다. 동일한 품질을 측정하기 위해 다른 AI 심판이 사용하는 점수는 비교할 수 없을 수도 있습니다. 모든 AI 응용 프로그램과 마찬가지로, AI 심판은 반복적으로 개선되어야 하며, 그들의 판단은 변화합니다. 이는 시간이 지남에 따라 애플리케이션의 변화를 추적하기 위한 벤치마크로는 신뢰할 수 없게 만듭니다. 유망하긴 하지만, AI 심판은 정확한 평가, 인간 평가, 혹은 둘 모두로 보완되어야 합니다.\n",
    "\n",
    "모델을 평가할 때, 각 모델을 독립적으로 평가하고 점수를 기반으로 순위를 매길 수 있습니다. 또는, 비교 신호를 사용하여 두 모델 중 어느 것이 더 나은지 순위를 매길 수도 있습니다. 비교 평가는 스포츠, 특히 체스와 같은 분야에서 일반적이며, AI 평가에서도 점점 인기를 얻고 있습니다. 비교 평가와 후속 훈련 정렬 프로세스는 선호 신호(preference signals)를 제공합니다. 이는 수집 비용이 비싸지만, 선호 모델의 개발을 촉진했습니다. 이러한 선호 모델은 사용자가 선호하는 응답을 예측하는 전문화된 AI 심판입니다.\n",
    "\n",
    "언어 모델링 지표와 직접 설계된 유사성 측정은 오래전부터 존재했지만, AI를 심판으로 활용하는 평가와 비교 평가는 기본 모델의 등장과 함께 채택되기 시작했습니다. 많은 팀들이 이를 평가 파이프라인에 통합하는 방법을 모색하고 있습니다. 개방형 응답 애플리케이션을 평가하기 위한 신뢰할 수 있는 평가 파이프라인을 구축하는 방법은 다음 장의 주제입니다.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 2023년 12월, OpenAI의 공동 창업자인 Greg Brockman은 \"평가는 놀랍도록 자주 필요한 전부입니다\"라는 내용의 트윗을 작성했습니다.\n",
    "\n",
    "2. 2023년 a16z가 진행한 연구에 따르면, 70명의 의사결정자 중 6명이 입소문으로 모델을 평가했다고 합니다.\n",
    "\n",
    "3. 이것은 흔히 **vibe check**라고도 알려져 있습니다.\n",
    "\n",
    "4. OpenAI의 GPT-σ1이 2024년 9월 출시되었을 때, 필즈 메달리스트 Terrence Tao는 이 모델을 사용하는 경험을 \"완전히 무능하지는 않지만 미흡한 대학원생과 일하는 것\"에 비유했습니다. 그는 AI가 \"유능한 대학원생\" 수준에 도달하려면 한두 번의 추가 개선이 더 필요할 수 있다고 추정했습니다. 그의 평가에 대해, 많은 사람들은 \"이미 AI 모델을 평가하기 위해 가장 똑똑한 인간의 지성이 필요한 상황이라면, 앞으로의 AI 모델을 평가할 자격이 있는 사람이 없을 것이다\"라는 농담을 했습니다.\n",
    "\n",
    "5. 저는 \"LLM\", \"GPT\", \"생성적(generative)\", \"변환기(transformer)\"라는 키워드를 사용해 최소 500개의 스타를 받은 저장소를 검색했습니다. 또한 제 웹사이트 [https://huyenchip.com](https://huyenchip.com)을 통해 누락된 저장소를 크라우드소싱했습니다.\n",
    "\n",
    "6. 강한 상관관계가 있긴 하지만, 언어 모델링 성능이 다운스트림 성능을 완전히 설명하지는 못합니다. 이는 연구가 활발히 진행 중인 영역입니다.\n",
    "\n",
    "7. **1장에서** 언급했듯이, 토큰은 문자, 단어, 또는 단어의 일부가 될 수 있습니다. Claude Shannon이 1951년에 엔트로피를 소개했을 때, 그가 다룬 토큰은 문자였습니다. 그의 말을 빌리자면: **\"엔트로피는 텍스트에서 평균적으로 각 문자에 대해 얼마나 많은 정보가 생성되는지를 측정하는 통계적 매개변수입니다. 언어가 이진 숫자(0 또는 1)로 변환될 때, 엔트로피는 원래 언어의 문자를 표현하기 위해 필요한 평균 이진 숫자의 수를 의미합니다.\"**\n",
    "\n",
    "8. 많은 사람들이 로그 기반 2보다 자연 로그를 선호하는 이유 중 하나는 자연 로그가 수학적으로 더 간단한 특성을 갖기 때문입니다. 예를 들어, 자연 로그 ln(x)의 도함수는 1/x입니다.\n",
    "\n",
    "9. SFT(지도형 미세 조정)와 RLHF(인간 피드백을 통한 강화 학습)의 의미가 확실하지 않다면, **2장을** 참조하세요.\n",
    "\n",
    "10. 양자화(Quantization)는 **7장에서** 논의됩니다.\n",
    "\n",
    "11. 문제는 많은 복잡한 작업이 측정 가능한 목표를 가지고 있더라도, AI가 복잡한 작업을 처음부터 끝까지 완전히 수행하기에 아직 충분히 발전하지 않았다는 점입니다. 따라서 AI는 일부 솔루션을 지원하는 데 사용될 수 있습니다. 때로는 솔루션의 일부를 평가하는 것이 결과(예: 승리/패배/무승부)를 평가하는 것보다 더 어려울 수 있습니다. 예를 들어, 누군가의 체스 실력을 평가하려고 한다고 상상해보세요. 한 번의 움직임을 평가하기보다 게임의 최종 결과를 평가하는 것이 더 쉽습니다.\n",
    "\n",
    "12. \"cats\"와 \"cat\" 또는 \"will not\"과 \"won't\"을 별도의 토큰으로 간주할지 여부에 따라 일부 처리가 필요할 수도 있습니다.\n",
    "\n",
    "13. 10,000차원 벡터 공간은 고차원으로 보일 수 있지만, 원시 데이터의 차원보다는 훨씬 낮습니다. 따라서 임베딩은 복잡한 데이터를 저차원 공간으로 표현한 것으로 간주됩니다.\n",
    "\n",
    "14. 문서 임베딩과는 달리, 단어 임베딩을 생성하는 모델도 있습니다. 예를 들어 word2vec(Mikolov et al., \"Efficient Estimation of Word Representations in Vector Space\", arXiv, 2013년 9월 7일)과 GloVe(Pennington et al., \"GloVe: Global Vectors for Word Representation\", Stanford University Natural Language Processing Group 블로그, 2014년)가 있습니다.\n",
    "\n",
    "15. **AI 심판(AI judge)**이라는 용어는 법원에서 AI가 판사로 사용되는 경우와 혼동하지 않아야 합니다.\n",
    "\n",
    "16. 2017년, 저는 NeurIPS 워크숍에서 MEWR(Machine Translation Evaluation Metric Without Reference Text)라는 자동 번역 평가 방법을 발표했습니다. 이 방법은 더 강력한 언어 모델을 활용하여 번역 품질을 자동으로 평가합니다. 안타깝게도, 개인적인 이유로 이 연구를 계속하지 못했습니다.\n",
    "\n",
    "17. 일부 경우, 평가가 예산의 대부분을 차지하며, 응답 생성보다 더 많은 비용이 들 수 있습니다.\n",
    "\n",
    "18. **스팟 체크(Spot-checking)**는 샘플링(sampling)과 동일합니다.\n",
    "\n",
    "19. Saito et al.(2023)은 사람들이 더 긴 응답을 선호하는 경향이 있음을 발견했지만, 그 정도는 비교적 적습니다.\n",
    "\n",
    "20. 이 기법은 때때로 **자기 비판(self-critique)** 또는 **자기 질문(self-ask)**이라고도 불립니다.\n",
    "\n",
    "21. BLEURT 점수 범위는 혼란을 줄 수 있습니다. 점수는 대략 **-2.5에서 1.0 사이**입니다. 이는 AI 심판과 관련된 기준의 모호성 문제를 강조합니다. 점수 범위는 임의적일 수 있습니다.\n",
    "\n",
    "22. Likert 척도와 같은 방식을 사용할 수 있습니다.\n",
    "\n",
    "23. Chatbot Arena는 Elo 평가 알고리즘 사용을 중단했지만, 개발자들은 한동안 여전히 모델 평가를 \"Elo 점수\"라고 불렀습니다. 그들은 Bradley-Terry 점수를 스케일링하여 Elo 점수처럼 보이게 만들었습니다. 스케일링 과정은 상당히 복잡합니다. 각 점수는 Elo에서 사용된 스케일(400)을 곱한 후, 초기 Elo 점수(1,000)를 더했습니다. 이후 모델 Llama-13b의 점수를 800으로 조정하기 위해 점수를 재조정했습니다.\n",
    "\n",
    "24. Chatbot Arena의 인기가 높아지면서, 점수를 조작하려는 시도가 더 흔해졌습니다. 누군가가 랭킹을 조작하려 했다고 인정한 적은 없지만, 몇몇 모델 개발자들은 경쟁 업체들이 랭킹을 조작하려 한다고 확신한다고 말했습니다."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
