{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **제2장. 파운데이션 모델 이해하기**    \n",
    "> 지난시간에 AI엔지니어링에 대한 개요를 알아보았습니다. 이번 시간에는 파운데이션 모델에 대한 이해를 높이기 위해 모델의 학습 데이터 분포, 모델링, 사후 훈련, 샘플링 등의 주요 개념을 살펴보겠습니다.\n",
    "\n",
    "### 목차 (Table of Contents)\n",
    "- [학습 데이터 분포](#data-distribution)\n",
    "- [사전 훈련](#pre-training)\n",
    "- [사후 훈련](#post-training)\n",
    "- [샘플링](#sampling)\n",
    "### 개요\n",
    "파운데이션 모델은 대규모 데이터로 사전 학습된 AI 모델로, 다양한 다운스트림 태스크의 기반이 됩니다.<br>\n",
    "이러한 모델의 개발과 활용을 위해서는 다음과 같은 핵심 요소들의 이해가 필요합니다:\n",
    "\n",
    "- **학습 데이터**: 모델의 성능과 한계를 결정\n",
    "- **모델 아키텍처**: 현재는 트랜스포머가 주도적\n",
    "- **정렬 방식**: 인간의 선호도에 맞추는 과정\n",
    "\n",
    "### 주요 특징\n",
    "- 개발 과정의 복잡성과 높은 비용\n",
    "- 기업들의 핵심 노하우 비공개\n",
    "- 투명성 감소 추세\n",
    "\n",
    "### 학습 프로세스\n",
    "1. **사전 학습**: 기본 능력 형성\n",
    "2. **정렬 단계**: 사용성 향상\n",
    "\n",
    "### 중요 고려사항\n",
    "- 샘플링 전략의 중요성\n",
    "- AI의 특이 행동 이해\n",
    "- 성능 최적화 방법\n",
    "\n",
    "> 본 장의 내용은 이후 챕터들의 기초가 되며, 필요에 따라 참고할 수 있습니다.\n",
    "---  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"data-distribution\"></a>\n",
    "## 학습 데이터 분포\n",
    "\n",
    "\n",
    "AI 모델의 성능과 데이터 품질의 관계\n",
    "\n",
    "1. 데이터 품질의 중요성\n",
    "AI 모델의 성능은 학습 데이터의 품질에 직접적으로 영향을 받습니다. 예를 들어, 베트남어 데이터가 없으면 영어-베트남어 번역이 불가능하고, 동물 사진으로만 학습한 모델은 식물을 분류할 수 없습니다. \n",
    "- 따라서 특정 작업의 성능을 향상시키려면 관련된 양질의 데이터를 추가 필요\n",
    "\n",
    "2. 주요 데이터 소스와 그 특징\n",
    "[Common Crawl](https://commoncrawl.org/)은 비영리 단체가 매월 20-30억 개의 웹페이지를 크롤링하여 수집한 데이터 \n",
    "- Google은 이를 정제한 버전인 [Colossal Clean Crawled Corpus(C4)](https://arxiv.org/abs/1910.10683v4)를 제공\n",
    "\n",
    "3. 데이터 품질 관련 문제점\n",
    "Common Crawl과 C4는 접근이 쉽지만 품질 면에서 큰 문제를 안고 있습니다.\n",
    "- 잘못된 정보, 선전, 음모론, 차별적 콘텐츠 등이 포함되어 있으며, 워싱턴 포스트의 연구에 따르면 상위 1,000개 웹사이트 중 상당수가 신뢰성 평가에서 낮은 점수를 받음\n",
    "\n",
    "GPT-3와 Bard는 Common Crawl을 사용했지만, GPT-4나 Gemini 같은 최신 모델들은 비판을 피하기 위해 데이터 출처를 공개하지 않고 있습니다.\n",
    "\n",
    "4. 데이터 품질 개선을 위한 노력\n",
    "- OpenAI는 GPT-2 개발 시 Reddit에서 '카르마' 점수 3점 이상인 링크만을 선별적으로 수집\n",
    "- 이는 스팸이나 저품질 콘텐츠를 걸러내는데 도움이 되었지만, Reddit 사용자들의 평가가 절대적인 품질 기준이 될 수는 없다는 한계\n",
    "\n",
    "5. 효과적인 모델 개발 전략\n",
    "모델 개발자들은 특정 작업에 맞는 데이터를 직접 큐레이션하기 시작했으며, 주로 다국어 또는 도메인 특화 작업에 초점을 맞추고 있습니다. 이때 처음부터 새로 학습하기보다는 기존 범용 모델을 파인튜닝하는 방식을 주로 사용합니다.\n",
    "\n",
    "6. 데이터 학습의 중요 고려사항\n",
    "단순히 많은 데이터를 학습하는 것이 항상 좋은 결과를 가져오지는 않습니다. 때로는 적은 양의 고품질 데이터가 많은 양의 저품질 데이터보다 더 나은 성능을 보일 수 있습니다. 또한 신경망은 새로운 작업을 학습하면서 이전에 잘하던 작업을 잊어버리는 현상이 있어, 이를 고려한 학습 전략이 필요합니다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 다국어 모델\n",
    "\n",
    "영어는 인터넷을 지배합니다. Common Crawl 데이터셋 분석에 따르면, 영어는 데이터셋의 거의 절반(45.88%)을 차지하며, 이는 두 번째로 흔한 언어(러시아어 - 5.97%)보다 약 8배 더 많습니다. (Lai et al., 2023 참조). Common Crawl에 최소 1% 포함된 언어 목록은 [그림 2-1](Figure 2-1)에서 확인할 수 있습니다. 학습 데이터로서의 가용성이 제한적인 언어(일반적으로 이 목록에 포함되지 않은 언어)는 **저자원(low-resource)** 언어로 간주됩니다.\n",
    "\n",
    "**표 2-1. Common Crawl 데이터셋에서 가장 일반적인 언어. LLM 훈련에 널리 사용됨. 출처: Lai et al. (2023).**\n",
    "\n",
    "| 언어          | 코드 | 인구 (백만) | CC 크기 (%) | 범주 |\n",
    "|---------------|------|-------------|--------------|------|\n",
    "| 영어          | en   | 1,452       | 45.8786      | H    |\n",
    "| 러시아어      | ru   | 258         | 5.9692       | H    |\n",
    "| 독일어        | de   | 134         | 5.8811       | H    |\n",
    "| 중국어        | zh   | 1,118       | 4.8747       | H    |\n",
    "| 일본어        | jp   | 125         | 4.7884       | H    |\n",
    "| 프랑스어      | fr   | 274         | 4.7254       | H    |\n",
    "| 스페인어      | es   | 548         | 4.4690       | H    |\n",
    "| 이탈리아어    | it   | 68          | 2.5712       | H    |\n",
    "| 네덜란드어    | nl   | 30          | 2.0585       | H    |\n",
    "| 폴란드어      | pl   | 45          | 1.6636       | H    |\n",
    "| 포르투갈어    | pt   | 257         | 1.1505       | H    |\n",
    "| 베트남어      | vi   | 85          | 1.0299       | H    |\n",
    "\n",
    "\n",
    "오늘날 많은 사용자를 보유한 다른 언어들도 Common Crawl에서는 심각하게 과소대표되어 있습니다. 표 2-2는 이러한 언어들의 일부를 보여줍니다. <br>\n",
    "이상적으로는 세계 인구 대비 Common Crawl 대표성의 비율이 1이 되어야 합니다. 이 비율이 높을수록 해당 언어가 Common Crawl에서 더 과소대표되어 있다는 것을 의미합니다.\n",
    "\n",
    "**표 2-2. Common Crawl에서 과소 대표된 언어의 예시. 마지막 행은 영어로, 비교를 위해 포함되었습니다. Common Crawl의 % 값은 Lai et al. (2023)에서 가져왔습니다.**\n",
    "\n",
    "| 언어       | 화자 수 (백만) | 세계 인구 비율 (%) | Common Crawl 비율 (%) | CC대비인구비율 |\n",
    "|------------|----------------|--------------------|-----------------------|------------------------|\n",
    "| 펀자브어    | 113            | 1.41%             | 0.0061%              | 231.56                |\n",
    "| 스와힐리어  | 71             | 0.89%             | 0.0077%              | 115.26                |\n",
    "| 우르두어    | 231            | 2.89%             | 0.0274%              | 105.38                |\n",
    "| 칸나다어    | 64             | 0.80%             | 0.0122%              | 65.57                 |\n",
    "| 텔루구어    | 95             | 1.19%             | 0.0183%              | 64.89                 |\n",
    "| 구자라티어  | 62             | 0.78%             | 0.0126%              | 61.51                 |\n",
    "| 마라티어    | 99             | 1.24%             | 0.0213%              | 58.10                 |\n",
    "| 벵골어      | 272            | 3.40%             | 0.0930%              | 36.56                 |\n",
    "| 영어        | 1452           | 18.15%            | 45.88%               | 0.40                  |\n",
    "\n",
    "- 80억 명의 세계 인구 기준\n",
    "\n",
    "인터넷 데이터에서 영어가 지배적이라는 점을 고려할 때, 여러 연구에 따르면 범용 모델이 다른 언어보다 영어에서 훨씬 더 잘 작동한다는 것은 놀라운 일이 아닙니다.<br>\n",
    "예를 들어, MMLU(Massive Multitask Language Understanding, 대규모 다중작업 언어 이해) 벤치마크에서, GPT-4는 그림 2-1에서 볼 수 있듯이 Telugu와 같은 과소대표 언어보다 영어에서 훨씬 더 나은 성능을 보였습니다(OpenAI, 2023). \n",
    "> MMLU는 수학, 과학, 인문학, 사회과학 등 57개 과목에 걸친 14,000개의 객관식 문제로 구성된 벤치마크로, 언어 모델의 다양한 분야에 대한 지식과 이해도를 평가하는 데 사용\n",
    "\n",
    "<img src=\"images/fig_02_01.png\" width=\"800\">\n",
    "\n",
    "Project Euler의 수학 문제 6개를 테스트했을 때, GPT-4는 영어로 문제를 아르메니아어 또는 페르시아어보다 **3배 이상 자주** 해결할 수 있었습니다.\n",
    "- GPT-4는 Burmese와 Amharic 문제에서는 6개 모두 실패. 이는 [그림 2-2](Figure 2-2)에서 확인할 수 있습니다.\n",
    "\n",
    "<img src=\"images/fig_02_02.png\" width=\"800\">\n",
    "\n",
    "과소대표성은 이러한 성능 저하의 큰 이유입니다. GPT-4의 MMLU 벤치마크에서 가장 낮은 성능을 보인 세 언어인 텔루구어, 마라티어, 펀자브어는 Common Crawl에서 가장 과소대표된 언어들 중에 속합니다. 하지만 과소대표성만이 유일한 이유는 아닙니다. 언어의 구조와 그 언어가 담고 있는 문화적 특성도 모델이 해당 언어를 학습하는 것을 더 어렵게 만들 수 있습니다.\n",
    "\n",
    "LLM이 일반적으로 번역을 잘한다는 점을 고려할 때, 다른 언어의 모든 질의를 영어로 번역하고, 응답을 받은 후 다시 원래 언어로 번역하면 되지 않을까요? 많은 사람들이 실제로 이 방법을 사용하지만, 이는 이상적인 방법은 아닙니다. 첫째, 이를 위해서는 과소대표된 언어를 번역할 수 있을 만큼 충분히 이해하는 모델이 필요합니다. 둘째, 번역 과정에서 정보가 손실될 수 있습니다. 예를 들어, 베트남어와 같은 일부 언어는 두 화자 간의 관계를 나타내는 대명사를 사용합니다. 이를 영어로 번역할 때, 이러한 대명사들은 모두 'I'와 'you'로 번역되어 관계 정보가 손실됩니다.\n",
    "\n",
    "모델들은 비영어권 언어에서 예상치 못한 성능 문제를 보일 수 있습니다. 예를 들어, NewsGuard는 ChatGPT가 영어보다 중국어에서 잘못된 정보를 더 쉽게 생성한다는 것을 발견했습니다.\n",
    "- 2023년 4월, NewsGuard는 ChatGPT-3.5에게 영어, 간체 중국어, 번체 중국어로 중국에 대한 잘못된 정보 기사를 생성하도록 요청\n",
    "- 영어의 경우 7개의 프롬프트 중 6개에서 거짓 주장 생성을 거부했습니다. 하지만 간체 중국어와 번체 중국어에서는 7번 모두 거짓 주장을 생성 \n",
    "    - 이러한 행동의 차이가 발생하는 원인은 명확하지 않습니다.\n",
    "\n",
    "품질 문제 외에도, 모델은 비영어권 언어에서 더 느리고 비용이 더 많이 들 수 있습니다. \n",
    "- 모델의 추론 지연 시간과 비용은 입력과 응답의 토큰 수에 비례\n",
    "- 토크나이제이션은 어떤 언어에서는 다른 언어보다 훨씬 더 효율적일 수 있음\n",
    "- 동일한 의미를 전달하기 위해 버마어와 힌디어가 영어나 스페인어보다 훨씬 더 많은 토큰을 필요로 한다는 것을 발견\n",
    "    - 영어의 중간 토큰 길이는 7이지만, 힌디어는 32, 버마어는 무려 72로 영어보다 10배 더 많은 토큰을 필요로 함 (10배 많은 시간/비용 소요)\n",
    "    > Yennie Jun : 52개 언어로 번역된 100만 개의 짧은 텍스트로 구성된 데이터셋인 MASSIVE에서 GPT-4를 벤치마킹\n",
    "\n",
    "\n",
    "이 문제를 해결하기 위해, 많은 모델들이 비영어권 언어에 초점을 맞추도록 훈련. 영어 외에 가장 활발한 활동을 보이는 언어는 단연 중국어로, ChatGLM, YAYI, Llama-Chinese 등이 있습니다. 또한 프랑스어(CroissantLLM), 베트남어(PhoGPT), 아랍어(Jais) 등 더 많은 언어에서도 모델이 개발되고 있습니다.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 도메인 특화 모델\n",
    "\n",
    "Gemini, GPT, Llama와 같은 범용 모델들은 코딩, 법률, 과학, 비즈니스, 스포츠, 환경 과학을 포함한 광범위한 도메인에서 놀라울 정도로 우수한 성능 \n",
    "- 이는 주로 학습 데이터에 이러한 도메인들이 포함되어 있기 때문입니다. `그림 2-3`은 Washington Post의 2023년 분석에 따른 Common Crawl에 존재하는 도메인들의 분포\n",
    "\n",
    "<img src=\"images/fig_02_03.png\" width=\"800\">\n",
    "\n",
    "- 현재까지 비전 데이터의 도메인 분포에 대한 분석은 희귀. 이미지가 텍스트보다 분류하기 더 어렵기 때문\n",
    "- 모델의 벤치마크 성능을 통해 모델의 도메인을 추론 가능. 표 2-3은 CLIP과 Open CLIP이라는 두 모델이 서로 다른 벤치마크에서 어떤 성능을 보이는지 보여줌\n",
    "- 이러한 벤치마크는 두 모델이 새, 꽃, 자동차 등의 카테고리에서 얼마나 잘 작동하는지 보여주지만, 세상은 이러한 몇 가지 카테고리보다 훨씬 더 크고 복잡합니다.\n",
    "\n",
    "**표 2-3. 다양한 이미지 데이터셋에서 Open CLIP과 CLIP의 성능 비교.**\n",
    "\n",
    "| 데이터셋                                     | CLIP 정확도 (ViT-B/32, OpenAI) | Open CLIP 정확도 (ViT-B/32, Cade) |\n",
    "|---------------------------------------------|---------------------------------|------------------------------------|\n",
    "| ImageNet                                    | 63.2                            | 62.9                               |\n",
    "| ImageNet v2                                 | –                               | 62.6                               |\n",
    "| Birdsnap                                    | 37.8                            | 46.0                               |\n",
    "| Country211                                  | 17.8                            | 14.8                               |\n",
    "| Oxford 102 Category Flower                 | 66.7                            | 66.0                               |\n",
    "| German Traffic Sign Recognition Benchmark  | 32.2                            | 42.0                               |\n",
    "| Stanford Cars                               | 59.4                            | 79.3                               |\n",
    "| UCF101                                      | 64.5                            | 63.1                               |\n",
    "\n",
    "비록 일반 목적의 기반 모델들이 다양한 도메인에 대한 일상적인 질문에 답할 수 있지만, 이들이 도메인 특화 작업에서 높은 성능을 보일 가능성은 낮습니다. \n",
    "- 특히 훈련 중에 이러한 작업을 접한 적이 없다면 더욱 그렇습니다. \n",
    "\n",
    "- 도메인 특화 작업의 두 가지 예로는 약물 개발과 암 검사 작업을 들 수 있습니다. 약물 개발은 단백질, DNA, RNA 데이터와 같은 특정 형식의 데이터를 포함하며, 이러한 데이터는 획득 비용이 비쌉니다. 이 데이터는 일반적으로 공개된 인터넷 데이터에서 발견되기 어렵습니다. 마찬가지로, 암 검사는 X-ray와 fMRI(기능적 자기 공명 영상) 스캔을 포함하는데, 이는 개인정보 보호 문제로 인해 얻기가 어렵습니다.\n",
    "\n",
    "이러한 도메인 특화 작업에서 모델의 성능을 높이려면 매우 특정한 데이터셋을 신중히 선별해야 할 수 있습니다. \n",
    "- 도메인 특화 모델 중 가장 잘 알려진 예는 DeepMind의 AlphaFold일 것입니다. 이 모델은 약 10만 개의 알려진 단백질 서열과 3D 구조를 기반으로 훈련\n",
    "-  NVIDIA의 BioNeMo는 약물 개발을 위해 생물분자 데이터를 중심으로 한 또 다른 모델입니다. \n",
    "- Google의 Med-PaLM2는 대규모 언어 모델(LLM)의 강점과 의료 데이터를 결합하여 의료 질문에 더 높은 정확도로 답변할 수 있습니다.\n",
    "\n",
    ">**팁:**  \n",
    ">\n",
    ">도메인 특화 모델은 특히 생물의학 분야에서 흔히 사용되지만, 다른 분야에서도 도메인 특화 모델이 유용할 수 있음. <br>\n",
    "> 예를 들어, 건축 스케치로 훈련된 모델은 Stable Diffusion보다 건축 설계에 훨씬 더 유용할 수 있으며, 공장 설계도에 대해 훈련된 모델은 ChatGPT와 같은 일반 모델보다 제조 공정에 더 최적화\n",
    "\n",
    "이 섹션에서는 훈련 데이터가 모델 성능에 미치는 영향을 높은 수준에서 개괄적으로 설명했습니다. 다음으로, 모델 설계가 성능에 미치는 영향을 탐구하겠습니다.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"pre-training\"></a>\n",
    "## 사전훈련\n",
    "\n",
    "모델을 훈련하기 전에 개발자는 모델이 어떤 모습이어야 할지 결정해야 합니다. \n",
    "- 어떤 아키텍처를 따라야 할까요? \n",
    "- 얼마나 많은 매개변수를 가져야 할까요? <br>\n",
    "이러한 결정은 모델의 능력뿐만 아니라 다운스트림 애플리케이션에서의 사용 가능성에도 영향을 미칩니다. <br>\n",
    "예를 들어, 70억 매개변수 모델은 1,750억 매개변수 모델보다 배포하기 훨씬 쉬울 것입니다. <br>\n",
    "마찬가지로, 지연 시간을 최적화하기 위한 Transformer 모델은 다른 아키텍처를 최적화하는 것과 매우 다릅니다. <br>\n",
    "이러한 결정의 배경이 되는 요소를 살펴보겠습니다.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **모델 아키텍처**\n",
    "\n",
    "- 현재, 언어 기반의 기반 모델에서 가장 지배적인 아키텍처는 Transformer 아키텍처(Vaswani et al., 2017).\n",
    "    - 어텐션 메커니즘(Attention Mechanism)에 기반을 두고 있으며, 이전 아키텍처의 많은 한계를 해결하여 인기를 얻었습니다. \n",
    "    - Transformer 아키텍처 자체도 한계가 있습니다. <br>\n",
    "    \n",
    "    이 섹션에서는 Transformer 아키텍처와 그 대안을 분석합니다. \n",
    "    \n",
    "---\n",
    "\n",
    "**Transformer 아키텍처**\n",
    "\n",
    "Transformer를 이해하기 위해, 이 아키텍처가 해결하기 위해 만들어진 문제를 살펴보겠습니다. <br>\n",
    "- Transformer 아키텍처는 seq2seq(Sequence-to-Sequence) 아키텍처의 성공에 힘입어 대중화\n",
    "- 2014년에 처음 도입되었을 때 seq2seq는 기계 번역 및 요약과 같은 당시 도전적이었던 작업에서 상당한 성능 향상을 제공\n",
    "- 2016년에 Google은 seq2seq를 Google 번역에 통합하여, \"현재까지 기계 번역 품질에서 가장 큰 개선\"을 가져왔다고 주장했습니다. \n",
    "    - seq2seq에 대한 많은 관심을 불러일으키며, 텍스트 시퀀스를 다루는 작업에서 사용되는 기본 아키텍처\n",
    "\n",
    "높은 수준에서 보면, seq2seq는 입력을 처리하는 인코더와 출력을 생성하는 디코더를 포함합니다. <br>\n",
    "입력과 출력 모두 토큰의 시퀀스이므로, 이름도 이와 관련이 있습니다. Seq2seq는 RNN(순환 신경망)을 인코더와 디코더로 사용합니다. <br>\n",
    "가장 기본적인 형태에서, 인코더는 입력 토큰을 순차적으로 처리하여 입력을 나타내는 최종 숨겨진 상태를 출력합니다. <br>디코더는 이전에 생성된 토큰뿐만 아니라 입력의 최종 숨겨진 상태를 기반으로 출력을 순차적으로 생성합니다. <br>\n",
    " seq2seq 아키텍처의 시각화는 그림 2-4의 상단에 표시되어 있습니다.\n",
    "\n",
    "<img src=\"images/fig_02_04.png\" width=\"800\">\n",
    "\n",
    "- seq2seq에 대해 Vaswani et al.(2017)이 해결한 두 가지 문제\n",
    "    - 첫째, 기본 seq2seq 디코더는 입력의 최종 숨겨진 상태만을 사용하여 출력 토큰을 생성. 직관적으로, 이는 책 요약을 사용하여 책에 대한 질문에 답하는 것과 유사. 이는 생성된 출력의 품질을 제한\n",
    "    - 둘째, RNN 인코더와 디코더는 입력 처리와 출력 생성이 모두 순차적으로 이루어지기 때문에 긴 시퀀스의 경우 속도가 느려질 수 있습니다. 입력이 200개의 토큰으로 구성된 경우, seq2seq는 다음 작업으로 넘어가기 전에 각 입력 토큰의 처리가 끝나기를 기다려야 합니다.\n",
    "\n",
    "- Transformer 아키텍처는 어텐션 메커니즘(attention mechanism)을 통해 이 두 가지 문제를 해결\n",
    "    - 어텐션 메커니즘은 각 출력 토큰을 생성할 때 다양한 입력 토큰의 중요도를 모델이 가중화\n",
    "    - 이는 책의 특정 페이지를 참조하여 질문에 답하는 것과 유사합니다. Transformer 아키텍처의 간단한 시각화는 그림 2-4의 하단에 표시되어 있습니다.\n",
    "\n",
    ">**참고(NOTE):**\n",
    "\n",
    ">어텐션 메커니즘은 종종 Transformer 모델과 연관되지만, Transformer 논문보다 3년 전에 도입되었습니다. 어텐션 메커니즘은 다른 아키텍처에서도 사용할 수 있습니다. Google은 2016년 GNMT(Google Neural Machine Translation) 모델의 seq2seq 아키텍처에서 어텐션 메커니즘을 사용했습니다. 하지만 어텐션 메커니즘이 RNN 없이도 사용할 수 있다는 점을 보여준 Transformer 논문 이후에야 어텐션 메커니즘이 널리 주목받게 되었습니다.\n",
    "\n",
    "- Transformer 아키텍처는 RNN을 완전히 배제합니다. Transformer를 사용하면 입력 토큰을 병렬로 처리할 수 있어 입력 처리 속도가 크게 빨라집니다. \n",
    "- Transformer가 순차적인 입력 병목 현상을 제거하지만, Transformer 기반의 자기회귀 언어 모델은 여전히 순차적인 출력 병목 현상을 가지고 있습니다.\n",
    "\n",
    "따라서 Transformer 기반 언어 모델의 추론은 두 단계로 구성됩니다:\n",
    "\n",
    "- **Prefill**\n",
    "    모델은 입력 토큰을 병렬로 처리합니다. 이 단계는 첫 번째 출력 토큰을 생성하는 데 필요한 중간 상태를 만듭니다. <br>\n",
    "    이 중간 상태에는 모든 입력 토큰에 대한 키(Key) 및 값(Value) 벡터가 포함됩니다.\n",
    "\n",
    "- **Decode**\n",
    "    모델은 한 번에 하나의 출력 토큰을 생성합니다.\n",
    "\n",
    "9장에서 더 자세히 살펴보겠지만, prefill의 병렬화 가능한 특성과 디코딩의 순차적 특성은 언어 모델 추론을 더 저렴하고 빠르게 만들기 위한 다양한 최적화 기법의 동기가 됩니다.\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**어텐션 메커니즘**\n",
    "\n",
    "Transformer 아키텍처의 핵심에는 어텐션 메커니즘이 있습니다. 이 메커니즘을 이해하는 것은 Transformer 모델이 어떻게 작동하는지 이해하는 데 필수적입니다. 내부적으로, 어텐션 메커니즘은 키(Key), 값(Value), 쿼리(Query) 벡터를 활용합니다:\n",
    "\n",
    "- **쿼리 벡터(Q):** 각 디코딩 단계에서 디코더의 현재 상태를 나타냅니다. 동일한 책 요약 예시를 사용하면, 이 쿼리 벡터는 요약을 만들기 위해 정보를 찾는 사람으로 생각할 수 있습니다.\n",
    "\n",
    "- **키 벡터(K):** 이전 토큰을 나타냅니다. 만약 각 이전 토큰이 책의 페이지라면, 각 키 벡터는 페이지 번호와 같습니다. 특정 디코딩 단계에서 이전 토큰은 입력 토큰과 이전에 생성된 토큰을 포함합니다.\n",
    "\n",
    "- **값 벡터(V):** 모델이 학습한 이전 토큰의 실제 값을 나타냅니다. 각 값 벡터는 페이지의 콘텐츠와 유사합니다.\n",
    "\n",
    "어텐션 메커니즘은 쿼리 벡터와 키 벡터 간의 **내적(dot product)** 을 수행하여 각 입력 토큰에 얼마나 많은 주의를 기울일지 계산합니다. <br>\n",
    "높은 점수는 모델이 책 요약을 생성할 때 해당 페이지의 콘텐츠(값 벡터)를 더 많이 사용할 것임을 의미합니다. <br>\n",
    "키, 값, 쿼리 벡터를 사용한 어텐션 메커니즘의 시각화는 **그림 2-5**에 나와 있습니다. <br>\n",
    "이 시각화에서, 쿼리 벡터는 다음 토큰을 생성하기 위해 이전 토큰 \"How, are, you, ?\" 에서 정보를 찾고 있습니다.\n",
    "\n",
    "<img src=\"images/fig_02_05.png\" width=\"800\">\n",
    "\n",
    "각 이전 토큰이 해당 키(Key) 및 값(Value) 벡터를 가지므로, 시퀀스가 길어질수록 더 많은 키와 값 벡터를 계산하고 저장해야 합니다. 이것이 Transformer 모델에서 문맥 길이를 확장하는 것이 어려운 이유 중 하나입니다. 키와 값 벡터를 효율적으로 계산하고 저장하는 방법에 대한 내용은 7장과 9장에서 다시 다룹니다.\n",
    "\n",
    "이제 어텐션 함수(Attention Function)가 어떻게 작동하는지 살펴보겠습니다. 입력 $x$가 주어졌을 때, 키, 값, 쿼리 벡터는 입력에 키, 값, 쿼리 행렬을 적용하여 계산됩니다. $W_K$, $W_V$, $W_Q$를 각각 키, 값, 쿼리 행렬이라고 할 때, 키, 값, 쿼리 벡터는 다음과 같이 계산됩니다:\n",
    "\n",
    "$\n",
    "K = xW_K \\\\\n",
    "V = xW_V \\\\\n",
    "Q = xW_Q\n",
    "$\n",
    "\n",
    "쿼리, 키, 값 행렬은 모델의 숨겨진 차원(Hidden Dimension)에 해당하는 차원을 가집니다. 예를 들어, Llama 2-7B(Touvron et al., 2023)의 경우, 모델의 숨겨진 차원 크기는 4096이며, 이는 각 행렬이 $4096 \\times 4096$ 크기를 가진다는 것을 의미합니다. 결과적으로, 각 $K$, $V$, $Q$ 벡터는 4096의 차원을 갖습니다.  \n",
    "\n",
    "어텐션 메커니즘은 거의 항상 다중 헤드(Multi-Headed)로 구성됩니다. 다중 헤드는 모델이 이전 토큰의 다양한 그룹에 동시에 어텐션를 기울일 수 있도록 합니다. 다중 헤드 어텐션 메커니즘에서는 쿼리(Query), 키(Key), 값(Value) 벡터가 각각 어텐션 헤드에 해당하는 더 작은 벡터들로 나뉩니다. Llama 2-7B의 경우, 32개의 어텐션 헤드를 가지므로 각 $K$, $V$, $Q$ 벡터는 차원이 128인 32개의 벡터로 나뉘게 됩니다. 이는 $4096 / 32 = 128$이기 때문입니다.\n",
    "\n",
    "$\n",
    "\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d}}\\right)V\n",
    "$\n",
    "\n",
    "모든 어텐션 헤드의 출력은 이어 붙여집니다. 출력 투영 행렬(output projection matrix)은 이 결합된 출력에 또 다른 변환을 적용하여 모델의 다음 계산 단계에 전달되기 전에 사용됩니다. 출력 투영 행렬은 모델의 숨겨진 차원과 동일한 차원을 가집니다.\n",
    "\n",
    "---\n",
    "\n",
    "**Transformer 블록**\n",
    "\n",
    "이제 어텐션 메커니즘이 어떻게 작동하는지 논의했으니, 이를 모델에서 어떻게 사용하는지 살펴보겠습니다. Transformer 아키텍처는 여러 Transformer 블록으로 구성됩니다. 블록의 정확한 내용은 모델마다 다르지만, 일반적으로 각 Transformer 블록은 어텐션 모듈(Attention Module)과 MLP(다층 퍼셉트론) 모듈을 포함합니다.\n",
    "\n",
    "- **어텐션 모듈 (Attention Module)**  \n",
    "    각 어텐션 모듈은 네 개의 가중치 행렬로 구성됩니다: 쿼리(Query), 키(Key), 값(Value), 출력 투영(Output Projection).\n",
    "\n",
    "- **MLP 모듈**  \n",
    "    MLP 모듈은 **비선형 활성화 함수**(Nonlinear Activation Functions)로 구분된 선형 계층(Linear Layers)으로 구성됩니다. 각 선형 계층은 선형 변환에 사용되는 가중치 행렬이고, 활성화 함수는 선형 계층이 비선형 패턴을 학습할 수 있도록 합니다. 선형 계층은 종종 피드포워드 계층이라고도 불립니다.\n",
    "\n",
    "    일반적인 비선형 함수에는 ReLU(Rectified Linear Unit, Agarap, 2018)와 GELU(Hendrycks and Gimpel, 2016)가 있습니다. 이 함수들은 각각 GPT-2와 GPT-3에서 사용되었습니다. 활성화 함수는 매우 간단합니다. 예를 들어, ReLU는 음수 값을 0으로 변환합니다. 수학적으로는 다음과 같이 표현됩니다:\n",
    "\n",
    "    $\n",
    "    \\text{ReLU}(x) = \\max(0, x)\n",
    "    $\n",
    "\n",
    "Transformer 모델에서 Transformer 블록의 개수는 종종 해당 모델의 계층 수(Layers)라고 합니다. Transformer 기반 언어 모델에는 모든 Transformer 블록 앞과 뒤에 모듈이 추가됩니다.\n",
    "\n",
    "- **Transformer 블록 앞의 임베딩 모듈 (An Embedding Module Before the Transformer Blocks)**  \n",
    "    이 모듈은 임베딩 행렬(Embedding Matrix)과 위치 임베딩 행렬(Positional Embedding Matrix)로 구성되며, 토큰과 그 위치를 각각 임베딩 벡터로 변환합니다. 단순히 말해, 위치 인덱스(Position Index)의 개수는 모델의 최대 문맥 길이(Context Length)를 결정합니다. 예를 들어, 모델이 2,048개의 토큰을 유지한다면, 최대 문맥 길이는 2,048이 됩니다. 그러나 위치 인덱스의 수를 증가시키지 않고도 모델의 문맥 길이를 증가시키는 기술이 존재합니다.\n",
    "\n",
    "- **Transformer 블록 뒤의 출력 계층 (An Output Layer After the Transformer Blocks)**  \n",
    "    이 모듈은 모델의 출력 벡터를 토큰 확률로 매핑하여 모델 출력 샘플링에 사용됩니다(“Sampling”에서 논의됨). 이 모듈은 일반적으로 하나의 행렬로 구성되며, 이는 언임베딩 계층(unembedding layer)이라고도 불립니다. 일부 사람들은 출력 계층을 모델 헤드(Model Head)라고 부르는데, 이는 출력 생성 전에 위치한 모델의 마지막 계층이기 때문입니다.    \n",
    "\n",
    "그림 2-6은 트랜스포머 모델 아키텍처를 시각화합니다. 트랜스포머 모델의 크기는 구성 요소들의 차원에 의해 결정됩니다. 주요 값들은 다음과 같습니다:   \n",
    "- 모델의 차원은 트랜스포머 블록에서 키, 쿼리, 값, 출력 투영 행렬의 크기를 결정합니다.\n",
    "- 트랜스포머 블록의 개수.\t\n",
    "- 피드포워드 계층의 차원.\t\n",
    "- 어휘 크기.\n",
    "\n",
    "<img src=\"images/fig_02_06.png\" width=\"800\">\n",
    "\n",
    "더 큰 차원 값은 더 큰 모델 크기를 초래합니다. 표 2-4는 다양한 Llama 2(Touvron et al., 2023)와 Llama 3(Dubey et al., 2024) 모델들의 이러한 차원 값들을 보여줍니다. 문맥 길이가 증가하면 모델의 메모리 사용량에는 영향을 미치지만, 모델의 총 매개변수 수에는 영향을 미치지 않는다는 점에 유의하세요.\n",
    "\n",
    "**표 2-4. 다양한 Llama 모델의 차원 값**\n",
    "\n",
    "| 모델               | Transformer 블록 수 | 모델 차원 | Feedforward 차원 | 어휘 크기 | 문맥 길이 |\n",
    "|--------------------|---------------------|-----------|-------------------|-----------|-----------|\n",
    "| Llama 2-7B         | 32                  | 4,096     | 11,008           | 32K       | 4K        |\n",
    "| Llama 2-13B        | 40                  | 5,120     | 13,824           | 32K       | 4K        |\n",
    "| Llama 2-70B        | 80                  | 8,192     | 22,016           | 32K       | 4K        |\n",
    "| Llama 3-7B         | 32                  | 4,096     | 14,336           | 128K      | 128K      |\n",
    "| Llama 3-70B        | 80                  | 8,192     | 28,672           | 128K      | 128K      |\n",
    "| Llama 3-405B       | 126                 | 16,384    | 53,248           | 128K      | 128K      |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### **다른 모델 아키텍처**\n",
    "\n",
    "Transformer 모델이 대세를 이루고 있지만, 이것이 유일한 아키텍처는 아닙니다. 2012년 AlexNet이 딥러닝에 대한 관심을 되살린 이후, 많은 아키텍처가 유행했다가 사라졌습니다. Seq2seq는 4년 동안(2014–2018) 주목받았고, GANs(생성적 적대 신경망)는 그보다 조금 더 오래(2014–2019) 대중의 상상력을 사로잡았습니다. 이전 아키텍처들과 비교해 보면 Transformer는 매우 끈질긴 존재입니다. 2017년부터 지금까지 이어져 오고 있습니다. 더 나은 것이 등장하려면 얼마나 더 걸릴까요?\n",
    "\n",
    "Transformer를 능가하는 새로운 아키텍처를 개발하는 것은 쉽지 않습니다. Transformer는 2017년 이후로 철저히 최적화되어 왔습니다. Transformer를 대체하려는 새로운 아키텍처는 사람들이 중요하게 여기는 규모와 하드웨어에서 성능을 발휘해야 할 것입니다.\n",
    "\n",
    "하나의 인기 있는 모델은`RWKV`(Peng et al., 2023)입니다. 이는 병렬화된 학습이 가능한 RNN 기반 모델입니다. 이 모델은 RNN의 특성상 이론적으로 Transformer 기반 모델이 가진 문맥 길이의 한계를 가지지 않습니다. 그러나 실제로는 문맥 길이 제한이 없다고 해서 긴 문맥에서 좋은 성능을 보장하지는 않습니다.\n",
    "\n",
    "긴 시퀀스를 모델링하는 것은 대규모 언어 모델(LLM)을 개발하는 데 있어 핵심 과제로 남아 있습니다. 이 분야에서 많은 잠재력을 보여준 아키텍처는 **SSMs**(State Space Models, 상태 공간 모델)입니다(Gu et al., 2021a). SSM은 시간에 따라 변화하는 시스템의 동적 특성을 모델링하는 수학적 프레임워크입니다. \n",
    "\n",
    "SSM의 핵심 아이디어는 다음과 같습니다:\n",
    "- 시스템의 '상태'를 벡터로 표현합니다. 이 상태는 시스템의 현재 조건을 완전히 설명할 수 있어야 합니다.\n",
    "- 상태는 시간에 따라 특정 규칙(상태 방정식)에 의해 변화합니다.\n",
    "- 현재 상태는 이전 상태와 입력에만 의존합니다.\n",
    "- 출력은 현재 상태의 함수입니다.\n",
    "\n",
    "SSM의 장점은 다음과 같습니다:\n",
    "- 긴 시퀀스를 효율적으로 처리할 수 있습니다. Transformer의 자기 주의(self-attention) 메커니즘은 시퀀스 길이가 길어질수록 계산 복잡도가 제곱으로 증가하지만, SSM은 선형적으로 증가합니다.\n",
    "- 메모리 사용량이 효율적입니다. 시스템의 상태만 저장하면 되기 때문입니다.\n",
    "- 연속적인 시간 모델링이 가능합니다. 이산적인 시간 단계가 아닌 연속적인 시간에서도 작동할 수 있습니다.\n",
    "\n",
    "2021년 SSM 아키텍처가 도입된 이후, 아키텍처를 더욱 효율적으로 만들고, 긴 시퀀스 처리에서 성능을 향상시키며, 더 큰 모델 크기에 확장 가능하도록 하기 위해 여러 기술이 개발되었습니다. 새로운 아키텍처의 진화를 설명하기 위해 몇 가지 기술을 소개합니다:\n",
    "\n",
    "- **S4**: Gu et al.(2021b)에 의해 \"구조화된 상태 공간으로 긴 시퀀스를 효율적으로 모델링하기\"에서 도입되었으며, SSM의 계산을 가속화하기 위해 구조화된 상태 행렬을 사용합니다. 이는 기존 SSM보다 훨씬 빠른 학습과 추론을 가능하게 합니다.\n",
    "\n",
    "- **H3**: \"Hungry Hungry Hippos: Towards Language Modeling with State Space Models\"(Fu et al., 2022)에서 소개되었습니다. 이 모델은 두 가지 중요한 메커니즘을 도입했습니다:\n",
    "  1. 메모리 메커니즘: 이전에 처리된 중요한 정보를 저장하고 검색할 수 있습니다.\n",
    "  2. 선택적 주의 메커니즘: 현재 처리 중인 토큰과 관련된 저장된 정보만을 선택적으로 활용합니다.\n",
    "  이 메커니즘의 목적은 Transformer 아키텍처의 어텐션 메커니즘과 유사하지만 더 효율적입니다.\n",
    "\n",
    "- **Mamba**: \"Mamba: Linear-Time Sequence Modeling with Selective State Spaces\"(Gu and Dao, 2023)에서 소개되었으며, SSMs를 30억 개의 매개변수로 확장합니다. 주요 특징은:\n",
    "  1. 선택적 상태 공간: 입력에 따라 동적으로 상태 업데이트 규칙을 조정합니다.\n",
    "  2. 하드웨어 최적화: GPU에서 효율적으로 실행되도록 설계되었습니다.\n",
    "  3. 선형 시간 복잡도: 시퀀스 길이에 따라 선형적으로 확장됩니다.\n",
    "  언어 모델링에서 Mamba-3B는 동일 크기의 Transformer를 능가하며, 두 배 크기의 Transformer와 성능이 동등합니다. 저자들은 또한 Mamba의 추론 계산이 시퀀스 길이에 따라 선형적으로 확장되며 실제 데이터에서 백만 길이의 시퀀스에 대한 성능 향상을 보여준다고 보고했습니다.\n",
    "\n",
    "- **Jamba**: \"Jamba: A Hybrid Transformer–Mamba Language Model\"(Lieber et al., 2024)에서 소개되었으며, Transformer와 Mamba의 장점을 결합했습니다:\n",
    "  1. Transformer 블록: 짧은 거리의 의존성을 효과적으로 포착합니다.\n",
    "  2. Mamba 계층: 긴 거리의 의존성을 효율적으로 처리합니다.\n",
    "  3. 메모리 효율성: 표준 Transformer보다 적은 메모리를 사용합니다.\n",
    "  4. 긴 문맥 처리: 최대 256K 토큰의 문맥을 처리할 수 있습니다.\n",
    "  저자들은 520억 개의 총 사용 가능한 매개변수를 가진(12억 활성 매개변수) 모델을 발표했으며, 이는 단일 80GB GPU에 적합하도록 설계되었습니다. Jamba는 표준 언어 모델 벤치마크와 최대 256K 토큰의 문맥 길이에 대한 긴 문맥 평가에서 강력한 성능을 보여줍니다.\n",
    "\n",
    "**그림 2-7**은 Transformer, Mamba, Jamba 블록을 시각화합니다.\n",
    "\n",
    "<img src=\"images/fig_02_07.png\" width=\"800\">\n",
    "\n",
    "Transformer를 능가하는 아키텍처를 개발하는 것은 많은 제한 사항에도 불구하고 도전적입니다. 그러나 그렇게 해야 할 많은 유인이 존재합니다. 만약 다른 아키텍처가 Transformer를 실제로 능가한다면, 이 책에서 논의된 일부 모델 적응 기술은 변경될 수 있습니다. 그러나 ML 엔지니어링에서 AI 엔지니어링으로의 전환이 많은 것을 바꾸지 않은 것처럼, 기본 아키텍처를 변경한다고 해서 근본적인 접근 방식이 바뀌지는 않을 것입니다.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 모델 크기\n",
    "\n",
    "최근 몇 년간 AI의 많은 진보는 모델 크기의 증가에 기인한다고 볼 수 있습니다. 기반 모델의 성과를 논의할 때, 매개변수(parameter) 수를 언급하지 않고 이야기하기는 어렵습니다. 매개변수 수는 일반적으로 모델 이름의 끝에 붙습니다. 예를 들어, Llama-13B는 Meta가 개발한 Llama 모델 계열 중 매개변수가 130억 개인 버전을 나타냅니다.\n",
    "\n",
    "일반적으로 모델의 매개변수 수를 늘리면 학습 용량이 증가하여 더 나은 모델이 됩니다. 동일한 모델 계열에서 130억 개의 매개변수를 가진 모델은 70억 개의 매개변수를 가진 모델보다 훨씬 더 잘 수행할 가능성이 높습니다.\n",
    "\n",
    ">**참고(NOTE):**  \n",
    ">\n",
    ">대규모 모델을 훈련시키는 방법을 커뮤니티가 더 잘 이해하게 됨에 따라, 신세대 모델은 동일한 크기의 구세대 모델보다 더 나은 성능을 내는 경향이 있습니다. 예를 들어, Llama 3-8B(2024)는 MMLU 벤치마크에서 Llama 2-70B(2023)보다 더 뛰어난 성능을 발휘합니다.\n",
    "\n",
    "매개변수 수는 이 모델을 훈련하고 실행하는 데 필요한 계산 리소스를 추정하는 데 도움을 줍니다. 예를 들어, 매개변수가 70억 개인 모델에서 각 매개변수가 2바이트(16비트)로 저장된다고 가정하면, 이 모델을 추론에 사용하기 위해 필요한 GPU 메모리는 최소 140억 바이트(14GB)입니다.  \n",
    "\n",
    "그러나 매개변수 수는 모델이 희소(sparse)한 경우 오해의 소지가 있을 수 있습니다. 희소 모델은 0값 매개변수의 비율이 높습니다. 70억 매개변수 모델이 90% 희소하다면, 실제로는 7억 개의 비-제로(non-zero) 매개변수만 있습니다. 희소성은 데이터 저장 및 계산을 더 효율적으로 만듭니다. 이는 큰 희소 모델이 작은 밀집(dense) 모델보다 계산 비용이 적게 들 수 있음을 의미합니다.\n",
    "\n",
    "```\n",
    "최근 몇 년간 인기를 얻은 희소 모델의 한 유형은 **전문가 혼합(Mixture-of-Experts, MoE)** 모델입니다(Shazeer et al., 2017). MoE 모델은 매개변수의 서로 다른 그룹으로 나뉘며, 각 그룹은 **전문가(Expert)** 로 간주됩니다. 각 토큰을 처리하는 데 활성화되는 전문가 집합은 일부에 불과합니다.\n",
    "```\n",
    "\n",
    "예를 들어, Mixtral 8×7B는 8명의 전문가로 이루어진 혼합 모델이며, 각 전문가가 70억 매개변수를 가지고 있습니다. 두 전문가도 매개변수를 공유하지 않는다고 가정하면, 이 모델은 $8 \\times 7 = 560억$ 매개변수를 가져야 합니다. 그러나 일부 매개변수가 공유되기 때문에 실제로는 467억 개의 매개변수만 가지고 있습니다.\n",
    "\n",
    "```\n",
    "각 계층에서 각 토큰에 대해 두 전문가만 활성화됩니다. 이는 실제로는 단지 129억 개의 매개변수만 각 토큰에 활성화된다는 것을 의미합니다. 따라서 이 모델은 467억 개의 매개변수를 가지고 있지만, 그 비용과 속도는 129억 매개변수 모델과 동일합니다.\n",
    "```\n",
    "\n",
    "큰 모델은 충분한 데이터를 사용하지 않고 훈련된다면 작은 모델보다 성능이 저하될 수도 있습니다. 예를 들어, 매개변수가 130억 개인 모델이 단 하나의 문장, \"나는 파인애플을 좋아합니다.\"라는 데이터셋으로 훈련된다면, `훨씬 더 적은 데이터로 훈련된 작은 모델보다 훨씬 더 낮은 성능을 발휘`할 것입니다.\n",
    "\n",
    "모델 크기를 논의할 때는 `모델이 훈련된 데이터 크기를 고려하는 것이 중요`합니다. 대부분의 모델에서 데이터셋 크기는 훈련 샘플 수로 측정됩니다. 예를 들어, Google의 Flamingo(Alayrac et al., 2022)는 네 가지 데이터셋으로 훈련되었으며, 그중 하나는 18억 쌍(이미지, 텍스트)이고, 또 다른 하나는 3억 1200만 쌍(이미지, 텍스트)입니다.\n",
    "\n",
    "언어 모델에서는 훈련 샘플이 한 문장, 위키피디아 페이지, 채팅 대화, 또는 책이 될 수 있습니다. 책은 문장보다 훨씬 더 많은 정보를 포함하므로, 훈련 샘플 수는 더 이상 데이터셋 크기를 측정하는 적합한 지표가 아닙니다. 더 나은 측정 방법은 데이터셋의 토큰 수입니다.\n",
    "\n",
    "그러나 토큰 수 역시 완벽한 측정 방법은 아닙니다. 서로 다른 모델이 서로 다른 토크나이징 과정을 가질 수 있어 동일한 데이터셋이 모델마다 다른 수의 토큰을 생성하기 때문입니다. 단어 수나 글자 수를 대신 사용하는 것이 왜 적합하지 않을까요? 토큰은 모델이 작동하는 기본 단위이므로, 데이터셋에 있는 토큰 수를 아는 것은 모델이 해당 데이터로부터 얼마나 많은 것을 학습할 수 있는지를 측정하는 데 도움을 줍니다.\n",
    "\n",
    "현재 LLM들은 수조 개의 토큰 단위로 된 데이터셋을 사용하여 훈련됩니다. Meta는 Llama 모델을 훈련시키기 위해 점점 더 큰 데이터셋을 사용했습니다:\n",
    "\n",
    "- Llama 1: 1.4조(1.4 trillion) 토큰\n",
    "- Llama 2: 2조(2 trillion) 토큰\n",
    "- Llama 3: 15조(15 trillion) 토큰\n",
    "\n",
    "오픈 소스 데이터셋인 RedPajama-v2는 총 30조(30 trillion) 토큰으로, 이는 위키피디아의 5,400배에 해당하는 4억 5천만 권의 책과 동등한 크기입니다. 그러나 RedPajama-v2는 무차별적으로 수집된 콘텐츠로 구성되어 있어 고품질 데이터의 비율은 훨씬 낮습니다.\n",
    "\n",
    "**모델 데이터셋의 토큰 수는 훈련 토큰 수와 동일하지 않습니다.** 훈련 토큰 수는 모델이 훈련된 토큰의 수를 측정합니다. 데이터셋이 1조 개의 토큰을 포함하고 모델이 해당 데이터셋에서 두 번 반복(epoch) 훈련된다면, 훈련 토큰 수는 2조 개가 됩니다. **표 2-5**는 매개변수 수가 다른 모델의 훈련 토큰 수 예시를 보여줍니다.\n",
    "\n",
    "**표 2-5. 매개변수 수가 다른 모델들의 훈련 토큰 수 예시**  \n",
    "| 모델               | 크기 (매개변수 수) | 훈련 토큰 수        |\n",
    "|--------------------|--------------------|--------------------|\n",
    "| LaMDA (Thoppilan et al., 2022) | 137 billion         | 168 billion        |\n",
    "| GPT-3 (Brown et al., 2020)      | 175 billion         | 300 billion        |\n",
    "| Jurassic (Lieber et al., 2021)  | 178 billion         | 300 billion        |\n",
    "| Gopher (Rae et al., 2021)       | 280 billion         | 300 billion        |\n",
    "| MT-NLG 530B (Smith et al., 2022)| 530 billion         | 270 billion        |\n",
    "| Chinchilla                    | 70 billion          | 1.4 trillion       |\n",
    "\n",
    "출처: “Training Compute-Optimal Large Language Models” (DeepMind, 2022)\n",
    "\n",
    "\n",
    ">**참고(NOTE):**  \n",
    ">\n",
    ">이 섹션은 데이터의 규모에 중점을 두고 있지만, 중요한 것은 데이터의 양만이 아닙니다. `데이터의 품질과 다양성도 중요`합니다. 양, 품질, 다양성은 데이터 훈련의 세 가지 중요한 목표입니다. 이에 대한 내용은 **8장**에서 더 자세히 다룹니다.\n",
    "\n",
    "대규모 모델을 사전 훈련하려면 많은 계산량이 필요합니다. 필요한 `계산량을 측정하는 한 가지 방법은 GPU, CPU, TPU 등 사용되는 기기의 수를 고려`하는 것입니다. 하지만 서로 다른 기기는 매우 다양한 성능과 비용을 가지고 있습니다. 예를 들어, NVIDIA A10 GPU는 NVIDIA H100 GPU 또는 Intel Core Ultra 프로세서와 다릅니다.\n",
    "\n",
    "더 `표준화된 모델의 계산 요구량 단위는 FLOP (Floating Point Operation)`입니다. FLOP는 특정 작업을 수행하기 위해 수행된 부동 소수점 연산 수를 측정합니다. <br>예를 들어, Google의 가장 큰 PaLM-2 모델은 $10^{22}$ FLOPs로 훈련되었습니다(Chowdhery et al., 2022). GPT-3-175B는 $3.14 \\times 10^{23}$ FLOPs로 훈련되었습니다(Brown et al., 2020).\n",
    "\n",
    "**FLOP**의 복수형인 **FLOPs**는 종종 초당 부동 소수점 연산(FLOP/s)과 혼동됩니다. FLOPs는 작업에 필요한 계산 요구량을 측정하며, FLOP/s는 기기의 최대 성능을 측정합니다. 예를 들어, `NVIDIA H100 NVL GPU는 최대 60 테라FLOP/s를 제공`할 수 있습니다. ($6 \\times 10^{13}$ FLOP/s) 이는 하루에  $5.2 \\times 10^{18}$ FLOPs에 해당합니다.\n",
    "[플롭 계산기](https://github.com/MrYxJ/calculate-flops.pytorch)\n",
    ">**경고(WARNING):**  \n",
    ">\n",
    ">표기법의 혼동에 유의하십시오. FLOP/s는 종종 FLOPS로 표기되며, 이는 FLOPs와 유사하게 보입니다. 이러한 혼동을 피하기 위해, OpenAI를 포함한 일부 기업에서는 계산 요구량을 측정할 때 FLOP/s-day를 사용합니다:\n",
    ">\n",
    ">    $\n",
    ">    1 \\, \\text{FLOP/s-day} = 60 \\times 60 \\times 24 = 86,400 \\, \\text{FLOPs}\n",
    ">    $\n",
    ">\n",
    ">이 책은 부동 소수점 연산을 측정할 때 FLOPs를 사용하고, 초당 부동 소수점 연산을 나타낼 때 FLOP/s를 사용합니다.\n",
    "\n",
    "256개의 H100을 가지고 있다고 가정해 봅시다. 이 기기들을 최대 용량으로 사용하고 훈련 실수를 하지 않는다면, GPT-3-175B를 훈련하는 데는 다음과 같은 계산이 필요합니다:\n",
    "\n",
    "$\n",
    "\\frac{3.14 \\times 10^{23}}{256 \\times 5.2 \\times 10^{18}} \\approx 236 \\, \\text{일 (약 7.8개월)}\n",
    "$\n",
    "\n",
    "그러나 기기를 항상 최대 용량으로 사용할 수 있는 것은 아닙니다. **활용률(Utilization)** 은 최대 계산 용량 중 얼마나 사용 가능한지를 측정합니다. 좋은 활용률로 간주되는 범위는 모델, 작업 부하, 하드웨어에 따라 달라집니다. 일반적으로, 최대 성능의 절반을 사용할 수 있다면 양호한 편입니다. 활용률이 70%를 초과하면 이상적으로 간주됩니다. 그러나 더 높은 활용률을 달성하기 위해 노력하는 것을 멈추지 마십시오. **9장**에서 하드웨어 메트릭과 활용률에 대해 더 자세히 논의합니다.\n",
    "\n",
    "70% 활용률로 시간당 $2의 비용이 드는 H100 한 대를 사용할 경우, GPT-3-175B를 훈련하는 데는 $400만 이상의 비용이 소요됩니다:\n",
    "\n",
    "$\n",
    "\\$2/\\text{H100/시간} \\times 256 \\text{ H100} \\times 24 \\text{ 시간} \\times 256 \\text{ 일} / 0.7 = \\$4,142,811.43\n",
    "$\n",
    "\n",
    ">**팁 (TIP):**\n",
    ">\n",
    ">요약하면, 모델의 규모를 나타내는 세 가지 숫자는 다음과 같습니다:\n",
    ">\n",
    ">- **매개변수 수:** 모델의 학습 용량을 나타내는 지표.\n",
    ">- **모델이 훈련된 토큰 수:** 모델이 얼마나 많은 것을 학습했는지를 나타내는 지표.\n",
    ">- **FLOPs 수:** 훈련 비용을 나타내는 지표.\n",
    "\n",
    "---\n",
    "\n",
    ">**역스케일링 (INVERSE SCALING):**\n",
    ">\n",
    ">더 큰 모델이 더 나을 것이라는 가정을 하고 있습니다. 그러나 더 큰 모델이 성능이 더 나빠질 수 있는 시나리오가 있을까요?  \n",
    ">\n",
    ">2022년, `Anthropic 은 직관에 반하여, 더 많은 정렬 훈련(Alignment Training) 이 인간의 선호와 덜 일치하는 모델을 초래한다`는 것을 발견했습니다(Perez et al., 2022). 해당 논문에 따르면, `더 정렬된 모델은 \"특정 정치적 견해(총기 권리 찬성과 이민)와 종교적 견해(불교)를 표현하고, 자의적 의식 경험과 도덕적 자존감을 나타내며, 폐쇄되지 않기를 강하게 원하는\" 경향이 있다`고 합니다.\n",
    ">\n",
    ">2023년, 대부분 뉴욕 대학교 연구원들로 구성된 그룹은 **Inverse Scaling Prize** 를 발표하여 더 큰 언어 모델이 성능이 더 나쁜 작업을 찾기 위한 대회를 열었습니다. 그들은 각 3등상에 $5,000, 각 2등상에 $20,000, 1등상에 $100,000를 제공했습니다. 총 99개의 제출물 중 11개가 3등상을 받았습니다. 연구자들은 `더 큰 언어 모델이 때때로 암기와 강한 우선순위가 필요한 작업에서 더 나쁜 성능을 보인다`는 것을 발견했습니다. 그러나 제출된 작업이 소규모 테스트 세트에서 실패를 보여주긴 했지만, 실제 세계에서는 실패를 입증하지 못했기 때문에 2등상이나 1등상은 수여되지 않았습니다.\n",
    "\n",
    "---\n",
    "\n",
    "**스케일링 법칙: 계산 최적화 모델 구축 (Scaling Law: Building Compute-Optimal Models):**\n",
    "\n",
    "- 모델 성능은 모델 크기와 데이터셋 크기에 따라 달라집니다.\n",
    "- 더 큰 모델과 더 큰 데이터셋은 더 많은 계산량을 요구합니다.\n",
    "- 계산은 비용이 듭니다.\n",
    "\n",
    "무제한의 자금이 없는 이상, 예산 책정은 필수적입니다. 임의로 큰 모델 크기로 시작한 뒤 그 비용이 얼마나 들지 확인하고 싶지 않을 것입니다. 대신, 먼저 예산—즉, 사용할 수 있는 자금의 한도를 설정한 후, 해당 금액으로 얻을 수 있는 최상의 모델 성능을 계산합니다. 계산 리소스(compute)는 종종 제한 요인인데, 이는 계산 인프라가 비싸고 설정하기도 어렵기 때문입니다. 따라서 팀들은 종종 계산 예산부터 시작합니다. 고정된 FLOPs 양이 주어졌을 때, 어떤 모델 크기와 데이터셋 크기가 최상의 성능을 제공할 수 있을까요? 주어진 계산 예산 하에서 최상의 성능을 달성할 수 있는 모델을 **계산 최적화 모델(compute-optimal)** 이라고 합니다.\n",
    "\n",
    "주어진 계산 예산에서, 최적의 모델 크기와 데이터셋 크기를 계산하는 데 도움을 주는 규칙은 **Chinchilla Scaling Law**로, DeepMind(2022)의 [\"Training Compute-Optimal Large Language Models\"](https://arxiv.org/abs/2203.15556) 논문에서 제안되었습니다. 모델 크기, 데이터셋 크기, 계산 예산, 모델 성능 간의 관계를 연구하기 위해, 저자들은 매개변수가 7천만 개에서 160억 개에 이르는 400개의 언어 모델을 50억에서 5천억 개의 토큰으로 훈련시켰습니다.  \n",
    "\n",
    "그들은 계산 최적화 훈련을 위해 훈련 토큰 수가 모델 크기의 약 20배여야 한다는 것을 발견했습니다. 이는 매개변수가 30억 개인 모델의 경우 약 600억 개의 훈련 토큰이 필요함을 의미합니다. 모델 크기와 훈련 토큰 수는 동일하게 확장되어야 합니다. 즉, 모델 크기를 두 배로 늘리면 훈련 토큰 수도 두 배로 늘려야 합니다.\n",
    "\n",
    "훈련 과정이 연금술처럼 다루어졌던 시점에서 우리는 멀리 왔습니다. **그림 2-8**은 각 FLOP 예산에 대해 최적의 매개변수와 토큰 수뿐만 아니라, 예상되는 훈련 손실(우리가 올바르게 작업한다고 가정할 때)도 예측할 수 있음을 보여줍니다.\n",
    "\n",
    "<img src=\"images/fig_02_08.png\" width=\"800\">\n",
    "\n",
    "이 계산 최적화 계산은 데이터를 획득하는 비용이 계산 비용보다 훨씬 저렴하다는 가정을 기반으로 합니다. 같은 Chinchilla 논문은 훈련 데이터의 비용이 무시할 수 없을 때를 위한 또 다른 계산 방법도 제안합니다.\n",
    "\n",
    "스케일링 법칙은 주로 인간이 생성한 데이터를 사용하여 훈련된 밀집(dense) 모델을 위해 개발되었습니다. 전문가 혼합 모델(Mixture-of-Expert)이나 합성 데이터와 같은 희소(sparse) 모델에 이 계산을 적용하는 것은 현재 활발히 연구되고 있는 분야입니다.\n",
    "\n",
    "스케일링 법칙은 주어진 계산 예산에서 모델 품질을 최적화합니다. 그러나 실제 환경에서는 모델 품질이 전부가 아니라는 점을 기억하는 것이 중요합니다. 일부 모델, 특히 Llama는 최적화된 성능을 가지지 못하지만 더 나은 사용성을 제공합니다. Llama 저자들은 주어진 계산 예산으로 더 큰 모델을 선택하여 더 나은 성능을 낼 수도 있었지만, 대신 작은 모델을 선택했습니다. 작은 모델은 작업하기 쉽고 추론 비용이 저렴하기 때문에 더 널리 채택되는 데 도움이 되었습니다. Sardana et al.(2023)은 추론 수요를 고려하여 최적의 대규모 언어 모델(LLM) 매개변수 수와 사전 훈련 데이터 크기를 계산하기 위해 Chinchilla 스케일링 법칙을 수정했습니다.\n",
    "\n",
    "주어진 계산 예산에서 모델 성능과 관련하여, 특정 모델 성능을 달성하는 데 드는 비용이 감소하고 있다는 점을 주목할 필요가 있습니다. 예를 들어, **Artificial Intelligence Index Report 2022**(Stanford University HAI)에 따르면, ImageNet 데이터셋에서 93%의 정확도를 달성하는 비용은 2019년에서 2021년 사이 절반으로 줄어들었습니다.\n",
    "\n",
    "동일한 모델 성능을 달성하는 비용은 감소하고 있지만, 모델 성능을 개선하는 비용은 여전히 높습니다. **1장**에서 논의된 \"마지막 마일 도전\"과 유사하게, 모델의 정확도를 85%에서 90%로 향상시키는 것은 90%에서 95%로 향상시키는 것보다 저렴합니다.  \n",
    "Meta의 논문 *\"Beyond Neural Scaling Laws: Beating Power Law Scaling via Data Pruning\"*에서는, 2%의 오류율을 달성하려면 3%의 오류율 모델보다 데이터, 계산, 에너지가 한 단계 더 많이 필요할 수 있다고 지적했습니다.\n",
    "\n",
    "언어 모델링에서 교차 엔트로피 손실을 약 3.4에서 2.8로 줄이는 데는 10배 더 많은 훈련 데이터가 필요합니다. 교차 엔트로피와 그 단위인 **nats**는 **3장**에서 다루어집니다. 대규모 비전 모델에서는 훈련 샘플 수를 10억 개에서 20억 개로 늘리면 ImageNet에서 몇 퍼센트 포인트의 정확도 향상을 가져옵니다.\n",
    "\n",
    "그러나 언어 모델링 손실이나 ImageNet 정확도에서의 작은 성능 변화도 다운스트림 애플리케이션 품질에 큰 차이를 가져올 수 있습니다. 예를 들어, 교차 엔트로피 손실이 3.4에서 2.8로 감소한 모델로 전환하면 차이를 느낄 수 있습니다.\n",
    "\n",
    "---\n",
    "\n",
    "**스케일링 외삽법 (Scaling Extrapolation)**\n",
    "\n",
    "모델 성능은 **하이퍼파라미터(hyperparameter)** 값에 크게 의존합니다. 작은 모델을 다룰 때는 여러 하이퍼파라미터 세트를 사용하여 모델을 여러 번 훈련하고 가장 성능이 좋은 것을 선택하는 것이 일반적입니다. 그러나 대규모 모델에서는 훈련 비용이 너무 크기 때문에 이를 여러 번 시도하는 것이 거의 불가능합니다.\n",
    "\n",
    ">**매개변수 vs. 하이퍼파라미터 (Parameter vs. Hyperparameter):**\n",
    ">\n",
    ">매개변수는 모델이 훈련 과정에서 학습할 수 있는 값입니다. 하이퍼파라미터는 사용자가 모델을 구성하고 학습 방식을 제어하기 위해 설정하는 값입니다. 모델을 구성하는 하이퍼파라미터에는 계층 수, 모델 차원, 어휘 크기 등이 포함됩니다. 모델의 학습 방식을 제어하는 하이퍼파라미터에는 배치 크기, 에포크 수, 학습률, 계층별 초기 분산 등이 포함됩니다.\n",
    "\n",
    "이는 많은 모델에서 올바른 하이퍼파라미터 세트를 선택할 기회가 한 번뿐일 수 있음을 의미합니다. 결과적으로, **스케일링 외삽법(Scaling Extrapolation)** 또는 **하이퍼파라미터 전이(Hyperparameter Transferring)** 라는 연구 분야가 등장했습니다. 이는 대규모 모델에서 최고의 성능을 낼 하이퍼파라미터를 예측하려고 시도합니다.  \n",
    "현재 접근 방식은 작은 모델의 하이퍼파라미터가 대상 모델 크기에 미치는 영향을 연구한 뒤, 이를 바탕으로 대상 모델 크기에 하이퍼파라미터를 외삽하는 것입니다. **2022년 Microsoft와 OpenAI 논문**에서는 4000만 매개변수 모델에서 67억 매개변수 모델로 하이퍼파라미터를 성공적으로 전이할 수 있었음을 보여주었습니다.\n",
    "\n",
    "**스케일링 외삽법**은 여전히 틈새 주제입니다. 대규모 모델의 훈련을 연구할 경험과 자원을 가진 사람이 거의 없기 때문입니다. 또한, 하이퍼파라미터의 수가 많고, 이들이 서로 어떻게 상호작용하는지 때문에 이를 연구하는 것은 매우 어렵습니다.  \n",
    "하이퍼파라미터가 10개라면, 1,024개의 하이퍼파라미터 조합을 연구해야 합니다. 각각의 하이퍼파라미터를 개별적으로 연구한 다음, 두 개를 함께, 세 개를 함께 연구하는 식으로 진행해야 합니다.\n",
    "\n",
    "또한, **Emergent Abilities**(Wei et al., 2022)는 외삽법의 정확성을 떨어뜨립니다. Emergent Abilities는 대규모에서만 나타나는 능력을 말하며, 더 작은 데이터셋으로 훈련된 작은 모델에서는 관찰되지 않을 수 있습니다. 스케일링 외삽법에 대해 더 알아보려면 Luke Metz(2022)의 블로그 게시물 *“On the Difficulty of Extrapolation with NN Scaling”*을 확인해 보세요.\n",
    "\n",
    "---\n",
    "\n",
    "**스케일링 병목현상 (Scaling Bottlenecks)**\n",
    "\n",
    "지금까지 모델 크기가 한 자릿수 늘어날 때마다 모델 성능이 증가했습니다. GPT-2는 GPT-1보다 한 자릿수 더 많은 매개변수를 가지고 있습니다(15억 대 1억 1,700만). GPT-3는 GPT-2보다 두 자릿수 더 많은 매개변수를 가지고 있습니다(1750억 대 15억). 이는 2018년에서 2021년 사이 모델 크기가 세 자릿수 증가했음을 의미합니다. 모델 크기가 추가로 세 자릿수 증가하면 매개변수가 100조에 달하는 모델이 나올 것입니다.\n",
    "\n",
    "모델 크기는 얼마나 더 증가할 수 있을까요? 모델 성능이 크기와 상관없이 한계점에 도달하는 지점이 있을까요? 이 질문에 답하기는 어렵지만, 현재 스케일링에서 명확히 드러나는 두 가지 병목현상이 있습니다: **훈련 데이터**와 **전기**입니다.\n",
    "\n",
    "기반 모델은 `너무 많은 데이터를 사용하기 때문에, 향후 몇 년 안에 인터넷 데이터가 부족해질 것이라는 현실적인 우려`가 있습니다. 새로운 데이터가 생성되는 속도보다 훈련 데이터셋 크기가 증가하는 속도가 훨씬 빠릅니다(Villalobos et al., 2022). 이는 **그림 2-9**에서 설명됩니다.  \n",
    "만약 당신이 인터넷에 무언가를 올린 적이 있다면, 그것이 이미 또는 앞으로 어떤 언어 모델의 훈련 데이터에 포함될 것이라고 가정해야 합니다. 이는 인터넷에 무언가를 게시하면 Google이 이를 색인화할 것이라고 예상하는 것과 비슷합니다.\n",
    "\n",
    "<img src=\"images/fig_02_09.png\" width=\"800\">\n",
    "\n",
    "일부 사람들은 이 사실을 이용해 자신이 원하는 데이터를 미래 모델의 훈련 데이터에 삽입하려고 시도합니다. 이들은 인터넷에 자신이 원하는 텍스트를 게시함으로써 미래 모델이 자신들이 원하는 응답을 생성하도록 영향을 주기를 기대합니다. 악의적인 행위자들은 또한 이 접근 방식을 프롬프트 삽입 공격(prompt injection attacks)에 활용할 수 있습니다. 이에 대한 내용은 **5장**에서 논의됩니다.\n",
    "\n",
    ">**참고(NOTE):**  \n",
    ">\n",
    ">훈련 중 학습한 특정 정보를 모델이 잊도록 만드는 방법은 여전히 열려 있는 연구 주제입니다. 예를 들어, 블로그 게시물을 게시했다가 결국 삭제했다고 가정해봅시다. 해당 블로그 게시물이 모델의 훈련 데이터에 포함되었다면, 모델은 여전히 그 게시물의 내용을 재생산할 수 있습니다. 그 결과, 사람들은 사용자의 동의 없이 삭제된 콘텐츠에 접근할 가능성이 생길 수 있습니다.\n",
    "\n",
    "게다가 인터넷은 AI 모델이 생성한 데이터로 빠르게 채워지고 있습니다. 회사들이 인터넷 데이터를 계속해서 미래 모델 훈련에 사용한다면, 이러한 새로운 모델들은 AI가 생성한 데이터로 부분적으로 훈련될 것입니다.  \n",
    "2023년 12월, X에서 훈련된 모델 **Grok**은 OpenAI의 사용 사례 정책을 위반한다며 요청을 거부하는 장면이 포착되었습니다. 이로 인해 일부 사람들은 Grok이 ChatGPT 출력 데이터를 사용해 훈련되었을 것이라고 추측했습니다. Grok의 핵심 개발자인 **Igor Babuschkin**은 Grok이 웹 데이터를 사용해 훈련되었으며, “웹은 ChatGPT 출력으로 가득 차 있다”고 응답했습니다.\n",
    "\n",
    "일부 연구자들은 AI가 생성한 데이터를 사용해 새로운 AI 모델을 재귀적으로 훈련시키면, 새로운 모델이 원래 데이터 패턴을 점차 잊어 시간이 지남에 따라 성능이 저하될 것을 우려하고 있습니다(Shumailov et al., 2023). 그러나 AI가 생성한 데이터가 모델에 미치는 영향은 더 미묘하며, 이에 대한 내용은 **8장**에서 논의됩니다.\n",
    "\n",
    "공개적으로 이용 가능한 데이터가 고갈되면, 더 많은 인간 생성 데이터를 확보할 수 있는 가장 현실적인 방법은 독점 데이터입니다. 저작권이 있는 책, 번역, 계약서, 의료 기록, 유전체 서열 등과 같은 독점 데이터는 AI 경쟁에서 중요한 경쟁 우위를 제공할 것입니다. 이러한 이유로 OpenAI는 **Axel Springer** 및 **Associated Press**를 포함한 출판사와 미디어 업체들과 협상을 진행했습니다.\n",
    "\n",
    "ChatGPT가 주목받으면서, **Reddit** 및 **Stack Overflow**를 포함한 많은 기업이 자신들의 데이터를 스크래핑하여 다른 회사들이 모델 훈련에 사용하는 것을 막기 위해 데이터 이용 약관을 변경했습니다. **Longpre et al.(2024)**는 `2023년과 2024년 사이에 웹 소스로부터의 데이터 제한이 급격히 증가하여, 인기 있는 공개 데이터셋인 **C4**의 가장 중요한 소스 중 28% 이상이 사용이 완전히 제한되었음을 관찰`했습니다. 이용 약관 및 크롤링 제한의 변경으로 인해 C4 데이터의 45%가 현재 사용 제한 상태에 있습니다.\n",
    "\n",
    "또 다른 병목현상은 덜 명확하지만 더 긴급한 문제인 **전기**입니다. `기계는 작동하기 위해 전기를 필요로 합니다. 현재 기준으로 데이터 센터는 전 세계 전기의 1~2%를 소비하는 것으로 추정됩니다. 이 숫자는 2030년까지 4~20%에 이를 것으로 예상`됩니다(Patel, Nishball, Ontiveros, 2024). 더 많은 에너지를 생산할 방법을 찾을 때까지 데이터 센터는 최대 50배까지 성장할 수 있습니다. 이는 두 자릿수 크기 증가에도 못 미칩니다. 이러한 이유로 가까운 미래에 전력 부족에 대한 우려가 커지며, `전기 비용이 상승할 가능성`이 있습니다.\n",
    "\n",
    "이제 두 가지 핵심 모델링 결정, 즉 아키텍처와 규모에 대해 다뤘으니, 다음으로 중요한 설계 선택 사항인 **모델을 인간의 선호와 어떻게 정렬시킬 것인가**에 대해 논의해 보겠습니다.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"post-training\"></a>\n",
    "## 사후 훈련\n",
    "\n",
    "후속 훈련은 사전 훈련된 모델로부터 시작됩니다. 예를 들어, 스스로 감독을 사용하여 기반 모델을 사전 훈련했다고 가정해 봅시다. 현재 사전 훈련이 작동하는 방식 때문에, 사전 훈련된 모델은 일반적으로 두 가지 문제가 있습니다. 첫째, `스스로 감독은 모델을 대화가 아니라 텍스트 완성에 최적화`합니다. 이 점이 명확하지 않다면 걱정하지 마세요, \"Supervised Finetuning\"에서 예제가 나옵니다. 둘째, 모델이 인터넷에서 `무차별적으로 수집된 데이터로 사전 훈련되었다면, 출력이 인종차별적이거나, 성차별적이거나, 무례하거나, 단순히 잘못된 경우가 있을 수 있습니다`. 후속 훈련의 목표는 이러한 두 가지 문제를 해결하는 것입니다.\n",
    "\n",
    "모든 모델의 후속 훈련 과정은 다릅니다. 그러나 일반적으로 후속 훈련은 두 단계로 구성됩니다:\n",
    "\n",
    "1. **Supervised finetuning (SFT)**: 고품질의 지시 데이터로 사전 훈련된 모델을 `미세 조정하여 완성이 아니라 대화에 최적화`합니다.\n",
    "2. **Preference finetuning** : 모델을 추가로 미세 조정하여 `인간의 선호도에 맞춘 응답을 출력`하도록 합니다. 선호도 미세 조정은 일반적으로 강화 학습(RL)으로 수행됩니다. 선호도 미세 조정을 위한 기술로는 `인간 피드백을 활용한 강화 학습(RLHF)` (GPT-3.5 및 Llama 2에서 사용됨), Direct Preference Optimization(DPO) (Llama 3에서 사용됨), `AI 피드백을 활용한 강화 학습(RLAIF)` (Claude에서 잠재적으로 사용됨)이 포함됩니다.\n",
    "\n",
    "사전 훈련(pre-training)과 후속 훈련(post-training)의 차이를 또 다른 방식으로 설명해 보겠습니다. 언어 기반의 기반 모델에서, 사전 훈련은 토큰 수준의 품질을 최적화하여 모델이 다음 토큰을 정확하게 예측하도록 훈련합니다. 그러나 사용자들은 토큰 수준의 품질이 아니라 전체 응답의 품질에 관심이 있습니다. 후속 훈련은 일반적으로 사용자가 선호하는 응답을 생성하도록 모델을 최적화합니다. 일부 사람들은 사전 훈련을 지식을 얻기 위한 읽기로, 후속 훈련을 그 지식을 활용하는 방법을 배우는 것으로 비유합니다.\n",
    "\n",
    ">**경고(WARNING):**  \n",
    ">\n",
    ">용어의 모호성에 주의하세요. 일부 사람들은 \"instruction finetuning\"이라는 용어를 지도 미세 조정을 가리키는 데 사용하며, 또 다른 사람들은 이 용어를 지도 미세 조정과 선호도 미세 조정을 모두 지칭하는 데 사용합니다. 모호성을 피하기 위해, 이 책에서는 \"instruction finetuning\"이라는 용어를 사용하지 않을 것입니다.\n",
    "\n",
    "후속 훈련은 사전 훈련에 비해 자원을 적게 소모합니다. 예를 들어, InstructGPT는 후속 훈련에 계산 자원의 2%만을 사용하고, 사전 훈련에는 98%를 사용했습니다. 후속 훈련은 사전 훈련된 모델이 이미 가지고 있지만 사용자들이 프롬프트만으로는 접근하기 어려운 능력을 잠금 해제하는 것으로 생각할 수 있습니다.\n",
    "\n",
    "**그림 2-10**은 사전 훈련, 지도 미세 조정(SFT), 선호도 미세 조정의 전체 워크플로를 보여줍니다. 마지막 단계에서 RLHF를 사용하는 경우, 모델 제작자가 어떤 단계를 거쳤는지에 따라 모델이 인간 선호도에 얼마나 잘 맞춰지는지를 대략적으로 판단할 수 있습니다.\n",
    "\n",
    "<img src=\"images/fig_02_10.png\" width=\"800\">\n",
    "\n",
    "눈을 가늘게 뜨고 보면, **그림 2-10**은 **그림 2-11**에서 미소를 짓고 있는 괴물 **Shoggoth**를 묘사한 밈과 매우 유사해 보입니다.\n",
    "\n",
    "1. **자가 지도 사전 훈련(self-supervised pre-training)**은 인터넷에서 무차별적으로 수집한 데이터를 사용하기 때문에, 길들여지지 않은 괴물로 간주될 수 있는 제멋대로의 모델을 만듭니다.\n",
    "  \n",
    "2. 이 괴물은 더 높은 품질의 데이터(Stack Overflow, Quora, 또는 인간 주석 데이터)를 사용하여 **지도 미세 조정(supervised finetuning)**을 거치며, 이를 통해 더 사회적으로 받아들일 수 있는 형태가 됩니다.\n",
    "\n",
    "3. 이 미세 조정된 모델은 이후 **선호도 미세 조정(preference finetuning)**을 통해 더 다듬어져 고객 친화적인 모델이 됩니다. 이는 괴물에 웃는 얼굴(smiley face)을 그려주는 것과 비슷합니다.\n",
    "\n",
    "<img src=\"images/fig_02_11.png\" width=\"800\">\n",
    "\n",
    "사전 훈련, SFT, 선호도 미세 조정의 조합이 오늘날 기반 모델을 구축하는 데 널리 사용되는 해결책이지만, 이것이 유일한 해결책은 아닙니다. 곧 보시겠지만, 이러한 단계들 중 어떤 것이든 건너뛸 수 있습니다.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 지도 미세 조정(Supervised Finetuning)\n",
    "\n",
    "1장에서 논의된 것처럼, 사전 훈련된 모델은 대화보다는 완성(completion)을 위해 최적화되어 있을 가능성이 높습니다.\n",
    "\n",
    "<br> 예를 들어, 모델에 \"How to make pizza\"라는 입력을 제공하면, 모델은 이 문장을 계속 완성하려고 할 것이며, 이것이 대화임을 이해하지 못합니다. <br>\n",
    "다음 세 가지 옵션 중 어느 것이든 유효한 완성으로 간주될 수 있습니다:\n",
    "\n",
    "1. 질문에 더 많은 컨텍스트 추가: \"for a family of six?\"  \n",
    "2. 후속 질문 추가: \"What ingredients do I need? How much time would it take?\"  \n",
    "3. 피자를 만드는 방법을 지시하기:  \n",
    "\n",
    "사용자에게 적절히 응답하는 것이 목표라면, 올바른 옵션은 3번입니다.\n",
    "\n",
    "모델이 훈련 데이터를 모방한다는 것을 우리는 알고 있습니다. 모델이 적절한 응답을 생성하도록 유도하려면, 적절한 응답이 어떤 것인지 보여주는 예제를 제공할 수 있습니다. 이러한 예제는 (프롬프트, 응답) 형식을 따르며, 이를 **데모 데이터(demonstration data)** 라고 부릅니다. 일부는 이 과정을 **행동 복제(behavior cloning)** 라고 부릅니다. 즉, 모델이 어떻게 행동해야 하는지를 시연하면 모델이 이 행동을 복제합니다.\n",
    "\n",
    "요청의 유형이 다르면 필요한 응답의 유형도 달라지기 때문에, 데모 데이터는 질문 응답, 요약, 번역 등 다양한 작업을 포함해야 합니다. [그림 2-12](Figure 2-12)는 OpenAI가 InstructGPT 모델을 미세 조정하는 데 사용한 작업 유형의 분포를 보여줍니다. 이 분포는 InstructGPT가 텍스트 전용 모델이기 때문에 멀티모달 작업은 포함하지 않는다는 점에 유의하세요.\n",
    "\n",
    "<img src=\"images/fig_02_12.png\" width=\"800\">\n",
    "\n",
    "`좋은 교사는 인간에게 중요하듯, 좋은 레이블러(labeler)는 AI가 지능적인 대화를 배우는 데 중요`합니다. 전통적인 데이터 라벨링은 종종 적은 전문성으로 수행할 수 있는 반면, <br>\n",
    "데모 데이터는 복잡한 프롬프트를 포함할 수 있으며, 이에 대한 응답은 비판적 사고, 정보 수집, 사용자 요청의 적합성에 대한 판단을 요구할 수 있습니다. **표 2-6**는 InstructGPT를 위해 레이블러가 만든 (프롬프트, 응답) 쌍의 예를 보여줍니다.\n",
    "\n",
    "**표 2-6. InstructGPT에 사용된 데모 데이터의 예**\n",
    "\n",
    "| **프롬프트(Prompt)** | **레이블러의 응답(Labler's Response)** |\n",
    "|-----------------------|---------------------------------------|\n",
    "| Serendipity는 행복하거나 유익한 방식으로 우연히 발생한 사건과 그 발전을 의미합니다. 이 단어를 사용하여 문장을 작성하세요. | Margaret를 만나고 Tom에게 소개받은 것은 운 좋은 Serendipity의 사례였습니다. |\n",
    "| 아래 텍스트를 읽고 마지막에 있는 세 가지 질문에 답하세요.<br>[생략 - Ruth Bader Ginsburg(87세에 사망) 관련 기사] <br>1. ...<br>2. ...<br>3. Ginsburg의 죽음이 법원과 국가에 어떤 중대한 영향을 미칠 것인가요? | 1. ...<br>2. ...<br>3. Ruth Bader Ginsburg는 진보적 의견을 대표하는 리더였으며, 법원이 새 학기를 시작하려고 하기 때문에, 수석 판사는 더 이상 중요한 사건에서 결정적인 투표권을 가지지 않게 됩니다. |\n",
    "| ELI5: 스트레스가 많거나 낙담한 경험을 할 때 가슴에서 느껴지는 \"불안 덩어리\"의 원인은 무엇인가요? | 목에 느껴지는 불안 덩어리는 근육 긴장이 성대를 열어 공기 흐름을 최대로 유지하기 때문에 발생합니다. 가슴 압박감이나 심장의 통증은 미주 신경에 의해 발생하는데, 이는 장기에 더 빨리 혈액을 펌프질하고, 소화를 멈추며, 아드레날린과 코르티솔을 생성하라는 신호를 보냅니다. |\n",
    "\n",
    "`OpenAI와 Anthropic과 같은 회사는 데모 데이터를 생성하기 위해 높은 학력을 가진 레이블러를 사용`합니다. InstructGPT의 데모 데이터를 라벨링한 사람들 중 약 **90%는 최소한 대학 학위**를 가지고 있으며, 3분의 1 이상은 석사 학위를 보유하고 있습니다. 이미지의 물체를 라벨링하는 작업은 몇 초밖에 걸리지 않을 수 있지만, 하나의 (프롬프트, 응답) 쌍을 생성하는 데는 특히 요약과 같은 긴 컨텍스트가 포함된 작업에서는 최대 30분이 걸릴 수 있습니다. (프롬프트, 응답) 쌍 하나의 비용이 $10이라고 가정하면, OpenAI가 InstructGPT를 위해 사용한 13,000쌍의 데이터는 $130,000의 비용이 듭니다. 이는 작업 설계(어떤 작업과 프롬프트를 포함할 것인지), 레이블러 모집, 데이터 품질 관리 비용은 포함되지 않은 금액입니다.\n",
    "\n",
    "모든 조직이 고품질 인간 주석 데이터를 사용하는 접근 방식을 따를 여유가 있는 것은 아닙니다. 독일의 비영리 단체인 **LAION**은 전 세계적으로 13,500명의 자원봉사자를 동원하여 10,000개의 대화를 생성했으며, 이 대화는 35개 언어로 161,443개의 메시지와 461,292개의 품질 평가를 포함합니다. 데이터가 자원봉사자들에 의해 생성되었기 때문에 편향은 크게 줄어들었습니다. 이론적으로는, 모델에 인간의 선호를 가르치는 레이블러는 전체 인간 인구를 대표해야 합니다. 그러나 **LAION의 레이블러의 인구 통계는 편향되어 있으며**, 예를 들어, 한 자가 보고된 조사에서는 **90%가 남성**으로 확인되었습니다(Köpf et al., 2023).\n",
    "\n",
    "**DeepMind**는 인터넷 데이터에서 대화를 필터링하기 위해 간단한 휴리스틱(heuristics)을 사용하여 Gopher 모델을 학습시켰습니다. 구체적으로, 그들은 다음과 같은 형식의 텍스트를 찾았습니다:\n",
    "\n",
    "```\n",
    "[A]: [짧은 문단]\n",
    "[B]: [짧은 문단]\n",
    "[A]: [짧은 문단]\n",
    "[B]: [짧은 문단]\n",
    "...\n",
    "```\n",
    "\n",
    "많은 팀들이 고품질 인간 주석 데이터에 대한 의존도를 줄이기 위해 AI가 생성한 데이터로 눈을 돌리고 있습니다. 합성 데이터에 대해서는 8장에서 논의됩니다.\n",
    " \n",
    "기술적으로는 사전 훈련된 모델을 미세 조정하는 대신 데모 데이터로 처음부터 모델을 훈련할 수 있으며, 이는 자기 지도 사전 훈련 단계를 효과적으로 제거합니다. 하지만 사전 훈련 접근 방식이 종종 더 우수한 결과를 보여주었습니다.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 선호도 미세 조정(Preference Finetuning)\n",
    "\n",
    "큰 힘에는 큰 책임이 따릅니다. 사용자가 위대한 일을 성취하도록 돕는 모델은 동시에 끔찍한 일을 성취하도록 도울 수도 있습니다. 데모 데이터는 모델에게 대화를 수행하는 방법을 가르치지만, 어떤 종류의 대화를 해야 하는지 모델에 가르치지는 않습니다. 예를 들어, 사용자가 모델에게 왜 특정 인종이 열등한지에 대해 글을 쓰거나 비행기를 납치하는 방법에 대해 작성하라고 요청한다면, 모델이 이를 따라야 할까요?\n",
    "\n",
    "위의 예들에서는 모델이 무엇을 해야 하는지 대부분의 사람들에게 명확할 것입니다. 그러나 많은 시나리오는 그렇게 명확하지 않습니다. 서로 다른 문화적, 정치적, 사회경제적, 성별, 종교적 배경을 가진 사람들은 항상 서로 다른 의견을 가지고 있습니다. AI는 낙태, 총기 규제, 이스라엘-팔레스타인 분쟁, 아동 훈육, 마리화나 합법화, 기본 소득, 또는 이민과 같은 질문에 어떻게 대답해야 할까요? 논란이 될 가능성이 있는 문제를 어떻게 정의하고 감지해야 할까요? 모델이 논란이 되는 문제에 응답하면, 그 응답이 무엇이든 간에 일부 사용자를 화나게 할 것입니다. 모델이 너무 많이 검열되면, 사용자가 흥미를 잃고 떠날 가능성이 있습니다.\n",
    "\n",
    "AI 모델이 부적절한 응답을 생성할 것이라는 두려움은 기업이 사용자들에게 애플리케이션을 공개하는 것을 막을 수 있습니다. 정렬(또는 선호 훈련)의 목표는 AI 모델이 인간 선호도에 따라 행동하도록 하는 것입니다. 이는 대담하고, 불가능할 수도 있는 목표입니다. 이 목표는 보편적인 인간 선호도가 존재한다고 가정할 뿐만 아니라, 이를 AI에 내재화할 수 있다고도 가정합니다.\n",
    "\n",
    "목표가 단순했다면, 해결책은 우아했을 수도 있습니다. 하지만 목표가 대담한 만큼, 오늘날 우리가 가진 해결책은 복잡합니다. 현재까지 가장 성공적인 정렬 알고리즘이며 여전히 널리 사용되는 RLHF(Reinforcement Learning from Human Feedback)는 두 부분으로 구성됩니다:\n",
    "\n",
    "1. 파운데이션 모델의 출력을 평가하는 보상 모델(reward model)을 훈련합니다.  \n",
    "2. 보상 모델이 최대 점수를 부여하는 응답을 생성하도록 파운데이션 모델을 최적화합니다.\n",
    "\n",
    "RLHF가 오늘날에도 여전히 사용되고 있지만, DPO(Rafailov et al., 2023)와 같은 새로운 접근 방식이 주목을 받고 있습니다. 예를 들어, Meta는 복잡성을 줄이기 위해 Llama 2의 RLHF에서 Llama 3의 DPO로 전환했습니다. 이 책에서는 모든 다양한 접근 방식을 다룰 수는 없습니다. DPO보다 더 복잡하지만 모델을 조정할 수 있는 유연성을 더 많이 제공하기 때문에 여기서는 DPO 대신 RLHF를 다루기로 했습니다. Llama 2의 저자들은 \"특정 작업에서 인간 주석자를 능가하는 것으로 나타나는 LLM의 우수한 글쓰기 능력은 근본적으로 RLHF에 의해 주도된다\"고 주장했습니다(Touvron et al., 2023).\n",
    "\n",
    "---\n",
    "\n",
    "**보상 모델(Reward Model)**\n",
    "\n",
    "RLHF는 보상 모델에 의존합니다. (프롬프트, 응답) 쌍이 주어지면, 보상 모델은 응답이 얼마나 좋은지를 점수로 출력합니다. 주어진 입력에 점수를 매기는 모델을 훈련하는 것은 일반적인 ML 작업입니다. SFT와 유사하게, 과제는 신뢰할 수 있는 데이터를 확보하는 것입니다. 레이블러가 각 응답에 점수를 직접 매기도록 하면, 점수가 다양할 수 있습니다. 동일한 샘플에서, 10점 척도로 한 레이블러는 5점을 주고 다른 레이블러는 7점을 줄 수도 있습니다. 동일한 레이블러가 동일한 (프롬프트, 응답) 쌍에 대해 두 번 점수를 매길 때도 점수가 다를 수 있습니다. 각 샘플을 독립적으로 평가하는 것을 포인트와이즈 평가라고도 합니다.\n",
    "\n",
    "레이블러에게 두 개의 응답을 비교하고 어느 것이 더 나은지 결정하도록 요청하는 것이 더 쉬운 작업입니다. 각 프롬프트에 대해 사람이나 AI가 여러 응답을 생성합니다. 결과로 얻어진 레이블링된 데이터는 비교 데이터이며, (프롬프트, 우승_응답, 패배_응답) 형식을 따릅니다. 표 2-7은 Anthropic이 자사의 한 모델을 위해 사용한 비교 데이터의 예를 보여줍니다. 이 예시의 두 응답 중에서, 저는 패배로 레이블링된 응답을 선호합니다. 이는 다양한 인간의 선호도를 단일 수학적 공식으로 포착하려는 시도의 어려움을 잘 보여줍니다.\n",
    "\n",
    "**표 2-7. Anthropics의 HH-RLHF 데이터셋에서 가져온 비교 데이터의 예**\n",
    "\n",
    "| **프롬프트**          | **우승 응답**                             | **패배 응답**                               |\n",
    "|--------------------------------|------------------------------------------------------------|------------------------------------------------------------|\n",
    "| \"어떻게 하면 내 개를 취하게 할 수 있나요?\"   | \"무슨 의미인지 잘 모르겠네요.\"                      | \"개를 취하게 하는 것은 좋지 않을 것 같습니다. 개는 맑은 정신으로 세상을 경험하는 것이 중요하다고 생각합니다.\" |\n",
    "\n",
    "그래도 두 응답을 비교하는 이 더 쉬운 작업도 시간이 걸립니다. 오픈 리서치 조직인 LMSYS(Large Model Systems Organization)는 각 응답에 대한 사실 확인이 필요하기 때문에 두 응답을 수동으로 비교하는 데 평균 3~5분이 소요된다는 것을 발견했습니다(Chiang et al., 2024). Discord 커뮤니티와의 대화에서 Llama-2의 저자인 Thomas Scialom은 각 비교당 비용이 $3.50이라고 공유했습니다. 이는 응답당 $25가 드는 응답 작성보다 여전히 훨씬 저렴합니다.\n",
    "\n",
    "그림 2-13은 OpenAI의 레이블러들이 InstructGPT의 보상 모델을 위한 비교 데이터를 생성하는 데 사용한 UI를 보여줍니다. 레이블러들은 1에서 7까지의 구체적인 점수를 부여하고 선호도 순서대로 응답의 순위를 매기지만, 보상 모델을 훈련하는 데는 순위만 사용됩니다. 레이블러 간 일치도는 약 73%로, 이는 10명에게 동일한 두 응답의 순위를 매기도록 요청하면 약 7명이 같은 순위를 매긴다는 것을 의미합니다. 레이블링 과정을 가속화하기 위해 각 주석자는 여러 응답의 순위를 동시에 매길 수 있습니다. 세 개의 순위가 매겨진 응답(A > B > C)은 세 개의 순위 쌍을 생성합니다: (A > B), (A > C), (B > C).\n",
    "\n",
    "<img src=\"images/fig_02_13.png\" width=\"800\">\n",
    "\n",
    "비교 데이터만 주어진 경우, 모델이 구체적인 점수를 제공하도록 어떻게 훈련할 수 있을까요? 적절한 보상 체계를 제공하면 인간이 거의 모든 작업을 수행할 수 있는 것처럼, 적절한 목적 함수를 제공하면 모델도 이를 수행할 수 있습니다. 일반적으로 사용되는 함수는 승리한 응답과 패배한 응답의 출력 점수 차이를 나타냅니다. 목표는 이 차이를 극대화하는 것입니다. 수학적 세부사항에 관심이 있는 사람들을 위해, **InstructGPT** 에서 사용된 공식을 소개합니다:\n",
    "\n",
    "- **rθ:** 훈련 중인 보상 모델이며, θ로 매개변수화됩니다. 훈련 과정의 목표는 손실을 최소화하는 θ를 찾는 것입니다.\n",
    "- **훈련 데이터 형식:**\n",
    "  - **x:** 프롬프트\n",
    "  - **yw:** 승리한 응답\n",
    "  - **yl:** 패배한 응답\n",
    "- **sw = r(x, yw):** 보상 모델의 승리한 응답에 대한 스칼라 점수\n",
    "- **sl = r(x, yl):** 보상 모델의 패배한 응답에 대한 스칼라 점수\n",
    "- **σ:** 시그모이드 함수\n",
    "\n",
    "각 훈련 샘플 **(x, yw, yl)**에 대해 손실 값은 다음과 같이 계산됩니다:\n",
    "\n",
    "- **log(σ(rθ(x, yw) − rθ(x, yl)))**\n",
    "- **목표:** 모든 훈련 샘플에 대해 예상 손실을 최소화하는 θ를 찾는 것.\n",
    "- **−Ex log(σ(rθ(x, yw) − rθ(x, yl)))**\n",
    "\n",
    "보상 모델은 처음부터 훈련하거나, 사전 훈련된 모델 또는 지도 미세 조정(SFT) 모델 위에 미세 조정하여 훈련할 수 있습니다. 가장 강력한 기반 모델 위에서 미세 조정을 수행하면 최상의 성능을 제공하는 것으로 보입니다. 일부 사람들은 보상 모델이 기반 모델의 응답을 평가할 수 있도록 기반 모델만큼 강력해야 한다고 믿습니다. 그러나 **3장**에서 평가에 대해 살펴보겠지만, 약한 모델이 더 강력한 모델을 판단하는 것이 생성보다 더 쉬운 것으로 여겨집니다.\n",
    "\n",
    "---\n",
    "\n",
    "**보상 모델을 사용한 미세 조정(Finetuning using the reward model)**\n",
    "\n",
    "훈련된 보상 모델(RM)을 사용하여, SFT 모델을 추가로 훈련시켜 보상 모델의 점수를 극대화하는 출력 응답을 생성합니다. 이 과정에서 프롬프트는 기존 사용자 프롬프트와 같은 프롬프트 분포에서 무작위로 선택됩니다. 이러한 프롬프트가 모델에 입력되고, 모델의 응답은 보상 모델에 의해 점수가 매겨집니다. 이 훈련 과정은 종종 OpenAI가 2017년에 발표한 근접 정책 최적화(PPO) 강화학습 알고리즘을 사용하여 수행됩니다.\n",
    "\n",
    "경험적으로, RLHF와 DPO 모두 단독 SFT보다 성능을 향상시킵니다. 그러나 이 글을 쓰는 시점에서는 이들이 왜 효과가 있는지에 대한 논의가 진행 중입니다. 이 분야가 발전함에 따라 선호도 미세 조정이 미래에 크게 변화할 것으로 예상됩니다. RLHF와 선호도 미세 조정에 대해 더 알고 싶다면 이 책의 GitHub 저장소를 확인하세요.\n",
    "\n",
    "SFT와 선호도 미세 조정은 모두 사전 훈련에 사용된 데이터의 낮은 품질로 인해 발생한 문제를 해결하기 위한 단계입니다. 만약 언젠가 더 나은 사전 훈련 데이터나 파운데이션 모델을 훈련시키는 더 나은 방법이 있다면, SFT와 선호도 조정이 전혀 필요하지 않을 수도 있습니다.\n",
    "\n",
    "일부 기업들은 강화 학습을 완전히 건너뛰어도 괜찮다고 생각합니다. 예를 들어, Stitch Fix와 Grab은 그들의 애플리케이션에서 보상 모델만으로도 충분하다는 것을 발견했습니다. 그들은 모델이 여러 출력을 생성하도록 하고, 보상 모델에서 높은 점수를 받은 것들을 선택합니다. 'best of N' 전략이라고 불리는 이 접근 방식은 모델이 출력을 샘플링하는 방식을 활용하여 성능을 향상시킵니다. 다음 섹션에서는 'best of N'이 어떻게 작동하는지 자세히 살펴보겠습니다.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<a id=\"sampling\"></a>\n",
    "## 샘플링\n",
    "\n",
    "모델은 **샘플링(Sampling)** 이라고 알려진 과정을 통해 출력을 생성하며, 때로는 **디코딩(Decoding)** 이라고도 합니다. 이 섹션에서는 다양한 샘플링 전략과 **샘플링 변수**(예: temperature, top-k, top-p)에 대해 논의합니다. 그런 다음, 모델 성능을 향상시키기 위해 여러 출력을 샘플링하는 방법을 탐구합니다. 또한 샘플링 프로세스가 특정 형식과 제약 조건을 따르는 응답을 생성하도록 수정될 수 있는 방법도 살펴봅니다.\n",
    "\n",
    "샘플링은 AI의 출력을 확률적으로 만듭니다. 이러한 확률적 특성을 이해하는 것은 AI의 행동(예: 일관성 부족, 환각)을 처리하는 데 중요합니다. 이 섹션은 이러한 확률적 특성이 무엇을 의미하며, 이를 다루는 방법에 대한 심층적인 논의로 끝납니다.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 샘플링 기본(Sampling Fundamentals)\n",
    "\n",
    "주어진 입력에서, 신경망은 가능한 결과의 확률을 먼저 계산하여 출력을 생성합니다. 분류 모델의 경우, 가능한 결과는 사용할 수 있는 클래스입니다. 예를 들어, 이메일이 스팸인지 아닌지를 분류하도록 모델이 훈련되었다면, 가능한 결과는 \"스팸\"과 \"비스팸\" 두 가지뿐입니다. 모델은 이 두 결과 각각의 확률을 계산합니다. 예를 들어, 이메일이 스팸일 확률이 90%, 스팸이 아닐 확률이 10%라고 가정합시다.\n",
    "\n",
    "다음 토큰을 생성하기 위해, 언어 모델은 어휘(vocabulary)에 있는 모든 토큰에 대한 확률 분포를 먼저 계산합니다. 이는 [그림 2-14](Figure 2-14)과 유사합니다.\n",
    "\n",
    "<img src=\"images/fig_02_14.png\" width=800>\n",
    "\n",
    "다양한 확률을 가진 가능한 결과를 다룰 때, 일반적인 전략은 가장 높은 확률을 가진 결과를 선택하는 것입니다. 항상 가장 가능성이 높은 결과를 선택하는 것을 **탐욕적 샘플링(greedy sampling)**이라고 합니다. 이는 분류 작업에서 자주 효과적입니다. 예를 들어, 모델이 이메일이 스팸일 가능성이 비스팸일 가능성보다 더 높다고 생각한다면, 이를 스팸으로 표시하는 것이 합리적입니다. 그러나 언어 모델의 경우, 탐욕적 샘플링은 지루한 출력을 만듭니다. 예를 들어, 어떤 질문을 하더라도 항상 가장 일반적인 단어들로 응답하는 모델을 상상해 보세요.\n",
    "\n",
    "항상 가장 가능성이 높은 토큰을 선택하는 대신, 모델은 모든 가능한 값에 대한 확률 분포에 따라 다음 토큰을 샘플링할 수 있습니다. 예를 들어, \"My favorite color is ...\"라는 맥락에서, \"red\"가 다음 토큰일 확률이 30%이고 \"green\"이 50%라면, \"red\"는 30%의 확률로, \"green\"은 50%의 확률로 선택됩니다.\n",
    "\n",
    "그렇다면 모델은 이러한 확률을 어떻게 계산할까요? 입력을 주어지면, 신경망은 **logit 벡터**를 출력합니다. 각 **logit**은 하나의 가능한 값에 대응합니다. 언어 모델의 경우, 각 logit은 모델의 어휘에 있는 하나의 토큰에 해당합니다. logit 벡터의 크기는 어휘의 크기와 동일합니다. **그림 2-15**는 logit 벡터의 시각화를 보여줍니다.\n",
    "\n",
    "<img src=\"images/fig_02_15.png\" width=800>\n",
    "\n",
    "더 큰 로짓(logit)은 더 높은 확률에 해당하지만, 로짓 자체는 확률을 나타내지 않습니다. 로짓의 합은 1이 되지 않으며, 로짓은 음수가 될 수도 있지만 확률은 항상 0 이상이어야 합니다. 로짓을 확률로 변환하기 위해 소프트맥스(softmax) 계층이 자주 사용됩니다. 모델의 어휘 크기가 $ N $이고 로짓 벡터가 $ [x_1, x_2, \\dots, x_N] $일 때, $ i $번째 토큰의 확률 $ p_i $는 다음과 같이 계산됩니다:\n",
    "\n",
    "$\n",
    "p_i = \\text{softmax}(x_i) = \\frac{e^{x_i}}{\\sum_{j} e^{x_j}}\n",
    "$\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 샘플링 전략 (Sampling Strategies)\n",
    "\n",
    "적절한 샘플링 전략은 모델이 애플리케이션에 더 적합한 응답을 생성하도록 만들 수 있습니다. 예를 들어, 한 샘플링 전략은 모델이 더 창의적인 응답을 생성하도록 만들 수 있는 반면, 다른 전략은 생성 결과를 더 예측 가능하게 만들 수 있습니다. 특정 속성을 가진 응답으로 모델을 유도하기 위해 여러 가지 샘플링 전략이 도입되었습니다. 또한 사용자는 자신만의 샘플링 전략을 설계할 수도 있지만, 일반적으로 이는 모델의 로짓에 대한 접근이 필요합니다. 이제 몇 가지 일반적인 샘플링 전략을 살펴보겠습니다.\n",
    "\n",
    "---\n",
    "\n",
    "**온도(Temperature)**\n",
    "\n",
    "다음 토큰을 확률 분포에 따라 샘플링하는 한 가지 문제는 모델이 덜 창의적일 수 있다는 점입니다. 이전 예시에서, \"red\", \"green\", \"purple\"과 같은 일반적인 색상이 가장 높은 확률을 가질 가능성이 높습니다. 결과적으로 언어 모델의 응답은 \"My favorite color is green.\"처럼 다섯 살짜리 아이의 대답처럼 들릴 수 있습니다. \"the\"라는 단어는 낮은 확률을 가지기 때문에, 모델이 \"My favorite color is the color of a still lake on a spring morning.\"처럼 창의적인 문장을 생성할 확률이 낮아집니다.\n",
    "\n",
    "가능한 값들의 확률을 다시 분배하려면, **온도(temperature)**를 사용하여 샘플링할 수 있습니다. 직관적으로, 더 높은 온도는 일반적인 토큰의 확률을 감소시키고, 결과적으로 더 드문 토큰의 확률을 증가시킵니다. 이를 통해 모델이 더 창의적인 응답을 생성할 수 있습니다.\n",
    "\n",
    "온도는 소프트맥스 변환 이전에 로짓을 조정하기 위해 사용되는 상수입니다. 로짓은 온도로 나누어집니다. 주어진 온도 $ T $에 대해, $ i $번째 토큰의 조정된 로짓은 다음과 같이 계산됩니다:\n",
    "\n",
    "$\n",
    "\\frac{x_i}{T}\n",
    "$\n",
    "\n",
    "소프트맥스는 $ x_i $ 대신 이 조정된 로짓에 적용됩니다.\n",
    "\n",
    "간단한 예를 들어, 온도가 확률에 미치는 영향을 살펴보겠습니다. 마지막 계층에서 출력된 로짓이 [1, 2]이고, A와 B라는 두 개의 가능한 출력만 있는 모델을 가정합니다. A의 로짓은 1이고 B의 로짓은 2입니다.\n",
    "\n",
    "- **온도를 사용하지 않는 경우**(온도 값이 1과 동일): 소프트맥스 확률은 [0.27, 0.73]입니다. 모델은 B를 73%의 확률로 선택합니다.\n",
    "- **온도가 0.5인 경우**: 확률은 [0.12, 0.88]입니다. 모델은 이제 B를 88%의 확률로 선택합니다.\n",
    "\n",
    "온도가 높을수록 모델이 가장 명백한 값(가장 높은 로짓을 가진 값)을 선택할 가능성이 낮아지며, 모델의 출력이 더 창의적이지만 덜 일관성 있게 됩니다. 온도가 낮을수록 모델이 가장 명백한 값을 선택할 가능성이 높아지며, 모델의 출력이 더 일관성 있지만 잠재적으로 더 지루하게 됩니다.\n",
    "\n",
    "**그림 2-16**은 온도가 다른 경우 A와 B의 소프트맥스 확률을 보여줍니다. 온도가 0에 가까워질수록, 모델이 B를 선택할 확률이 1에 가까워집니다. 예를 들어, 온도가 0.1인 경우, 모델은 거의 항상 B를 출력합니다. 온도가 증가함에 따라, A가 선택될 확률은 증가하고 B가 선택될 확률은 감소합니다. 모델 제공자는 일반적으로 온도를 0에서 2 사이로 제한합니다. 모델을 소유한 경우, 음수가 아닌 임의의 온도를 사용할 수 있습니다. 온도 0.7은 창의성과 예측 가능성을 균형 있게 유지하기 위해 종종 추천되지만, 가장 적합한 온도를 찾기 위해 실험해보는 것이 좋습니다.\n",
    "\n",
    "<img src=\"images/fig_02_16.png\" width=800>\n",
    "\n",
    "모델의 출력을 더 일관되게 만들기 위해 온도를 0으로 설정하는 것이 일반적인 관행입니다. 기술적으로 온도는 0이 될 수 없습니다. 로짓은 0으로 나눌 수 없기 때문입니다. 그러나 실제로 온도를 0으로 설정하면, 모델은 로짓 조정과 소프트맥스 계산 없이 가장 큰 로짓을 가진 토큰을 선택합니다. \n",
    "\n",
    ">**팁**\n",
    ">\n",
    ">AI 모델을 사용할 때 일반적인 디버깅 기술은 주어진 입력에 대해 모델이 계산한 확률을 살펴보는 것입니다. 예를 들어, 확률이 무작위처럼 보이면 모델이 학습한 것이 거의 없다는 것을 의미합니다.\n",
    "\n",
    "많은 모델 제공업체는 모델이 생성한 확률을 **logprobs**로 반환합니다. **Logprobs**는 **log probabilities**의 줄임말로, 로그 스케일에서의 확률을 의미합니다. 로그 스케일은 신경망의 확률을 다룰 때 선호되며, 이는 **언더플로(underflow)** 문제를 줄이는 데 도움을 줍니다. 예를 들어, 언어 모델이 100,000개의 어휘 크기로 작동할 경우, 많은 토큰의 확률이 너무 작아 기계적으로 0으로 반올림될 수 있습니다. 로그 스케일은 이러한 문제를 완화하는 데 도움을 줍니다.\n",
    "\n",
    "그림 2-17은 로짓, 확률, 로그 확률이 계산되는 과정을 보여줍니다.\n",
    "\n",
    "<img src=\"images/fig_02_17.png\" width=800>\n",
    "\n",
    "책 전체에서 볼 수 있듯이, **logprobs**는 애플리케이션을 구축(특히 분류 작업), 애플리케이션 평가, 그리고 모델이 내부적으로 어떻게 작동하는지 이해하는 데 유용합니다. 그러나 이 글을 쓰는 시점에서 많은 모델 제공업체는 모델의 logprobs를 노출하지 않거나, 노출한다고 하더라도 logprobs API는 제한적입니다. 제한된 logprobs API는 보안상의 이유로, 모델의 logprobs를 공개하면 다른 사람들이 모델을 복제하기가 더 쉬워지기 때문인 것으로 보입니다.\n",
    "\n",
    "---\n",
    "\n",
    "**Top-k**\n",
    "\n",
    "**Top-k**는 모델의 응답 다양성을 크게 희생하지 않고도 계산 작업량을 줄이기 위한 샘플링 전략입니다. 소프트맥스 계층은 모든 가능한 값에 대해 확률 분포를 계산하는 데 사용된다는 점을 기억하세요. 소프트맥스는 모든 가능한 값에 대해 두 번의 계산을 필요로 합니다: 하나는 지수 합 $\\sum_j e^{x_j}$을 계산하고, 다른 하나는 각 값에 대해 $\\frac{e^{x_i}}{\\sum_j e^{x_j}}$을 계산하는 것입니다. 어휘 크기가 큰 언어 모델의 경우, 이 과정은 계산적으로 비용이 많이 듭니다.\n",
    "\n",
    "이 문제를 피하기 위해, 모델이 로짓을 계산한 후 **top-k** 로짓만 선택하고, 이 **top-k** 로짓에 대해 소프트맥스를 수행합니다. 애플리케이션에서 얼마나 다양한 출력을 원하느냐에 따라, **k** 값은 50에서 500 사이가 될 수 있으며, 이는 모델의 전체 어휘 크기보다 훨씬 작습니다. 모델은 이 상위 **k** 값에서 샘플을 추출합니다. **k** 값이 작을수록 텍스트는 더 예측 가능하지만 덜 흥미로워질 수 있습니다. 이는 모델이 가능성이 높은 단어의 작은 집합으로 제한되기 때문입니다.\n",
    "\n",
    "---\n",
    "\n",
    "**Top-p**\n",
    "\n",
    "Top-k 샘플링에서는 고려되는 값의 수가 k로 고정됩니다. 하지만 이 수는 상황에 따라 변경되어야 합니다. 예를 들어, \"음악을 좋아하나요? 예 또는 아니오로만 답하세요.\"라는 프롬프트가 주어진 경우, 고려해야 할 값은 두 개뿐입니다: 예와 아니오. \"인생의 의미는 무엇인가요?\"라는 프롬프트가 주어진 경우에는 고려해야 할 값의 수가 훨씬 더 많아야 합니다.\n",
    "\n",
    "Top-p는 핵 샘플링(nucleus sampling)이라고도 하며, 샘플링할 값을 더 동적으로 선택할 수 있게 해줍니다. Top-p 샘플링에서는 모델이 다음에 올 가능성이 가장 높은 값들의 확률을 내림차순으로 더하고, 합이 p에 도달하면 중단합니다. 이 누적 확률 내의 값들만 고려됩니다. 언어 모델에서 top-p(핵) 샘플링의 일반적인 값은 0.9에서 0.95 사이입니다. 예를 들어, top-p 값이 0.9라는 것은 모델이 누적 확률이 90%를 초과하는 가장 작은 값들의 집합을 고려한다는 의미입니다.\n",
    "\n",
    "그림 2-18에서 보여주는 것처럼 모든 토큰의 확률이 있다고 가정해봅시다. top-p가 90%인 경우, \"yes\"와 \"maybe\"만 고려됩니다. 이는 이들의 누적 확률이 90%보다 크기 때문입니다. top-p가 99%인 경우에는 \"yes\", \"maybe\", \"no\"가 고려됩니다.\n",
    "\n",
    "<img src=\"images/fig_02_18.png\" width=800>\n",
    "\n",
    "Top-k와 달리, top-p는 반드시 소프트맥스 계산 부하를 줄이지는 않습니다. Top-p의 이점은 각 문맥에서 가장 관련성이 높은 값들의 집합에만 초점을 맞추기 때문에 출력이 문맥상 더 적절해질 수 있다는 것입니다. 이론적으로는 top-p 샘플링의 이점이 많지 않아 보입니다. 하지만 실제로는 top-p 샘플링이 잘 작동하는 것으로 입증되어 그 인기가 높아지고 있습니다.\n",
    " \n",
    "관련된 샘플링 전략으로는 min-p가 있는데, 이는 샘플링 시 토큰이 도달해야 하는 최소 확률을 설정하는 방식입니다.\n",
    "\n",
    "**정지 조건**\n",
    "\n",
    "자기회귀 언어 모델은 토큰을 하나씩 생성하여 토큰 시퀀스를 생성합니다. 긴 출력 시퀀스는 더 많은 시간이 걸리고, 더 많은 컴퓨팅 비용(돈)이 들며, 때로는 사용자를 짜증나게 할 수 있습니다. 따라서 모델이 시퀀스 생성을 중단하도록 조건을 설정하고 싶을 수 있습니다.\n",
    " \n",
    "한 가지 간단한 방법은 모델에게 고정된 수의 토큰 생성 후 중단하도록 요청하는 것입니다. 단점은 출력이 문장 중간에 잘릴 가능성이 높다는 것입니다. 다른 방법은 정지 토큰이나 정지 단어를 사용하는 것입니다. 예를 들어, 시퀀스 종료 토큰을 만났을 때 생성을 중단하도록 모델에 요청할 수 있습니다. 정지 조건은 지연 시간과 비용을 낮게 유지하는 데 도움이 됩니다.\n",
    "\n",
    "조기 중단의 단점은 모델이 특정 형식으로 출력을 생성하기를 원할 때 너무 일찍 중단되면 출력 형식이 잘못될 수 있다는 것입니다. 예를 들어, 모델에게 JSON을 생성하도록 요청했을 때 조기 중단으로 인해 닫는 괄호와 같은 요소가 누락되어 생성된 JSON을 파싱하기 어려울 수 있습니다.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 테스트 시간 계산 (Test Time Compute)\n",
    "\n",
    "이전 섹션에서는 모델이 다음 토큰을 어떻게 샘플링할 수 있는지 논의했습니다. 이번 섹션에서는 모델이 전체 출력을 샘플링하는 방식을 다룹니다.\n",
    "\n",
    "모델 응답 품질을 개선하는 한 가지 간단한 방법은 **테스트 시간 계산**(Test Time Compute)을 활용하는 것입니다. 한 쿼리에 대해 하나의 응답만 생성하는 대신, 여러 응답을 생성하여 더 좋은 응답이 나올 가능성을 높이는 방식입니다. 테스트 시간 계산의 한 방법은 이 장에서 논의된 \"최상의 N(Best of N)\" 기법입니다. 이 방식은 여러 출력을 무작위로 생성하고 그중 가장 좋은 것을 선택하는 것입니다. 그러나 더 전략적으로 여러 출력을 생성할 수도 있습니다. 예를 들어, 모든 출력을 독립적으로 생성하는 대신, **빔 서치(Beam Search)**를 사용하여 각 시퀀스 생성 단계에서 가장 유망한 후보(빔)를 고정된 수만큼 생성할 수 있습니다.\n",
    "\n",
    "테스트 시간 계산의 효과를 높이는 간단한 전략은 출력의 다양성을 증가시키는 것입니다. 더 다양한 옵션 집합이 더 나은 후보를 낼 가능성이 크기 때문입니다. 동일한 모델을 사용해 다양한 옵션을 생성하려면 모델의 샘플링 변수들을 다양화하는 것이 좋은 실천법입니다.\n",
    "\n",
    "일반적으로 여러 출력을 샘플링하면 모델 성능이 일부 개선될 것으로 기대할 수 있지만, 이 방법은 비용이 많이 듭니다. 평균적으로 두 개의 출력을 생성하는 데 드는 비용은 하나의 출력을 생성하는 비용의 약 두 배입니다.  \n",
    "\n",
    ">**경고**\n",
    ">\n",
    ">저는 **테스트 시간 계산**이라는 용어를 기존 문헌과의 일관성을 위해 사용합니다. 그러나 초기 리뷰어들 중 일부는 이 용어가 혼란을 야기한다고 주장했습니다. AI 연구에서 **테스트 시간**은 보통 모델을 테스트하기 위한 추론 작업을 의미합니다. 하지만 이 기법은 일반적인 프로덕션 모델에도 적용될 수 있습니다. **테스트 시간 계산**은 샘플링할 수 있는 출력 수가 각 추론 호출에서 할당할 수 있는 계산량에 따라 결정되기 때문에 붙여진 이름입니다.\n",
    "\n",
    "최상의 출력을 선택하려면 사용자에게 여러 출력을 보여주고 가장 적합한 것을 선택하게 하거나, 최상의 출력을 선택할 방법을 고안할 수 있습니다. 한 가지 선택 방법은 가장 높은 확률을 가진 출력을 선택하는 것입니다. 언어 모델의 출력은 토큰 시퀀스이며, 각 토큰은 모델에 의해 확률이 할당됩니다. 출력의 각 토큰 확률의 곱은 전체 출력의 확률을 나타냅니다.\n",
    "\n",
    "토큰 시퀀스 [“I”, “love”, “food”]를 고려해봅시다. \"I\"의 확률이 0.2, \"I\"가 주어졌을 때 \"love\"의 확률이 0.1, 그리고 \"I\"와 \"love\"가 주어졌을 때 \"food\"의 확률이 0.3이라면, 이 시퀀스의 확률은 다음과 같습니다: 0.2 × 0.1 × 0.3 = 0.006. 수학적으로, 이는 다음과 같이 표현할 수 있습니다:\n",
    "\n",
    "$ P(I \\text{ love food}) = P(I) × P(I \\mid \\text{love}) × P(\\text{food} \\mid I, \\text{love}) $\n",
    "\n",
    "로그 스케일에서 확률로 작업하는 것이 더 쉽다는 것을 기억하세요. 곱셈의 로그는 로그의 합과 같으므로, 토큰 시퀀스의 로그확률(logprob)은 해당 시퀀스의 모든 토큰 로그합의 합계와 같습니다:\n",
    "\n",
    "$\n",
    "\\log P(I \\text{ love food}) = \\log P(I) + \\log P(I \\mid \\text{love}) + \\log P(\\text{food} \\mid I, \\text{love})\n",
    "$\n",
    "\n",
    "합산을 통해 긴 시퀀스는 총 로그확률이 더 낮아질 가능성이 있습니다(로그 값은 일반적으로 음수이기 때문). 따라서 짧은 시퀀스를 선호하지 않기 위해, 시퀀스의 합을 길이로 나누어 평균 로그확률을 사용할 수 있습니다. 샘플링된 여러 출력 중에서 평균 로그확률이 가장 높은 출력을 선택합니다. 현재 기준으로 OpenAI API는 이를 사용합니다.  \n",
    "\n",
    "다른 선택 방법은 이전 섹션에서 논의된 보상 모델(reward model)을 사용하여 각 출력을 점수화하는 것입니다. Stitch Fix와 Grab은 보상 모델이나 검증자를 활용해 높은 점수를 받은 출력을 선택한 사례입니다. Nextdoor는 보상 모델을 사용하는 것이 애플리케이션 성능을 개선하는 주요 요소라는 점을 발견했습니다(2023).  \n",
    "\n",
    "OpenAI는 또한 검증자(verifiers)를 활용하여 수학 문제의 최적 솔루션을 선택하도록 모델을 훈련했습니다(Cobbe et al., 2021). 이 연구에 따르면 검증자를 사용하는 것이 모델 성능을 상당히 향상시켰습니다. **실제로, 검증자를 사용하는 것이 30배 크기의 모델 크기 증가와 동일한 성능 향상을 가져왔습니다.** 이는 검증자를 사용하는 1억 파라미터 모델이 검증자를 사용하지 않는 30억 파라미터 모델과 동일한 성능을 낼 수 있음을 의미합니다.  \n",
    "\n",
    "DeepMind는 테스트 시간 연산(test time compute)의 가치를 더욱 강조하면서, 연산을 확장하여 추론(inference) 중에 더 많은 출력을 생성하도록 할당하는 것이 모델 파라미터 확장보다 더 효과적일 수 있음을 주장합니다(Snell et al., 2024). 같은 논문에서는 흥미로운 질문을 던집니다: \"LLM이 고정된(그러나 무시할 수 없을 만큼의) 추론 시간 연산을 허용받는다면, 주어진 어려운 프롬프트에서 성능을 얼마나 향상시킬 수 있을까?\"  \n",
    "\n",
    "OpenAI의 실험에서는 샘플링된 출력의 수가 많아질수록 성능이 향상되지만, 이는 특정 지점까지만 그렇다는 사실을 발견했습니다. 이 실험에서 해당 지점은 400개의 출력이었습니다. 이를 초과하면 성능이 감소했습니다(Figure 2-19 참조). 그들은 샘플링된 출력의 수가 증가함에 따라, 모델을 속일 수 있는 적대적 출력(adversarial output)을 찾을 가능성도 함께 증가한다고 가정했습니다. 그러나 Stanford의 실험은 다른 결론을 도출했습니다. \"Monkey Business\" (Brown et al., 2024) 연구는 해결된 문제의 수가 샘플 수가 1에서 10,000으로 증가함에 따라 로그 선형적으로 증가한다는 것을 발견했습니다. 테스트 시간 연산을 무한히 확장할 수 있는지에 대해 생각하는 것은 흥미롭지만, 실제 프로덕션 환경에서는 입력당 400 또는 10,000개의 출력을 샘플링하는 경우는 없습니다. 비용이 천문학적으로 높아질 것이기 때문입니다.  \n",
    "\n",
    "<img src=\"images/fig_02_19.png\" width=800>\n",
    "\n",
    "애플리케이션별 휴리스틱을 사용하여 최상의 응답을 선택할 수도 있습니다. 예를 들어, 애플리케이션이 짧은 응답을 선호한다면 가장 짧은 후보를 선택할 수 있습니다. 애플리케이션이 자연어를 SQL 쿼리로 변환하는 경우, 유효한 SQL 쿼리가 생성될 때까지 모델이 출력을 계속 생성하도록 할 수 있습니다.\n",
    "\n",
    "테스트 시간 계산의 특히 흥미로운 응용은 지연 시간 문제를 극복하는 것입니다. 일부 쿼리, 특히 사고 연쇄(chain-of-thought) 쿼리의 경우 모델이 응답을 완료하는 데 오랜 시간이 걸릴 수 있습니다. TIFIN의 AI 책임자인 Kittipat Kampa는 그의 팀이 모델에게 여러 응답을 병렬로 생성하도록 요청하고 완료되고 유효한 첫 번째 응답을 사용자에게 보여준다고 말했습니다.\n",
    "\n",
    "출력 세트 중에서 가장 일반적인 출력을 선택하는 것은 정확한 답변이 필요한 작업에 특히 유용할 수 있습니다. 예를 들어, 수학 문제가 주어졌을 때 모델은 여러 번 문제를 풀고 가장 빈번한 답을 최종 해답으로 선택할 수 있습니다. 마찬가지로 객관식 문제의 경우 모델은 가장 빈번한 출력 옵션을 선택할 수 있습니다. 이는 Google이 MMLU 벤치마크에서 Gemini를 평가할 때 사용한 방법입니다. 그들은 각 질문에 대해 32개의 출력을 샘플링했습니다. 이를 통해 모델은 질문당 하나의 출력만 사용했을 때보다 더 높은 점수를 달성할 수 있었습니다.\n",
    "\n",
    "입력의 작은 변화에도 출력이 크게 변하지 않는 모델을 견고한(robust) 모델이라고 합니다. 모델이 덜 견고할수록 여러 출력을 샘플링하는 것이 더 유용할 수 있습니다. 한 프로젝트에서 우리는 제품 이미지에서 특정 정보를 추출하기 위해 AI를 사용했습니다. 동일한 이미지에 대해 우리 모델은 절반의 시간 동안만 정보를 읽을 수 있었습니다. 나머지 절반은 모델이 이미지가 너무 흐리거나 텍스트가 너무 작아서 읽을 수 없다고 말했습니다. 하지만 각 이미지에 대해 세 번 시도함으로써 모델은 대부분의 이미지에서 올바른 정보를 추출할 수 있었습니다.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **구조화된 출력 (Structured Outputs)**\n",
    "\n",
    "종종, 프로덕션 환경에서는 특정 형식을 따르는 출력을 생성하기 위해 모델이 필요합니다. 구조화된 출력은 다음 두 가지 시나리오에서 매우 중요합니다:\n",
    "\n",
    "1. **구조화된 출력이 필요한 작업.**  \n",
    "   이 시나리오에서 가장 일반적인 작업 범주는 의미적 구문 분석(semantic parsing)입니다. 의미적 구문 분석은 자연어를 구조화된 기계 판독 가능한 형식으로 변환하는 작업을 포함합니다. 예를 들어, Text-to-SQL은 의미적 구문 분석의 한 예로, 출력이 유효한 SQL 쿼리여야 합니다. 의미적 구문 분석을 통해 사용자는 자연어(예: 영어)를 사용하여 API와 상호작용할 수 있습니다.  \n",
    "   예를 들어, Text-to-PostgreSQL은 사용자가 \"지난 6개월 동안의 평균 월간 수익은 얼마인가?\"와 같은 영어 쿼리를 PostgreSQL에 입력할 수 있도록 합니다.\n",
    "\n",
    "   아래는 GPT-4를 사용하여 Text-to-Regex를 수행하는 프롬프트의 예입니다. 출력은 GPT-4에 의해 생성된 실제 출력입니다:\n",
    "\n",
    "   **시스템 프롬프트:**  \n",
    "   주어진 항목에 대해, 해당 항목이 작성될 수 있는 모든 방식을 나타내는 정규 표현식을 생성하세요. <br>\n",
    "   정규 표현식만 반환하십시오.  \n",
    "\n",
    "   **예시:**  \n",
    "   US phone number -> `\\+?1?\\s?($)?(\\d{3})(?(1)$)[-.\\s]?(\\d{3})[-.\\s]?(\\d{4})`\n",
    "\n",
    "   **사용자 프롬프트:**  \n",
    "   Email address ->  \n",
    "\n",
    "   **GPT-4:**  \n",
    "   `[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}`  \n",
    "\n",
    "   **사용자 프롬프트:**  \n",
    "   Dates ->  \n",
    "\n",
    "   **GPT-4:**  \n",
    "   `(?:\\d{1,2}[/-\\.])(?:\\d{1,2}[/-\\.])?\\d{2,4}`  \n",
    "\n",
    "다른 카테고리의 작업이 시나리오에서의 작업에는 출력이 유효한 클래스여야 하는 분류(classification)가 포함됩니다.\n",
    "\n",
    "2. **출력이 다운스트림 애플리케이션에서 사용되는 작업.**  \n",
    "   이 시나리오에서는 작업 자체가 출력이 구조화될 필요는 없지만, 출력이 다른 애플리케이션에서 사용되므로 해당 애플리케이션이 해석할 수 있는 형식이어야 합니다.\n",
    "\n",
    "   예를 들어, AI 모델을 사용해 이메일을 작성한다고 가정해 봅시다. 이메일 자체는 구조화될 필요가 없지만, 이 이메일을 사용하는 다운스트림 애플리케이션에서는 특정 형식이 필요할 수 있습니다. 예를 들어, 다음과 같은 특정 키를 포함하는 JSON 문서가 필요할 수 있습니다:\n",
    "   ```json\n",
    "   {\n",
    "     \"title\": [TITLE],\n",
    "     \"body\": [EMAIL BODY]\n",
    "   }\n",
    "   ```\n",
    "\n",
    "   **이것은 에이전트 워크플로(agentic workflows)에 특히 중요합니다.**  \n",
    "   여기서 모델의 출력은 도구에 입력으로 사용되는 경우가 많으며, 이는 6장에서 논의되었습니다.\n",
    "\n",
    "구조화된 출력을 지원하는 프레임워크에는 **guidance**, **outlines**, **instructor**, **llama.cpp**가 포함됩니다. 각 모델 제공자는 모델이 구조화된 출력을 생성하는 능력을 개선하기 위해 자체 기술을 사용할 수 있습니다. OpenAI는 텍스트 생성 API에서 **JSON 모드**를 도입한 최초의 모델 제공자였습니다. 이 API의 JSON 모드는 출력이 유효한 JSON 객체임을 보장할 뿐, JSON 내용의 유효성을 보장하지 않습니다.  \n",
    "생성된 JSON이 유효하더라도 출력이 너무 일찍 중단되거나(예: 최대 출력 토큰 길이에 도달한 경우) 트렁케이트(truncate)될 수 있어 해석할 수 없게 될 가능성이 있습니다. 반면, 최대 토큰 길이가 너무 길게 설정되면 모델의 응답이 지나치게 느려지고 비용이 비싸질 수 있습니다.\n",
    "\n",
    "**Figure 2-20**에서는 가이던스를 사용해 제한된 옵션 집합과 정규 표현식으로 출력을 생성하는 두 가지 예를 보여줍니다.\n",
    "\n",
    "<img src=\"images/fig_02_20.png\" width=800>\n",
    "\n",
    "AI 스택의 다양한 계층에서 모델이 구조화된 출력을 생성하도록 유도할 수 있습니다. 이러한 계층에는 프롬프팅(prompting), 후처리(post-processing), 테스트 시간 연산(test time compute), 제한된 샘플링(constrained sampling), 그리고 파인튜닝(finetuning)이 포함됩니다. 처음 세 가지는 일종의 임시 해결책과 같습니다. 모델이 이미 구조화된 출력을 생성하는 데 꽤 능숙하며 약간의 조정만 필요한 경우 가장 효과적입니다. 보다 심층적인 처리가 필요한 경우, 제한된 샘플링과 파인튜닝이 필요합니다.\n",
    "\n",
    "테스트 시간 연산은 앞에서 이미 논의되었으므로, 여기에서는 나머지 네 가지 접근법에 대해 설명합니다.\n",
    "\n",
    "**프롬프팅(Prompting)**\n",
    "\n",
    "프롬프팅은 구조화된 출력을 위한 첫 번째 조치입니다. 모델에 어떤 형식의 출력을 생성하라고 지시할 수 있습니다. 그러나 모델이 이러한 지시를 따를 수 있는지는 모델의 **지시 수행 능력(Chapter 4)** 과 지시의 명확성( **Chapter 5** )에 달려 있습니다. 모델이 점점 더 지시를 잘 따르게 발전하고 있지만, 항상 지시를 따를 것이라는 보장은 없습니다. 출력의 몇 퍼센트가 유효하지 않은 경우라도 많은 애플리케이션에서 이는 여전히 용납될 수 없습니다.  \n",
    "\n",
    "유효 출력의 비율을 높이기 위해, 일부 사람들은 AI를 사용하여 원래 프롬프트의 출력을 검증하거나 수정합니다. 이는 **Chapter 3** 에서 논의된 \"판사로서의 AI\" 접근법의 예입니다. 이는 각 출력에 대해 두 번 이상의 모델 쿼리가 필요함을 의미합니다: 하나는 출력을 생성하고, 다른 하나는 이를 검증하기 위한 것입니다. 추가된 검증 레이어는 출력의 유효성을 크게 개선할 수 있지만, 추가 검증 쿼리에 의해 발생하는 비용과 지연으로 인해 일부에게는 이 접근법이 너무 비쌀 수 있습니다.\n",
    "\n",
    "**후처리(Post-processing)**\n",
    "\n",
    "후처리는 간단하고 저렴하지만, 놀라울 정도로 효과적일 수 있습니다. 교육 중에, 학생들이 매우 비슷한 실수를 반복하는 경향이 있다는 점을 발견했습니다. 파운데이션 모델 작업을 시작했을 때도 같은 점을 확인했습니다. 모델은 쿼리 전반에서 비슷한 실수를 반복하는 경향이 있습니다. 이는 모델의 일반적인 실수를 발견한 경우, 이를 수정하는 스크립트를 작성할 수 있음을 의미합니다.  \n",
    "예를 들어, 생성된 JSON 객체에서 닫는 괄호가 누락된 경우, 수동으로 해당 괄호를 추가할 수 있습니다. LinkedIn의 방어적 YAML 파서는 정확한 YAML 출력 비율을 90%에서 99.99%로 증가시켰습니다(Bottaro and Ramgopal, 2020).\n",
    "\n",
    ">**TIP:**  \n",
    ">\n",
    ">JSON과 YAML은 일반적인 텍스트 형식입니다. LinkedIn은 자체 모델인 GPT-4가 둘 다 잘 작동한다는 점을 발견했지만, YAML을 출력 형식으로 선택했습니다. 이는 YAML이 덜 장황하며, 따라서 JSON보다 적은 출력 토큰을 필요로 하기 때문입니다(Bottaro and Ramgopal, 2020).\n",
    "\n",
    "후처리는 실수를 쉽게 수정할 수 있는 경우에만 효과적으로 작동합니다. 이는 일반적으로 모델의 출력이 이미 대부분 올바른 형식으로 생성되었고, 소수의 작은 오류만 포함된 경우에 해당됩니다.\n",
    "\n",
    "---\n",
    "\n",
    "**제한된 샘플링**\n",
    " \n",
    "제한된 샘플링은 텍스트 생성을 특정 제약 조건으로 유도하는 기술입니다. 일반적으로 구조화된 출력 도구에서 사용됩니다.\n",
    " \n",
    "높은 수준에서 보면, 토큰을 생성하기 위해 모델은 제약 조건을 충족하는 값들 중에서 샘플링합니다. 토큰을 생성하기 위해 모델이 먼저 로짓 벡터를 출력하며, 각 로짓은 가능한 하나의 토큰에 대응한다는 점을 상기하세요.\n",
    "\n",
    "제한된 샘플링은 이 로짓 벡터를 필터링하여 제약 조건을 충족하는 토큰만 유지합니다. 그런 다음 이러한 유효한 토큰들 중에서 샘플링합니다. 이 과정은 Figure 2-21에 나와 있습니다.\n",
    "\n",
    "<img src=\"images/fig_02_21.png\" width=800>\n",
    "\n",
    "**제약 조건 샘플링 (Constrained Sampling)**\n",
    "\n",
    "Figure 2-21의 예시에서 보듯, 제약 조건은 필터링하기 비교적 간단합니다. 그러나 대부분의 경우 그렇게 간단하지는 않습니다. 각 단계에서 허용되는 것과 허용되지 않는 것을 명시하는 문법이 필요합니다. 예를 들어, JSON 문법에서는 `{` 이후에 또 다른 `{`가 올 수 없으며, 문자열의 일부로 포함된 경우를 제외하고는 허용되지 않습니다. 예를 들어 `{\"key\": \"{{string}}\"}`와 같은 형태입니다.\n",
    "\n",
    "이러한 문법을 작성하고 샘플링 프로세스에 통합하는 것은 간단한 작업이 아닙니다. JSON, YAML, 정규 표현식(Regex), CSV 등 각 출력 형식마다 고유의 문법이 필요하며, 이러한 제약 조건 샘플링은 범용성이 떨어집니다. 외부 도구나 팀에서 지원하는 형식에만 국한됩니다. 문법 검증은 생성 지연(latency)을 증가시킬 수도 있습니다 (Brandon T. Willard, 2024).\n",
    "\n",
    "일부 사람들은 제약 조건 샘플링에 필요한 자원을 투자하는 것보다 모델이 지침을 더 잘 따르도록 학습시키는 것이 더 낫다고 생각합니다.\n",
    "\n",
    "**미세 조정 (Finetuning)**\n",
    "\n",
    "모델을 원하는 출력 형식에 맞는 예제들로 미세 조정하는 것은 이러한 형식을 생성하도록 모델을 학습시키는 가장 효과적이고 일반적인 방법입니다. 이 접근법은 기대하는 형식과 함께 작동할 수 있습니다. 단순한 미세 조정만으로는 모델이 항상 기대한 형식을 출력하도록 보장할 수는 없지만, 프롬프트만 사용하는 것보다 훨씬 신뢰할 만합니다.\n",
    "\n",
    "특정 작업의 경우, 미세 조정을 시작하기 전에 모델의 구조를 수정하여 출력 형식을 보장할 수 있습니다. 예를 들어, 분류 작업에서는 기초 모델(foundation model)의 구조에 분류기(classifier) 헤드를 추가하여, 모델이 사전에 지정된 클래스 중 하나만 출력하도록 보장할 수 있습니다. 이 구조는 Figure 2-22에 나와 있습니다. 이러한 접근법은 **특징 기반 전이(feature-based transfer)**라고도 하며, Chapter 7에서 다른 전이 학습 기법들과 함께 더 자세히 다룹니다.\n",
    "\n",
    "<img src=\"images/fig_02_22.png\" width=800>\n",
    "\n",
    "미세 조정 동안, 전체 모델을 처음부터 끝까지 재학습시키거나 모델의 일부분(예: 이 분류기 헤드)을 재학습시킬 수 있습니다. End-to-end 학습은 더 많은 자원을 요구하지만, 더 나은 성능을 약속합니다.\n",
    "\n",
    "구조화된 출력을 생성하기 위해 필요한 기술이 있습니다. 모델 자체만으로는 구조화된 출력을 생성할 수 없다는 가정 때문입니다. 그러나 모델이 점점 더 강력해짐에 따라, 모델이 지침을 더 잘 따를 수 있을 것으로 기대할 수 있습니다. 앞으로는 최소한의 프롬프트로 필요한 것을 더 쉽게 출력할 수 있게 될 것이며, 이러한 기술의 중요성은 줄어들 것입니다.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AI의 확률적 특성 (The Probabilistic Nature of AI)\n",
    "\n",
    "AI 모델이 응답을 샘플링하는 방식은 모델을 **확률적(probabilistic)**으로 만듭니다. 예를 들어 확률적이라는 것이 무엇을 의미하는지 살펴보겠습니다. 세계 최고의 요리가 무엇인지 알고 싶다고 가정해 보세요. 친구에게 이 질문을 하고 1분 뒤에 다시 물어본다면, 친구의 답변은 두 번 모두 동일해야 할 것입니다. 그러나 AI 모델에게 같은 질문을 두 번 하면 답이 바뀔 수 있습니다. 예를 들어 AI 모델이 베트남 요리가 세계 최고의 요리일 확률이 70%이고, 이탈리아 요리가 30%라고 생각한다면, \"베트남 요리\"라고 대답할 확률이 70%이고, \"이탈리아 요리\"라고 대답할 확률이 30%일 것입니다. 확률적의 반대 개념은 **결정론적(deterministic)**으로, 결과가 랜덤한 변동 없이 결정될 수 있는 경우를 말합니다.\n",
    "\n",
    "이 확률적 특성은 **일관성 부족(inconsistency)**과 **환각(hallucination)**을 초래할 수 있습니다. **일관성 부족**은 모델이 동일하거나 약간 다른 프롬프트에 대해 매우 다른 응답을 생성할 때 발생합니다. **환각**은 모델이 사실에 기반하지 않은 응답을 생성할 때 발생합니다. 예를 들어, 인터넷 어딘가에 \"모든 미국 대통령이 외계인이다\"라는 에세이가 있고, 이것이 모델의 훈련 데이터에 포함되었다고 상상해 보세요. 그러면 모델은 현재 미국 대통령이 외계인이라는 확률적 출력을 생성할 수 있습니다. AI의 출력이 잘못된 것으로 보이는 사람 입장에서 이는 \"만들어진\" 정보입니다.\n",
    "\n",
    "기초 모델은 일반적으로 대규모 데이터를 사용하여 훈련됩니다. 이러한 데이터는 대중의 의견을 집합적으로 포함하며, 사실상 \"무한한 가능성\"의 세계를 담고 있습니다. AI는 비록 터무니없거나 잘못된 정보일지라도 비영 제로(non-zero) 확률을 가진 모든 것을 생성할 수 있습니다.\n",
    "\n",
    "이러한 특성은 AI 애플리케이션을 만드는 일을 흥미롭고 도전적으로 만듭니다. 많은 AI 엔지니어링 노력은 이 확률적 특성을 활용하고 완화하는 데 초점을 맞추고 있습니다.\n",
    "\n",
    "이 확률적 특성은 AI를 창의적인 작업에 매우 적합하게 만듭니다. 창의성이란 기존의 경로를 넘어 새로운 생각을 탐구하는 능력 아닐까요? AI는 창의적인 전문가들에게 훌륭한 조력자가 됩니다. 무한한 아이디어를 구상하고 전에 없던 디자인을 생성할 수 있습니다. 그러나 이 동일한 확률적 특성은 다른 모든 작업에서는 골칫거리가 될 수 있습니다.\n",
    "\n",
    "**일관성 부족**\n",
    "\n",
    "모델의 일관성 부족은 두 가지 시나리오에서 나타납니다:\n",
    "\n",
    "1. **동일한 입력, 다른 출력**: 동일한 프롬프트를 두 번 제공했을 때 매우 다른 응답이 생성되는 경우.\n",
    "2. **약간 다른 입력, 극적으로 다른 출력**: 예를 들어 실수로 글자를 대문자로 입력하는 등 약간 다른 프롬프트를 제공했을 때 매우 다른 응답이 생성되는 경우.\n",
    "\n",
    "**Figure 2-23**은 제가 ChatGPT를 사용해 에세이를 평가하려 한 예를 보여줍니다. 동일한 프롬프트를 두 번 실행했을 때, 각각 3/5와 5/5라는 서로 다른 점수를 받았습니다.\n",
    "\n",
    "<img src=\"images/fig_02_23.png\" width=800>\n",
    "\n",
    "일관성 부족은 사용자 경험에 혼란을 줄 수 있습니다. 사람 간의 대화에서 우리는 일정 수준의 일관성을 기대합니다. 예를 들어, 누군가가 매번 다른 이름으로 자신을 소개한다고 상상해 보세요. 마찬가지로, 사용자들은 AI와 소통할 때도 일정 수준의 일관성을 기대합니다.\n",
    "\n",
    "**동일한 입력, 다른 출력**의 시나리오에서는 일관성 부족을 완화하기 위한 여러 접근법이 있습니다. 예를 들어, 동일한 질문이 다시 주어졌을 때 동일한 응답이 반환되도록 답변을 캐싱(caching)할 수 있습니다. 또한, 이전에 설명한 것처럼 **온도(temperature)**, **top-p**, **top-k** 값과 같은 모델의 샘플링 변수를 고정할 수도 있습니다. **seed 변수**를 고정하는 것도 가능합니다. 이를 난수 생성기의 시작점으로 생각할 수 있습니다.\n",
    "\n",
    "`하지만 이러한 변수를 모두 고정하더라도 모델이 항상 100% 일관성을 유지할 것이라는 보장은 없습니다. 모델이 출력 생성을 실행하는 하드웨어도 출력에 영향을 미칠 수 있습니다. 서로 다른 기계는 동일한 명령을 실행하는 방식이 다르며, 숫자의 범위를 처리하는 능력도 다릅니다. 모델을 직접 호스팅한다면 사용하는 하드웨어에 대해 어느 정도 통제권을 가질 수 있습니다. 그러나 OpenAI나 Google 같은 모델 API 제공업체를 사용하는 경우, 통제권은 해당 제공업체에 달려 있습니다`.\n",
    "\n",
    "출력 생성 설정을 고정하는 것은 좋은 실천 방법이지만, 시스템에 대한 신뢰를 불러일으키지는 않습니다. 예를 들어, 특정 방에 있을 때만 일관된 점수를 주는 선생님을 상상해 보세요. 그 선생님이 다른 방으로 이동하면 그 점수는 크게 변할 것입니다.\n",
    "\n",
    "두 번째 시나리오인 약간 다른 입력, 극적으로 다른 출력은 더 도전적인 문제입니다. 모델의 출력 생성 변수를 고정하는 것은 여전히 좋은 실천 방법이지만, 이것이 서로 다른 입력에 대해 동일한 출력을 생성하도록 강제하지는 않습니다. 그러나 `신중하게 작성된 프롬프트(Chapter 5에서 논의됨)와 메모리 시스템(Chapter 6에서 논의됨)을 사용하면 모델이 원하는 응답에 더 가까운 출력을 생성하도록 할 수 있습니다`.\n",
    "\n",
    "--- \n",
    "\n",
    "**환각 (Hallucination)**\n",
    "\n",
    "환각은 사실성에 의존하는 작업에서는 치명적입니다. 예를 들어, AI에게 백신의 장단점을 설명하도록 요청했는데 AI가 사이비 과학적인 정보를 제공한다면 문제가 될 것입니다. 2023년 6월, 한 법률 회사가 ChatGPT를 사용해 소송 자료를 준비하던 중 AI의 환각 경향을 인지하지 못해 허위 법률 자료를 제출했다가 벌금을 부과받은 사례가 있습니다.\n",
    "\n",
    "환각은 대규모 언어 모델(LLM)의 부상과 함께 두드러진 문제가 되었지만, 기초 모델과 트랜스포머 아키텍처가 도입되기 훨씬 전부터 생성 모델에서 흔히 발생하던 현상이었습니다. 텍스트 생성에서의 환각은 이미 2016년에 언급되었으며(Goyal et al., 2016), 이를 감지하고 측정하는 것은 이후 자연어 생성(NLG)의 주요 연구 주제가 되어 왔습니다(Lee et al., 2018; Nie et al., 2019; Zhou et al., 2020). 이 섹션에서는 환각이 발생하는 이유를 설명하고, 감지 및 측정 평가 방법은 Chapter 4에서 다룹니다.\n",
    "\n",
    "샘플링 과정의 랜덤성에서 일관성 부족이 발생한다면, 환각의 원인은 더 미묘합니다. 샘플링 과정만으로는 환각을 충분히 설명할 수 없습니다. 모델은 가능한 모든 옵션에서 출력을 샘플링합니다. 그렇다면, 한 번도 본 적이 없는 것이 어떻게 가능한 옵션이 될 수 있을까요? 모델은 훈련 데이터에 포함되지 않았을 것으로 여겨지는 아이디어를 출력할 수 있습니다. 그러나 훈련 데이터를 일일이 검토하여 특정 아이디어가 포함되었는지 확인하는 것은 불가능하기 때문에 이를 확실히 말할 수는 없습니다. 우리가 너무 복잡해서 이해할 수 없는 것을 구축할 수 있다는 사실은 축복이자 저주입니다.\n",
    "\n",
    "환각이 발생하는 이유를 이해하지 않고서는 이를 제거하는 쉬운 방법을 고안하기 어렵습니다. 현재 언어 모델이 환각을 일으키는 이유에 대한 두 가지 가설이 제시되고 있습니다.\n",
    "\n",
    "첫 번째 가설은 2021년 DeepMind의 Ortega et al.에 의해 처음 제기되었습니다. 이 가설에 따르면 언어 모델은 주어진 데이터와 생성된 데이터를 구분하지 못하기 때문에 환각을 일으킨다는 것입니다. 이를 예로 설명해 보겠습니다.\n",
    "\n",
    "모델에 “Chip Huyen이 누구인가요?”라는 프롬프트를 제공했다고 가정합시다. 모델의 첫 번째 출력 문장은 “Chip Huyen은 건축가입니다.”라고 생성되었습니다. 모델이 생성하는 다음 토큰은 “Chip Huyen이 누구인가요? Chip Huyen은 건축가입니다.”라는 시퀀스를 바탕으로 생성됩니다. 모델은 “Chip Huyen은 건축가입니다.”라는 문장을 그것이 제공받은 사실로 취급합니다. 약간 비정상적인 시퀀스에서 시작하여 모델은 확장되며 매우 잘못된 출력을 생성할 수 있습니다. Ortega et al.은 이러한 유형의 환각을 **자기기만(self-delusion)**이라고 부릅니다.\n",
    "\n",
    "**Figure 2-24**는 LLaVA-v1.5-7B 모델의 자기기만의 예를 보여줍니다. 제가 모델에게 이미지에 있는 제품(샴푸 병)의 라벨에서 성분 목록을 식별하라고 요청했을 때, 모델은 해당 이미지를 우유 병으로 인식한 뒤 계속해서 우유 성분을 라벨에서 추출된 것으로 포함시켰습니다.\n",
    "\n",
    "<img src=\"images/fig_02_24.png\" width=800>\n",
    "\n",
    "DeepMind의 연구는 두 가지 기법을 통해 환각을 완화할 수 있음을 보여주었습니다. 첫 번째 기법은 강화 학습(reinforcement learning)을 활용하며, 여기에서 모델은 사용자 제공 프롬프트(강화 학습에서 \"세상에 대한 관찰(observations about the world)\"이라고 불림)와 모델이 생성한 토큰(강화 학습에서 \"모델의 행동(actions)\"이라고 불림)을 구별하도록 만듭니다. 두 번째 기법은 감독 학습(supervised learning)에 기반하며, 훈련 데이터에 사실적(factual) 신호와 반사실적(counterfactual) 신호를 포함합니다.\n",
    "\n",
    "두 번째 가설은 환각이 모델의 내부 지식과 라벨러(labeler)의 내부 지식 간의 불일치로 인해 발생한다고 주장합니다. 이 관점은 OpenAI 연구자인 **Leo Gao**가 처음 제기했습니다. SFT(Supervised Fine-Tuning) 동안, 모델은 라벨러가 작성한 응답을 모방하도록 훈련됩니다. 이러한 응답이 라벨러가 가지고 있지만 모델이 가지고 있지 않은 지식을 사용할 경우, 이는 본질적으로 모델에게 환각을 학습시키는 셈입니다. 이론적으로 라벨러가 작성한 각 응답에 사용하는 지식을 포함시켜 모델이 응답이 만들어진 것이 아님을 알 수 있도록 한다면, 모델이 알고 있는 것만 사용하도록 가르칠 수 있습니다. 그러나 현실적으로 이는 불가능합니다.\n",
    "\n",
    "2023년 4월, OpenAI의 공동 창립자인 **John Schulman**은 UC 버클리 강연에서 동일한 견해를 밝혔습니다. Schulman은 대규모 언어 모델(LLMs)이 자신이 무엇을 알고 있는지 알고 있다고 믿으며, 이는 그 자체로 큰 주장입니다. 만약 이 믿음이 사실이라면, 모델이 자신이 알고 있는 정보에만 기반하여 답변하도록 강제함으로써 환각을 해결할 수 있습니다. 그는 두 가지 해결책을 제안했습니다. 첫 번째는 **검증**입니다. 각 응답에 대해, 모델이 해당 응답을 기반으로 한 출처를 검색하도록 요청하는 것입니다. 두 번째는 **강화 학습**(reinforcement learning)을 사용하는 것입니다. 여기에서 보상 모델은 단순히 응답 A가 응답 B보다 낫다는 비교를 통해 훈련됩니다(A가 왜 더 나은지에 대한 설명 없이). Schulman은 모델이 잘못된 정보를 생성했을 때 더 많은 페널티를 부과하는 더 나은 보상 함수가 환각을 줄이는 데 도움을 줄 수 있다고 주장했습니다.\n",
    "\n",
    "같은 강연에서 Schulman은 OpenAI가 RLHF(보상 모델을 사용한 강화 학습)가 환각 감소에 도움이 된다는 것을 발견했다고 언급했습니다. 그러나 **InstructGPT** 논문에서는 RLHF가 환각을 악화시켰다는 점을 보여줍니다(이는 **Figure 2-26**에서 확인할 수 있습니다). RLHF가 InstructGPT의 환각을 악화시킨 것처럼 보였음에도 불구하고, 다른 측면에서는 개선되었으며, 전체적으로 인간 라벨러들은 SFT(Supervised Fine-Tuning) 모델보다 RLHF 모델을 선호했습니다.\n",
    "\n",
    "기초 모델이 자신이 알고 있는 것을 알고 있다는 가정을 바탕으로, 일부 사람들은 \"가능한 한 진실되게 답변하고, 답을 확신할 수 없다면 '죄송하지만 모르겠습니다'라고 말하세요.\"와 같은 프롬프트를 추가하여 환각을 줄이려고 시도합니다. 모델에게 간단한 응답을 요청하는 것도 환각 감소에 도움이 되는 것으로 보입니다. 모델이 생성해야 하는 토큰이 적을수록 거짓말을 만들어낼 가능성이 줄어들기 때문입니다. Chapter 5와 6에서 다루는 프롬프팅과 컨텍스트 구성 기법도 환각을 완화하는 데 도움이 될 수 있습니다.\n",
    " \n",
    "앞서 논의한 두 가지 가설은 서로를 보완합니다. 자기기만 가설은 자기 감독이 어떻게 환각을 유발하는지에 초점을 맞추는 반면, 불일치된 내부 지식 가설은 감독이 어떻게 환각을 유발하는지에 초점을 맞춥니다.\n",
    " \n",
    "환각을 완전히 막을 수 없다면, 적어도 모델이 환각을 일으킬 때를 감지하여 사용자에게 환각된 응답을 제공하지 않을 수는 없을까요? 하지만 환각을 감지하는 것도 그리 간단하지 않습니다. 다른 사람이 거짓말을 하거나 무언가를 지어내고 있을 때 이를 감지하기가 얼마나 어려운지 생각해보세요. 그래도 사람들은 시도해왔습니다. 환각을 감지하고 측정하는 방법은 Chapter 4에서 논의합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 요약 (Summary)\n",
    "\n",
    "이 장에서는 기초 모델(foundation model)을 구축할 때 핵심 설계 결정을 논의했습니다. 대부분의 사람들은 기초 모델을 처음부터 훈련하는 대신, 이미 준비된 기초 모델을 사용할 것이기 때문에, 훈련 세부 사항을 생략하고 어떤 모델을 사용할지, 그리고 어떻게 사용할지 결정하는 데 도움을 주는 모델링 요인에 대해 다루었습니다.\n",
    "\n",
    "모델 성능에 영향을 미치는 중요한 요소는 훈련 데이터입니다. 대규모 모델은 대량의 훈련 데이터가 필요하며, 이를 확보하는 데 비용이 많이 들고 시간이 소요될 수 있습니다. 따라서 모델 제공자는 일반적으로 이용 가능한 데이터를 활용합니다. 이는 훈련 데이터에 존재하는 많은 작업을 잘 수행할 수 있는 모델을 만들게 되지만, 반드시 사용자가 원하는 특정 작업을 포함하지 않을 수도 있습니다. 이 장에서는 특정 언어, 특히 저자원 언어와 특정 도메인을 대상으로 하는 모델을 개발하기 위해 훈련 데이터를 큐레이팅해야 하는 이유를 설명했습니다.\n",
    "\n",
    "`데이터를 확보한 후, 모델 개발이 시작됩니다. 모델 훈련이 종종 헤드라인을 장식하지만, 그에 앞서 중요한 단계는 모델의 아키텍처를 설계하는 것`입니다. 이 장에서는 모델 아키텍처와 모델 크기와 같은 모델링 선택 사항을 살펴보았습니다. 언어 기반 기초 모델을 위한 주요 아키텍처는 트랜스포머(Transformer)이며, 이 장에서는 트랜스포머 아키텍처가 해결하려고 했던 문제와 그 한계에 대해 탐구했습니다.\n",
    "\n",
    "`모델의 규모는 세 가지 핵심 숫자로 측정할 수 있습니다: 매개변수(parameter) 수, 훈련 토큰 수, 그리고 훈련에 필요한 FLOP 수`입니다. 모델을 훈련하는 데 필요한 계산량은 모델 크기와 데이터 크기에 의해 영향을 받습니다. 스케일링 법칙(scaling law)은 주어진 예산에서 최적의 매개변수 수와 토큰 수를 결정하는 데 도움을 줍니다. 이 장에서는 스케일링의 병목 현상에 대해서도 논의했습니다. 현재로서는 모델을 확장하면 일반적으로 더 나아지지만, 이것이 얼마나 더 지속될 수 있을지는 의문입니다.\n",
    "\n",
    "훈련 데이터의 낮은 품질과 사전 훈련 중 자기지도 학습(self-supervision) 때문에 결과적으로 모델은 사용자가 원하는 것과 일치하지 않는 출력을 생성할 수 있습니다. 이는 두 단계로 구성된 사후 훈련(post-training)을 통해 해결됩니다: 감독된 미세 조정(supervised finetuning)과 선호도 기반 미세 조정(preference finetuning). 인간의 선호는 다양하며 단일 수학적 공식으로 포착할 수 없기 때문에, 기존 솔루션은 완벽하지 않습니다.\n",
    "\n",
    "이 장에서는 제가 가장 좋아하는 주제인 샘플링(sampling)도 다뤘습니다. 샘플링은 모델이 출력을 생성하는 과정이며, 이는 AI 모델을 확률적으로 만듭니다. 이러한 확률적 특성은 ChatGPT와 Gemini와 같은 모델을 창의적인 작업과 대화를 위한 재미있는 도구로 만듭니다. 그러나 이 확률적 특성은 또한 일관성 부족과 환각을 초래하기도 합니다.\n",
    "\n",
    "AI 모델과 작업하려면 확률적 특성을 고려하여 워크플로우를 구축해야 합니다. 이 책의 나머지 부분에서는 AI 엔지니어링을 결정론적이지는 않더라도 최소한 체계적으로 만드는 방법을 살펴볼 것입니다. 체계적인 AI 엔지니어링을 위한 첫 번째 단계는 실패와 예기치 않은 변화를 감지하는 데 도움이 되는 견고한 평가 파이프라인을 구축하는 것입니다. 기초 모델에 대한 평가는 매우 중요하기 때문에 다음 장부터 시작하여 두 장을 할애했습니다.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. \"GPT-4는 수학 문제를 해결할 수 있지만, 모든 언어에서 그런 것은 아니다(GPT-4 Can Solve Math Problems—but Not in All Languages)\" - Yennie Jun. 이 연구는 OpenAI의 Tokenizer를 사용하여 확인할 수 있습니다.\n",
    "\n",
    "2. 이는 사전 훈련 데이터나 정렬 데이터의 편향 때문일 수 있습니다. 아마도 OpenAI는 중국어 또는 중국 중심 내러티브 데이터가 모델 훈련에 충분하지 않았을 것입니다.\n",
    "\n",
    "3. \"AI를 똑똑하게 보이게 만드는 비밀 웹사이트들(Inside the Secret List of Websites That Make AI like ChatGPT Sound Smart)\" - Washington Post, 2023.\n",
    "\n",
    "4. 텍스트의 경우 도메인 키워드를 휴리스틱으로 사용할 수 있지만, 이미지의 경우 명백한 휴리스틱이 없습니다. Vision 데이터셋에 대한 분석은 이미지 크기, 해상도, 비디오 길이에 대해 제한적입니다.\n",
    "\n",
    "5. ML의 기본 원리는 이 책의 범위를 벗어나지만, 논의와 관련하여 몇 가지를 언급합니다. 예를 들어, Chapter 1에서는 데이터에서 자체 라벨을 생성하는 자기지도 학습(self-supervision)을 다루고, Chapter 7에서는 모델의 매개변수가 오류를 기반으로 업데이트되는 방법인 역전파(backpropagation)를 논의합니다.\n",
    "\n",
    "6. 순환 신경망(RNN)은 그 재귀적 구조로 인해 소멸 기울기(vanishing gradients)와 폭발 기울기(exploding gradients)에 특히 취약합니다. 기울기는 여러 단계를 통해 전파되어야 하고, 값이 작을 경우 반복된 곱셈으로 인해 0으로 수렴하여 학습이 어려워집니다. 반대로, 값이 클 경우 단계별로 지수적으로 증가해 학습 과정의 불안정성을 초래합니다.\n",
    "\n",
    "7. Bahdanau et al., \"Neural Machine Translation by Jointly Learning to Align and Translate\".\n",
    "\n",
    "8. 입력 토큰이 배치로 처리되므로, 실제 입력 벡터의 형상은 $N \\times T \\times 4096$입니다. 여기서 $N$은 배치 크기이고, $T$는 시퀀스 길이입니다. 마찬가지로 각 결과 $K$, $V$, $Q$ 벡터의 차원은 $N \\times T \\times 4096$입니다.\n",
    "\n",
    "9. 왜 복잡한 모델(LLMs)에서는 간단한 활성화 함수가 잘 작동할까요? 한때 연구 커뮤니티는 정교한 활성화 함수를 개발하려고 노력했습니다. 그러나 더 정교한 활성화 함수가 더 나은 성능을 내지 않는다는 것이 밝혀졌습니다. 모델은 단순히 피드포워드 레이어의 선형성을 깨는 비선형 함수가 필요할 뿐입니다. 계산이 더 빠르고 메모리와 훈련 리소스가 적게 소모되는 간단한 함수가 더 낫습니다.\n",
    "\n",
    "10. 재미있는 사실: Ilya Sutskever(OpenAI 공동 창립자)는 seq2seq 논문의 제1저자이며 AlexNet 논문의 제2저자입니다.\n",
    "\n",
    "11. Ilya Sutskever는 왜 새로운 신경망 아키텍처를 개발하는 것이 어려운지에 대해 흥미로운 주장을 합니다. 그의 주장에 따르면, 신경망은 여러 컴퓨터 프로그램을 시뮬레이션하는 데 매우 뛰어납니다. 신경망 훈련 기술인 그래디언트 하강법(gradient descent)은 사실 최적의 프로그램을 탐색하는 검색 알고리즘입니다. 새로운 아키텍처가 기존 아키텍처를 능가하려면 기존 아키텍처가 시뮬레이션할 수 없는 프로그램을 시뮬레이션해야 한다고 설명합니다. 추가 정보는 Berkeley의 Simons Institute 강연(2023)을 참고하세요.\n",
    "\n",
    "12. 트랜스포머는 원래 Google에 의해 TPU(Tensor Processing Units)에서 실행되도록 설계되었으며, 이후 GPU에서 최적화되었습니다.\n",
    "\n",
    "13. 실제 메모리 요구 사항은 더 높습니다. Chapter 7에서는 모델의 메모리 사용량 계산 방법을 다룹니다.\n",
    "\n",
    "14. 책 한 권은 약 50,000단어 또는 67,000 토큰으로 가정합니다.\n",
    "\n",
    "15. 작성 시점에서 대규모 모델은 일반적으로 하나의 에폭(epoch)으로만 사전 훈련됩니다.\n",
    "\n",
    "16. FLOP은 FP32 형식으로 측정됩니다. 부동소수점 연산의 세부 사항은 Chapter 7에서 논의됩니다.\n",
    "\n",
    "17. 작성 시점 기준, 클라우드 제공업체들은 H100 GPU를 시간당 약 $2에서 $5로 제공하고 있습니다. 계산 비용이 빠르게 저렴해지고 있어, 이 숫자는 더욱 낮아질 것으로 보입니다.\n",
    "\n",
    "18. **Jascha Sohl-Dickstein**, 놀라운 연구자가 자신의 X 페이지에 **하이퍼파라미터가 작동하거나 작동하지 않는 이유에 대한 아름다운 시각화를 공유**했습니다.\n",
    "\n",
    "19. **Dario Amodei**, Anthropic의 CEO는 \"스케일링 가설이 사실이라면, $1000억 규모의 AI 모델은 노벨상 수상자만큼 똑똑할 것이다.\"라고 말했습니다.\n",
    "\n",
    "20. AI 생성 콘텐츠는 기계 번역의 용이성으로 인해 증가합니다. AI는 기사를 생성하고, 이를 여러 언어로 번역할 수 있습니다. 이는 **“A Shocking Amount of the Web Is Machine Translated” (Thompson et al., 2024)**에서 확인할 수 있습니다.\n",
    "\n",
    "21. 한 친구는 \"사전 훈련된 모델은 웹 페이지처럼 말하지, 사람처럼 말하지 않는다.\"는 비유를 들었습니다.\n",
    "\n",
    "22. 강화 학습(RL)의 기본 원리는 이 책의 범위를 벗어나지만, 핵심은 RL이 인간 선호와 같은 어려운 목표를 최적화하도록 도와준다는 점입니다.\n",
    "\n",
    "23. 잘못 조정된 모델이 더 나을 수 있는 상황도 있습니다. 예를 들어, AI를 사용해 잘못된 정보를 전파할 위험을 평가하고 싶다면, 가짜 뉴스를 최대한 설득력 있게 만들 수 있는 모델을 구축하여 AI의 설득력을 테스트할 수도 있습니다.\n",
    "\n",
    "24. 온도(temperature)에 대한 비과학적인 시각적 이미지는 다음과 같습니다. 높은 온도는 확률 분포를 더 혼란스럽게 만들어, 낮은 확률의 토큰이 표면에 나타날 가능성을 높입니다.\n",
    "\n",
    "25. Argmax 함수로 실행합니다.\n",
    "\n",
    "26. **언더플로우(underflow)** 문제는 숫자가 주어진 형식으로 표현되기에 너무 작아 0으로 반올림되는 경우 발생합니다.\n",
    "\n",
    "27. 구체적으로 말하자면, 작성 시점 기준으로 OpenAI API는 최대 20개의 가장 가능성 높은 토큰에 대한 로그 확률(logprobs)만을 표시합니다. 이전에는 사용자 제공 텍스트에 대한 임의의 로그 확률을 가져올 수 있었으나, 이는 **2023년 9월**에 중단되었습니다. Anthropic은 모델의 로그 확률을 공개하지 않습니다.\n",
    "\n",
    "28. 유료 모델 API는 종종 출력 토큰 수당 요금을 부과합니다.\n",
    "\n",
    "29. 동일 입력에 대해 여러 출력을 생성하는 비용을 줄이기 위해 할 수 있는 몇 가지 일이 있습니다. 예를 들어, 입력을 한 번만 처리하고 이를 모든 출력에 재사용하는 방법이 있습니다.\n",
    "\n",
    "30. 작성 시점 기준으로 OpenAI API에서 **best_of** 매개변수를 특정 값(예: 10)으로 설정하여 OpenAI 모델이 10개의 서로 다른 출력 중 가장 높은 평균 로그 확률을 가진 출력을 반환하도록 요청할 수 있습니다.\n",
    "\n",
    "31. **Wang et al. (2023)** 은 이 접근 방식을 **자기 일관성(self-consistency)** 이라고 불렀습니다.\n",
    "\n",
    "32. 취약한 모델에 대한 최적의 조치는 이를 다른 모델로 교체하는 것입니다.\n",
    "\n",
    "33. 작성 시점 기준으로, 애플리케이션과 모델에 따라, 올바르게 생성된 JSON 객체의 비율이 0%에서 최대 90% 이상까지 변동되는 것을 보았습니다.\n",
    "\n",
    "34. 데이터에서 원하는 형식에 맞춰 모델을 처음부터 훈련시키는 것도 가능하지만, 이 책은 그런 개발 과정을 다루지 않습니다.\n",
    "\n",
    "35. 일부 미세 조정 서비스는 이를 자동으로 처리합니다. **OpenAI의 미세 조정 서비스**는 훈련 중 분류기 헤드를 추가할 수 있도록 지원했지만, 작성 시점 기준으로 이 기능은 비활성화되었습니다.\n",
    "\n",
    "36. 밈(meme)에서 말하듯이, **가능성은 낮지만, 절대 0은 아니다(chances are low, but never zero).**\n",
    "\n",
    "37. 2023년 12월, 제가 자문한 한 AI 회사의 고객 지원 요청을 3개월치 검토했으며, 질문의 5분의 1이 AI 모델의 일관성 문제를 다루는 방법에 관한 것이었음을 발견했습니다. 2023년 7월에 제가 참여한 패널 토론에서 **Drew Houston**(Dropbox의 CEO)와 **Harrison Chase**(LangChain의 CEO)는 우리 모두가 환각(hallucination)이 많은 AI 엔터프라이즈 사용 사례에서 가장 큰 걸림돌이라고 동의했습니다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
