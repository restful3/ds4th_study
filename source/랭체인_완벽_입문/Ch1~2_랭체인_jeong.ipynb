{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "882ea644-96f4-459e-b319-a8c83bf1a5de",
   "metadata": {},
   "source": [
    "# 목차\n",
    "- Chapter1. 챗GPT와 랭체인\n",
    "    + 1. 챗GPT와 언어모델 알아보기\n",
    "    + 2. 랭체인개요 \n",
    "    + 3. 랭체인 활용예시 \n",
    "    + 4. 실습준비\n",
    "    + 5. OPENAI의 API 호출"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "555679b9-3c61-4293-889e-64814590668e",
   "metadata": {},
   "source": [
    "# 1장. 챗GPT와 랭체인"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f87a8ef7-bdd5-4b23-872d-a6c7521a612a",
   "metadata": {},
   "source": [
    "## 1. 챗GPT와 언어모델 알아보기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b1cc9a7-3ad7-488f-b8d9-2f73be980775",
   "metadata": {},
   "source": [
    "- OpenAI의 API에서 개발하는 언어모델은 크게 두가지 분류 : 'Chat', 'Complete'\n",
    "    + Chat : 대화형 상호작용 특화, 질문 댓글 의견 등에 대한 답변을 생성하고 그 답변 바탕으로 대화 나눔\n",
    "    + Complete : 주어진 텍스트에 이어 텍스트 생성, 어느정도 정보/이야기를 시작 제공하면 이를 바탕으로 자동 보완\n",
    "        - 현재 gpt-4나 claud-3는 complete 모델이 존재하지 않음\n",
    "- 모델 선택 시 컨텍스트 길이 (토큰수) 고려 필요 : 일반모델은 4k, 16k는 16000개까지 처리가능\n",
    "    + gpt-3.5-turbo 등 뒤에 4자리가 없는 모델은 최신모델임을 의미, 업데이트시 자동으로 반영\n",
    "    + gpt-3.5-turbo-0613 등 뒤에 4자리 붙은 모델은 특정 버전이 고정된 것으로 업데이트가 반영되지 않음 . 특정결과 필요하거나 변동성 피하고 싶을때 선택\n",
    "    + gpt-3.5-turbo-instruct는 문제해결, 문장생성, 질문응답, 대화생성 등 다양한 작업에 활용\n",
    "    + claude2는 100k 토큰까지 입력가능 : 프로젝트 전체 소스코드를 생성하고 버그 수정도 가능\n",
    "    + LLaMa3는 오픈소스이나, 성능면에서 타오픈소스 모델 압도, 특히 70b모델은 claude3나 GPT4의 성능에 근접"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffe86032-637a-456d-8db1-a712b879940f",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Groq을 써서 로컬모델도 무료로 돌리기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deed605c-1049-4496-b14b-1593572f200b",
   "metadata": {},
   "source": [
    "- Groq을 쓰면 모델을 다운로드 받을 필요가 없습니다. 속도는 심지어 AWS, Azure보다 10배이상 빠릅니다.\n",
    "- 무료이며, 기업용은 별도로 비용이 발생 예상합니다, [API 발급](https://console.groq.com/docs/quickstart) 후 사용하세요"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "64f501b2-b80b-4afd-9eb8-2a7cdf3ff480",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "의치아 하nin! AI시대에 준비할 가치가 있는 가장 기초적인 것 같지는 않나요?󠁧󠁢󠁥󠁮󠁧󠁿\n",
      "\n",
      "우선, AI의 발전하고 있는 속도에 맞춰 우리 스스로를 업그레이드하는 것은 정말 중요할 것 같아요. 예를 들어, 새로운 기술과 같은 정보를 해부하고, AI를 접목시킬 수 있는 다양한 프로젝트에 참가하는 것이 좋을 것 같아요. тим에 있는 친구들은 어떤 기술을 사용합니까?🤔\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from groq import Groq\n",
    "load_dotenv()\n",
    "\n",
    "client = Groq(\n",
    "    api_key=os.getenv(\"GROQ_API_KEY\"),\n",
    ")\n",
    "\n",
    "chat_completion = client.chat.completions.create(\n",
    "    messages=[\n",
    "               {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"존댓말 쓰지말고, 친한친구처럼 한국어로 답변\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"AI시대에 우린 무엇을 준비해야 할까?\",\n",
    "        }\n",
    "    ],\n",
    "    model=\"llama3-8b-8192\",\n",
    ")\n",
    "\n",
    "print(chat_completion.choices[0].message.content) # 정적으로 찍을 때 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "2fd031fb-81d9-4471-8f4f-7edebf764e0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "와우, 진짜 좋은 질문이야! 😊\n",
      "\n",
      "솔직히, 결혼하고 애 키우는 건 선택의 문제야. 하지만, 많은 사람들이 그렇게 선택하는 이유는 여러 가지야.\n",
      "\n",
      "첫째, 사랑하는 사람과 함께 살아남는 건 인간의 기본적인 욕구야. 우리는 사회적 동물이라, 가족이나 친척들과의 유대감이 필요해. 결혼하고 애 키우는 건 이러한 유대감을 강화하고, 서로를 지지하고 지원하는 관계를 형성하는 거야.\n",
      "\n",
      "둘째, 인간은 생존과 번영을 위해 노력하는 생물이야. 결혼하고 애 키우는 건 이러한 생존 본능을 충족하는 방법이야. 우리는 가족을 통해 다음 세대를 이어나가고, 우리의 유산을 남기는 거야.\n",
      "\n",
      "셋째, 결혼하고 애 키우는 건 개인의 성장과 발전을 위해 필요한 거야. 우리는 아이들을 통해 새로운 경험과 지식을 얻을 수 있고, 부모로서의 책임감을 느껴. 이러한 경험은 우리의 성장과 발전을 위해 필요한 거야.\n",
      "\n",
      "넷째, 결혼하고 애 키우는 건 사회적 책임감을 느껴. 우리는 사회의 구성원으로서, 다음 세대를 이어나가고, 사회의 발전을 위해 기여하는 거야.\n",
      "\n",
      "물론, 이러한 이유들이 모든 사람에게 적용되는 건 아니야. 하지만, 많은 사람들이 결혼하고 애 키우는 것은 이러한 이유들 때문이야. 🤔None"
     ]
    }
   ],
   "source": [
    "from groq import Groq\n",
    "load_dotenv()\n",
    "\n",
    "client = Groq(\n",
    " api_key=os.getenv(\"GROQ_API_KEY\"),\n",
    ")\n",
    "\n",
    "stream = client.chat.completions.create(\n",
    "    #\n",
    "    # Required parameters\n",
    "    #\n",
    "    messages=[\n",
    "        # Set an optional system message. This sets the behavior of the\n",
    "        # assistant and can be used to provide specific instructions for\n",
    "        # how it should behave throughout the conversation.\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"친한친구처럼, 존댓말 쓰지말고 답변\"\n",
    "        },\n",
    "        # Set a user message for the assistant to respond to.\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"왜 결혼을 하고, 애를 키우며 살아야 할까?\",\n",
    "        }\n",
    "    ],\n",
    "\n",
    "    model=\"llama3-70b-8192\",\n",
    "\n",
    "    # 파라미터 셋팅\n",
    "    #\n",
    "    # Controls randomness: lowering results in less random completions.\n",
    "    # As the temperature approaches zero, the model will become deterministic\n",
    "    # and repetitive.\n",
    "    temperature=0.5,\n",
    "\n",
    "    # The maximum number of tokens to generate. Requests can use up to\n",
    "    # 32,768 tokens shared between prompt and completion.\n",
    "    max_tokens=1024,\n",
    "\n",
    "    # Controls diversity via nucleus sampling: 0.5 means half of all\n",
    "    # likelihood-weighted options are considered.\n",
    "    top_p=0.7,\n",
    "\n",
    "    # A stop sequence is a predefined or user-specified text string that\n",
    "    # signals an AI to stop generating content, ensuring its responses\n",
    "    # remain focused and concise. Examples include punctuation marks and\n",
    "    # markers like \"[end]\".\n",
    "    stop=None,\n",
    "\n",
    "    # If set, partial message deltas will be sent.\n",
    "    stream=True,\n",
    ")\n",
    "\n",
    "# stream=True 로 설정시, chunk로 루프돌려서 choices[0].delta로 가져옴\n",
    "for chunk in stream:\n",
    "    print(chunk.choices[0].delta.content, end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88b1d6ef-1e0b-41d9-abdb-6f5c04293b5d",
   "metadata": {},
   "source": [
    "## 2. 랭체인개요"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eef3fb6-ff16-4caf-81fa-24950cc46cd2",
   "metadata": {},
   "source": [
    "- 고성능 언어모델의 등장으로 기존 절차적 프로그래밍에서 어려웠던 기능을 쉽게 처리 (자연어 처리 및 표현 수정 등)\n",
    "    - 한계 :  논리적 복잡한 문제나 학습지식의 범위 벗어나는 정보 대잡 어려움\n",
    "- 이런 한계를 극복하기 위해 언어모델이 알지못하는 정보도 대답할 수 있게 하는 RAG (Retrieval-Augmented Generation), 추론과 행동을 언어모델 스스로 판단하여 인터넷검색이나 파일 저장 등을 자율적으로 수행하게 하는 React(Reasoming And Acting, 추론 및 행동)\n",
    "- Langchain의 6개 모듈 : [랭체인 다큐먼트 참고](https://python.langchain.com/docs/get_started/introduction.html)\n",
    "    - Model I/O : 언어모델 호출/프롬프트 준비/결과수신\n",
    "    - Retrieval : PDF, CSV, VectorDB 등에서 연관된 정보를 입출력저장\n",
    "    - Memory : 대화를 장/단기적 저장\n",
    "    - Chains : 여러 프로세스 통합/복잡한 기능개발을 쉽게 진행\n",
    "    - Agents : 모델외부와 상호작용하여 기능 확장 (예> 관련된 논문 결과를 크롤링, Rag로 저장, 이미지 인식)\n",
    "    - Callbacks : 이벤트 발생시 처리 수행 (로그 출력이나 외부라이브러리연동사용)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "244977b5-0435-488b-833e-722c743c9002",
   "metadata": {},
   "source": [
    "## 3. 랭체인 활용예시\n",
    "> 랭체인으로 어떤 서비스를 만들수 있을까\n",
    "- 언어모델이 모르는 정보가 있는 PDF를 불러와서 질문하거나 요약할 수 있는 챗봇 애플리케이션 생성\n",
    "- 명령을 통해 행동하는 비서역할의 서비스 \n",
    "    + 예 > 부산에 갈만한 곳을 검색하고 2박3일 일정표를 짜서 iternery.csv파일에 한국어로 저장해줘\n",
    "- 챗지피티는 일반인이 플러그인을 설치해서 단순 사용한다면, 랭체인은 개발자가 보다 확장성 있게 언어모델이 할수 없는 일을 가능하게 만들수 있다\n",
    "    + 예 > 구글캘린더, 지메일에서 정보를 가져와서 매일 아침 slack에서 오늘 일정과 할일 목록을 제안\n",
    "    \n",
    "> 나만의 어플리케이션을 만들어 보자\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e22f330-c6c3-4c38-8bab-dd290415def0",
   "metadata": {},
   "source": [
    "## 4. 실습준비"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db4e2415-778c-47db-87aa-f3c1fe83ca83",
   "metadata": {},
   "source": [
    "- [1. 비주얼 스튜디오 설치](https://azure.microsoft.com/ko-kr/products/visual-studio-code) 및 [파이썬다운로드](https://www.python.org/downloads/)\n",
    "- [2.주피터랩 설치](https://youtu.be/kuhtXwYlvjc?si=UWjIARPutkrWffa8)\n",
    "- [3.가상환경설치 및 주피터랩에띄우기](https://github.com/restful3/ds4th_study/blob/main/source/%EB%9E%AD%EC%B2%B4%EC%9D%B8_%EC%99%84%EB%B2%BD_%EC%9E%85%EB%AC%B8/langchain/langchain%20%ED%99%98%EA%B2%BD%20%EC%84%A4%EC%A0%95.md)\n",
    "- [4. OPENAI API다운받기](https://openai.com/blog/openai-api)\n",
    "- [5. dotenv 설정하기](https://hyunhp.tistory.com/718)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86aaee27-3528-4080-93ca-b1ae43d46d4a",
   "metadata": {},
   "source": [
    "## 5. OPENAI의 API를 호출해보자"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78cd8282-01e1-4e3e-96cb-9103b0bf4bf7",
   "metadata": {},
   "source": [
    "[dotenv](https://hyunhp.tistory.com/718) 설명을 참조하여 API Password를 저장할 수 있도록 합니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1e8bd9f2-512a-4aba-b0a9-43eb499f7a61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "사용자분이 물어보신 LLM이란 용어가 무엇인지 명확하게 알려주셔야 정확한 답변을 드릴 수 있을 것 같습니다. LLM이 특정한 분야나 주제와 관련이 있는 경우 이에 대해서 확인 후 답변을 제공해 드릴 수 있습니다. 추가 정보를 제공해주시면 도움을 드릴 수 있을 것입니다.\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "기본 챗팅모델을 호출하여 답변을 불러와 봅시다.\n",
    "'''\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import openai\n",
    "# from openai import OpenAI\n",
    "\n",
    "# 모델별 API pw를 저장합니다 .env 파일에 저장하고 불러옵니다.\n",
    "load_dotenv()\n",
    "os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "# \n",
    "completion = openai.chat.completions.create(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"LLM이 갑자기 뜬 이유를 알려주세요\",\n",
    "        },\n",
    "    ],\n",
    ")\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1dd8ce56-8938-4e75-af4a-7f55641d7e87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatCompletion(id='chatcmpl-9JIRejDL26MwB2GacjLOE3OtXT9XU', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='사용자분이 물어보신 LLM이란 용어가 무엇인지 명확하게 알려주셔야 정확한 답변을 드릴 수 있을 것 같습니다. LLM이 특정한 분야나 주제와 관련이 있는 경우 이에 대해서 확인 후 답변을 제공해 드릴 수 있습니다. 추가 정보를 제공해주시면 도움을 드릴 수 있을 것입니다.', role='assistant', function_call=None, tool_calls=None))], created=1714386162, model='gpt-3.5-turbo-0125', object='chat.completion', system_fingerprint='fp_3b956da36b', usage=CompletionUsage(completion_tokens=127, prompt_tokens=26, total_tokens=153))\n"
     ]
    }
   ],
   "source": [
    "# completion의 형태를 확인해봅시다\n",
    "print(completion)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9322102-e52f-4c36-bb22-67f037aa594b",
   "metadata": {},
   "source": [
    "- 좋은 결과를 위해서는 더  맥락을 고려한 프롬프트 입력이 필요"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4f101232-d386-408c-96d4-7393163b58d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM인 ChatGPT는 OpenAI에서 개발한 대화형 언어 모델입니다. 이 모델은 GPT-3 모델에 대화 관련 능력을 추가한 것으로, 만약 아직 LLM이 대두되기 전이라면, 그 배경은 GPT-3의 성공적인 출시와 대화 생성 분야에서의 대단한 성과에 기인합니다.\n",
      "\n",
      "ChatGPT는 놀라운 성능을 보여주어 기업 및 개인이 다양한 사용 사례에서 활용할 수 있게 되었습니다. 비즈니스 분야에서는 고객 서비스 질을 향상시키기 위해, ChatGPT를 상담원과 함께 사용하여 실시간 대화를 제공할 수 있습니다. ChatGPT는 특정 도메인에 특화된 문제 해결을 위한 지식을 학습하고 사용자에게 도움을 줄 수 있는 역할도 합니다.\n",
      "\n",
      "또한 ChatGPT는 개별적인 사용자 스타일에 맞게 학습될 수 있는 기능을 제공함으로써 개인화된 서비스를 제공할 수 있습니다. 이 모델은 사용자의 쓰는 스타일과 관심사를 이해하는 능력을 갖추고 있습니다.\n",
      "\n",
      "ChatGPT의 영향력은 이미 상당히 큽니다. 상용화되면서 많은 기업들은 ChatGPT로 고객 서비스와 인터랙션을 개선하거나, 대화 데이터를 생성함으로써 대화 데이터셋을 확장하고 있습니다. ChatGPT는 또한 엔터테인먼트 분야에서도 인기를 끌고 있습니다. ChatGPT는 넓은 범위의 질문에 대답하거나, 가상의 인물과 대화하는 등 현실 세계와 가상 세계 간의 상호작용을 가능하게 합니다.\n",
      "\n",
      "앞으로는 ChatGPT와 같은 LLM의 발전이 기대됩니다. OpenAI는 계속해서 모델의 성능을 향상시키고 있으며, 개발자들에게 다양한 API와 도구를 제공하여 애플리케이션을 개발하는 데 도움을 주고 있습니다. LLM은 지식의 전달, 개인화된 컨텐츠 생성, 고객 서비스 등 다양한 분야에 적용될 것으로 예상됩니다.\n",
      "\n",
      "또한, OpenAI는 LLM의 사용에 관한 윤리적 고려사항과 가능한 한 부정적인 영향을 최소화하기 위해 다양한 기술적 및 제한적 제어 방법을 연구하고 있습니다. 따라서 ChatGPT와 LLM과 관련된 이러한 진보들은 시간이 흐른 뒤 우리의 일상 생활과 상호작용하는 모습을 넓게 바꿀 수 있을 것입니다.\n"
     ]
    }
   ],
   "source": [
    "# 더 긴문장을 호출할 때 모델을 바꿈\n",
    "completion = openai.chat.completions.create(\n",
    "    model=\"gpt-3.5-turbo-16k\",\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"ChatGPT 같은 LLM이 요즘에 핫한데, 대두가 된 배경과 어떤 영향을 미치고 있으며, 앞으로는 어떻게 바뀔지 상세히 알려줘 \",\n",
    "        },\n",
    "    ],\n",
    ")\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0ed8724-1ad4-43f0-8400-761f04f79bd8",
   "metadata": {},
   "source": [
    "- openai에서 antroprophic으로 바꿀 경우 코드호출이 불가능\n",
    "- 그래서 language model(langchain등)을 쓰는이유임"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98fd5954-b92a-4374-992e-900cb707f7c0",
   "metadata": {},
   "source": [
    "# 2장. Model I/O"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9756212d-2ab2-4e42-a681-2d1a68e5ee3a",
   "metadata": {},
   "source": [
    "## 랭귀지모델 활용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e6fc7c25-3444-4329-9c7f-208488f7d6a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\miniconda\\envs\\tw311\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:117: LangChainDeprecationWarning: The class `langchain_community.chat_models.openai.ChatOpenAI` was deprecated in langchain-community 0.0.10 and will be removed in 0.2.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import ChatOpenAI`.\n",
      "  warn_deprecated(\n",
      "E:\\miniconda\\envs\\tw311\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:117: LangChainDeprecationWarning: The function `__call__` was deprecated in LangChain 0.1.7 and will be removed in 0.2.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "안녕하세요! 도와드릴게요. 있어서 좋아요!\n"
     ]
    }
   ],
   "source": [
    "from langchain.chat_models import ChatOpenAI  #← 모듈 가져오기\n",
    "from langchain.schema import HumanMessage  #← 사용자의 메시지인 HumanMessage 가져오기\n",
    "\n",
    "## 모델객체 만들기\n",
    "chat = ChatOpenAI(  #← 클라이언트를 만들고 chat에 저장\n",
    "    model=\"gpt-3.5-turbo\",  #← 호출할 모델 지정\n",
    ")\n",
    "\n",
    "## 위에서 만든 객체 실행하기\n",
    "result = chat( #← 실행하기\n",
    "    [\n",
    "        HumanMessage(content=\"안녕하세요!\"),\n",
    "    ]\n",
    ")\n",
    "print(result.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b20061e-01c1-4f5e-a005-3bdf02629dca",
   "metadata": {},
   "source": [
    "### AIMessage를 사용하여 언어모델의 응답을 표현할 수 있음\n",
    "- 대화형식의 상로작용을 표현 위해 AI Message도 준비됨. \n",
    "    - 첫번째 HumanMessage에 레시피를 반환,\n",
    "    - 아래와 같은 대화흐름에서 어떻게 표현하는지 보자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "40f10afc-8a01-4251-9d91-7f8aa8e5ac09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "죄송합니다. 현재 저는 한국어와 영어로만 대화할 수 있어요. 다른 언어로는 도와드릴 수 없어요. 혹시 다른 질문이 있으시면 알려주세요. 함께 도와드릴게요.\n"
     ]
    }
   ],
   "source": [
    "from langchain.schema import HumanMessage, AIMessage\n",
    "\n",
    "result = chat( #← 실행하기\n",
    "    [\n",
    "        HumanMessage(content=\"Kpop문화에 대해 알려줘\"),\n",
    "        AIMessage(content=\"{ChatModel의 답변}\"),\n",
    "        HumanMessage(content=\"인도네시아어로 번역해줘\"),\n",
    "        \n",
    "    ]\n",
    ")\n",
    "print(result.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88c465e6-e9eb-4139-bba6-832e69c34f4c",
   "metadata": {},
   "source": [
    "> HumanMessage, AIMessage를 통해 상호작용을 표현할 수 있다.\n",
    "위의 랭귀지 모델만으로는 매번 소스코드를 다시 작성해야 하므로 번거로움\n",
    "- 상호작용을 지원하기 위해 Memory모듈이 있음"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6ae5981-315e-4219-8e08-1a419f5eee11",
   "metadata": {},
   "source": [
    "### SystemMessage를 통해 메타 지시 가능"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c8cc92f6-f4f7-404f-8480-4aa2941c1f6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "안녕! 밥은 아직 안 먹었어. 너는 먹었니?\n"
     ]
    }
   ],
   "source": [
    "from langchain.schema import HumanMessage, SystemMessage\n",
    "result = chat( #← 실행하기\n",
    "    [\n",
    "        SystemMessage(content=\"친한친구처럼, 존댓말 쓰지말고 솔직하게 답변\"),\n",
    "        HumanMessage(content=\"안녕? 밥은 먹었니\"),\n",
    "        \n",
    "    ]\n",
    ")\n",
    "print(result.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03907583-0565-4a16-8063-0c7bb1392554",
   "metadata": {},
   "source": [
    "### 언어모델 바꿔보자 앤트로픽으로!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "830094b3-4501-4b6c-91bf-e30644e4570c",
   "metadata": {},
   "source": [
    "[엔트로픽API](https://console.anthropic.com/settings/keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5cd52c76-5cc2-4141-9480-cbfee6e6e5b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\miniconda\\envs\\tw311\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:117: LangChainDeprecationWarning: The class `langchain_community.chat_models.anthropic.ChatAnthropic` was deprecated in langchain-community 0.0.28 and will be removed in 0.2. An updated version of the class exists in the langchain-anthropic package and should be used instead. To use it run `pip install -U langchain-anthropic` and import as `from langchain_anthropic import ChatAnthropic`.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 네, 밥은 잘 먹었어요.\n"
     ]
    }
   ],
   "source": [
    "from langchain.chat_models import ChatAnthropic\n",
    "from langchain.schema import SystemMessage, HumanMessage\n",
    "\n",
    "load_dotenv()\n",
    "anthropic_api_key = os.getenv(\"Anthropic_API_KEY\")\n",
    "\n",
    "chat = ChatAnthropic(\n",
    "    model=\"claude-2\",\n",
    "    anthropic_api_key=anthropic_api_key\n",
    ")\n",
    "\n",
    "result = chat([\n",
    "    SystemMessage(content=\"친한친구처럼, 존댓말 쓰지말고 솔직하게 답변\"),\n",
    "    HumanMessage(content=\"안녕? 밥은 먹었니\"),\n",
    "])\n",
    "\n",
    "print(result.content) # claude2는 좀 멍청"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a285843d-db5c-46d1-99a5-ebc91b94be83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ContentBlock(text='아직 밥은 안 먹었어. 배고프긴 한데 너무 귀찮아서 말이야. 너는 밥 먹었어?', type='text')]\n"
     ]
    }
   ],
   "source": [
    "import anthropic\n",
    "\n",
    "client = anthropic.Anthropic(\n",
    "    # defaults to os.environ.get(\"ANTHROPIC_API_KEY\")\n",
    "    api_key=anthropic_api_key\n",
    ")\n",
    "message = client.messages.create(\n",
    "    model=\"claude-3-sonnet-20240229\",\n",
    "    max_tokens=1000,\n",
    "    temperature=0,\n",
    "    system=\"친한친구처럼, 존댓말 쓰지말고 솔직하게 답변\",\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\n",
    "                    \"type\": \"text\",\n",
    "                    \"text\": \"안녕? 밥은 먹었니\"\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    ")\n",
    "print(message.content) # claude3-sonnet 똑똑한데 랭체인에서 아직 사용 불가능"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab95ec9b-d70a-43c6-a820-23a78ec9e157",
   "metadata": {},
   "source": [
    "## PromptTemplate을 쓰면 쉽게 변수를 바꿔서 프롬프트를 만들수 있다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d0d26783-0ac4-4e1e-9851-6ea5d72ef213",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "빈지노는 어느 학교 출신？\n"
     ]
    }
   ],
   "source": [
    "from langchain import PromptTemplate  #← PromptTemplate 가져오기\n",
    "\n",
    "prompt = PromptTemplate(  #← PromptTemplate 초기화하기\n",
    "    template=\"{influencer}는 어느 학교 출신？\", \n",
    "    input_variables=[\n",
    "        \"product\"  #← influencer 입력할 변수 지정\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(prompt.format(influencer=\"빈지노\")) # influencer= 로 매개변수 입력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bd224576-c31b-4289-85bd-0ac8151e0aef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "김구라는 어느 학교 출신？\n"
     ]
    }
   ],
   "source": [
    "print(prompt.format(influencer=\"김구라\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3f150573-d0e3-4996-ab15-bcbccb1e3901",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'influencer'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mprompt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[1;32mE:\\miniconda\\envs\\tw311\\Lib\\site-packages\\langchain_core\\prompts\\prompt.py:132\u001b[0m, in \u001b[0;36mPromptTemplate.format\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m    117\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Format the prompt with the inputs.\u001b[39;00m\n\u001b[0;32m    118\u001b[0m \n\u001b[0;32m    119\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    129\u001b[0m \u001b[38;5;124;03m        prompt.format(variable1=\"foo\")\u001b[39;00m\n\u001b[0;32m    130\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    131\u001b[0m kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_merge_partial_and_user_variables(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m--> 132\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mDEFAULT_FORMATTER_MAPPING\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtemplate_format\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtemplate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mE:\\miniconda\\envs\\tw311\\Lib\\string.py:190\u001b[0m, in \u001b[0;36mFormatter.format\u001b[1;34m(self, format_string, *args, **kwargs)\u001b[0m\n\u001b[0;32m    189\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mformat\u001b[39m(\u001b[38;5;28mself\u001b[39m, format_string, \u001b[38;5;241m/\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvformat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mformat_string\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mE:\\miniconda\\envs\\tw311\\Lib\\site-packages\\langchain_core\\utils\\formatting.py:18\u001b[0m, in \u001b[0;36mStrictFormatter.vformat\u001b[1;34m(self, format_string, args, kwargs)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m     14\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m     15\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo arguments should be provided, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     16\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124meverything should be passed as keyword arguments.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     17\u001b[0m     )\n\u001b[1;32m---> 18\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvformat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mformat_string\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mE:\\miniconda\\envs\\tw311\\Lib\\string.py:194\u001b[0m, in \u001b[0;36mFormatter.vformat\u001b[1;34m(self, format_string, args, kwargs)\u001b[0m\n\u001b[0;32m    192\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mvformat\u001b[39m(\u001b[38;5;28mself\u001b[39m, format_string, args, kwargs):\n\u001b[0;32m    193\u001b[0m     used_args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n\u001b[1;32m--> 194\u001b[0m     result, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_vformat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mformat_string\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mused_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    195\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheck_unused_args(used_args, args, kwargs)\n\u001b[0;32m    196\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[1;32mE:\\miniconda\\envs\\tw311\\Lib\\string.py:234\u001b[0m, in \u001b[0;36mFormatter._vformat\u001b[1;34m(self, format_string, args, kwargs, used_args, recursion_depth, auto_arg_index)\u001b[0m\n\u001b[0;32m    230\u001b[0m     auto_arg_index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    232\u001b[0m \u001b[38;5;66;03m# given the field_name, find the object it references\u001b[39;00m\n\u001b[0;32m    233\u001b[0m \u001b[38;5;66;03m#  and the argument it came from\u001b[39;00m\n\u001b[1;32m--> 234\u001b[0m obj, arg_used \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_field\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfield_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    235\u001b[0m used_args\u001b[38;5;241m.\u001b[39madd(arg_used)\n\u001b[0;32m    237\u001b[0m \u001b[38;5;66;03m# do any conversion on the resulting object\u001b[39;00m\n",
      "File \u001b[1;32mE:\\miniconda\\envs\\tw311\\Lib\\string.py:299\u001b[0m, in \u001b[0;36mFormatter.get_field\u001b[1;34m(self, field_name, args, kwargs)\u001b[0m\n\u001b[0;32m    296\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_field\u001b[39m(\u001b[38;5;28mself\u001b[39m, field_name, args, kwargs):\n\u001b[0;32m    297\u001b[0m     first, rest \u001b[38;5;241m=\u001b[39m _string\u001b[38;5;241m.\u001b[39mformatter_field_name_split(field_name)\n\u001b[1;32m--> 299\u001b[0m     obj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_value\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfirst\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    301\u001b[0m     \u001b[38;5;66;03m# loop through the rest of the field_name, doing\u001b[39;00m\n\u001b[0;32m    302\u001b[0m     \u001b[38;5;66;03m#  getattr or getitem as needed\u001b[39;00m\n\u001b[0;32m    303\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m is_attr, i \u001b[38;5;129;01min\u001b[39;00m rest:\n",
      "File \u001b[1;32mE:\\miniconda\\envs\\tw311\\Lib\\string.py:256\u001b[0m, in \u001b[0;36mFormatter.get_value\u001b[1;34m(self, key, args, kwargs)\u001b[0m\n\u001b[0;32m    254\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m args[key]\n\u001b[0;32m    255\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 256\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mkwargs\u001b[49m\u001b[43m[\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m]\u001b[49m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'influencer'"
     ]
    }
   ],
   "source": [
    "print(prompt.format())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87fa0371-1360-4c50-bdae-46c05ad95d9d",
   "metadata": {},
   "source": [
    "> 키를 넣지 않으면 에러 발생"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e0d219ec-64c6-40a3-a3b3-486b508602e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "아이유는 대구여자고등학교를 졸업한 후 가수 활동을 시작했어.\n"
     ]
    }
   ],
   "source": [
    "from langchain import PromptTemplate\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.schema import HumanMessage\n",
    "\n",
    "chat = ChatOpenAI(  #← 클라이언트 생성 및 chat에 저장\n",
    "    model=\"gpt-3.5-turbo\",  #← 호출할 모델 지정\n",
    ")\n",
    "\n",
    "prompt = PromptTemplate(  #← PromptTemplate을 작성\n",
    "    template=\"{influencer}는 어느 학교 출신이야\",  #← {product}라는 변수를 포함하는 프롬프트 작성하기\n",
    "    input_variables=[\n",
    "        \"influencer\"  #← product에 입력할 변수 지정\n",
    "    ]\n",
    ")\n",
    "\n",
    "result = chat( #← 실행\n",
    "    [\n",
    "        SystemMessage(content=\"친한친구처럼, 존댓말 쓰지말고 솔직하게 답변\"),\n",
    "        HumanMessage(content=prompt.format(influencer=\"가수 아이유\")),\n",
    "        AIMessage(content=\"{ChatModel의 답변}\"),\n",
    "        SystemMessage(content=\"친한친구처럼, 존댓말 쓰지말고 솔직하게 답변\"),\n",
    "        HumanMessage(content=\"맞는지 다시 확인하고 답변해줘\"),\n",
    "    ]\n",
    ")\n",
    "print(result.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5f7e2bc4-56fc-42cd-a360-8a7b8d8e87c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_json = prompt.save('prompt.json') # 프롬프트템플릿을 json으로 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "df1d9dea-de3e-4d83-a0c2-8bcba2c109e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "박명수는 어느 학교 출신이야\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import load_prompt\n",
    "prompt = load_prompt('prompt.json')\n",
    "print(prompt.format(influencer='박명수'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tw311",
   "language": "python",
   "name": "tw311"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
