{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "882ea644-96f4-459e-b319-a8c83bf1a5de",
   "metadata": {},
   "source": [
    "# 목차\n",
    "- Chapter1. 챗GPT와 랭체인\n",
    "    + 1. 챗GPT와 언어모델 알아보기\n",
    "    + 2. 랭체인개요 \n",
    "    + 3. 랭체인 활용예시 \n",
    "    + 4. 실습준비\n",
    "    + 5. OPENAI의 API 호출\n",
    "- Chapter2. Model I/O\n",
    "    + 1. 언어모델을 이용한 응용프로그램 작동방식\n",
    "    + 2. 사용하기 쉬운 Language Models\n",
    "    + 3. Templates\n",
    "    + 4. Output parsers\n",
    "- Chapter3. Retrieval\n",
    "- Chapter4. Memory\n",
    "- Chapter5. Chains\n",
    "- Chapter6. Agents\n",
    "- Chapter7. Callbacks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "555679b9-3c61-4293-889e-64814590668e",
   "metadata": {},
   "source": [
    "# Ch1. 챗GPT와 랭체인\n",
    "    + 1. 챗GPT와 언어모델 알아보기\n",
    "    + 2. 랭체인개요 \n",
    "    + 3. 랭체인 활용예시 \n",
    "    + 4. 실습준비\n",
    "    + 5. OPENAI의 API 호출"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f87a8ef7-bdd5-4b23-872d-a6c7521a612a",
   "metadata": {},
   "source": [
    "## 1. 챗GPT와 언어모델 알아보기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b1cc9a7-3ad7-488f-b8d9-2f73be980775",
   "metadata": {},
   "source": [
    "- OpenAI의 API에서 개발하는 언어모델은 크게 두가지 분류 : 'Chat', 'Complete'\n",
    "    + Chat : 대화형 상호작용 특화, 질문 댓글 의견 등에 대한 답변을 생성하고 그 답변 바탕으로 대화 나눔\n",
    "    + Complete : 주어진 텍스트에 이어 텍스트 생성, 어느정도 정보/이야기를 시작 제공하면 이를 바탕으로 자동 보완\n",
    "        - 현재 gpt-4나 claud-3는 complete 모델이 존재하지 않음\n",
    "- 모델 선택 시 컨텍스트 길이 (토큰수) 고려 필요 : 일반모델은 4k, 16k는 16000개까지 처리가능\n",
    "    + gpt-3.5-turbo 등 뒤에 4자리가 없는 모델은 최신모델임을 의미, 업데이트시 자동으로 반영\n",
    "    + gpt-3.5-turbo-0613 등 뒤에 4자리 붙은 모델은 특정 버전이 고정된 것으로 업데이트가 반영되지 않음 . 특정결과 필요하거나 변동성 피하고 싶을때 선택\n",
    "    + gpt-3.5-turbo-instruct는 문제해결, 문장생성, 질문응답, 대화생성 등 다양한 작업에 활용\n",
    "    + claude2는 100k 토큰까지 입력가능 : 프로젝트 전체 소스코드를 생성하고 버그 수정도 가능\n",
    "    + LLaMa3는 오픈소스이나, 성능면에서 타오픈소스 모델 압도, 특히 70b모델은 claude3나 GPT4의 성능에 근접"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffe86032-637a-456d-8db1-a712b879940f",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Groq을 써서 로컬모델도 무료로 돌리기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deed605c-1049-4496-b14b-1593572f200b",
   "metadata": {},
   "source": [
    "- Groq을 쓰면 모델을 다운로드 받을 필요가 없습니다. 속도는 심지어 AWS, Azure보다 10배이상 빠릅니다.\n",
    "- 무료이며, 기업용은 별도로 비용이 발생 예상합니다, [API 발급](https://console.groq.com/docs/quickstart) 후 사용하세요"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "64f501b2-b80b-4afd-9eb8-2a7cdf3ff480",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "의치아 하nin! AI시대에 준비할 가치가 있는 가장 기초적인 것 같지는 않나요?󠁧󠁢󠁥󠁮󠁧󠁿\n",
      "\n",
      "우선, AI의 발전하고 있는 속도에 맞춰 우리 스스로를 업그레이드하는 것은 정말 중요할 것 같아요. 예를 들어, 새로운 기술과 같은 정보를 해부하고, AI를 접목시킬 수 있는 다양한 프로젝트에 참가하는 것이 좋을 것 같아요. тим에 있는 친구들은 어떤 기술을 사용합니까?🤔\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from groq import Groq\n",
    "load_dotenv()\n",
    "\n",
    "client = Groq(\n",
    "    api_key=os.getenv(\"GROQ_API_KEY\"),\n",
    ")\n",
    "\n",
    "chat_completion = client.chat.completions.create(\n",
    "    messages=[\n",
    "               {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"존댓말 쓰지말고, 친한친구처럼 한국어로 답변\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"AI시대에 우린 무엇을 준비해야 할까?\",\n",
    "        }\n",
    "    ],\n",
    "    model=\"llama3-8b-8192\",\n",
    ")\n",
    "\n",
    "print(chat_completion.choices[0].message.content) # 정적으로 찍을 때 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "2fd031fb-81d9-4471-8f4f-7edebf764e0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "와우, 진짜 좋은 질문이야! 😊\n",
      "\n",
      "솔직히, 결혼하고 애 키우는 건 선택의 문제야. 하지만, 많은 사람들이 그렇게 선택하는 이유는 여러 가지야.\n",
      "\n",
      "첫째, 사랑하는 사람과 함께 살아남는 건 인간의 기본적인 욕구야. 우리는 사회적 동물이라, 가족이나 친척들과의 유대감이 필요해. 결혼하고 애 키우는 건 이러한 유대감을 강화하고, 서로를 지지하고 지원하는 관계를 형성하는 거야.\n",
      "\n",
      "둘째, 인간은 생존과 번영을 위해 노력하는 생물이야. 결혼하고 애 키우는 건 이러한 생존 본능을 충족하는 방법이야. 우리는 가족을 통해 다음 세대를 이어나가고, 우리의 유산을 남기는 거야.\n",
      "\n",
      "셋째, 결혼하고 애 키우는 건 개인의 성장과 발전을 위해 필요한 거야. 우리는 아이들을 통해 새로운 경험과 지식을 얻을 수 있고, 부모로서의 책임감을 느껴. 이러한 경험은 우리의 성장과 발전을 위해 필요한 거야.\n",
      "\n",
      "넷째, 결혼하고 애 키우는 건 사회적 책임감을 느껴. 우리는 사회의 구성원으로서, 다음 세대를 이어나가고, 사회의 발전을 위해 기여하는 거야.\n",
      "\n",
      "물론, 이러한 이유들이 모든 사람에게 적용되는 건 아니야. 하지만, 많은 사람들이 결혼하고 애 키우는 것은 이러한 이유들 때문이야. 🤔None"
     ]
    }
   ],
   "source": [
    "from groq import Groq\n",
    "load_dotenv()\n",
    "\n",
    "client = Groq(\n",
    " api_key=os.getenv(\"GROQ_API_KEY\"),\n",
    ")\n",
    "\n",
    "stream = client.chat.completions.create(\n",
    "    #\n",
    "    # Required parameters\n",
    "    #\n",
    "    messages=[\n",
    "        # Set an optional system message. This sets the behavior of the\n",
    "        # assistant and can be used to provide specific instructions for\n",
    "        # how it should behave throughout the conversation.\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"친한친구처럼, 존댓말 쓰지말고 답변\"\n",
    "        },\n",
    "        # Set a user message for the assistant to respond to.\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"왜 결혼을 하고, 애를 키우며 살아야 할까?\",\n",
    "        }\n",
    "    ],\n",
    "\n",
    "    model=\"llama3-70b-8192\",\n",
    "\n",
    "    # 파라미터 셋팅\n",
    "    #\n",
    "    # Controls randomness: lowering results in less random completions.\n",
    "    # As the temperature approaches zero, the model will become deterministic\n",
    "    # and repetitive.\n",
    "    temperature=0.5,\n",
    "\n",
    "    # The maximum number of tokens to generate. Requests can use up to\n",
    "    # 32,768 tokens shared between prompt and completion.\n",
    "    max_tokens=1024,\n",
    "\n",
    "    # Controls diversity via nucleus sampling: 0.5 means half of all\n",
    "    # likelihood-weighted options are considered.\n",
    "    top_p=0.7,\n",
    "\n",
    "    # A stop sequence is a predefined or user-specified text string that\n",
    "    # signals an AI to stop generating content, ensuring its responses\n",
    "    # remain focused and concise. Examples include punctuation marks and\n",
    "    # markers like \"[end]\".\n",
    "    stop=None,\n",
    "\n",
    "    # If set, partial message deltas will be sent.\n",
    "    stream=True,\n",
    ")\n",
    "\n",
    "# stream=True 로 설정시, chunk로 루프돌려서 choices[0].delta로 가져옴\n",
    "for chunk in stream:\n",
    "    print(chunk.choices[0].delta.content, end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88b1d6ef-1e0b-41d9-abdb-6f5c04293b5d",
   "metadata": {},
   "source": [
    "## 2. 랭체인개요"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eef3fb6-ff16-4caf-81fa-24950cc46cd2",
   "metadata": {},
   "source": [
    "- 고성능 언어모델의 등장으로 기존 절차적 프로그래밍에서 어려웠던 기능을 쉽게 처리 (자연어 처리 및 표현 수정 등)\n",
    "    - 한계 :  논리적 복잡한 문제나 학습지식의 범위 벗어나는 정보 대잡 어려움\n",
    "- 이런 한계를 극복하기 위해 언어모델이 알지못하는 정보도 대답할 수 있게 하는 RAG (Retrieval-Augmented Generation), 추론과 행동을 언어모델 스스로 판단하여 인터넷검색이나 파일 저장 등을 자율적으로 수행하게 하는 React(Reasoming And Acting, 추론 및 행동)\n",
    "- Langchain의 6개 모듈 : [랭체인 다큐먼트 참고](https://python.langchain.com/docs/get_started/introduction.html)\n",
    "    - Model I/O : 언어모델 호출/프롬프트 준비/결과수신\n",
    "    - Retrieval : PDF, CSV, VectorDB 등에서 연관된 정보를 입출력저장\n",
    "    - Memory : 대화를 장/단기적 저장\n",
    "    - Chains : 여러 프로세스 통합/복잡한 기능개발을 쉽게 진행\n",
    "    - Agents : 모델외부와 상호작용하여 기능 확장 (예> 관련된 논문 결과를 크롤링, Rag로 저장, 이미지 인식)\n",
    "    - Callbacks : 이벤트 발생시 처리 수행 (로그 출력이나 외부라이브러리연동사용)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "244977b5-0435-488b-833e-722c743c9002",
   "metadata": {},
   "source": [
    "## 3. 랭체인 활용예시\n",
    "> 랭체인으로 어떤 서비스를 만들수 있을까\n",
    "- 언어모델이 모르는 정보가 있는 PDF를 불러와서 질문하거나 요약할 수 있는 챗봇 애플리케이션 생성\n",
    "- 명령을 통해 행동하는 비서역할의 서비스 \n",
    "    + 예 > 부산에 갈만한 곳을 검색하고 2박3일 일정표를 짜서 iternery.csv파일에 한국어로 저장해줘\n",
    "- 챗지피티는 일반인이 플러그인을 설치해서 단순 사용한다면, 랭체인은 개발자가 보다 확장성 있게 언어모델이 할수 없는 일을 가능하게 만들수 있다\n",
    "    + 예 > 구글캘린더, 지메일에서 정보를 가져와서 매일 아침 slack에서 오늘 일정과 할일 목록을 제안\n",
    "    \n",
    "> 나만의 어플리케이션을 만들어 보자\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e22f330-c6c3-4c38-8bab-dd290415def0",
   "metadata": {},
   "source": [
    "## 4. 실습준비"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db4e2415-778c-47db-87aa-f3c1fe83ca83",
   "metadata": {},
   "source": [
    "- [1. 비주얼 스튜디오 설치](https://azure.microsoft.com/ko-kr/products/visual-studio-code) 및 [파이썬다운로드](https://www.python.org/downloads/)\n",
    "- [2.주피터랩 설치](https://youtu.be/kuhtXwYlvjc?si=UWjIARPutkrWffa8)\n",
    "- [3.가상환경설치 및 주피터랩에띄우기](https://github.com/restful3/ds4th_study/blob/main/source/%EB%9E%AD%EC%B2%B4%EC%9D%B8_%EC%99%84%EB%B2%BD_%EC%9E%85%EB%AC%B8/langchain/langchain%20%ED%99%98%EA%B2%BD%20%EC%84%A4%EC%A0%95.md)\n",
    "- [4. OPENAI API다운받기](https://openai.com/blog/openai-api)\n",
    "- [5. dotenv 설정하기](https://hyunhp.tistory.com/718)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86aaee27-3528-4080-93ca-b1ae43d46d4a",
   "metadata": {},
   "source": [
    "## 5. OPENAI의 API를 호출해보자"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78cd8282-01e1-4e3e-96cb-9103b0bf4bf7",
   "metadata": {},
   "source": [
    "[dotenv](https://hyunhp.tistory.com/718) 설명을 참조하여 API Password를 저장할 수 있도록 합니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "1e8bd9f2-512a-4aba-b0a9-43eb499f7a61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM이 갑자기 뜬 이유는 다양할 수 있겠지만, 그 중 일반적으로는 다음과 같은 이유가 있을 수 있습니다.\n",
      "\n",
      "1. 마케팅 활동: LLM이 갑자기 뜬 이유 중 하나는 마케팅 활동이 성공적으로 이뤄졌기 때문일 수 있습니다. 새로운 광고 캠페인, 소셜 미디어 홍보, 혹은 새로운 제품 런칭 등이 있을 수 있습니다.\n",
      "\n",
      "2. 이벤트나 행사 참여: LLM이 갑자기 뜨게 된 이유 중 하나로는 어떤 이벤트나 행사에 참여하거나 주목을 받았기 때문일 수 있습니다. 이를 통해 인지도가 증대되었을 가능성이 있습니다.\n",
      "\n",
      "3. 커뮤니티나 온라인 컨텐츠에서 주목을 받음: LLM이 커뮤니티나 온라인 컨텐츠에서 화제가 되었기 때문에 갑자기 뜬 것일 수 있습니다. 만약 어떤 이슈에 대한 논란이 있었다면, 해당 이슈에 관련된 LLM이 관련성을 얻었을 가능성이 있습니다.\n",
      "\n",
      "이러한 이유들이 LLM이 갑자기 뜬 이유 중 일부가 될 수 있습니다. 추가적인 상황이나 배경 정보에 따라 다른 이유가 있을 수도 있습니다.\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "기본 챗팅모델을 호출하여 답변을 불러와 봅시다.\n",
    "'''\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import openai\n",
    "# from openai import OpenAI\n",
    "\n",
    "# 모델별 API pw를 저장합니다 .env 파일에 저장하고 불러옵니다.\n",
    "load_dotenv()\n",
    "os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "# \n",
    "completion = openai.chat.completions.create(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"LLM이 갑자기 뜬 이유를 알려주세요\",\n",
    "        },\n",
    "    ],\n",
    ")\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "6508f577-07c2-4854-9491-56e3044b2aa3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 'cmpl-9JKtM2LT6Vi9GFC8Qf9ZuTFKzIxl3', 'choices': [CompletionChoice(finish_reason='stop', index=0, logprobs=None, text=' 좋다'), CompletionChoice(finish_reason='stop', index=1, logprobs=None, text=' 좋다\\n\\n오늘 날씨가 매우 좋아서 기분도 좋아요')], 'created': 1714395568, 'model': 'gpt-3.5-turbo-instruct', 'object': 'text_completion', 'system_fingerprint': None, 'usage': CompletionUsage(completion_tokens=30, prompt_tokens=18, total_tokens=48)}\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "client = OpenAI()\n",
    "\n",
    "response = client.completions.create(\n",
    "    model=\"gpt-3.5-turbo-instruct\",\n",
    "    prompt=\"오늘 날씨가 매우 좋고 기분이\",\n",
    "    stop=\".\",  #←문자가 나타나면 문장 종료\n",
    "    max_tokens=100,  #← 최대 토큰 수\n",
    "    n=2,  #← 생성할 문장 수\n",
    "    temperature=0.5  #←다양성을 나타내는 매개변수\n",
    "    \n",
    ")\n",
    "# Convert the Completion object to a dictionary\n",
    "response_dict = response.__dict__\n",
    "\n",
    "# Print the dictionary as JSON\n",
    "print(response_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9322102-e52f-4c36-bb22-67f037aa594b",
   "metadata": {},
   "source": [
    "- 좋은 결과를 위해서는 더  맥락을 고려한 프롬프트 입력이 필요"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4f101232-d386-408c-96d4-7393163b58d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM인 ChatGPT는 OpenAI에서 개발한 대화형 언어 모델입니다. 이 모델은 GPT-3 모델에 대화 관련 능력을 추가한 것으로, 만약 아직 LLM이 대두되기 전이라면, 그 배경은 GPT-3의 성공적인 출시와 대화 생성 분야에서의 대단한 성과에 기인합니다.\n",
      "\n",
      "ChatGPT는 놀라운 성능을 보여주어 기업 및 개인이 다양한 사용 사례에서 활용할 수 있게 되었습니다. 비즈니스 분야에서는 고객 서비스 질을 향상시키기 위해, ChatGPT를 상담원과 함께 사용하여 실시간 대화를 제공할 수 있습니다. ChatGPT는 특정 도메인에 특화된 문제 해결을 위한 지식을 학습하고 사용자에게 도움을 줄 수 있는 역할도 합니다.\n",
      "\n",
      "또한 ChatGPT는 개별적인 사용자 스타일에 맞게 학습될 수 있는 기능을 제공함으로써 개인화된 서비스를 제공할 수 있습니다. 이 모델은 사용자의 쓰는 스타일과 관심사를 이해하는 능력을 갖추고 있습니다.\n",
      "\n",
      "ChatGPT의 영향력은 이미 상당히 큽니다. 상용화되면서 많은 기업들은 ChatGPT로 고객 서비스와 인터랙션을 개선하거나, 대화 데이터를 생성함으로써 대화 데이터셋을 확장하고 있습니다. ChatGPT는 또한 엔터테인먼트 분야에서도 인기를 끌고 있습니다. ChatGPT는 넓은 범위의 질문에 대답하거나, 가상의 인물과 대화하는 등 현실 세계와 가상 세계 간의 상호작용을 가능하게 합니다.\n",
      "\n",
      "앞으로는 ChatGPT와 같은 LLM의 발전이 기대됩니다. OpenAI는 계속해서 모델의 성능을 향상시키고 있으며, 개발자들에게 다양한 API와 도구를 제공하여 애플리케이션을 개발하는 데 도움을 주고 있습니다. LLM은 지식의 전달, 개인화된 컨텐츠 생성, 고객 서비스 등 다양한 분야에 적용될 것으로 예상됩니다.\n",
      "\n",
      "또한, OpenAI는 LLM의 사용에 관한 윤리적 고려사항과 가능한 한 부정적인 영향을 최소화하기 위해 다양한 기술적 및 제한적 제어 방법을 연구하고 있습니다. 따라서 ChatGPT와 LLM과 관련된 이러한 진보들은 시간이 흐른 뒤 우리의 일상 생활과 상호작용하는 모습을 넓게 바꿀 수 있을 것입니다.\n"
     ]
    }
   ],
   "source": [
    "# 더 긴문장을 호출할 때 모델을 바꿈\n",
    "completion = openai.chat.completions.create(\n",
    "    model=\"gpt-3.5-turbo-16k\",\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"ChatGPT 같은 LLM이 요즘에 핫한데, 대두가 된 배경과 어떤 영향을 미치고 있으며, \\\n",
    "             앞으로는 어떻게 바뀔지 상세히 알려줘 \",\n",
    "        },\n",
    "    ],\n",
    ")\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0ed8724-1ad4-43f0-8400-761f04f79bd8",
   "metadata": {},
   "source": [
    "- openai에서 antroprophic으로 바꿀 경우 코드호출이 불가능\n",
    "- 그래서 language model(langchain등)을 쓰는이유임"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98fd5954-b92a-4374-992e-900cb707f7c0",
   "metadata": {},
   "source": [
    "# Ch2. Model I/O\n",
    "    + 언어모델을 이용한 응용프로그램 작동방식\n",
    "    + 사용하기 쉬운 Language Models\n",
    "    + Templates\n",
    "    + Output parsers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9756212d-2ab2-4e42-a681-2d1a68e5ee3a",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "## 1. 언어모델을 이용한 응용프로그램 작동방식\n",
    " - 각 회사의 모델을 쉽게 가져다 쓸 수 있음\n",
    " - 프롬프트템플릿을 통해 변수를 바꿔가며 입력 가능\n",
    " - output parsers 모듈은 언어모델에서 얻은 출력을 분석/애플리케이션에서 사용하기 쉬운 형태로 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "e6fc7c25-3444-4329-9c7f-208488f7d6a8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "안녕하세요! 도와드릴 일이 있나요? :)\n"
     ]
    }
   ],
   "source": [
    "from langchain.chat_models import ChatOpenAI  #chat 모듈 가져오기 : openai뿐 아니라 claude도 가져올 수 있음\n",
    "from langchain.schema import HumanMessage  #← 사용자의 메시지인 HumanMessage 가져오기\n",
    "\n",
    "## 모델객체 만들기\n",
    "chat = ChatOpenAI(  #← 클라이언트를 만들고 chat에 저장\n",
    "    model=\"gpt-3.5-turbo\",  #← 호출할 모델 지정\n",
    ")\n",
    "\n",
    "## 위에서 만든 객체 실행하기\n",
    "result = chat( #← 실행하기\n",
    "    [\n",
    "        HumanMessage(content=\"안녕하세요!\"),\n",
    "    ]\n",
    ")\n",
    "print(result.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b20061e-01c1-4f5e-a005-3bdf02629dca",
   "metadata": {},
   "source": [
    "### AIMessage를 사용하여 언어모델의 응답을 표현할 수 있음\n",
    "- 대화형식의 상로작용을 표현 위해 AI Message도 준비됨. \n",
    "    - 첫번째 HumanMessage에 레시피를 반환,\n",
    "    - 아래와 같은 대화흐름에서 어떻게 표현하는지 보자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "40f10afc-8a01-4251-9d91-7f8aa8e5ac09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "저는 한국어만 지원하고 있어서 인도네시아어로 번역해 드릴 수 없습니다. 죄송합니다. 혹시 다른 질문이 있으시면 도와드리겠습니다.\n"
     ]
    }
   ],
   "source": [
    "from langchain.schema import HumanMessage, AIMessage\n",
    "\n",
    "result = chat( #← 실행하기\n",
    "    [\n",
    "        HumanMessage(content=\"Kpop문화에 대해 알려줘\"),\n",
    "        AIMessage(content=\"{ChatModel의 답변}\"),\n",
    "        HumanMessage(content=\"인도네시아어로 번역해줘\"),\n",
    "        \n",
    "    ]\n",
    ")\n",
    "print(result.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88c465e6-e9eb-4139-bba6-832e69c34f4c",
   "metadata": {},
   "source": [
    "> HumanMessage, AIMessage를 통해 상호작용을 표현할 수 있다.\n",
    "위의 랭귀지 모델만으로는 매번 소스코드를 다시 작성해야 하므로 번거로움\n",
    "- 상호작용을 지원하기 위해 Memory모듈이 있음"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6ae5981-315e-4219-8e08-1a419f5eee11",
   "metadata": {},
   "source": [
    "### SystemMessage를 통해 메타 지시 가능"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c8cc92f6-f4f7-404f-8480-4aa2941c1f6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "안녕! 밥은 아직 안 먹었어. 너는 먹었니?\n"
     ]
    }
   ],
   "source": [
    "from langchain.schema import HumanMessage, SystemMessage\n",
    "result = chat( #← 실행하기\n",
    "    [\n",
    "        SystemMessage(content=\"친한친구처럼, 존댓말 쓰지말고 솔직하게 답변\"),\n",
    "        HumanMessage(content=\"안녕? 밥은 먹었니\"),\n",
    "        \n",
    "    ]\n",
    ")\n",
    "print(result.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03907583-0565-4a16-8063-0c7bb1392554",
   "metadata": {},
   "source": [
    "### 언어모델 바꿔보자 앤트로픽으로!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "830094b3-4501-4b6c-91bf-e30644e4570c",
   "metadata": {},
   "source": [
    "[엔트로픽API](https://console.anthropic.com/settings/keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5cd52c76-5cc2-4141-9480-cbfee6e6e5b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\miniconda\\envs\\tw311\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:117: LangChainDeprecationWarning: The class `langchain_community.chat_models.anthropic.ChatAnthropic` was deprecated in langchain-community 0.0.28 and will be removed in 0.2. An updated version of the class exists in the langchain-anthropic package and should be used instead. To use it run `pip install -U langchain-anthropic` and import as `from langchain_anthropic import ChatAnthropic`.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 네, 밥은 잘 먹었어요.\n"
     ]
    }
   ],
   "source": [
    "from langchain.chat_models import ChatAnthropic\n",
    "from langchain.schema import SystemMessage, HumanMessage\n",
    "\n",
    "load_dotenv()\n",
    "anthropic_api_key = os.getenv(\"Anthropic_API_KEY\")\n",
    "\n",
    "chat = ChatAnthropic(\n",
    "    model=\"claude-2\",\n",
    "    anthropic_api_key=anthropic_api_key\n",
    ")\n",
    "\n",
    "result = chat([\n",
    "    SystemMessage(content=\"친한친구처럼, 존댓말 쓰지말고 솔직하게 답변\"),\n",
    "    HumanMessage(content=\"안녕? 밥은 먹었니\"),\n",
    "])\n",
    "\n",
    "print(result.content) # claude2는 좀 멍청"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "a285843d-db5c-46d1-99a5-ebc91b94be83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ContentBlock(text='Tentu saja, saya akan menjelaskan tentang budaya K-pop (Korean pop) dalam bahasa Indonesia.\\n\\nK-pop atau Korean pop adalah genre musik pop yang berasal dari Korea Selatan. Budaya K-pop telah menjadi fenomena global yang sangat populer di kalangan remaja dan anak muda di seluruh dunia, termasuk di Indonesia.\\n\\nBeberapa karakteristik utama budaya K-pop antara lain:\\n\\n1. Musik dan tarian: Lagu-lagu K-pop dikenal dengan aransemen musik yang kaya, lirik yang catchy, dan tarian koreografi yang enerjik dan terkoordinasi dengan baik.\\n\\n2. Idola K-pop: Para idola K-pop biasanya tergabung dalam grup vokal atau grup tari yang beranggotakan remaja atau anak muda yang tampan dan cantik. Mereka digemari karena talenta menyanyi, menari, dan penampilan yang menarik.\\n\\n3. Fandom yang besar: Budaya K-pop didukung oleh basis penggemar (fandom) yang sangat besar dan loyal di seluruh dunia yang disebut K-popers.\\n\\n4. Industri hiburan yang terorganisir: Budaya K-pop didukung oleh industri hiburan Korea yang terorganisir dengan baik, seperti agensi manajemen besar yang menaungi grup idola.\\n\\n5. Pengaruh budaya Korea: K-pop juga mempopulerkan budaya Korea lainnya seperti drama, film, makanan, fashion, dan bahasa Korea.\\n\\nDemikianlah penjelasan singkat tentang budaya K-pop yang telah menjadi fenomena global dan digemari oleh banyak orang di berbagai belahan dunia, termasuk di Indonesia.', type='text')]\n"
     ]
    }
   ],
   "source": [
    "import anthropic\n",
    "\n",
    "client = anthropic.Anthropic(\n",
    "    # defaults to os.environ.get(\"ANTHROPIC_API_KEY\")\n",
    "    api_key=anthropic_api_key\n",
    ")\n",
    "message = client.messages.create(\n",
    "    model=\"claude-3-sonnet-20240229\",\n",
    "    max_tokens=1000,\n",
    "    temperature=0,\n",
    "    system=\"인도네시아어로 답변해줘\",\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\n",
    "                    \"type\": \"text\",\n",
    "                    \"text\": \"Kpop문화에 대해 알려줘\"\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    ")\n",
    "print(message.content) # claude3-sonnet 똑똑한데 랭체인에서 아직 사용 불가능"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab95ec9b-d70a-43c6-a820-23a78ec9e157",
   "metadata": {},
   "source": [
    "### PromptTemplate을 쓰면 쉽게 변수를 바꿔서 프롬프트를 만들수 있다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "d0d26783-0ac4-4e1e-9851-6ea5d72ef213",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "빈지노는 어느 학교 출신？\n"
     ]
    }
   ],
   "source": [
    "from langchain import PromptTemplate  #← PromptTemplate 가져오기\n",
    "\n",
    "prompt = PromptTemplate(  #← PromptTemplate 초기화하기\n",
    "    template=\"{influencer}는 어느 학교 출신？\", \n",
    "    input_variables=[\n",
    "        \"product\"  #← influencer 입력할 변수 지정\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(prompt.format(influencer=\"빈지노\")) # influencer= 로 매개변수 입력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "bd224576-c31b-4289-85bd-0ac8151e0aef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "김구라는 어느 학교 출신？\n"
     ]
    }
   ],
   "source": [
    "print(prompt.format(influencer=\"김구라\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3f150573-d0e3-4996-ab15-bcbccb1e3901",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'influencer'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mprompt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[1;32mE:\\miniconda\\envs\\tw311\\Lib\\site-packages\\langchain_core\\prompts\\prompt.py:132\u001b[0m, in \u001b[0;36mPromptTemplate.format\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m    117\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Format the prompt with the inputs.\u001b[39;00m\n\u001b[0;32m    118\u001b[0m \n\u001b[0;32m    119\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    129\u001b[0m \u001b[38;5;124;03m        prompt.format(variable1=\"foo\")\u001b[39;00m\n\u001b[0;32m    130\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    131\u001b[0m kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_merge_partial_and_user_variables(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m--> 132\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mDEFAULT_FORMATTER_MAPPING\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtemplate_format\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtemplate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mE:\\miniconda\\envs\\tw311\\Lib\\string.py:190\u001b[0m, in \u001b[0;36mFormatter.format\u001b[1;34m(self, format_string, *args, **kwargs)\u001b[0m\n\u001b[0;32m    189\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mformat\u001b[39m(\u001b[38;5;28mself\u001b[39m, format_string, \u001b[38;5;241m/\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvformat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mformat_string\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mE:\\miniconda\\envs\\tw311\\Lib\\site-packages\\langchain_core\\utils\\formatting.py:18\u001b[0m, in \u001b[0;36mStrictFormatter.vformat\u001b[1;34m(self, format_string, args, kwargs)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m     14\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m     15\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo arguments should be provided, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     16\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124meverything should be passed as keyword arguments.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     17\u001b[0m     )\n\u001b[1;32m---> 18\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvformat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mformat_string\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mE:\\miniconda\\envs\\tw311\\Lib\\string.py:194\u001b[0m, in \u001b[0;36mFormatter.vformat\u001b[1;34m(self, format_string, args, kwargs)\u001b[0m\n\u001b[0;32m    192\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mvformat\u001b[39m(\u001b[38;5;28mself\u001b[39m, format_string, args, kwargs):\n\u001b[0;32m    193\u001b[0m     used_args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n\u001b[1;32m--> 194\u001b[0m     result, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_vformat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mformat_string\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mused_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    195\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheck_unused_args(used_args, args, kwargs)\n\u001b[0;32m    196\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[1;32mE:\\miniconda\\envs\\tw311\\Lib\\string.py:234\u001b[0m, in \u001b[0;36mFormatter._vformat\u001b[1;34m(self, format_string, args, kwargs, used_args, recursion_depth, auto_arg_index)\u001b[0m\n\u001b[0;32m    230\u001b[0m     auto_arg_index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    232\u001b[0m \u001b[38;5;66;03m# given the field_name, find the object it references\u001b[39;00m\n\u001b[0;32m    233\u001b[0m \u001b[38;5;66;03m#  and the argument it came from\u001b[39;00m\n\u001b[1;32m--> 234\u001b[0m obj, arg_used \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_field\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfield_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    235\u001b[0m used_args\u001b[38;5;241m.\u001b[39madd(arg_used)\n\u001b[0;32m    237\u001b[0m \u001b[38;5;66;03m# do any conversion on the resulting object\u001b[39;00m\n",
      "File \u001b[1;32mE:\\miniconda\\envs\\tw311\\Lib\\string.py:299\u001b[0m, in \u001b[0;36mFormatter.get_field\u001b[1;34m(self, field_name, args, kwargs)\u001b[0m\n\u001b[0;32m    296\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_field\u001b[39m(\u001b[38;5;28mself\u001b[39m, field_name, args, kwargs):\n\u001b[0;32m    297\u001b[0m     first, rest \u001b[38;5;241m=\u001b[39m _string\u001b[38;5;241m.\u001b[39mformatter_field_name_split(field_name)\n\u001b[1;32m--> 299\u001b[0m     obj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_value\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfirst\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    301\u001b[0m     \u001b[38;5;66;03m# loop through the rest of the field_name, doing\u001b[39;00m\n\u001b[0;32m    302\u001b[0m     \u001b[38;5;66;03m#  getattr or getitem as needed\u001b[39;00m\n\u001b[0;32m    303\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m is_attr, i \u001b[38;5;129;01min\u001b[39;00m rest:\n",
      "File \u001b[1;32mE:\\miniconda\\envs\\tw311\\Lib\\string.py:256\u001b[0m, in \u001b[0;36mFormatter.get_value\u001b[1;34m(self, key, args, kwargs)\u001b[0m\n\u001b[0;32m    254\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m args[key]\n\u001b[0;32m    255\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 256\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mkwargs\u001b[49m\u001b[43m[\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m]\u001b[49m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'influencer'"
     ]
    }
   ],
   "source": [
    "print(prompt.format())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87fa0371-1360-4c50-bdae-46c05ad95d9d",
   "metadata": {},
   "source": [
    "> 키를 넣지 않으면 에러 발생"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8950e3fd-c4a0-4a23-8a71-d31ef49e4ec2",
   "metadata": {},
   "source": [
    "### LanguageModel+PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "e0d219ec-64c6-40a3-a3b3-486b508602e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "아이유는 경기고등학교를 졸업한 후 한국예술종합학교에 입학했습니다.\n"
     ]
    }
   ],
   "source": [
    "from langchain import PromptTemplate\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.schema import HumanMessage\n",
    "\n",
    "chat = ChatOpenAI(  #← 클라이언트 생성 및 chat에 저장\n",
    "    model=\"gpt-3.5-turbo\",  #← 호출할 모델 지정\n",
    ")\n",
    "\n",
    "prompt = PromptTemplate(  #← PromptTemplate을 작성\n",
    "    template=\"{influencer}는 어느 학교 출신이야\",  #← {product}라는 변수를 포함하는 프롬프트 작성하기\n",
    "    input_variables=[\n",
    "        \"influencer\"  #← product에 입력할 변수 지정\n",
    "    ]\n",
    ")\n",
    "\n",
    "result = chat( #← 실행\n",
    "    [\n",
    "        SystemMessage(content=\"친한친구처럼, 존댓말 쓰지말고 솔직하게 답변\"),\n",
    "        HumanMessage(content=prompt.format(influencer=\"가수 아이유\")),\n",
    "        AIMessage(content=\"{ChatModel의 답변}\"),\n",
    "        SystemMessage(content=\"친한친구처럼, 존댓말 쓰지말고 솔직하게 답변\"),\n",
    "        HumanMessage(content=\"맞는지 다시 확인하고 답변해줘\"),\n",
    "    ]\n",
    ")\n",
    "print(result.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f6b1738-8217-437e-a578-1fddcb3c0446",
   "metadata": {},
   "source": [
    "### Prompt저장/활용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f7e2bc4-56fc-42cd-a360-8a7b8d8e87c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = PromptTemplate(  #← PromptTemplate을 작성\n",
    "    template=\"{influencer}는 어느 학교 출신이야\",  #← {product}라는 변수를 포함하는 프롬프트 작성하기\n",
    "    input_variables=[\n",
    "        \"influencer\"  #← product에 입력할 변수 지정\n",
    "    ]\n",
    "\n",
    "prompt_json = prompt.save('prompt.json') # 프롬프트템플릿을 json으로 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "df1d9dea-de3e-4d83-a0c2-8bcba2c109e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "박명수는 어느 학교 출신이야\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import load_prompt\n",
    "prompt = load_prompt('prompt.json')\n",
    "print(prompt.format(influencer='박명수'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0306d396-42aa-4e1d-90aa-336fc502209b",
   "metadata": {},
   "source": [
    "### Output Parsers\n",
    "- 언어모델에서 받은 결과를 원하는 출력의 형태로 구조화\n",
    "- CommaSeparatedListOutputParser는 결과를 목록형태로 받아 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "945b31a9-b25d-4391-a457-c42ae1257769",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "대표 상품 => 아이폰\n",
      "대표 상품 => 아이패드\n",
      "대표 상품 => 맥북\n"
     ]
    }
   ],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.output_parsers import \\\n",
    "    CommaSeparatedListOutputParser  #← Output Parser인 CommaSeparatedListOutputParser를 가져옵니다.\n",
    "from langchain.schema import HumanMessage\n",
    "\n",
    "output_parser = CommaSeparatedListOutputParser() #← CommaSeparatedListOutputParser 초기화\n",
    "\n",
    "chat = ChatOpenAI(model=\"gpt-3.5-turbo\", )\n",
    "\n",
    "result = chat(\n",
    "    [\n",
    "        HumanMessage(content=\"애플이 개발한 대표적인 제품 3개를 알려주세요\"),\n",
    "        HumanMessage(content=output_parser.get_format_instructions()),  #← output_parser.get_format_instructions()를 실행하여 언어모델에 지시사항 추가하기\n",
    "    ]\n",
    ")\n",
    "\n",
    "output = output_parser.parse(result.content) #← 출력 결과를 분석하여 목록 형식으로 변환한다.\n",
    "\n",
    "for item in output: #← 목록을 하나씩 꺼내어 출력한다.\n",
    "    print(\"대표 상품 => \" + item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "a99ba081-31a1-4918-8271-2f394876e539",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='아이폰, 아이패드, 맥북', response_metadata={'token_usage': {'completion_tokens': 16, 'prompt_tokens': 58, 'total_tokens': 74}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': 'fp_3b956da36b', 'finish_reason': 'stop', 'logprobs': None})"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "a3f57030-19d9-4575-ab89-3687d7684475",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['아이폰', '아이패드', '맥북']"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e41a674-3ab2-4691-9f91-e2496eed7eaf",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "## 2.사용하기 쉬운 Language Models\n",
    "- 랭체인의 모델 : 대화형식으로 사용하기 위한 chat_models, complete 모델과 같은 모델의 연속을 준비하는 llms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "14d33e31-fe29-4193-95bb-64072dfc275d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\miniconda\\envs\\tw311\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:117: LangChainDeprecationWarning: The class `langchain_community.llms.openai.OpenAI` was deprecated in langchain-community 0.0.10 and will be removed in 0.2.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import OpenAI`.\n",
      "  warn_deprecated(\n",
      "E:\\miniconda\\envs\\tw311\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:117: LangChainDeprecationWarning: The function `__call__` was deprecated in LangChain 0.1.7 and will be removed in 0.2.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 끓여먹는다\n"
     ]
    }
   ],
   "source": [
    "from langchain.llms import OpenAI\n",
    "\n",
    "llm = OpenAI(model=\"gpt-3.5-turbo-instruct\" #← 호출할 모델 지정\n",
    "             )\n",
    "\n",
    "result = llm(\n",
    "    \"맛있는 라면을\",  #← 언어모델에 입력되는 텍스트\n",
    "    stop=\".\"  #← \".\" 가 출력된 시점에서 계속을 생성하지 않도록\n",
    ")\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96843c31-58de-4150-bea3-23eca4d0b207",
   "metadata": {},
   "source": [
    "### 중복되지 않게 재사용, cashe\n",
    "- 전송 횟수가 늘어날수록 api사용비 많아짐. 효율적으로 캐싱하는 기능 cashe\n",
    "    + InMemoryCache 를 llm_cashe로 설정해두면 동일한 요청에 대해 캐시에 저장된 내용을 바로 사용하기에 API에 불필요한 호출을 하지 않는다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "41633ed0-ef5a-472c-a984-5a61fe63d689",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\miniconda\\envs\\tw311\\Lib\\typing.py:864: RuntimeWarning: coroutine 'main' was never awaited\n",
      "  code = compile(arg_to_compile, '<string>', 'eval')\n",
      "RuntimeWarning: Enable tracemalloc to get the object allocation traceback\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "안녕하세요! 도와드릴게 있나요?\n",
      "실행 시간: 3.3740170001983643초\n",
      "안녕하세요! 도와드릴게 있나요?\n",
      "실행 시간: 0.00099945068359375초\n"
     ]
    }
   ],
   "source": [
    "import time  #← 실행 시간을 측정하기 위해 time 모듈 가져오기\n",
    "import langchain\n",
    "from langchain.cache import InMemoryCache  #← InMemoryCache 가져오기\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.schema import HumanMessage\n",
    "\n",
    "langchain.llm_cache = InMemoryCache() #← llm_cache에 InMemoryCache 설정\n",
    "\n",
    "chat = ChatOpenAI()\n",
    "start = time.time() #← 실행 시작 시간 기록\n",
    "result = chat([ #← 첫 번째 실행을 수행\n",
    "    HumanMessage(content=\"안녕하세요!\")\n",
    "])\n",
    "\n",
    "end = time.time() #← 실행 종료 시간 기록\n",
    "print(result.content)\n",
    "print(f\"실행 시간: {end - start}초\")\n",
    "\n",
    "start = time.time() #← 실행 시작 시간 기록\n",
    "result = chat([ #← 같은 내용으로 두 번째 실행을 함으로써 캐시가 활용되어 즉시 실행 완료됨\n",
    "    HumanMessage(content=\"안녕하세요!\")\n",
    "])\n",
    "\n",
    "end = time.time() #← 실행 종료 시간 기록\n",
    "print(result.content)\n",
    "print(f\"실행 시간: {end - start}초\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05ab35d7-0245-4da4-a24d-956713b95e1d",
   "metadata": {},
   "source": [
    "### 프로세스를 순차적 표시, Straming + callback\n",
    "- chatgpt사이트 처럼 순차적으로 답변을 해준다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "c30a1371-d098-4653-a94b-09a9c3f0da99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "맛있는 스테이크를 굽는 법은 다음과 같습니다:\n",
      "\n",
      "1. 스테이크는 냉장고에서 꺼내어 냉기를 식힌 후 룸온 온도로 약 30분 정도 방치해주세요.\n",
      "2. 팬이나 그릴을 미리 예열해주세요. 온도는 고온으로 설정해야 합니다.\n",
      "3. 스테이크에 소금과 후추를 골고루 뿌려주세요. 다양한 양념을 사용해도 좋습니다.\n",
      "4. 팬이나 그릴에 식용유를 두르고 스테이크를 올려주세요. 한쪽 면은 2~3분 정도 익혀주세요.\n",
      "5. 스테이크를 뒤집어 다른 면도 2~3분 정도 익혀주세요. 이때 고기가 고기를 고기가 빠질 때까지 뒤집어주세요.\n",
      "6. 고기가 익는 정도에 따라 더 익히고 싶으면 시간을 조절해주세요.\n",
      "7. 익은 스테이크를 미리 준비한 접시에 옮겨 두어 5분 정도 쉬어주세요. 이렇게 하면 고기의 주스가 고르게 퍼지게 됩니다.\n",
      "8. 스테이크를 잘라서 그릇에 담고 채소나 감자 등과 함께 내어주세요.맛있는 스테이크가 완성되었습니다.맛있는 스테이크를 즐겨 보세요!"
     ]
    }
   ],
   "source": [
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.schema import HumanMessage\n",
    "\n",
    "chat = ChatOpenAI(\n",
    "    streaming=True,  #← streaming을 True로 설정하여 스트리밍 모드로 실행\n",
    "    callbacks=[\n",
    "        StreamingStdOutCallbackHandler()  #← StreamingStdOutCallbackHandler를 콜백으로 설정\n",
    "    ]\n",
    ")\n",
    "resp = chat([ #← 요청 보내기\n",
    "    HumanMessage(content=\"맛있는 스테이크 굽는 법을 알려주세요\")\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e1d36d0-8234-4748-b9fc-fcc6acfe27b3",
   "metadata": {},
   "source": [
    "결과를 표시한 후에 소스코드를 다루고 싶다면~~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "8c07028e-4d82-4637-a685-423237d29db6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'맛있는 스테이크를 굽는 법은 다음과 같습니다:\\n\\n1. 스테이크는 냉장고에서 꺼내어 냉기를 식힌 후 룸온 온도로 약 30분 정도 방치해주세요.\\n2. 팬이나 그릴을 미리 예열해주세요. 온도는 고온으로 설정해야 합니다.\\n3. 스테이크에 소금과 후추를 골고루 뿌려주세요. 다양한 양념을 사용해도 좋습니다.\\n4. 팬이나 그릴에 식용유를 두르고 스테이크를 올려주세요. 한쪽 면은 2~3분 정도 익혀주세요.\\n5. 스테이크를 뒤집어 다른 면도 2~3분 정도 익혀주세요. 이때 고기가 고기를 고기가 빠질 때까지 뒤집어주세요.\\n6. 고기가 익는 정도에 따라 더 익히고 싶으면 시간을 조절해주세요.\\n7. 익은 스테이크를 미리 준비한 접시에 옮겨 두어 5분 정도 쉬어주세요. 이렇게 하면 고기의 주스가 고르게 퍼지게 됩니다.\\n8. 스테이크를 잘라서 그릇에 담고 채소나 감자 등과 함께 내어주세요.맛있는 스테이크가 완성되었습니다.맛있는 스테이크를 즐겨 보세요!'"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_text = resp.content\n",
    "res_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aed8206-603f-4e0c-8af9-f2c233ad97d3",
   "metadata": {},
   "source": [
    "## 3. Templates-프롬프트구축의 효율향상"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "606e9c50-e31a-4513-ab97-82b6f42bf5f0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tw311",
   "language": "python",
   "name": "tw311"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
