{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "1YbMvIBRycktUn48ThxKsQzePS9Qy3ApI",
      "authorship_tag": "ABX9TyMTw5+UHub2/+gwm9PCi6iJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/restful3/ds4th_study/blob/main/source/%ED%85%90%EC%B4%88%EC%9D%98%20%ED%8C%8C%EC%9D%B4%ED%86%A0%EC%B9%98%20%EB%94%A5%EB%9F%AC%EB%8B%9D%20%ED%8A%B9%EA%B0%95/LSTM%ED%85%8D%EC%8A%A4%ED%8A%B8%EC%83%9D%EC%84%B1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9lTA_n7z1Uy7",
        "outputId": "a037354a-818e-41ce-b3e5-f7a99293b41b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Index(['abstract', 'articleID', 'articleWordCount', 'byline', 'documentType',\n",
            "       'headline', 'keywords', 'multimedia', 'newDesk', 'printPage', 'pubDate',\n",
            "       'sectionName', 'snippet', 'source', 'typeOfMaterial', 'webURL'],\n",
            "      dtype='object')\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "import string\n",
        "df= pd.read_csv('/content/drive/MyDrive/SelfStudy/딥러닝기초/data/CH10/ArticlesApril2017.csv')\n",
        "print(df.columns)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "헤드라인만 가져와서 텍스트분석을 진행한다.\n",
        "\n",
        "![](https://drive.google.com/uc?id=15kGb1FM8HLEiLeypYdOGnu4sF35A2GsU)"
      ],
      "metadata": {
        "id": "qz7smLhnyOfE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- BOW (Bag of Words)\n",
        "> 모든 단어를 겹치지 않도록 고유번호로 인식한다.\n",
        "- [corpus ](https://ko.wikipedia.org/wiki/%EB%A7%90%EB%AD%89%EC%B9%98)\n",
        "> 말뭉치라고도 하며, 자연어처리를 위해 구성되는 기본집단\n",
        "\n",
        "![](https://drive.google.com/uc?id=1uqEw3liB1LOmktYxf-T7C613xskD196j)\n"
      ],
      "metadata": {
        "id": "teYndNttxlqd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import glob\n",
        "from torch.utils.data.dataset import Dataset\n",
        "\n",
        "\n",
        "class TextGeneration(Dataset):\n",
        "    def clean_text(self, txt):\n",
        "        # 모든 단어를 소문자로 바꾸고 특수문자를 제거\n",
        "        txt = \"\".join(v for v in txt if v not in string.punctuation).lower()\n",
        "        return txt\n",
        "\n",
        "    def __init__(self):\n",
        "        all_headlines = []\n",
        "\n",
        "        # 모든 헤드라인의 텍스트를 불러옴\n",
        "        for filename in glob.glob(\"/content/drive/MyDrive/SelfStudy/딥러닝기초/data/CH10/*.csv\"):\n",
        "            if 'Articles' in filename:\n",
        "                article_df = pd.read_csv(filename)\n",
        "\n",
        "                # 데이터셋의 headline의 값을 all_headlines에 추가\n",
        "                all_headlines.extend(list(article_df.headline.values))\n",
        "                break\n",
        "\n",
        "        # headline 중 unknown 값은 제거\n",
        "        all_headlines = [h for h in all_headlines if h != \"Unknown\"]\n",
        "\n",
        "        # 구두점 제거 및 전처리가 된 문장들을 리스트로 반환\n",
        "        self.corpus = [self.clean_text(x) for x in all_headlines]\n",
        "        self.BOW = {}\n",
        "\n",
        "        # 모든 문장의 단어를 추출해 고유번호 지정\n",
        "        for line in self.corpus: # 코퍼스에서 하나씩 불러와서\n",
        "            for word in line.split(): #쪼갠 word를\n",
        "                if word not in self.BOW.keys(): # 아직 키로 저장 안된것만 골라서\n",
        "                    self.BOW[word] = len(self.BOW.keys()) # 새로운 키번호로 저장함\n",
        "\n",
        "        # 모델의 입력으로 사용할 데이터\n",
        "        self.data = self.generate_sequence(self.corpus)\n",
        "\n",
        "\n",
        "    def generate_sequence(self, txt):\n",
        "        seq = []\n",
        "\n",
        "        for line in txt:\n",
        "            line = line.split() # line 분해리스트\n",
        "            line_bow = [self.BOW[word] for word in line] #BOW[word]의 고유번호 리스트\n",
        "\n",
        "            # 단어 2개를 입력으로, 그다음 단어를 정답으로\n",
        "            data = [([line_bow[i], line_bow[i+1]], line_bow[i+2])\n",
        "            for i in range(len(line_bow)-2)]\n",
        "\n",
        "            seq.extend(data)\n",
        "\n",
        "        return seq\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "    def __getitem__(self, i):\n",
        "        data = np.array(self.data[i][0])  # ❶ 입력 데이터\n",
        "        label = np.array(self.data[i][1]).astype(np.float32)  # ❷ 출력 데이터\n",
        "\n",
        "        return data, label"
      ],
      "metadata": {
        "id": "rGzibkUt2Hxn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "LSTM 모델 정의<br>\n",
        "![](https://drive.google.com/uc?id=1CHLvEcJBBaxhvw08oRyHaPuZZJ50IhUA)\n"
      ],
      "metadata": {
        "id": "Tm_2SPLl8NSO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "class LSTM(nn.Module):\n",
        "  def __init__(self, num_embeddings):\n",
        "    super(LSTM, self).__init__()\n",
        "\n",
        "    # 밀집표현을 위한 임베딩 층\n",
        "    self.embed = nn.Embedding(\n",
        "        num_embeddings = num_embeddings, embedding_dim = 16\n",
        "    )\n",
        "\n",
        "    # LSTM을 5개 층으로 쌓음\n",
        "    self.lstm = nn.LSTM(\n",
        "        input_size = 16,\n",
        "        hidden_size = 64,\n",
        "        num_layers =5,\n",
        "        batch_first = True\n",
        "    )\n",
        "\n",
        "    # 분류를 위한 MLP층\n",
        "    self.fc1 = nn.Linear(128, num_embeddings)\n",
        "    self.fc2 = nn.Linear(num_embeddings,num_embeddings)\n",
        "    # 활성화함수\n",
        "    self.relu = nn.ReLU()\n",
        "\n",
        "  def forward(self, x):\n",
        "      x = self.embed(x)\n",
        "\n",
        "      # LSTM 모델의 예측값\n",
        "      x, _ = self.lstm(x)\n",
        "      x = torch.reshape(x, (x.shape[0], -1))\n",
        "      x = self.fc1(x)\n",
        "      x = self.relu(x)\n",
        "      x = self.fc2(x)\n",
        "\n",
        "      return x"
      ],
      "metadata": {
        "id": "nbmtBDhS8IbV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tqdm\n",
        "from torch.utils.data.dataloader import DataLoader\n",
        "from torch.optim.adam import Adam\n",
        "import torch\n",
        "\n",
        "# 학습을 진행할 프로세서 정의\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "dataset = TextGeneration() # 데이터셋 정의\n",
        "model = LSTM(num_embeddings=len(dataset.BOW)).to(device) # 모델정의\n",
        "loader = DataLoader(dataset, batch_size=64)\n",
        "optim = Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "for epoch in range(200):\n",
        "  iterator = tqdm.tqdm(loader)\n",
        "  for data, label in iterator:\n",
        "    # 기울기 초기화\n",
        "    optim.zero_grad()\n",
        "    # 모델의 예측값\n",
        "    pred =model(torch.tensor(data, dtype=torch.long).to(device))\n",
        "    # 정답레이블은 long텐서로반환필요ㅕ\n",
        "    loss = nn.CrossEntropyLoss()(\n",
        "        pred, torch.tensor(label,dtype=torch.long).to(device))\n",
        "\n",
        "    # 오차역전파\n",
        "    loss.backward()\n",
        "    optim.step()\n",
        "    iterator.set_description(f'epoch{epoch} loss: {loss.item()}')\n",
        "torch.save(model.state_dict(), 'lstm.pth')"
      ],
      "metadata": {
        "id": "r_kTyxgp9coh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate(model, BOW, string=\"finding an \", strlen=10):\n",
        "   device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "   print(f\"input word: {string}\")\n",
        "\n",
        "   with torch.no_grad():\n",
        "       for p in range(strlen):\n",
        "           # 입력 문장을 텐서로 변경\n",
        "           words = torch.tensor(\n",
        "               [BOW[w] for w in string.split()], dtype=torch.long).to(device)\n",
        "\n",
        "           #\n",
        "           input_tensor = torch.unsqueeze(words[-2:], dim=0)\n",
        "           output = model(input_tensor)  # 모델을 이용해 예측\n",
        "           output_word = (torch.argmax(output).cpu().numpy())\n",
        "           string += list(BOW.keys())[output_word]  # 문장에 예측된 단어를 추가\n",
        "           string += \" \"\n",
        "\n",
        "   print(f\"predicted sentence: {string}\")\n",
        "\n",
        "# model = torch.load(\"lstm.pth\", map_location=device)\n",
        "\n",
        "model.load_state_dict(torch.load(\"lstm.pth\", map_location=device))\n",
        "pred = generate(model, dataset.BOW)"
      ],
      "metadata": {
        "id": "3mbRbM55B5D5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1ab89871-be65-4110-d6f0-4fef343251be"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "input word: finding an \n",
            "predicted sentence: finding an of … therapy step meet cuomo to gop for 2018 \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "generate(model, dataset.BOW, string = \"a church \")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PLWrF5Fvw6Eh",
        "outputId": "ef1da40d-87b1-49ed-d7fa-8f4f6f936449"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "input word: a church \n",
            "predicted sentence: a church of melting’ uprising of your without is france up put \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "psVqTRaJw6r4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}