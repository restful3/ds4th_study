{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ff3e8082-340f-49c3-bb51-744e77cfdeec",
   "metadata": {},
   "source": [
    "# 2. 마르코프 결정 과정(MDP)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51a9f161",
   "metadata": {},
   "source": [
    "2장에서는 에이전트의 행동에 따라 상태가 변하는 문제를 다룹니다. 이러한 문제의 대표적인 예로 마르코프 결정 과정 MDP 이 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5335a7b",
   "metadata": {},
   "source": [
    "## 2.1 마르코프 결정 과정(MDP)이란?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c50eac6",
   "metadata": {},
   "source": [
    "마르코프 결정 과정에서 '결정 과정'이란 '에이전트가 (환경과 상호작용하면서) 행동을 결정하는 과정'을 뜻합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07d07061",
   "metadata": {},
   "source": [
    "### 2.1.1 구체적인 예"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11533865",
   "metadata": {},
   "source": [
    "<p align=\"center\"><img src=\"./images/fig_02_01.png\" width=700></p>\n",
    "<p align=\"center\"><img src=\"./images/fig_02_02.png\" width=700></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f291815",
   "metadata": {},
   "source": [
    "로봇이 에이전트 agent 이고 주변이 환경 environment 입니다. 그림과 같이 에이전트의 행동에 따라 에이전트가 처하는 상황이 달라집니다. 이 상황을 강화학습에서는 **상태** state 라고 합니다. MDP에서는 에이전트의 행동에 따라 상태가 바뀌고, 상태가 바뀐 곳에서 새로운 행동을 하게 됩니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27a65843",
   "metadata": {},
   "source": [
    "<p align=\"center\"><img src=\"./images/fig_02_03.png\" width=700></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d674b6f",
   "metadata": {},
   "source": [
    "이 문제에서 오른쪽으로 이동하면 즉시 얻는 보상은 마이너스지만, 한 번 더 오른쪽으로 이동하면 +6짜리 사과 더미를 얻을 수 있습니다. 따라서 이 문제에서 최선의 행동은 오른쪽으로 두 번 이동하는 것입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "454d036c",
   "metadata": {},
   "source": [
    "이 예에서 알 수 있듯이 에이전트는 눈앞의 보상이 아니라 미래에 얻을 수 있는 보상의 총합을 고려해야 합니다. 즉, 보상의 총합을 극대화하려 노력해야 합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb71aec2",
   "metadata": {},
   "source": [
    "### 2.1.2 에이전트와 환경의 상호작용"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1feeaf9e",
   "metadata": {},
   "source": [
    "<p align=\"center\"><img src=\"./images/fig_02_04.png\" width=500></p>\n",
    "<p align=\"center\"><img src=\"./images/fig_02_04_1.png\" width=500></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfd88af9",
   "metadata": {},
   "source": [
    "## 2.2 환경과 에이전트를 수식으로"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e075397",
   "metadata": {},
   "source": [
    "MDP는 에이전트와 환경의 상호작용을 수식으로 표현합니다. 다음의 세 요소를 수식으로 표현해야 합니다.\n",
    "- 상태 전이 : 상태는 어떻게 전이되는가?\n",
    "- 보상 : 보상은 어떻게 주어지는가?\n",
    "- 정책 : 에이전트는 행동을 어떻게 결정하는가?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b6d2c8b",
   "metadata": {},
   "source": [
    "### 2.2.1 상태 전이"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fc5be6b",
   "metadata": {},
   "source": [
    "<p align=\"center\"><img src=\"./images/fig_02_05.png\" width=700></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41ca071a",
   "metadata": {},
   "source": [
    "상태 전이가 **결정적** deterministic 일 경우 다음 상태 s'는 현재 상태 s와 행동 a에 의해 '단 하나'로 결정됩니다. 따라서 함수로는 다음처럼 표현할 수 있습니다.\n",
    "$$ s' = f(s,a) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "401067c2",
   "metadata": {},
   "source": [
    "$f(s,a)$는 상태 s와 행동 a를 입력하면 다음 상태 s'를 출력하는 함수입니다. 이 함수를 가리켜 **상태 전이 함수** state transition function 라고 합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "149d0754",
   "metadata": {},
   "source": [
    "반면 [그림2-5]의 오른쪽은 이동을 **확률적** stochastic 으로 표현하고 있습니다. 에이전트는 왼쪽으로 0.9의 확률로 이동하고, 0.1의 확률로는 그 자리에 머물러 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b130e92",
   "metadata": {},
   "source": [
    "확률적 상태 전이를 표기하는 방법을 설명하겠습니다. 에이전트가 상태 s에서 행동 a를 선택한다고 해봅시다. 이 경우 다음 상태 s'로 이동할 확률은 다음처럼 나타냅니다.\n",
    "$$ p(s'|s,a) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b6a8f7d",
   "metadata": {},
   "source": [
    "상태 s에서 행동 a를 선택했다는 조건이 주어졌을 때 s'로 전이될 확률을 $ p(s'|s,a) $로 나타내며, 이를 **상태 전이 확률** state transition probability 라고 합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72a9a347",
   "metadata": {},
   "source": [
    "<p align=\"center\"><img src=\"./images/fig_02_06.png\" width=700></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "852af0e5",
   "metadata": {},
   "source": [
    "에이전트가 L3에 있을 때, 즉 상태가 L3일 때 행동으로 Left를 선택했다고 해봅시니다.  \n",
    "그리고 이 경우의 상태 전이 확률 $ p(s'|s = L3, a = Left) $ 가 오른쪽 표와 같다고 합시다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04df4cc2",
   "metadata": {},
   "source": [
    "**마르코프 성질** markov property  \n",
    "> $ p(s'|s,a) $ 가 다음 상태 s'를 결정하는 데는 '현재' 상태 s와 행동 a만이 영향을 줍니다. 즉 지금까지 어떤 상태들을 거쳐 왔고 어떤 행동들을 취해 왔는지는 신경 쓰지 않습니다. 이처럼 현재의 정보만 고려하는 성질을 마르코프 성질이라고 합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2298f171",
   "metadata": {},
   "source": [
    "MDP는 마르코프 성질을 만족한다고 가정하고 상태 전이(와 보상)를 모델링합닏. 마르코프 성질을 도입하는 가장 큰 이유는 문제를 더 쉽게 풀기 위해서입니다. 만약 마르코프 성질을 따른다고 가정하지 않는다면 과거의 모든 상태와 행동까지 고려해야 해서, 그 조합이 기하급수적으로 많아집니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9a9740b",
   "metadata": {},
   "source": [
    "### 2.2.2 보상 함수"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ad4f74f",
   "metadata": {},
   "source": [
    "에이전트가 상태 s에서 행동 a를 수행하여 다음 상태 s'가 되었을 때 얻는 보상을 $r(s,a,s')$ 라는 함수로 정의합니다.  \n",
    "그럼 **보상 함수** reward function의 예로 [그림2-7]을 보겠습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb70c8a3",
   "metadata": {},
   "source": [
    "<p align=\"center\"><img src=\"./images/fig_02_07.png\" width=700></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9d8ad31",
   "metadata": {},
   "source": [
    "그림에는 에이전트가 상태 L2에서 (s=L2), 행동 Left를 선택하여 상태 L1로 전이된 예가 그려져 있습니다.  \n",
    "이 경우의 보상 함수 $r(s,a,s')$ 를 통해 1임을 알 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9af9a3e",
   "metadata": {},
   "source": [
    "### 2.2.3 에이전트의 정책"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "050021c3",
   "metadata": {},
   "source": [
    "**정책** policy 은 에이전트가 행동을 결정하는 방식을 말합니다. 정책에서 중요한 점은 마르코프 성질에 의해 에이전트는 '현재 상태'만으로 행동을 결정할 수 있다는 것입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a912e6b",
   "metadata": {},
   "source": [
    "환경의 상태 전이에서는 현재 상태 s와 행동 a만을 고려하여 다음 상태 s'가 결정됩니다. 보상도 마찬가지로 현재 상태 s와 행동 a 그리고 전이된 상태 s'만으로 결정됩니다. 이상이 의미하는 바는 '환경에 대해 필요한 정보는 모두 현재 상태에 있다'는 것입니다. 따라서 에이전트가 '현재 상태'으로 행동을 결정할 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63916c19",
   "metadata": {},
   "source": [
    "<p align=\"center\"><img src=\"./images/fig_02_08.png\" width=700></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dec6aa72",
   "metadata": {},
   "source": [
    "왼쪽 그림에서 에이전트는 L3에 있을 때는 반드시 왼쪽으로 이동합니다. 이러한 결정적 정책은 함수로 다음과 같이 정의할 수 있습니다. \n",
    "$$ a = \\mu(s) $$\n",
    "$\\mu(s)$는 매개변수로 상태를 건네주면 행동 a를 반환하는 함수입니다. 이 예에서 $\\mu(s=L3)$은 Left를 반환합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a51a38ad",
   "metadata": {},
   "source": [
    "[그림2-8]의 오른쪽은 확률적 정책의 예입니다. 에이전트가 왼쪽으로 이동할 확률은 0.4이고, 오른쪽으로 이동할 확률은 0.6입니다. 이렇게 에이전트의 행동이 확률적으로 결정되는 정책은 수식으로 다음과 같이 표현할 수 있습니다. \n",
    "$$ \\pi(a|s) $$\n",
    "$ \\pi(a|s) $는 상태 s에서 행동 a를 취할 확률을 나타냅니다. 예를 수식으로 표현하면 다음과 같습니다.\n",
    "<p align=\"center\"><img src=\"./images/fig_02_08_1.png\" width=400></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a98b9377",
   "metadata": {},
   "source": [
    "## 2.3 MDP의 목표"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c4c028b",
   "metadata": {},
   "source": [
    "- 에이전트는 정책 $\\pi(a|s)$에 따라 행동합니다.\n",
    "- 그 행동과 상태 전이 확률 $p(s'|s,a)$에 의해 다음 상태가 결정됩니다.\n",
    "- 보상은 보상 함수 $r(s,a,s')가 결정합니다.\n",
    "\n",
    "이 틀 안에서 최적 정책을 찾는 것이 **MDP의 목표** 입니다. **최적 정책** optimal policy 이란 수익이 최대가 되는 정책입니다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4be6d846",
   "metadata": {},
   "source": [
    "### 2.3.1 일회성 과제와 지속적 과제"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66f64fd5",
   "metadata": {},
   "source": [
    "<p align=\"center\"><img src=\"./images/fig_02_09.png\" width=700></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a38d549",
   "metadata": {},
   "source": [
    "**일회성 과제** episode task 에서는 시작부터 끝까지의 일련의 시도를 **에피소드** episode라고 합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7750e118",
   "metadata": {},
   "source": [
    "반면 **지속적 과제** continuous task는 끝이 없는 문제입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d3f5a6f",
   "metadata": {},
   "source": [
    "### 2.3.2 수익"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30cc41a8",
   "metadata": {},
   "source": [
    "다음으로 새로운 용어 **수익** return을 소개하겠습니다. 이 수익을 극대화하는 것이 에이전트의 목표입니다.\n",
    "<p align=\"center\"><img src=\"./images/fig_02_04.png\" width=500></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c7cb56d",
   "metadata": {},
   "source": [
    "시간 t에서의 상태를 $S_t$라고 해보죠. 그리고 에이전트가 정책 $\\pi$에 따라 행동 $A_t$를 하고, 보상 $R_t$를 얻고, 새로운 상태 $S_{t+1}$로 전이하는 흐름이 이어집니다. 이때 수익 $G_t$는 다음과 같이 정의됩니다.\n",
    "<p align=\"center\"><img src=\"./images/eq_02_01.png\" width=700></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6da387fc",
   "metadata": {},
   "source": [
    "[식2.1]과 같이 수익은 에이전트가 얻는 보상의 합입니다. 하지만 시간이 지날수록 보상은 $\\gamma$에 의해 기하급수적으로 줄어듭니다. 이 $\\gamma$를 **할인율** discount rate 이라고 하며 0.0에서 1.0사이의 실수로 설정합니다.\n",
    "<p align=\"center\"><img src=\"./images/eq_02_01_1.png\" width=400></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "452dd465",
   "metadata": {},
   "source": [
    "할인율을 도입하는 주된 이유는 지속적 과제에서 수익이 무한대가 되지 않도록 방지하기 위해서입니다.  \n",
    "또한 할인율은 가까운 미래의 보상을 더 중요하게 보이도록 합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc2f3d9d",
   "metadata": {},
   "source": [
    "### 2.3.3 상태 가치 함수"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3031086a",
   "metadata": {},
   "source": [
    "방금 정의한 '수익'을 극대화하는 것이 에이전트의 목표라고 했습니다. 여기서 주의할 것은 에어전트와 환경이 '확률적'으로 동작할 수 있다는 점입니다. 에이전트는 다음 행동을 확률적으로 결정할 수 있고, 상태 역시 확률적으로 전이될 수 있습니다. 그렇다면 얻는 수익도 '확률적'으로 달라질 것입니다. 비록 같은 상태에서 시작하더라도 수익이 에피소드마다 확률적으로 달라질 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c5d2d59",
   "metadata": {},
   "source": [
    "이러한 확률적 동작에 대응하기 위해서는 기댓값, 즉 '수익의 기댓값'을 지료로 삼아야 합니다. 상태 $S_t$가 s이고 (시간 t는 임의의 값), 에이전트의 정책이 $\\pi$일 때, 에이전트가 얻을 수 있는 기대 수익을 다음처럼 표현할 수 있습니다.\n",
    "<p align=\"center\"><img src=\"./images/eq_02_02.png\" width=700></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afa32fe9",
   "metadata": {},
   "source": [
    "이처럼 수익의 기댓값을 $v_\\pi(s)$를 **상태 가치 함수** state-value function 라고 합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8246151",
   "metadata": {},
   "source": [
    "<p align=\"center\"><img src=\"./images/eq_02_03.png\" width=700></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4224674",
   "metadata": {},
   "source": [
    "[식2.3]에서는 우변에서 $\\pi$의 위치가 $E_\\pi$로 이동했습니다. 의미는 [식2.2]에서와 마찬가지로 $\\pi$라는 정책이 조건으로 주어져 있음을 나타냅니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6830b94c",
   "metadata": {},
   "source": [
    "### 2.3.4 최적 정책과 최적 가치 함수"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7fd0720",
   "metadata": {},
   "source": [
    "강화 학습의 목표는 최적 정책을 찾는 것입니다. 이번 절에서는 최적 정책이란 무엇이며 애초에 '최적'을 어떻게 표현할 수 있을지를 생각해보겠습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d12cc925",
   "metadata": {},
   "source": [
    "<p align=\"center\"><img src=\"./images/fig_02_10.png\" width=700></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c47d296",
   "metadata": {},
   "source": [
    "상태에 따라 상태 가치 함수의 크고 작음이 달라지는 경우에는 두 정책의 우열을 가릴 수 없습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4993fe6",
   "metadata": {},
   "source": [
    "<p align=\"center\"><img src=\"./images/fig_02_11.png\" width=500></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eb549b4",
   "metadata": {},
   "source": [
    "이 그림에서는 모든 상태에서 $v_{\\pi'}(s) ≥ v_{\\pi}(s)$ 가 성립됩니다. 따라서 $\\pi'$가 $\\pi$보다 나은 정책이라고 할 수 있습니다. 이렇게 두 정책의 우열을 가리려면 하나의 정책이 다른 정책보다 '모든 상태'에서 더 좋거나 최소한 똑같아야 합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1374407",
   "metadata": {},
   "source": [
    "최적 정책을 $\\pi_*$로 표현한다면 정책 $\\pi_*$는 다른 정책과 비교하여 모든 상태에서 상태 가치 함수 $v_{\\pi_*}(s)$의 값이 더 큰 정책이라는 뜻입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c73475f",
   "metadata": {},
   "source": [
    "<p align=\"center\"><img src=\"./images/fig_02_12.png\" width=500></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "992c4eb3",
   "metadata": {},
   "source": [
    "중요한 사실은 MDP에서는 최적 정책이 적어도 하나는 존재한다는 사실입니다. 그리고 그 최적 정책은 '결정적 정책'입니다. 결정적 정책에서는 각 상태에서의 행동이 유일하게 결정됩니다.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52920987",
   "metadata": {},
   "source": [
    "수식으로는 $a = \\mu_*(s)$와 같이, 상태 s를 입력하면 행동 a를 출력하는 함수 $\\mu_*$로 나타낼 수 있습니다.  \n",
    "\n",
    "최적 정책의 상태 가치 함수를 **최적 상태 가치 함수** optimal state-value function ($v_*$로 표기) 라고 합니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "589d129c",
   "metadata": {},
   "source": [
    "## 2.4 MDP 예제"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc0c23d8",
   "metadata": {},
   "source": [
    "<p align=\"center\"><img src=\"./images/fig_02_13.png\" width=700></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4413f4f0",
   "metadata": {},
   "source": [
    "### 2.4.1 백업 다이어그램"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9a9ab75",
   "metadata": {},
   "source": [
    "**백업 다이어그램** backup diagram\n",
    "- '방향 있는 그래프(노드와 화살표로 구성된 그래프)'를 활용하여 '상태, 행동, 보상'의 전이를 표현한 그래프입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e31e7a39",
   "metadata": {},
   "source": [
    "에이전트가 현재 상태에 상관없이 무조건 오른쪽으로 이동한다고 해봅시다.\n",
    "<p align=\"center\"><img src=\"./images/fig_02_14.png\" width=200></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f3098b3",
   "metadata": {},
   "source": [
    "위 그림에서 에이전트의 정책이 결정적입니다. 즉, 항상 정해진 행동을 취합니다. 게다가 환경의 상태 전이도 결정적이기 때문에 백업 다이어그램의 전이는 일직선으로 뻗어나갑니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5686915",
   "metadata": {},
   "source": [
    "만약 에이전트가 50%의 확률로 오른쪽, 나머지 50%의 확률로 왼쪽으로 이동한다면 백업 다이어그램을 [그림2-15]처럼 그릴 수 있습니다.\n",
    "<p align=\"center\"><img src=\"./images/fig_02_15.png\" width=400></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee72b044",
   "metadata": {},
   "source": [
    "### 2.4.2 최적 정책 찾기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63682a56",
   "metadata": {},
   "source": [
    "그렇다면 두 칸짜리 그리드 월드에서 최적 정책은 무엇일까요? 최적 정책은 결정적 정책으로 존재한다고 알려져 있습니다. 결정적 정책은 $a=\\mu(s)$와 같이 함수로 표현됩니다. 그리고 이번 문제는 상태와 행동의 가짓수가 적기 때문에 존재하는 모든 결정적 정책을 알아낼 수 있습니다. 상태와 행동이 각 2개씩이므로 결정적 정책은 총 $2^2 = 4$개가 존재합니다.  \n",
    "그림의 네 가지 정책 중 최적 정책이 존재합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84fb57fa",
   "metadata": {},
   "source": [
    "<p align=\"center\">\n",
    "<img src=\"./images/fig_02_13_1.png\" height=200>\n",
    "<img src=\"./images/fig_02_16.png\" height=200>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d7542ba",
   "metadata": {},
   "source": [
    "이제 정책 $\\mu_1$의 상태 가치 함수를 계산해봅시다\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"./images/fig_02_16_1.png\" height=150>\n",
    "<img src=\"./images/fig_02_16_2.png\" height=150>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "44e151a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "v_mu1(S=L1) = -7.999734386011124\n",
      "v_mu1(S=L2) = -9.999734386011122\n"
     ]
    }
   ],
   "source": [
    "## 정책 v_mu1(S=L1)\n",
    "V = 1\n",
    "for i in range(1, 100):\n",
    "    V += -1 * (0.9 ** i)\n",
    "print('v_mu1(S=L1) =',V)\n",
    "\n",
    "## 정책 v_mu1(S=L2)\n",
    "V = -1\n",
    "for i in range(1, 100):\n",
    "    V += -1 * (0.9 ** i)\n",
    "print('v_mu1(S=L2) =',V)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2aba4b0",
   "metadata": {},
   "source": [
    "이렇게 해서 정책 $\\mu_1$의 가치 함수를 구했습니다. 이상의 작업을 다른 정책에도 모두 동일하게 진행하면 [그림2-17]의 결과를 얻을 수 있습니다.\n",
    "<p align=\"center\"><img src=\"./images/fig_02_17.png\" height=400></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb47833d",
   "metadata": {},
   "source": [
    "그래프를 보면 정책 $\\mu_2$가 모든 상태에서 다른 정책들보다 상태 가치 함수의 값이 더 큽니다. 따라서 정책 $\\mu_2$가 바로 우리가 찾는 최적 정책입니다. 정책 $\\mu_2$는 벽에 부딪히지 않고 오른쪽으로 갔다가 왼쪽으로 돌아오는 행동을 반복합니다. 그러면서 사과를 받복해서 얻는 것이죠. 최적 정책을 찾았으니 마침내 MDP의 목표를 달성한 것입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a978cc6",
   "metadata": {},
   "source": [
    "## 2.5 정리"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54d1eed9",
   "metadata": {},
   "source": [
    "이번 장에서는 마르코프 결정 과정(MDP)에 대해 알아보았습니다. MDP는 에이전트와 환경의 상호작용을 수식으로 표현한 것입니다.  \n",
    "환경에는 상태 전이 확률(또는 상태 전이 함수)과 보상 함수가 있고, 에이전트에는 정책이 있습니다. 그리고 환경과 에이전트가 영향을 주고 받습니다. 이러한 틀 안에서 최적 정책을 찾는 것이 MDP의 목표입니다.  \n",
    "최적 정책이란 모든 상태에서 다른 어떤 정책보다 상태 가치 함수의 값이 더 크거나 같은 정책을 말합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1c94399",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
