{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DJkHzjAw9i4Q"
      },
      "source": [
        "**15장 – RNN과 CNN을 사용해 시퀀스 처리하기**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rm41fEdF9i4b"
      },
      "source": [
        "_이 노트북에는 15장의 모든 샘플 코드와 연습 문제에 대한 해답이 포함되어 있습니다._"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VHXdWQ679i4c"
      },
      "source": [
        "<table align=\"left\">\n",
        "  <td>\n",
        "    <a href=\"https://colab.research.google.com/github/rickiepark/handson-ml3/blob/main/15_processing_sequences_using_rnns_and_cnns.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
        "  </td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dFXIv9qNpKzt",
        "tags": []
      },
      "source": [
        "# 설정"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8IPbJEmZpKzu"
      },
      "source": [
        "이 프로젝트에는 Python 3.7 이상이 필요합니다:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TFSU3FCOpKzu"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "\n",
        "assert sys.version_info >= (3, 7)\n",
        "\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GJtVEqxfpKzw"
      },
      "source": [
        "그리고 TensorFlow ≥ 2.8:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0Piq5se2pKzx"
      },
      "outputs": [],
      "source": [
        "from packaging import version\n",
        "import tensorflow as tf\n",
        "\n",
        "assert version.parse(tf.__version__) >= version.parse(\"2.8.0\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DDaDoLQTpKzx"
      },
      "source": [
        "이전 챕터에서 했던 것처럼 기본 글꼴 크기를 정의하여 그림을 더 예쁘게 만들어 보겠습니다:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8d4TH3NbpKzx",
        "outputId": "55a3a298-b8da-4e17-b2b9-f2c29ffac949"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "plt.rc('font', size=14)\n",
        "plt.rc('axes', labelsize=14, titlesize=14)\n",
        "plt.rc('legend', fontsize=14)\n",
        "plt.rc('xtick', labelsize=10)\n",
        "plt.rc('ytick', labelsize=10)\n",
        "\n",
        "import sys\n",
        "# 코랩의 경우 나눔 폰트를 설치합니다.\n",
        "if 'google.colab' in sys.modules:\n",
        "    !sudo apt-get -qq -y install fonts-nanum\n",
        "    import matplotlib.font_manager as fm\n",
        "    font_files = fm.findSystemFonts(fontpaths=['/usr/share/fonts/truetype/nanum'])\n",
        "    for fpath in font_files:\n",
        "        fm.fontManager.addfont(fpath)\n",
        "\n",
        "# 나눔 폰트를 사용합니다.\n",
        "import matplotlib\n",
        "\n",
        "matplotlib.rc('font', family='NanumBarunGothic')\n",
        "matplotlib.rcParams['axes.unicode_minus'] = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "def get_font_family():\n",
        "    import platform\n",
        "    system_name = platform.system()\n",
        "    if system_name == \"Darwin\" :\n",
        "        font_family = \"AppleGothic\"\n",
        "    elif system_name == \"Windows\":\n",
        "        font_family = \"Malgun Gothic\"\n",
        "    elif system_name == \"Linux\":\n",
        "        font_family = \"NanumGothic\"\n",
        "    return font_family\n",
        "\n",
        "\n",
        "font_family = get_font_family()\n",
        "plt.rc(\"font\", family=font_family)\n",
        "plt.rc('axes', unicode_minus=False)\n",
        "%config InlineBackend.figure_format='retina'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RcoUIRsvpKzy"
      },
      "source": [
        "그리고 `images/rnn` 폴더를 만들고(아직 존재하지 않는 경우), 이 노트북을 통해 책에 사용할 그림을 고해상도로 저장하는 데 사용되는 `save_fig()` 함수를 정의해 보겠습니다:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PQFH5Y9PpKzy"
      },
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "\n",
        "IMAGES_PATH = Path() / \"images\" / \"rnn\"\n",
        "IMAGES_PATH.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "def save_fig(fig_id, tight_layout=True, fig_extension=\"png\", resolution=300):\n",
        "    path = IMAGES_PATH / f\"{fig_id}.{fig_extension}\"\n",
        "    if tight_layout:\n",
        "        plt.tight_layout()\n",
        "    # plt.savefig(path, format=fig_extension, dpi=resolution)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YTsawKlapKzy"
      },
      "source": [
        "이 챕터는 GPU가 없으면 매우 느려질 수 있으므로 GPU가 있는지 확인하거나 그렇지 않으면 경고를 표시합니다:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ekxzo6pOpKzy"
      },
      "outputs": [],
      "source": [
        "if not tf.config.list_physical_devices('GPU'):\n",
        "    print(\"GPU가 감지되지 않았습니다. 신경망은 GPU가 없으면 매우 느릴 수 있습니다.\")\n",
        "    if \"google.colab\" in sys.modules:\n",
        "        print(\"런타임 > 런타임 유형 변경으로 이동하여 GPU를 선택하세요.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y7p0gDSP9i4r"
      },
      "source": [
        "# 기본 RNN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zqSZA_lD9i4r"
      },
      "source": [
        "`ageron/data` 저장소트에서 승객 데이터를 다운로드해 보겠습니다. 원래는 시카고 교통국에서 제공한 것으로, [시카고 데이터 포털](https://homl.info/ridership)에서 다운로드할 수 있습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "id": "sPAgnesQ9i40",
        "outputId": "b416d6cf-abd3-4783-a173-1ce2f58d6a61"
      },
      "outputs": [],
      "source": [
        "tf.keras.utils.get_file(\n",
        "    \"ridership.tgz\",\n",
        "    \"https://github.com/ageron/data/raw/main/ridership.tgz\",\n",
        "    cache_dir=\".\",\n",
        "    extract=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GOYIXxRZ9i47"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "\n",
        "path = Path(\"datasets/ridership/CTA_-_Ridership_-_Daily_Boarding_Totals.csv\")\n",
        "df = pd.read_csv(path, parse_dates=[\"service_date\"])\n",
        "df.columns = [\"date\", \"day_type\", \"bus\", \"rail\", \"total\"]  # 더 짧은 이름\n",
        "df = df.sort_values(\"date\").set_index(\"date\")\n",
        "df = df.drop(\"total\", axis=1)  # total은 단순히 bus + rail 이므로 삭제합니다.\n",
        "df = df.drop_duplicates()  # 중복된 월 제거 (2011-10와 2014-07)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "id": "99aKJDKo9i47",
        "outputId": "766c2cb7-d38c-4d76-e894-ecd064667655"
      },
      "outputs": [],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aA_XW6oS9i48"
      },
      "source": [
        "2019년의 첫 몇 달을 살펴봅시다(판다스에서는 범위의 경계를 포함합니다):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 345
        },
        "id": "yA0Lrd519i48",
        "outputId": "7242d985-8361-494e-a999-3acd4e30cc6e"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "df[\"2019-03\":\"2019-05\"].plot(grid=True, marker=\".\", figsize=(8, 3.5))\n",
        "plt.legend([\"버스\", \"열차\"]);\n",
        "plt.xlabel(\"날짜\")\n",
        "save_fig(\"daily_ridership_plot\")  # 추가 코드 - 책의 그림을 저장합니다.\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 494
        },
        "id": "h6TOSbhl9i48",
        "outputId": "62644756-9d01-4a5f-cba5-b4769620404e"
      },
      "outputs": [],
      "source": [
        "diff_7 = df[[\"bus\", \"rail\"]].diff(7)[\"2019-03\":\"2019-05\"]\n",
        "\n",
        "fig, axs = plt.subplots(2, 1, sharex=True, figsize=(8, 5))\n",
        "df.plot(ax=axs[0], legend=False, marker=\".\")  # 원본 시계열\n",
        "df.shift(7).plot(ax=axs[0], grid=True, legend=False, linestyle=\":\")  # 지연\n",
        "diff_7.plot(ax=axs[1], grid=True, marker=\".\")  # 7일 차분 시계열\n",
        "axs[1].legend([\"버스\", \"열차\"])\n",
        "axs[1].set_xlabel(\"날짜\")\n",
        "axs[0].set_ylim([170_000, 900_000])  # 추가 코드 - 그래프를 아름답게 꾸미기\n",
        "save_fig(\"differencing_plot\")  # 추가 코드 - 책의 그림을 저장합니다.\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "emEdmLkS9i49",
        "outputId": "40101350-1f38-487d-dc2d-0c9dba04726c",
        "tags": []
      },
      "outputs": [],
      "source": [
        "list(df.loc[\"2019-05-25\":\"2019-05-27\"][\"day_type\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HeIK90K_9i49"
      },
      "source": [
        "평균 절대 오차(MAE)는 평균 절대 편차(MAD)라고도 합니다:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5INjvZkW9i49",
        "outputId": "b2b6d72f-5e0d-4b67-c0cc-06c7c180b271"
      },
      "outputs": [],
      "source": [
        "diff_7.abs().mean()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jwQuXFfi9i4-"
      },
      "source": [
        "평균 절대 비율 오류(MAPE):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X8FJEWJK9i4-",
        "outputId": "bad14fcc-b4be-4ae3-c140-9c98b6c4a7ee"
      },
      "outputs": [],
      "source": [
        "targets = df[[\"bus\", \"rail\"]][\"2019-03\":\"2019-05\"]\n",
        "(diff_7 / targets).abs().mean()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LuXlris19i4-"
      },
      "source": [
        "이제 연도별 계절성과 장기 트렌드를 살펴보겠습니다:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 395
        },
        "id": "YJWxsYJ99i4-",
        "outputId": "c80256f7-3981-413a-c643-ee4f1139d5a0"
      },
      "outputs": [],
      "source": [
        "period = slice(\"2001\", \"2019\")\n",
        "df_monthly = df.resample('M').mean(numeric_only=True)  # 월별 평균을 계산합니다.\n",
        "rolling_average_12_months = df_monthly[period].rolling(window=12).mean()\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(8, 4))\n",
        "df_monthly[period].plot(ax=ax, marker=\".\")\n",
        "rolling_average_12_months.plot(ax=ax, grid=True, legend=False)\n",
        "ax.legend([\"버스\", \"열차\"]);\n",
        "ax.set_xlabel('날짜')\n",
        "save_fig(\"long_term_ridership_plot\")  # 추가 코드 - 책의 그림을 저장합니다.\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "xyDaWVz49i4_",
        "outputId": "8057ab3e-b76d-44dc-e5a7-628ad287ff1a"
      },
      "outputs": [],
      "source": [
        "df_monthly.diff(12)[period].plot(grid=True, marker=\".\", figsize=(8, 3))\n",
        "plt.legend([\"버스\", \"열차\"])\n",
        "plt.xlabel(\"날짜\")\n",
        "save_fig(\"yearly_diff_plot\")  # 추가 코드 - 책의 그림을 저장합니다.\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "euGEf9Ih9i4_"
      },
      "source": [
        "Colab에서 실행하는 경우 `statsmodels` 라이브러리를 설치합니다:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "34MnZNRj9i4_"
      },
      "outputs": [],
      "source": [
        "if \"google.colab\" in sys.modules:\n",
        "    %pip install -q -U statsmodels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LPmzraWY9i4_"
      },
      "outputs": [],
      "source": [
        "from statsmodels.tsa.arima.model import ARIMA\n",
        "\n",
        "origin, today = \"2019-01-01\", \"2019-05-31\"\n",
        "rail_series = df.loc[origin:today][\"rail\"].asfreq(\"D\")\n",
        "model = ARIMA(rail_series,\n",
        "              order=(1, 0, 0),\n",
        "              seasonal_order=(0, 1, 1, 7))\n",
        "model = model.fit()\n",
        "y_pred = model.forecast()  # 427,758.6 반환"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H3ZOpuil9i5A",
        "outputId": "68691f30-814e-4e0e-a767-3274c3b7a876"
      },
      "outputs": [],
      "source": [
        "y_pred[0]  # ARIMA 예측"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gpn622cY9i5B",
        "outputId": "42ded4e2-5f84-46e3-bc2b-dd914bd5afd3"
      },
      "outputs": [],
      "source": [
        "df[\"rail\"].loc[\"2019-06-01\"]  # 타깃 값"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z77JLBDk9i5B",
        "outputId": "76ed3451-b6cf-46eb-ae38-eb477223536f"
      },
      "outputs": [],
      "source": [
        "df[\"rail\"].loc[\"2019-05-25\"]  # 순진한 예측(1주일 전의 값)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bMSLkVg39i5C"
      },
      "outputs": [],
      "source": [
        "origin, start_date, end_date = \"2019-01-01\", \"2019-03-01\", \"2019-05-31\"\n",
        "time_period = pd.date_range(start_date, end_date)\n",
        "rail_series = df.loc[origin:end_date][\"rail\"].asfreq(\"D\")\n",
        "y_preds = []\n",
        "for today in time_period.shift(-1):\n",
        "    model = ARIMA(rail_series[origin:today],  # \"오늘\"까지의 데이터로 훈련\n",
        "                  order=(1, 0, 0),\n",
        "                  seasonal_order=(0, 1, 1, 7))\n",
        "    model = model.fit()  # 매일 모델을 재훈련한다는 점에 유의하세요!\n",
        "    y_pred = model.forecast()[0]\n",
        "    y_preds.append(y_pred)\n",
        "\n",
        "y_preds = pd.Series(y_preds, index=time_period)\n",
        "mae = (y_preds - rail_series[time_period]).abs().mean()  # 32,040.7 반환"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n4nDhrBH9i5C",
        "outputId": "6029e74d-0a30-4bc6-e784-0887a3af7a73"
      },
      "outputs": [],
      "source": [
        "mae"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 333
        },
        "id": "uyPUYHxs9i5C",
        "outputId": "a011af0a-a802-4e90-e13c-67f6fc8adf88"
      },
      "outputs": [],
      "source": [
        "# 추가 코드 - SARIMA 예측을 표시합니다.\n",
        "fig, ax = plt.subplots(figsize=(8, 3))\n",
        "rail_series.loc[time_period].plot(label=\"실제\", ax=ax, marker=\".\", grid=True)\n",
        "ax.plot(y_preds, color=\"r\", marker=\".\", label=\"SARIMA 예측\")\n",
        "ax.set_xlabel(\"날짜\")\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 442
        },
        "id": "lNXbBuR29i5D",
        "outputId": "a5cda9b8-8d52-46c4-a8ff-f45a926ff59d"
      },
      "outputs": [],
      "source": [
        "# 추가 코드 - 자기 상관관계 함수(ACF) 및\n",
        "# 부분 자기 상관관계 함수(PACF)를 그리는 방법을 보여줍니다.\n",
        "\n",
        "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
        "\n",
        "fig, axs = plt.subplots(nrows=1, ncols=2, figsize=(15, 5))\n",
        "plot_acf(df[period][\"rail\"], ax=axs[0], lags=35, title=\"자기상관\")\n",
        "axs[0].grid()\n",
        "plot_pacf(df[period][\"rail\"], ax=axs[1], lags=35, method=\"ywm\",\n",
        "          title=\"부분 자기상관\")\n",
        "axs[1].grid()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lEqIt0EM9i5D",
        "outputId": "22ae2d2c-96e4-459f-8393-5aaab6b16b22"
      },
      "outputs": [],
      "source": [
        "# TensorFlow 라이브러리를 가져옵니다.\n",
        "import tensorflow as tf\n",
        "\n",
        "# 간단한 시계열 데이터를 생성합니다.\n",
        "my_series = [0, 1, 2, 3, 4, 5]\n",
        "\n",
        "# 시계열 데이터셋을 생성합니다.\n",
        "my_dataset = tf.keras.utils.timeseries_dataset_from_array(\n",
        "    my_series,  # 입력 시계열 데이터\n",
        "    targets=my_series[3:],  # 타깃은 입력보다 3 스텝 앞선 값입니다.\n",
        "    sequence_length=3,  # 각 시퀀스의 길이를 3으로 설정합니다.\n",
        "    batch_size=2  # 배치 크기를 2로 설정합니다.\n",
        ")\n",
        "\n",
        "# 생성된 데이터셋을 리스트로 변환하여 출력합니다.\n",
        "# 이를 통해 데이터셋의 구조를 확인할 수 있습니다.\n",
        "list(my_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "my_series = [0, 1, 2, 3, 4, 5]\n",
        "\n",
        "for window_dataset in tf.data.Dataset.from_tensor_slices(my_series).window(4, shift=1):\n",
        "    for element in window_dataset:\n",
        "        print(f\"{element}\", end=\" \")\n",
        "    print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "55c8P2pH9i5D",
        "outputId": "65f86c3e-1d40-48f8-e4fb-4719e723633c"
      },
      "outputs": [],
      "source": [
        "for window_dataset in tf.data.Dataset.range(6).window(4, shift=1):\n",
        "    for element in window_dataset:\n",
        "        print(f\"{element}\", end=\" \")\n",
        "    print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L9AOQWbD9i5E",
        "outputId": "7983ff81-1dc4-412d-e9dc-8f43c9429549"
      },
      "outputs": [],
      "source": [
        "# 0부터 5까지의 숫자로 데이터셋을 생성합니다.\n",
        "dataset = tf.data.Dataset.range(6)\n",
        "\n",
        "# 윈도우 크기 4, 이동 간격 1로 윈도우를 생성합니다. \n",
        "# drop_remainder=True는 마지막에 크기가 4 미만인 윈도우를 버립니다.\n",
        "dataset = dataset.window(4, shift=1, drop_remainder=True)\n",
        "\n",
        "# 각 윈도우를 하나의 텐서로 변환합니다.\n",
        "dataset = dataset.flat_map(lambda window_dataset: window_dataset.batch(4))\n",
        "\n",
        "# 각 윈도우 텐서를 출력합니다.\n",
        "for window_tensor in dataset:\n",
        "    print(f\"{window_tensor}\")\n",
        "\n",
        "# 결과로 [0 1 2 3], [1 2 3 4], [2 3 4 5] 세 개의 윈도우가 생성됩니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WFwpIDhR9i5E"
      },
      "outputs": [],
      "source": [
        "def to_windows(dataset, length):\n",
        "    # 데이터셋을 주어진 길이의 윈도우로 변환합니다.\n",
        "    # length: 각 윈도우의 길이\n",
        "    # shift=1: 각 윈도우는 1씩 이동하며 생성됩니다.\n",
        "    # drop_remainder=True: 마지막에 length보다 작은 윈도우는 버립니다.\n",
        "    dataset = dataset.window(length, shift=1, drop_remainder=True)\n",
        "    \n",
        "    # 각 윈도우를 하나의 텐서로 변환합니다.\n",
        "    # flat_map을 사용하여 윈도우 데이터셋을 펼치고, 각 윈도우를 배치로 만듭니다.\n",
        "    return dataset.flat_map(lambda window_ds: window_ds.batch(length))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d3CercaN9i5E",
        "outputId": "fa1c5b5c-cd8c-4cfd-c863-657ffcb395fd"
      },
      "outputs": [],
      "source": [
        "dataset = to_windows(tf.data.Dataset.range(6), 4)\n",
        "dataset = dataset.map(lambda window: (window[:-1], window[-1]))\n",
        "list(dataset.batch(2))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wae4FNZv9i5F"
      },
      "source": [
        "데이터를 계속 살펴보기 전에 훈련, 검증 및 테스트를 위해 시계열을 세 가지 기간으로 나누어 보겠습니다. 지금은 테스트 데이터를 살펴보지 않겠습니다:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "17qcHzqF9i5F"
      },
      "outputs": [],
      "source": [
        "rail_train = df[\"rail\"][\"2016-01\":\"2018-12\"] / 1e6\n",
        "rail_valid = df[\"rail\"][\"2019-01\":\"2019-05\"] / 1e6\n",
        "rail_test = df[\"rail\"][\"2019-06\":] / 1e6"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hkulHlvq9i5F"
      },
      "outputs": [],
      "source": [
        "# 시퀀스의 길이를 56으로 설정합니다. 이는 8주(56일) 동안의 데이터를 사용하여 다음 날을 예측하겠다는 의미입니다.\n",
        "seq_length = 56\n",
        "\n",
        "# 난수 생성을 위한 시드를 설정하여 결과의 재현성을 보장합니다.\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "# 훈련 데이터셋을 생성합니다.\n",
        "train_ds = tf.keras.utils.timeseries_dataset_from_array(\n",
        "    rail_train.to_numpy(),  # 입력 데이터: 2016년부터 2018년까지의 철도 이용객 수\n",
        "    targets=rail_train[seq_length:],  # 타겟 데이터: 입력 데이터에서 seq_length만큼 이동한 값들\n",
        "    sequence_length=seq_length,  # 각 시퀀스의 길이\n",
        "    batch_size=32,  # 배치 크기\n",
        "    shuffle=True,  # 데이터를 섞어서 과적합을 방지\n",
        "    seed=42  # 셔플링을 위한 시드 설정\n",
        ")\n",
        "\n",
        "# 검증 데이터셋을 생성합니다.\n",
        "valid_ds = tf.keras.utils.timeseries_dataset_from_array(\n",
        "    rail_valid.to_numpy(),  # 입력 데이터: 2019년 1월부터 5월까지의 철도 이용객 수\n",
        "    targets=rail_valid[seq_length:],  # 타겟 데이터: 입력 데이터에서 seq_length만큼 이동한 값들\n",
        "    sequence_length=seq_length,  # 각 시퀀스의 길이\n",
        "    batch_size=32  # 배치 크기 (검증 데이터는 셔플링하지 않습니다)\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 선형 모델로 예측하기"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vM7YHzxv9i5I",
        "outputId": "f2bf4abf-9710-42de-e505-4750f40cd081"
      },
      "outputs": [],
      "source": [
        "# 재현성을 위해 랜덤 시드를 설정합니다.\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "# 선형 모델을 정의합니다.\n",
        "model = tf.keras.Sequential([\n",
        "    # 입력 시퀀스 길이(seq_length)를 받아 하나의 출력을 내는 Dense 층을 추가합니다.\n",
        "    tf.keras.layers.Dense(1, input_shape=[seq_length])\n",
        "])\n",
        "\n",
        "# 조기 종료 콜백을 정의합니다.\n",
        "early_stopping_cb = tf.keras.callbacks.EarlyStopping(\n",
        "    monitor=\"val_mae\",  # 검증 세트의 평균 절대 오차를 모니터링합니다.\n",
        "    patience=50,  # 50 에포크 동안 개선이 없으면 훈련을 중단합니다.\n",
        "    restore_best_weights=True  # 최상의 모델 가중치를 복원합니다.\n",
        ")\n",
        "\n",
        "# 확률적 경사 하강법(SGD) 옵티마이저를 정의합니다.\n",
        "opt = tf.keras.optimizers.SGD(learning_rate=0.02, momentum=0.9)\n",
        "\n",
        "# 모델을 컴파일합니다.\n",
        "model.compile(\n",
        "    loss=tf.keras.losses.Huber(),  # Huber 손실 함수를 사용합니다.\n",
        "    optimizer=opt,\n",
        "    metrics=[\"mae\"]  # 평균 절대 오차를 메트릭으로 사용합니다.\n",
        ")\n",
        "\n",
        "# 모델을 훈련시킵니다.\n",
        "history = model.fit(\n",
        "    train_ds,  # 훈련 데이터셋\n",
        "    validation_data=valid_ds,  # 검증 데이터셋\n",
        "    epochs=500,  # 최대 500 에포크 동안 훈련합니다.\n",
        "    callbacks=[early_stopping_cb]  # 조기 종료 콜백을 사용합니다.\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y2swfjfh9i5I",
        "outputId": "dd51b6c9-6d56-4bdc-db0a-8588b16b1347"
      },
      "outputs": [],
      "source": [
        "# 추가 코드 - 모델을 평가합니다.\n",
        "valid_loss, valid_mae = model.evaluate(valid_ds)\n",
        "valid_mae * 1e6"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DwIX-i0R9i5I"
      },
      "source": [
        "## 간단한 RNN 사용"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LxMxuoqx9i5M"
      },
      "outputs": [],
      "source": [
        "# 재현성을 위해 랜덤 시드를 설정합니다.\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "# 간단한 RNN 모델을 정의합니다.\n",
        "model = tf.keras.Sequential([\n",
        "    # SimpleRNN 층을 추가합니다.\n",
        "    # - 1개의 유닛(뉴런)을 가집니다.\n",
        "    # - input_shape=[None, 1]은 가변 길이의 시퀀스와 각 시점마다 1개의 특성을 가진 입력을 의미합니다.\n",
        "    #   None은 시퀀스 길이가 가변적임을 나타냅니다.\n",
        "    tf.keras.layers.SimpleRNN(1, input_shape=[None, 1])\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J9kwvI6o9i5M"
      },
      "outputs": [],
      "source": [
        "# 여러 번 재사용할 유틸리티 함수를 정의합니다.\n",
        "\n",
        "def fit_and_evaluate(model, train_set, valid_set, learning_rate, epochs=500):\n",
        "    # 조기 종료 콜백을 설정합니다.\n",
        "    # 검증 세트의 MAE가 50 에포크 동안 개선되지 않으면 훈련을 중단합니다.\n",
        "    early_stopping_cb = tf.keras.callbacks.EarlyStopping(\n",
        "        monitor=\"val_mae\", patience=50, restore_best_weights=True)\n",
        "    \n",
        "    # SGD 옵티마이저를 설정합니다. 모멘텀을 사용하여 학습 속도를 개선합니다.\n",
        "    opt = tf.keras.optimizers.SGD(learning_rate=learning_rate, momentum=0.9)\n",
        "    \n",
        "    # 모델을 컴파일합니다. Huber 손실 함수와 MAE 메트릭을 사용합니다.\n",
        "    model.compile(loss=tf.keras.losses.Huber(), optimizer=opt, metrics=[\"mae\"])\n",
        "    \n",
        "    # 모델을 훈련시킵니다.\n",
        "    history = model.fit(train_set, validation_data=valid_set, epochs=epochs,\n",
        "                        callbacks=[early_stopping_cb])\n",
        "    \n",
        "    # 검증 세트에 대해 모델을 평가합니다.\n",
        "    valid_loss, valid_mae = model.evaluate(valid_set)\n",
        "    \n",
        "    # MAE를 마이크로초 단위로 변환하여 반환합니다.\n",
        "    return valid_mae * 1e6"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vfK4onlh9i5N",
        "outputId": "f8ad29fa-346f-44ef-e938-774a627f0f75"
      },
      "outputs": [],
      "source": [
        "fit_and_evaluate(model, train_ds, valid_ds, learning_rate=0.02)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_7Byyqcu9i5N"
      },
      "outputs": [],
      "source": [
        "tf.random.set_seed(42)  # 추가 코드 - 재현성 보장\n",
        "univar_model = tf.keras.Sequential([\n",
        "    tf.keras.layers.SimpleRNN(32, input_shape=[None, 1]),\n",
        "    tf.keras.layers.Dense(1)  # 기본적으로 활성화 함수 없음\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JZ9HP7pH9i5N",
        "outputId": "49f91218-a9eb-45cf-d719-9c94f2966626"
      },
      "outputs": [],
      "source": [
        "# 추가 코드 - 이전과 같이 모델을 컴파일, 훈련 및 평가합니다.\n",
        "fit_and_evaluate(univar_model, train_ds, valid_ds, learning_rate=0.05)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WkKTNSf79i5O"
      },
      "source": [
        "## 심층 RNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5MnrN76G9i5O"
      },
      "outputs": [],
      "source": [
        "tf.random.set_seed(42)  # 추가 코드 - 재현성 보장\n",
        "deep_model = tf.keras.Sequential([\n",
        "    tf.keras.layers.SimpleRNN(32, return_sequences=True, input_shape=[None, 1]),\n",
        "    tf.keras.layers.SimpleRNN(32, return_sequences=True),\n",
        "    tf.keras.layers.SimpleRNN(32),\n",
        "    tf.keras.layers.Dense(1)\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MaQtAbgv9i5O",
        "outputId": "606f6a65-0193-4645-b264-ea0969cc169e"
      },
      "outputs": [],
      "source": [
        "# 추가 코드 - 이전과 같이 모델을 컴파일, 훈련 및 평가합니다.\n",
        "fit_and_evaluate(deep_model, train_ds, valid_ds, learning_rate=0.01)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X2na3s3M9i5O"
      },
      "source": [
        "## 다변량 시계열"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O44xIqKA9i5P"
      },
      "outputs": [],
      "source": [
        "df_mulvar = df[[\"bus\", \"rail\"]] / 1e6  # 버스 및 열차 시계열을 모두 입력으로 사용\n",
        "df_mulvar[\"next_day_type\"] = df[\"day_type\"].shift(-1)  # 내일의 유형을 알고 있습니다.\n",
        "df_mulvar = pd.get_dummies(df_mulvar)  # 요일 유형을 원-핫 인코딩합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cb7Pm-Zw9i5P"
      },
      "outputs": [],
      "source": [
        "mulvar_train = df_mulvar[\"2016-01\":\"2018-12\"]\n",
        "mulvar_valid = df_mulvar[\"2019-01\":\"2019-05\"]\n",
        "mulvar_test = df_mulvar[\"2019-06\":]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I3XiUVI39i5P"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "# 재현성을 위해 랜덤 시드 설정\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "# 다변량 시계열 데이터셋 생성 (훈련용)\n",
        "train_mulvar_ds = tf.keras.utils.timeseries_dataset_from_array(\n",
        "    mulvar_train.to_numpy(np.float32),  # 모든 특성(5개 열)을 입력으로 사용\n",
        "    targets=mulvar_train[\"rail\"][seq_length:],  # 열차 승객 수만 예측 대상으로 설정\n",
        "    sequence_length=seq_length,  # 시퀀스 길이 설정\n",
        "    batch_size=32,  # 배치 크기 설정\n",
        "    shuffle=True,  # 데이터 섞기 활성화\n",
        "    seed=42  # 셔플링의 재현성을 위한 시드 설정\n",
        ")\n",
        "\n",
        "# 다변량 시계열 데이터셋 생성 (검증용)\n",
        "valid_mulvar_ds = tf.keras.utils.timeseries_dataset_from_array(\n",
        "    mulvar_valid.to_numpy(np.float32),  # 검증 데이터의 모든 특성 사용\n",
        "    targets=mulvar_valid[\"rail\"][seq_length:],  # 검증 데이터의 열차 승객 수 예측\n",
        "    sequence_length=seq_length,\n",
        "    batch_size=32  # 검증 데이터는 셔플링하지 않음\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LaY12mmL9i5P"
      },
      "outputs": [],
      "source": [
        "tf.random.set_seed(42)  # 추가 코드 - 재현성 보장\n",
        "mulvar_model = tf.keras.Sequential([\n",
        "    tf.keras.layers.SimpleRNN(32, input_shape=[None, mulvar_train.shape[1]]),\n",
        "    tf.keras.layers.Dense(1)\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m4F_cHju9i5P",
        "outputId": "fba810b7-9e25-404c-a0e3-4d0831f7eba4"
      },
      "outputs": [],
      "source": [
        "# 추가 코드 - 이전과 같이 모델을 컴파일, 훈련 및 평가합니다.\n",
        "fit_and_evaluate(mulvar_model, train_mulvar_ds, valid_mulvar_ds,\n",
        "                 learning_rate=0.05)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P-2hs7y99i5Q",
        "outputId": "b187ab0a-0eee-461f-883e-0606ab5b0548"
      },
      "outputs": [],
      "source": [
        "# 추가 코드 - 버스와 열차를 모두 예측하는 멀티태스크 RNN 구축 및 훈련\n",
        "\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "seq_length = 56\n",
        "train_multask_ds = tf.keras.utils.timeseries_dataset_from_array(\n",
        "    mulvar_train.to_numpy(np.float32),\n",
        "    targets=mulvar_train[[\"bus\", \"rail\"]][seq_length:],  # 타깃 2개\n",
        "    sequence_length=seq_length,\n",
        "    batch_size=32,\n",
        "    shuffle=True,\n",
        "    seed=42\n",
        ")\n",
        "valid_multask_ds = tf.keras.utils.timeseries_dataset_from_array(\n",
        "    mulvar_valid.to_numpy(np.float32),\n",
        "    targets=mulvar_valid[[\"bus\", \"rail\"]][seq_length:],\n",
        "    sequence_length=seq_length,\n",
        "    batch_size=32\n",
        ")\n",
        "\n",
        "tf.random.set_seed(42)\n",
        "multask_model = tf.keras.Sequential([\n",
        "    tf.keras.layers.SimpleRNN(32, input_shape=[None, 5]),\n",
        "    tf.keras.layers.Dense(2)\n",
        "])\n",
        "\n",
        "fit_and_evaluate(multask_model, train_multask_ds, valid_multask_ds,\n",
        "                 learning_rate=0.02)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XHno6SRT9i5Q",
        "outputId": "610bbaf0-0036-422e-82bf-1e9c4ed41a72"
      },
      "outputs": [],
      "source": [
        "# 추가 코드 - 버스에 대한 순진한 예측을 평가합니다.\n",
        "bus_naive = mulvar_valid[\"bus\"].shift(7)[seq_length:]\n",
        "bus_target = mulvar_valid[\"bus\"][seq_length:]\n",
        "(bus_target - bus_naive).abs().mean() * 1e6"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EHLgRQfE9i5Q",
        "outputId": "08ef32b2-07b6-4655-ea42-e257f73e5248"
      },
      "outputs": [],
      "source": [
        "# 추가 코드 - 버스와 열차 모두에 대한 멀티태스크 RNN의 예측을 평가합니다.\n",
        "Y_preds_valid = multask_model.predict(valid_multask_ds)\n",
        "for idx, name in enumerate([\"bus\", \"rail\"]):\n",
        "    mae = 1e6 * tf.reduce_mean(tf.abs(\n",
        "        mulvar_valid[name][seq_length:].values - Y_preds_valid[:, idx]))\n",
        "    print(name, int(mae))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vqknH78k9i5Q"
      },
      "source": [
        "## 여러 스텝 앞을 예측하기"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KjU7ODjq9i5Q",
        "outputId": "d8a977bb-5d58-446f-9865-276c1ecfa538",
        "tags": []
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "# 유효성 검증 데이터셋에서 초기 시퀀스를 추출합니다.\n",
        "X = rail_valid.to_numpy()[np.newaxis, :seq_length, np.newaxis]\n",
        "\n",
        "# 14일 동안의 예측을 수행합니다.\n",
        "for step_ahead in range(14):\n",
        "    # 현재 시퀀스에 대해 다음 날의 예측을 수행합니다.\n",
        "    y_pred_one = univar_model.predict(X)\n",
        "    \n",
        "    # 예측 결과를 기존 시퀀스에 추가합니다.\n",
        "    # reshape(1, 1, 1)은 예측값을 3차원 배열로 변환합니다 (배치 크기 1, 시퀀스 길이 1, 특성 수 1).\n",
        "    X = np.concatenate([X, y_pred_one.reshape(1, 1, 1)], axis=1)\n",
        "    \n",
        "    # 이 과정을 반복하면 매 스텝마다 이전 예측을 포함한 새로운 시퀀스로 다음 날을 예측합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 345
        },
        "id": "mC02tAVx9i5R",
        "outputId": "dc6cfabb-fc0f-4bdd-db28-1fec7e876cb7"
      },
      "outputs": [],
      "source": [
        "# 그림 15-11 생성 및 저장\n",
        "\n",
        "# 예측 데이터 준비\n",
        "# 2019-02-26부터 2019-03-11까지 14일간의 예측 데이터를 시리즈로 변환합니다.\n",
        "Y_pred = pd.Series(X[0, -14:, 0],\n",
        "                   index=pd.date_range(\"2019-02-26\", \"2019-03-11\"))\n",
        "\n",
        "# 그래프 생성\n",
        "fig, ax = plt.subplots(figsize=(8, 3.5))\n",
        "\n",
        "# 실제 데이터 플롯\n",
        "# 2019-02-01부터 2019-03-11까지의 실제 데이터를 그립니다. (단위: 백만)\n",
        "(rail_valid * 1e6)[\"2019-02-01\":\"2019-03-11\"].plot(\n",
        "    label=\"진짜\", marker=\".\", ax=ax)\n",
        "\n",
        "# 예측 데이터 플롯\n",
        "# 예측 데이터를 빨간색 'x' 마커로 그립니다. (단위: 백만)\n",
        "(Y_pred * 1e6).plot(\n",
        "    label=\"예측\", grid=True, marker=\"x\", color=\"r\", ax=ax)\n",
        "\n",
        "# '오늘' 표시선 추가\n",
        "# 2019-02-25를 '오늘'로 표시하는 수직선을 그립니다.\n",
        "ax.vlines(\"2019-02-25\", 0, 1e6, color=\"k\", linestyle=\"--\", label=\"오늘\")\n",
        "\n",
        "# y축 범위 설정\n",
        "ax.set_ylim([200_000, 800_000])\n",
        "\n",
        "# x축 레이블 설정\n",
        "ax.set_xlabel(\"날짜\")\n",
        "\n",
        "# 범례 위치 설정\n",
        "plt.legend(loc=\"center left\")\n",
        "\n",
        "# 그래프 저장\n",
        "save_fig(\"forecast_ahead_plot\")\n",
        "\n",
        "# 그래프 표시\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xpHWSzic9i5R"
      },
      "source": [
        "이제 14개의 다음 값을 한 번에 모두 예측하는 RNN을 만들어 보겠습니다:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sFyNwxz89i5R"
      },
      "outputs": [],
      "source": [
        "# 재현성을 위해 랜덤 시드 설정\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "# 입력과 타겟을 분리하는 함수 정의\n",
        "def split_inputs_and_targets(mulvar_series, ahead=14, target_col=1):\n",
        "    # 입력: 시계열 데이터의 처음부터 ahead 만큼 이전까지\n",
        "    # 타겟: 마지막 ahead 개의 데이터 중 target_col 열\n",
        "    return mulvar_series[:, :-ahead], mulvar_series[:, -ahead:, target_col]\n",
        "\n",
        "# 훈련 데이터셋 생성\n",
        "ahead_train_ds = tf.keras.utils.timeseries_dataset_from_array(\n",
        "    mulvar_train.to_numpy(np.float32),  # 다변량 훈련 데이터를 numpy 배열로 변환\n",
        "    targets=None,  # 타겟을 별도로 지정하지 않음 (split_inputs_and_targets에서 처리)\n",
        "    sequence_length=seq_length + 14,  # 입력 시퀀스 길이 + 예측할 미래 시점 수\n",
        "    batch_size=32,  # 배치 크기\n",
        "    shuffle=True,  # 데이터 섞기\n",
        "    seed=42  # 재현성을 위한 랜덤 시드\n",
        ").map(split_inputs_and_targets)  # 입력과 타겟 분리 함수 적용\n",
        "\n",
        "# 검증 데이터셋 생성 (훈련 데이터셋과 유사하지만 셔플하지 않음)\n",
        "ahead_valid_ds = tf.keras.utils.timeseries_dataset_from_array(\n",
        "    mulvar_valid.to_numpy(np.float32),\n",
        "    targets=None,\n",
        "    sequence_length=seq_length + 14,\n",
        "    batch_size=32\n",
        ").map(split_inputs_and_targets)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f3-ifzkF9i5R"
      },
      "outputs": [],
      "source": [
        "tf.random.set_seed(42)\n",
        "\n",
        "ahead_model = tf.keras.Sequential([\n",
        "    tf.keras.layers.SimpleRNN(32, input_shape=[None, 5]),\n",
        "    tf.keras.layers.Dense(14)\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XgSudVKo9i5S",
        "outputId": "ed760f3e-c1d8-462a-fba1-5c4863140ba5"
      },
      "outputs": [],
      "source": [
        "# 추가 코드 - 이전과 같이 모델을 컴파일, 훈련 및 평가합니다.\n",
        "fit_and_evaluate(ahead_model, ahead_train_ds, ahead_valid_ds,\n",
        "                 learning_rate=0.02)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tO8rDQTF9i5S",
        "outputId": "2f34fefb-b870-4962-e4f9-8e4f6c880983"
      },
      "outputs": [],
      "source": [
        "X = mulvar_valid.to_numpy(np.float32)[np.newaxis, :seq_length]  # 크기 [1, 56, 5]\n",
        "Y_pred = ahead_model.predict(X)  # 크기 [1, 14]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 시퀀스-투-시퀀스 모델로 예측하기"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2JQ5QiEW9i5S"
      },
      "source": [
        "이제 각 타임 스텝에서 다음 14스텝을 예측하는 RNN을 만들어 보겠습니다. 즉, 타임 스텝 0\\~55를 기준으로 타임 스텝 56\\~69를 예측하는 대신 타임 스텝 0에서 타임 스텝 1\\~14를 예측하고, 타임 스텝 1에서 타임 스텝 2\\~15를 예측하는 식으로 예측하고, 마지막 타임 스텝에서 타임 스텝 56\\~69를 예측합니다. 이 모델은 코잘(causal) 모델이므로 어떤 타임 스텝에서든 예측을 할 때는 과거의 타임 스텝만 볼 수 있습니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1yynMbfu9i5S"
      },
      "source": [
        "데이터셋을 준비하기 위해 `to_windows()`를 두 번 사용하여 다음과 같이 연속된 윈도의 시퀀스를 가져올 수 있습니다:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N8QKPGAq9i5S",
        "outputId": "d00134b4-1e22-48ff-e74c-4a85d3e4e2f1"
      },
      "outputs": [],
      "source": [
        "# 0부터 6까지의 숫자로 구성된 데이터셋 생성\n",
        "my_series = tf.data.Dataset.range(7)\n",
        "\n",
        "# 첫 번째 윈도우 생성: 크기 3의 윈도우로 데이터 분할\n",
        "# 두 번째 윈도우 생성: 첫 번째 윈도우의 결과를 크기 4의 윈도우로 다시 분할\n",
        "dataset = to_windows(to_windows(my_series, 3), 4)\n",
        "\n",
        "# 결과 데이터셋을 리스트로 변환하여 출력\n",
        "# 이는 4개의 연속된 3-요소 윈도우를 포함하는 시퀀스를 생성합니다\n",
        "list(dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yJ9R3MEt9i5T"
      },
      "source": [
        "그런 다음 원소를 원하는 입력과 타깃으로 분할할 수 있습니다:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e58kNsTU9i5T",
        "outputId": "f409321d-7a39-4fbc-faca-d97a2a50f4de"
      },
      "outputs": [],
      "source": [
        "# 데이터셋의 각 요소를 입력과 타깃으로 분할합니다.\n",
        "# S[:, 0]은 입력 시퀀스의 첫 번째 요소를 의미하고,\n",
        "# S[:, 1:]은 입력 시퀀스의 나머지 요소들을 의미합니다.\n",
        "dataset = dataset.map(lambda S: (S[:, 0], S[:, 1:]))\n",
        "# 데이터셋을 리스트로 변환하여 출력합니다.\n",
        "list(dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O64sr4xl9i5T"
      },
      "source": [
        "이 아이디어를 유틸리티 함수로 만들어 봅시다. 이 함수는 셔플(선택 사항)과 배치 처리도 담당합니다:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g2i9e7169i5T"
      },
      "outputs": [],
      "source": [
        "def to_seq2seq_dataset(series, seq_length=56, ahead=14, target_col=1,\n",
        "                       batch_size=32, shuffle=False, seed=None):\n",
        "    # 시리즈 데이터를 float32 형식의 텐서로 변환하고, 윈도우 크기를 ahead + 1로 설정하여 윈도우를 만듭니다.\n",
        "    ds = to_windows(tf.data.Dataset.from_tensor_slices(\n",
        "        series.to_numpy(np.float32)), ahead + 1)\n",
        "    \n",
        "    # 생성된 윈도우를 다시 seq_length 크기로 윈도우를 만들고, 각 윈도우의 첫 번째 요소를 입력으로,\n",
        "    # 나머지 요소들을 타깃으로 설정합니다. 여기서 S[:, 0]은 입력 시퀀스의 첫 번째 요소를 의미하고,\n",
        "    # S[:, 1:, 1]은 타깃 시퀀스를 의미합니다.\n",
        "    ds = to_windows(ds, seq_length).map(lambda S: (S[:, 0], S[:, 1:, 1]))\n",
        "    \n",
        "    # 셔플이 True로 설정된 경우, 데이터셋을 섞습니다. 이때 배치 크기의 8배만큼의 버퍼 크기를 사용합니다.\n",
        "    if shuffle:\n",
        "        ds = ds.shuffle(8 * batch_size, seed=seed)\n",
        "    \n",
        "    # 데이터셋을 배치 크기로 나누어 반환합니다.\n",
        "    return ds.batch(batch_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t7UZYySX9i5T"
      },
      "outputs": [],
      "source": [
        "seq2seq_train = to_seq2seq_dataset(mulvar_train, shuffle=True, seed=42)\n",
        "seq2seq_valid = to_seq2seq_dataset(mulvar_valid)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HDgasbLn9i5U"
      },
      "outputs": [],
      "source": [
        "tf.random.set_seed(42)  # 추가 코드 - 재현성 보장을 위해 난수 시드 설정\n",
        "\n",
        "# 시퀀스-투-시퀀스 모델을 정의합니다.\n",
        "seq2seq_model = tf.keras.Sequential([\n",
        "    # SimpleRNN 층을 추가합니다. 이 층은 32개의 유닛을 가지고 있으며, \n",
        "    # return_sequences=True로 설정하여 모든 타임 스텝의 출력을 반환합니다.\n",
        "    # input_shape=[None, 5]는 입력 시퀀스의 길이가 가변적이고, 각 타임 스텝에 5개의 특성이 있음을 의미합니다.\n",
        "    tf.keras.layers.SimpleRNN(32, return_sequences=True, input_shape=[None, 5]),\n",
        "    \n",
        "    # Dense 층을 추가합니다. 이 층은 14개의 출력을 생성합니다.\n",
        "    # tf.keras.layers.TimeDistributed(tf.keras.layers.Dense(14))나\n",
        "    # tf.keras.layers.Conv1D(14, kernel_size=1) 와 동일한 역할을 합니다.\n",
        "    tf.keras.layers.Dense(14)\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "84URicBY9i5U",
        "outputId": "2b61a2e0-a370-4d81-a7dd-e7cd4446f53e"
      },
      "outputs": [],
      "source": [
        "fit_and_evaluate(seq2seq_model, seq2seq_train, seq2seq_valid,\n",
        "                 learning_rate=0.1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ExtQNKrU9i5U",
        "outputId": "48c5a8bc-76ab-4219-e1f6-5544b8a50156"
      },
      "outputs": [],
      "source": [
        "X = mulvar_valid.to_numpy(np.float32)[np.newaxis, :seq_length]\n",
        "y_pred_14 = seq2seq_model.predict(X)[0, -1]  # 마지막 타임 스텝의 출력만"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NhIojJ2I9i5U",
        "outputId": "df365f72-df97-4610-91a6-76dfcaa03cf8"
      },
      "outputs": [],
      "source": [
        "Y_pred_valid = seq2seq_model.predict(seq2seq_valid)\n",
        "for ahead in range(14):\n",
        "    preds = pd.Series(Y_pred_valid[:-1, -1, ahead],\n",
        "                      index=mulvar_valid.index[56 + ahead : -14 + ahead])\n",
        "    mae = (preds - mulvar_valid[\"rail\"]).abs().mean() * 1e6\n",
        "    print(f\"MAE for +{ahead + 1}: {mae:,.0f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 긴 시퀀스 다루기"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "201p6fy59i5U"
      },
      "source": [
        "## 층 정규화를 사용한 심층 RNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Epx-G5Fs9i5V"
      },
      "outputs": [],
      "source": [
        "class LNSimpleRNNCell(tf.keras.layers.Layer):\n",
        "    def __init__(self, units, activation=\"tanh\", **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.state_size = units  # 상태 크기를 설정합니다.\n",
        "        self.output_size = units  # 출력 크기를 설정합니다.\n",
        "        # SimpleRNNCell을 생성합니다. 활성화 함수는 나중에 적용하기 위해 None으로 설정합니다.\n",
        "        self.simple_rnn_cell = tf.keras.layers.SimpleRNNCell(units, activation=None)\n",
        "        # 층 정규화를 위한 LayerNormalization 층을 생성합니다.\n",
        "        self.layer_norm = tf.keras.layers.LayerNormalization()\n",
        "        # 활성화 함수를 설정합니다.\n",
        "        self.activation = tf.keras.activations.get(activation)\n",
        "\n",
        "    def call(self, inputs, states):\n",
        "        # SimpleRNNCell을 호출하여 출력과 새로운 상태를 얻습니다.\n",
        "        outputs, new_states = self.simple_rnn_cell(inputs, states)\n",
        "        # 층 정규화를 적용하고 활성화 함수를 적용합니다.\n",
        "        norm_outputs = self.activation(self.layer_norm(outputs))\n",
        "        # 정규화된 출력을 반환합니다. 상태도 함께 반환합니다.\n",
        "        return norm_outputs, [norm_outputs]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "56bYpEfz9i5V"
      },
      "outputs": [],
      "source": [
        "tf.random.set_seed(42)  # 추가 코드 - 재현성 보장\n",
        "custom_ln_model = tf.keras.Sequential([\n",
        "    tf.keras.layers.RNN(LNSimpleRNNCell(32), return_sequences=True,\n",
        "                        input_shape=[None, 5]),\n",
        "    tf.keras.layers.Dense(14)\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xbJu88qL9i5V"
      },
      "source": [
        "5에포크 동안만 훈련하면 효과가 있다는 것을 알 수 있습니다(원하는 경우 더 늘릴 수 있습니다):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L98VdSmn9i5V",
        "outputId": "273d5de9-c4e3-4a39-ea97-74b81eeae6f8"
      },
      "outputs": [],
      "source": [
        "fit_and_evaluate(custom_ln_model, seq2seq_train, seq2seq_valid,\n",
        "                 learning_rate=0.1, epochs=5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0P9Bfym99i5W"
      },
      "source": [
        "# 추가 자료 - 사용자 정의 RNN 클래스 만들기"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dtzcsbrF9i5W"
      },
      "source": [
        "RNN 클래스는 마법이 아닙니다. 사실, 자신만의 RNN 클래스를 구현하는 것은 그리 어렵지 않습니다:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZgZ20BwA9i5W"
      },
      "outputs": [],
      "source": [
        "class MyRNN(tf.keras.layers.Layer):\n",
        "    def __init__(self, cell, return_sequences=False, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.cell = cell  # RNN 셀 저장\n",
        "        self.return_sequences = return_sequences  # 시퀀스 반환 여부 저장\n",
        "\n",
        "    def get_initial_state(self, inputs):\n",
        "        try:\n",
        "            # 셀에 초기 상태 메서드가 있으면 그것을 사용\n",
        "            return self.cell.get_initial_state(inputs)\n",
        "        except AttributeError:\n",
        "            # 셀에 초기 상태 메서드가 없으면 0으로 초기화된 상태 생성\n",
        "            batch_size = tf.shape(inputs)[0]\n",
        "            return [tf.zeros([batch_size, self.cell.state_size],\n",
        "                             dtype=inputs.dtype)]\n",
        "\n",
        "    @tf.function\n",
        "    def call(self, inputs):\n",
        "        states = self.get_initial_state(inputs)  # 초기 상태 가져오기\n",
        "        shape = tf.shape(inputs)\n",
        "        batch_size = shape[0]  # 배치 크기\n",
        "        n_steps = shape[1]  # 시퀀스 길이\n",
        "        \n",
        "        # 시퀀스를 저장할 TensorArray 생성\n",
        "        sequences = tf.TensorArray(\n",
        "            inputs.dtype, size=(n_steps if self.return_sequences else 0))\n",
        "        \n",
        "        # 출력 텐서 초기화 (tf.function 요구사항)\n",
        "        outputs = tf.zeros(shape=[batch_size, self.cell.output_size],\n",
        "                           dtype=inputs.dtype)\n",
        "        \n",
        "        # 각 타임스텝에 대해 반복\n",
        "        for step in tf.range(n_steps):\n",
        "            # 현재 타임스텝의 입력으로 셀 실행\n",
        "            outputs, states = self.cell(inputs[:, step], states)\n",
        "            if self.return_sequences:\n",
        "                # 모든 타임스텝의 출력을 저장\n",
        "                sequences = sequences.write(step, outputs)\n",
        "\n",
        "        if self.return_sequences:\n",
        "            # 모든 타임스텝의 출력을 반환\n",
        "            return tf.transpose(sequences.stack(), [1, 0, 2])\n",
        "        else:\n",
        "            # 마지막 타임스텝의 출력만 반환\n",
        "            return outputs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0n93GnG69i5Y"
      },
      "source": [
        "`@tf.function`은 `for` 루프 전에 `output` 변수를 생성해야 하므로, 해당 값을 전혀 사용하지 않더라도 그 값을 0 텐서로 초기화합니다. 함수가 그래프로 변환되면 이 사용되지 않은 값은 그래프에서 잘려나가므로 성능에 영향을 미치지 않습니다. 마찬가지로 `@tf.function`은 `self.return_sequences`가 `False`인 경우에도 `sequences` 변수가 사용되는 `if` 문 앞에 생성되어야 하므로, 이 경우에는 0 크기의 `TensorArray`를 생성합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PqZPWbNK9i5Z"
      },
      "outputs": [],
      "source": [
        "tf.random.set_seed(42)\n",
        "\n",
        "custom_model = tf.keras.Sequential([\n",
        "    MyRNN(LNSimpleRNNCell(32), return_sequences=True, input_shape=[None, 5]),\n",
        "    tf.keras.layers.Dense(14)\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V6-M7w_Q9i5Z"
      },
      "source": [
        "5에포크 동안만 훈련하면 효과가 있다는 것을 알 수 있습니다(원하는 경우 더 늘릴 수 있습니다):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "errz27w39i5Z",
        "outputId": "cbd7cf3d-fbc5-4ee2-9f89-6c5dd9cde5f7"
      },
      "outputs": [],
      "source": [
        "fit_and_evaluate(custom_model, seq2seq_train, seq2seq_valid,\n",
        "                 learning_rate=0.1, epochs=5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iEWE3qH69i5Z"
      },
      "source": [
        "# LSTM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DOZJDQpN9i5c",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# 랜덤 시드 설정으로 실험 결과의 재현성 보장\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "# LSTM 모델 정의\n",
        "lstm_model = tf.keras.models.Sequential([\n",
        "    # LSTM 층 추가\n",
        "    # - 32개의 유닛(뉴런)\n",
        "    # - return_sequences=True: 모든 타임스텝의 출력을 반환\n",
        "    # - input_shape=[None, 5]: 가변 길이의 시퀀스, 각 타임스텝마다 5개의 특성\n",
        "    tf.keras.layers.LSTM(32, return_sequences=True, input_shape=[None, 5]),\n",
        "    \n",
        "    # 완전연결층 추가\n",
        "    # - 14개의 유닛(뉴런): 출력 차원\n",
        "    tf.keras.layers.Dense(14)\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kvqGJ-719i5c"
      },
      "source": [
        "5에포크 동안만 훈련하면 효과가 있다는 것을 알 수 있습니다(원하는 경우 더 늘릴 수 있습니다):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CcsRQM8s9i5c",
        "outputId": "0de3fcbf-e3d2-4005-f5b8-b29088f0b978"
      },
      "outputs": [],
      "source": [
        "fit_and_evaluate(lstm_model, seq2seq_train, seq2seq_valid,\n",
        "                 learning_rate=0.1, epochs=5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "__8j-sp-9i5c"
      },
      "source": [
        "# GRU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JnKZ-acx9i5d",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# 랜덤 시드 설정으로 실험 결과의 재현성 보장\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "# GRU 모델 정의\n",
        "gru_model = tf.keras.Sequential([\n",
        "    # GRU 층 추가\n",
        "    # - 32개의 유닛(뉴런)\n",
        "    # - return_sequences=True: 모든 타임스텝의 출력을 반환\n",
        "    # - input_shape=[None, 5]: 가변 길이의 시퀀스, 각 타임스텝마다 5개의 특성\n",
        "    tf.keras.layers.GRU(32, return_sequences=True, input_shape=[None, 5]),\n",
        "    \n",
        "    # 완전연결층 추가\n",
        "    # - 14개의 유닛(뉴런): 출력 차원\n",
        "    tf.keras.layers.Dense(14)\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zysqjKfW9i5d"
      },
      "source": [
        "5에포크 동안만 훈련하면 효과가 있다는 것을 알 수 있습니다(원하는 경우 더 늘릴 수 있습니다):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Q3I8ib89i5d",
        "outputId": "f11f2e51-30bc-418a-c15e-c0450812ed69"
      },
      "outputs": [],
      "source": [
        "fit_and_evaluate(gru_model, seq2seq_train, seq2seq_valid,\n",
        "                 learning_rate=0.1, epochs=5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "usV26kdX9i5d"
      },
      "source": [
        "## 1D 합성곱 층으로 시퀀스 다루기"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lR1pA0BP9i5d"
      },
      "source": [
        "```\n",
        "  |-----0-----|      |-----3----|      |--... |-------52------|\n",
        "         |-----1----|      |-----4----|   ... |       |-------53------|\n",
        "               |-----2----|     |------5--...-51------|       |-------54------|\n",
        "X:  0  1  2  3  4  5  6  7  8  9 10 11 12 ...  104 105 106 107 108 109 110 111\n",
        "Y:      from 4     6     8    10    12    ...      106     108     110     112\n",
        "         to 17    19    21    23    25    ...      119     121     123     125\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oNVcZ37z9i5e"
      },
      "outputs": [],
      "source": [
        "# 랜덤 시드 설정으로 실험 결과의 재현성 보장\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "# Conv1D와 GRU를 결합한 모델 정의\n",
        "conv_rnn_model = tf.keras.Sequential([\n",
        "    # 1D 합성곱 층 추가\n",
        "    # - filters=32: 32개의 필터 사용\n",
        "    # - kernel_size=4: 4개의 연속된 입력을 한 번에 처리\n",
        "    # - strides=2: 2칸씩 이동하며 합성곱 수행\n",
        "    # - activation=\"relu\": ReLU 활성화 함수 사용\n",
        "    # - input_shape=[None, 5]: 가변 길이의 시퀀스, 각 타임스텝마다 5개의 특성\n",
        "    tf.keras.layers.Conv1D(filters=32, kernel_size=4, strides=2,\n",
        "                           activation=\"relu\", input_shape=[None, 5]),\n",
        "    \n",
        "    # GRU 층 추가\n",
        "    # - 32개의 유닛(뉴런)\n",
        "    # - return_sequences=True: 모든 타임스텝의 출력을 반환\n",
        "    tf.keras.layers.GRU(32, return_sequences=True),\n",
        "    \n",
        "    # 완전연결층 추가\n",
        "    # - 14개의 유닛(뉴런): 출력 차원\n",
        "    tf.keras.layers.Dense(14)\n",
        "])\n",
        "\n",
        "# 더 긴 시퀀스로 훈련 데이터셋 생성\n",
        "longer_train = to_seq2seq_dataset(mulvar_train, seq_length=112,\n",
        "                                       shuffle=True, seed=42)\n",
        "# 더 긴 시퀀스로 검증 데이터셋 생성\n",
        "longer_valid = to_seq2seq_dataset(mulvar_valid, seq_length=112)\n",
        "\n",
        "# 훈련 데이터 다운샘플링: 3번째 타임스텝부터 2칸씩 건너뛰며 선택\n",
        "downsampled_train = longer_train.map(lambda X, Y: (X, Y[:, 3::2]))\n",
        "# 검증 데이터 다운샘플링: 3번째 타임스텝부터 2칸씩 건너뛰며 선택\n",
        "downsampled_valid = longer_valid.map(lambda X, Y: (X, Y[:, 3::2]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lMrdj-LV9i5e"
      },
      "source": [
        "5에포크 동안만 훈련하면 효과가 있다는 것을 알 수 있습니다(원하는 경우 더 늘릴 수 있습니다):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r9IMTzrn9i5e",
        "outputId": "35a5fd4d-23e7-4c7d-deae-22d150fbb496"
      },
      "outputs": [],
      "source": [
        "fit_and_evaluate(conv_rnn_model, downsampled_train, downsampled_valid,\n",
        "                 learning_rate=0.1, epochs=5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qaEYQYnK9i5e"
      },
      "source": [
        "## WaveNet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "njxwZk4G9i5f"
      },
      "source": [
        "```\n",
        " ⋮\n",
        "C2  /\\ /\\ /\\ /\\ /\\ /\\ /\\ /\\ /\\ /\\ /\\ /\\ /\\...\n",
        "   \\  /  \\  /  \\  /  \\  /  \\  /  \\  /  \\     \n",
        "     /    \\      /    \\      /    \\          \n",
        "C1  /\\ /\\ /\\ /\\ /\\ /\\ /\\ /\\ /\\ /\\ /\\  /\\ /...\\\n",
        "X: 0  1  2  3  4  5  6  7  8  9  10 11 12 ... 111\n",
        "Y: 1  2  3  4  5  6  7  8  9  10 11 12 13 ... 112\n",
        " /14 15 16 17 18 19 20 21 22  23 24 25 26 ... 125\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z8XluLz-9i5f"
      },
      "outputs": [],
      "source": [
        "# 재현성을 위해 랜덤 시드 설정\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "# WaveNet 모델 생성\n",
        "wavenet_model = tf.keras.Sequential()\n",
        "\n",
        "# 입력 레이어 추가: 가변 길이의 시퀀스와 5개의 특성을 가진 입력을 받음\n",
        "wavenet_model.add(tf.keras.layers.InputLayer(input_shape=[None, 5]))\n",
        "\n",
        "# 확장된 합성곱 레이어 추가\n",
        "for rate in (1, 2, 4, 8) * 2:  # 확장 비율을 1, 2, 4, 8로 두 번 반복\n",
        "    wavenet_model.add(tf.keras.layers.Conv1D(\n",
        "        filters=32,  # 32개의 필터 사용\n",
        "        kernel_size=2,  # 커널 크기 2\n",
        "        padding=\"causal\",  # 인과적 패딩 사용 (미래 정보 사용 방지)\n",
        "        activation=\"relu\",  # ReLU 활성화 함수 사용\n",
        "        dilation_rate=rate  # 현재 확장 비율 적용\n",
        "    ))\n",
        "\n",
        "# 출력 레이어 추가: 14개의 특성을 가진 출력을 생성\n",
        "wavenet_model.add(tf.keras.layers.Conv1D(filters=14, kernel_size=1))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TVXr9vnR9i5f"
      },
      "source": [
        "5에포크 동안만 훈련하면 효과가 있다는 것을 알 수 있습니다(원하는 경우 더 늘릴 수 있습니다):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wwYlVMHE9i5f",
        "outputId": "8847b72b-87cc-4708-9714-d285a2b18c62"
      },
      "outputs": [],
      "source": [
        "fit_and_evaluate(wavenet_model, longer_train, longer_valid,\n",
        "                 learning_rate=0.1, epochs=5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ZEjGrRa9i5f"
      },
      "source": [
        "# 추가 자료 – Wavenet 구현"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "conbBcCe9i5f"
      },
      "source": [
        "다음은 논문에 정의된 WaveNet 구현입니다. ReLU와 파라미터를 가진 스킵 연결 대신 게이트 활성화 유닛(Gated Activation Unit)을 사용하며, 시퀀스가 점점 짧아지는 것을 방지하기 위해 왼쪽에 0으로 패딩합니다:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nyuDXZNi9i5g"
      },
      "outputs": [],
      "source": [
        "class GatedActivationUnit(tf.keras.layers.Layer):\n",
        "    def __init__(self, activation=\"tanh\", **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        # 활성화 함수를 설정합니다. 기본값은 tanh입니다.\n",
        "        self.activation = tf.keras.activations.get(activation)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        # 입력의 절반을 필터로 사용합니다.\n",
        "        n_filters = inputs.shape[-1] // 2\n",
        "        \n",
        "        # 선형 출력: 입력의 앞쪽 절반에 활성화 함수를 적용합니다.\n",
        "        linear_output = self.activation(inputs[..., :n_filters])\n",
        "        \n",
        "        # 게이트: 입력의 뒤쪽 절반에 시그모이드 함수를 적용합니다.\n",
        "        gate = tf.keras.activations.sigmoid(inputs[..., n_filters:])\n",
        "        \n",
        "        # 선형 출력과 게이트를 곱하여 최종 출력을 생성합니다.\n",
        "        return self.activation(linear_output) * gate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CLO-fC_N9i5g"
      },
      "outputs": [],
      "source": [
        "def wavenet_residual_block(inputs, n_filters, dilation_rate):\n",
        "    # 1. 확장된 인과적 합성곱 적용\n",
        "    # 입력에 대해 2배의 필터를 사용하여 확장된 인과적 합성곱을 수행합니다.\n",
        "    z = tf.keras.layers.Conv1D(2 * n_filters, kernel_size=2, padding=\"causal\",\n",
        "                            dilation_rate=dilation_rate)(inputs)\n",
        "    \n",
        "    # 2. 게이트 활성화 유닛 적용\n",
        "    # 게이트 활성화 유닛을 통해 비선형성을 추가합니다.\n",
        "    z = GatedActivationUnit()(z)\n",
        "    \n",
        "    # 3. 1x1 합성곱 적용\n",
        "    # 채널 수를 원래의 n_filters로 줄이기 위해 1x1 합성곱을 적용합니다.\n",
        "    z = tf.keras.layers.Conv1D(n_filters, kernel_size=1)(z)\n",
        "    \n",
        "    # 4. 잔차 연결 및 스킵 연결 반환\n",
        "    # 입력과 처리된 출력을 더하여 잔차 연결을 만들고,\n",
        "    # 원본 출력 z를 스킵 연결로 반환합니다.\n",
        "    return tf.keras.layers.Add()([z, inputs]), z"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0MzwlLu19i5g"
      },
      "outputs": [],
      "source": [
        "# 랜덤 시드 설정\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "# WaveNet 모델 구성을 위한 하이퍼파라미터 설정\n",
        "n_layers_per_block = 3  # 각 블록당 레이어 수 (논문에서는 10)\n",
        "n_blocks = 1  # 블록 수 (논문에서는 3)\n",
        "n_filters = 32  # 필터 수 (논문에서는 128)\n",
        "n_outputs = 14  # 출력 차원 (논문에서는 256)\n",
        "\n",
        "# 입력 레이어 정의\n",
        "inputs = tf.keras.layers.Input(shape=[None, 5])\n",
        "\n",
        "# 초기 합성곱 레이어\n",
        "z = tf.keras.layers.Conv1D(n_filters, kernel_size=2, padding=\"causal\")(inputs)\n",
        "\n",
        "# 스킵 연결을 저장할 리스트\n",
        "skip_to_last = []\n",
        "\n",
        "# WaveNet 잔차 블록 구성\n",
        "for dilation_rate in [2**i for i in range(n_layers_per_block)] * n_blocks:\n",
        "    z, skip = wavenet_residual_block(z, n_filters, dilation_rate)\n",
        "    skip_to_last.append(skip)\n",
        "\n",
        "# 스킵 연결 합산 및 ReLU 활성화\n",
        "z = tf.keras.activations.relu(tf.keras.layers.Add()(skip_to_last))\n",
        "\n",
        "# 1x1 합성곱 레이어 추가\n",
        "z = tf.keras.layers.Conv1D(n_filters, kernel_size=1, activation=\"relu\")(z)\n",
        "\n",
        "# 최종 출력 레이어\n",
        "Y_preds = tf.keras.layers.Conv1D(n_outputs, kernel_size=1)(z)\n",
        "\n",
        "# 전체 WaveNet 모델 정의\n",
        "full_wavenet_model = tf.keras.Model(inputs=[inputs], outputs=[Y_preds])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "scJkNRZ_9i5g"
      },
      "source": [
        "5에포크 동안만 훈련하면 효과가 있다는 것을 알 수 있습니다(원하는 경우 더 늘릴 수 있습니다):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EEYcEAQ39i5g",
        "outputId": "2709ee16-d63f-4e89-d508-424bb40663b7"
      },
      "outputs": [],
      "source": [
        "fit_and_evaluate(full_wavenet_model, longer_train, longer_valid,\n",
        "                 learning_rate=0.1, epochs=5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DNCJjPlg9i5g"
      },
      "source": [
        "이 장에서는 RNN의 기초를 살펴보고 이를 사용하여 시퀀스(즉, 시계열)를 처리하는 방법을 살펴보았습니다. 이 과정에서 CNN을 포함한 시퀀스를 처리하는 다른 방법도 살펴봤습니다. 다음 장에서는 자연어 처리를 위해 RNN을 사용하고, RNN에 대해 자세히 알아볼 것입니다(양방향 RNN, 상태가 있는 RNN과 상태가 없는 RNN, 인코더-디코더, 어텐션 기반 인코더-디코더). 또한 어텐션 전용 아키텍처인 트랜스포머에 대해서도 살펴볼 것입니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZhrX9H2X9i5h"
      },
      "source": [
        "# 연습문제"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fXrJjUrc9i5h"
      },
      "source": [
        "## 1. to 8."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AWy-rYVt9i5h"
      },
      "source": [
        "부록 A 참조."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qcelEXat9i5h"
      },
      "source": [
        "## 9. SketchRNN 데이터셋 다루기"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sch1fD4S9i5h"
      },
      "source": [
        "_연습문제: 텐서플로 데이터셋에서 제공하는 SketchRNN 데이터셋으로 분류 모델을 훈련해보세요._"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KuQeeNbO9i5j"
      },
      "source": [
        "이 데이터셋은 아직 TFDS에서 제공하지 않습니다. 아직 [풀 리퀘스트](https://github.com/tensorflow/datasets/pull/361)가 진행 중입니다. 다행히 이 데이터는 TFRecord로 제공되므로 다운로드해보죠(3,450,000 훈련 스케치와 345,000 테스트 스케치가 포함된 이 데이터셋은 1GB 정도되기 때문에 다운로드 시간이 조금 걸립니다):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k64P6YUW9i5k",
        "outputId": "0a1f14e4-d08a-4468-ffc7-e97706003c15"
      },
      "outputs": [],
      "source": [
        "tf_download_root = \"http://download.tensorflow.org/data/\"\n",
        "filename = \"quickdraw_tutorial_dataset_v1.tar.gz\"\n",
        "filepath = tf.keras.utils.get_file(filename,\n",
        "                                   tf_download_root + filename,\n",
        "                                   cache_dir=\".\",\n",
        "                                   extract=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0gPOyTLl9i5k"
      },
      "outputs": [],
      "source": [
        "quickdraw_dir = Path(filepath).parent\n",
        "train_files = sorted(\n",
        "    [str(path) for path in quickdraw_dir.glob(\"training.tfrecord-*\")]\n",
        ")\n",
        "eval_files = sorted(\n",
        "    [str(path) for path in quickdraw_dir.glob(\"eval.tfrecord-*\")]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xe6HDrC59i5k",
        "outputId": "cb7d0ee0-992d-4957-91cb-4843a3f12ee3"
      },
      "outputs": [],
      "source": [
        "train_files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xNJurGkq9i5k",
        "outputId": "1519fd41-0ed2-493f-fdf8-e5f20352c9db"
      },
      "outputs": [],
      "source": [
        "eval_files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X45qSSG39i5l"
      },
      "outputs": [],
      "source": [
        "with open(quickdraw_dir / \"eval.tfrecord.classes\") as test_classes_file:\n",
        "    test_classes = test_classes_file.readlines()\n",
        "\n",
        "with open(quickdraw_dir / \"training.tfrecord.classes\") as train_classes_file:\n",
        "    train_classes = train_classes_file.readlines()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zV3hDSEh9i5l"
      },
      "outputs": [],
      "source": [
        "assert train_classes == test_classes\n",
        "class_names = [name.strip().lower() for name in train_classes]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QXI-Ydo99i5l",
        "outputId": "d44b2430-a540-493b-ae52-e0e5d4b1909a"
      },
      "outputs": [],
      "source": [
        "sorted(class_names)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RNHGe-PL9i5l"
      },
      "outputs": [],
      "source": [
        "def parse(data_batch):\n",
        "    feature_descriptions = {\n",
        "        \"ink\": tf.io.VarLenFeature(dtype=tf.float32),\n",
        "        \"shape\": tf.io.FixedLenFeature([2], dtype=tf.int64),\n",
        "        \"class_index\": tf.io.FixedLenFeature([1], dtype=tf.int64)\n",
        "    }\n",
        "    examples = tf.io.parse_example(data_batch, feature_descriptions)\n",
        "    flat_sketches = tf.sparse.to_dense(examples[\"ink\"])\n",
        "    sketches = tf.reshape(flat_sketches, shape=[tf.size(data_batch), -1, 3])\n",
        "    lengths = examples[\"shape\"][:, 0]\n",
        "    labels = examples[\"class_index\"][:, 0]\n",
        "    return sketches, lengths, labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pImOdY959i5l"
      },
      "outputs": [],
      "source": [
        "def quickdraw_dataset(filepaths, batch_size=32, shuffle_buffer_size=None,\n",
        "                      n_parse_threads=5, n_read_threads=5, cache=False):\n",
        "    dataset = tf.data.TFRecordDataset(filepaths,\n",
        "                                      num_parallel_reads=n_read_threads)\n",
        "    if cache:\n",
        "        dataset = dataset.cache()\n",
        "    if shuffle_buffer_size:\n",
        "        dataset = dataset.shuffle(shuffle_buffer_size)\n",
        "    dataset = dataset.batch(batch_size)\n",
        "    dataset = dataset.map(parse, num_parallel_calls=n_parse_threads)\n",
        "    return dataset.prefetch(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MD9miC7d9i5l"
      },
      "outputs": [],
      "source": [
        "train_set = quickdraw_dataset(train_files, shuffle_buffer_size=10000)\n",
        "valid_set = quickdraw_dataset(eval_files[:5])\n",
        "test_set = quickdraw_dataset(eval_files[5:])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s8E73Pkc9i5m",
        "outputId": "eedd3578-ef2f-4d6a-d244-31b1940c5618"
      },
      "outputs": [],
      "source": [
        "for sketches, lengths, labels in train_set.take(1):\n",
        "    print(\"sketches =\", sketches)\n",
        "    print(\"lengths =\", lengths)\n",
        "    print(\"labels =\", labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "9LSfQM4v9i5m",
        "outputId": "6c266045-5430-41df-d376-4f6cc2c2fa57"
      },
      "outputs": [],
      "source": [
        "def draw_sketch(sketch, label=None):\n",
        "    origin = np.array([[0., 0., 0.]])\n",
        "    sketch = np.r_[origin, sketch]\n",
        "    stroke_end_indices = np.argwhere(sketch[:, -1]==1.)[:, 0]\n",
        "    coordinates = sketch[:, :2].cumsum(axis=0)\n",
        "    strokes = np.split(coordinates, stroke_end_indices + 1)\n",
        "    title = class_names[label.numpy()] if label is not None else \"Try to guess\"\n",
        "    plt.title(title)\n",
        "    plt.plot(coordinates[:, 0], -coordinates[:, 1], \"y:\")\n",
        "    for stroke in strokes:\n",
        "        plt.plot(stroke[:, 0], -stroke[:, 1], \".-\")\n",
        "    plt.axis(\"off\")\n",
        "\n",
        "def draw_sketches(sketches, lengths, labels):\n",
        "    n_sketches = len(sketches)\n",
        "    n_cols = 4\n",
        "    n_rows = (n_sketches - 1) // n_cols + 1\n",
        "    plt.figure(figsize=(n_cols * 3, n_rows * 3.5))\n",
        "    for index, sketch, length, label in zip(range(n_sketches), sketches, lengths, labels):\n",
        "        plt.subplot(n_rows, n_cols, index + 1)\n",
        "        draw_sketch(sketch[:length], label)\n",
        "    plt.show()\n",
        "\n",
        "for sketches, lengths, labels in train_set.take(1):\n",
        "    draw_sketches(sketches, lengths, labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zD29Lqxc9i5m"
      },
      "source": [
        "대부분의 스케치는 100개 포인트 이하로 구성되어 있습니다:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 460
        },
        "id": "jraKdgZ59i5w",
        "outputId": "fb1fb9cc-257d-45c6-940e-6d308015ead1"
      },
      "outputs": [],
      "source": [
        "lengths = np.concatenate([lengths for _, lengths, _ in train_set.take(1000)])\n",
        "plt.hist(lengths, bins=150, density=True)\n",
        "plt.axis([0, 200, 0, 0.03])\n",
        "plt.xlabel(\"길이\")\n",
        "plt.ylabel(\"밀도\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "noOaYgmW9i5w"
      },
      "outputs": [],
      "source": [
        "def crop_long_sketches(dataset, max_length=100):\n",
        "    return dataset.map(lambda inks, lengths, labels: (inks[:, :max_length], labels))\n",
        "\n",
        "cropped_train_set = crop_long_sketches(train_set)\n",
        "cropped_valid_set = crop_long_sketches(valid_set)\n",
        "cropped_test_set = crop_long_sketches(test_set)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8tn7GmcQ9i5x",
        "outputId": "c8c85d31-366a-4acf-eb79-06c0ba990c4f"
      },
      "outputs": [],
      "source": [
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Conv1D(32, kernel_size=5, strides=2, activation=\"relu\"),\n",
        "    tf.keras.layers.BatchNormalization(),\n",
        "    tf.keras.layers.Conv1D(64, kernel_size=5, strides=2, activation=\"relu\"),\n",
        "    tf.keras.layers.BatchNormalization(),\n",
        "    tf.keras.layers.Conv1D(128, kernel_size=3, strides=2, activation=\"relu\"),\n",
        "    tf.keras.layers.BatchNormalization(),\n",
        "    tf.keras.layers.LSTM(128, return_sequences=True),\n",
        "    tf.keras.layers.LSTM(128),\n",
        "    tf.keras.layers.Dense(len(class_names), activation=\"softmax\")\n",
        "])\n",
        "optimizer = tf.keras.optimizers.SGD(learning_rate=1e-2, clipnorm=1.)\n",
        "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
        "              optimizer=optimizer,\n",
        "              metrics=[\"accuracy\", \"sparse_top_k_categorical_accuracy\"])\n",
        "history = model.fit(cropped_train_set, epochs=2,\n",
        "                    validation_data=cropped_valid_set)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NdqxsYv89i5x",
        "outputId": "e7f50149-f992-4dfd-94de-3cbd7c82c23d"
      },
      "outputs": [],
      "source": [
        "y_test = np.concatenate([labels for _, _, labels in test_set])\n",
        "y_probas = model.predict(test_set)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WkpmgIzx9i5x",
        "outputId": "61bc7812-42a7-4a69-b0ca-872378634fee"
      },
      "outputs": [],
      "source": [
        "np.mean(tf.keras.metrics.sparse_top_k_categorical_accuracy(y_test, y_probas))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "LwUkKkCX9i5x",
        "outputId": "6c80072d-8fec-4a4b-bc02-fc3cb26ef5ca"
      },
      "outputs": [],
      "source": [
        "n_new = 10\n",
        "Y_probas = model.predict(sketches)\n",
        "top_k = tf.nn.top_k(Y_probas, k=5)\n",
        "for index in range(n_new):\n",
        "    plt.figure(figsize=(3, 3.5))\n",
        "    draw_sketch(sketches[index])\n",
        "    plt.show()\n",
        "    print(\"Top-5 predictions:\".format(index + 1))\n",
        "    for k in range(5):\n",
        "        class_name = class_names[top_k.indices[index, k]]\n",
        "        proba = 100 * top_k.values[index, k]\n",
        "        print(\"  {}. {} {:.3f}%\".format(k + 1, class_name, proba))\n",
        "    print(\"Answer: {}\".format(class_names[labels[index].numpy()]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RuG1-Fe09i5y"
      },
      "outputs": [],
      "source": [
        "model.save(\"my_sketchrnn\", save_format=\"tf\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G6arU0oL9i5y"
      },
      "source": [
        "## 10. 바흐 합창곡\n",
        "\n",
        "_연습문제: [바흐 합창곡](https://homl.info/bach) 데이터셋을 다운로드하여 압축을 풉니다. 이 데이터셋은 요한 제바스티안 바흐가 작곡한 382개의 합창곡으로 구성되어 있습니다. 각 곡은 100에서 640까지 타임 스텝 길이입니다. 각 타임 스텝은 4개의 정수를 담고 있습니다. 각 정수는 피아노 음표의 인덱스에 해당합니다(연주되는 음표가 없다는 것을 의미하는 0은 제외). 코랄의 타임 스텝 시퀀스가 주어지면 다음 타임 스텝(4개의 음표)을 예측할 수 있는 순환 모델, 합성곱 모델 또는 두 가지를 합친 모델을 훈련하세요. 그 다음 이 모델을 사용해 한 번에 하나의 음표씩 바흐와 같은 음악을 생성하세요. 코랄의 시작 부분을 모델에 주입하고 다음 타임 스텝을 예측합니다. 이 타임 스텝을 입력 시퀀스에 추가하여 모델이 다음 음표를 예측하게 만드는 식입니다. 또 바흐를 위한 [구글 두들](https://www.google.com/doodles/celebrating-johann-sebastian-bach)에 사용한 구글의 [Coconet 모델](https://homl.info/coconet)을 확인해보세요._"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "id": "qkor4S2m9i5y",
        "outputId": "31232c3b-9f4d-494e-9dd8-775f608a0cc7"
      },
      "outputs": [],
      "source": [
        "tf.keras.utils.get_file(\n",
        "    \"jsb_chorales.tgz\",\n",
        "    \"https://github.com/ageron/data/raw/main/jsb_chorales.tgz\",\n",
        "    cache_dir=\".\",\n",
        "    extract=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C_uRoydq9i5y"
      },
      "outputs": [],
      "source": [
        "jsb_chorales_dir = Path(\"datasets/jsb_chorales\")\n",
        "train_files = sorted(jsb_chorales_dir.glob(\"train/chorale_*.csv\"))\n",
        "valid_files = sorted(jsb_chorales_dir.glob(\"valid/chorale_*.csv\"))\n",
        "test_files = sorted(jsb_chorales_dir.glob(\"test/chorale_*.csv\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7e3Xjuye9i5y"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "def load_chorales(filepaths):\n",
        "    return [pd.read_csv(filepath).values.tolist() for filepath in filepaths]\n",
        "\n",
        "train_chorales = load_chorales(train_files)\n",
        "valid_chorales = load_chorales(valid_files)\n",
        "test_chorales = load_chorales(test_files)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iFc-vV7C9i5z",
        "outputId": "87c2b175-9fe3-4c29-d566-0ed59caac926"
      },
      "outputs": [],
      "source": [
        "train_chorales[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "obNXmgJ39i5z"
      },
      "source": [
        "음표의 범위는 36(C1 = 옥타브 1의 C)에서 81(A5 = 옥타브 5의 A)까지이고 무음을 위해 0을 추가합니다:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EpYPwl-r9i5z"
      },
      "outputs": [],
      "source": [
        "notes = set()\n",
        "for chorales in (train_chorales, valid_chorales, test_chorales):\n",
        "    for chorale in chorales:\n",
        "        for chord in chorale:\n",
        "            notes |= set(chord)\n",
        "\n",
        "n_notes = len(notes)\n",
        "min_note = min(notes - {0})\n",
        "max_note = max(notes)\n",
        "\n",
        "assert min_note == 36\n",
        "assert max_note == 81"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mL9Pbzff9i5z"
      },
      "source": [
        "이 코랄을 듣기 위한 몇 개의 함수를 만들어 보죠(자세한 내용을 이해할 필요는 없습니다. 사실 MIDI 플레이어처럼 더 간단한 방법이 있지만 그냥 하나의 합성기(synthesizer)를 만들어 보고 싶었습니다):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1KURemiW9i5z"
      },
      "outputs": [],
      "source": [
        "from IPython.display import Audio\n",
        "\n",
        "def notes_to_frequencies(notes):\n",
        "    # 한 옥타브 올라갈 때 주파수는 두배가 됩니다; 옥타브마다 12개의 반음이 있습니다;\n",
        "    # 옥타브 4의 A는 440Hz이고 음표 번호는 69입니다.\n",
        "    return 2 ** ((np.array(notes) - 69) / 12) * 440\n",
        "\n",
        "def frequencies_to_samples(frequencies, tempo, sample_rate):\n",
        "    note_duration = 60 / tempo # tempo는 분당 박자 수로 측정합니다\n",
        "    # 매 박자마다 딸깍거리는 소리를 줄이기 위해 주파수를 반올림하여 각 음의 끝에서 샘플을 0에 가깝게 만듭니다.\n",
        "    frequencies = (note_duration * frequencies).round() / note_duration\n",
        "    n_samples = int(note_duration * sample_rate)\n",
        "    time = np.linspace(0, note_duration, n_samples)\n",
        "    sine_waves = np.sin(2 * np.pi * frequencies.reshape(-1, 1) * time)\n",
        "    # (음표 0 = 무음을 포함해) 9Hz 이하인 주파수는 모두 삭제합니다\n",
        "    sine_waves *= (frequencies > 9.).reshape(-1, 1)\n",
        "    return sine_waves.reshape(-1)\n",
        "\n",
        "def chords_to_samples(chords, tempo, sample_rate):\n",
        "    freqs = notes_to_frequencies(chords)\n",
        "    freqs = np.r_[freqs, freqs[-1:]] # 마지막 음표를 조금 더 길게합니다\n",
        "    merged = np.mean([frequencies_to_samples(melody, tempo, sample_rate)\n",
        "                     for melody in freqs.T], axis=0)\n",
        "    n_fade_out_samples = sample_rate * 60 // tempo # 마지막 음을 희미하게 합니다\n",
        "    fade_out = np.linspace(1., 0., n_fade_out_samples)**2\n",
        "    merged[-n_fade_out_samples:] *= fade_out\n",
        "    return merged\n",
        "\n",
        "def play_chords(chords, tempo=160, amplitude=0.1, sample_rate=44100, filepath=None):\n",
        "    samples = amplitude * chords_to_samples(chords, tempo, sample_rate)\n",
        "    if filepath:\n",
        "        from scipy.io import wavfile\n",
        "        samples = (2**15 * samples).astype(np.int16)\n",
        "        wavfile.write(filepath, sample_rate, samples)\n",
        "        return display(Audio(filepath))\n",
        "    else:\n",
        "        return display(Audio(samples, rate=sample_rate))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iH4JWxt69i5z"
      },
      "source": [
        "이제 몇 개의 코랄을 들어 보죠:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 194
        },
        "id": "cJGpElrW9i50",
        "outputId": "b4641041-55aa-48a0-bd6d-4fd32657d06b"
      },
      "outputs": [],
      "source": [
        "for index in range(3):\n",
        "    play_chords(train_chorales[index])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rx4te05H9i50"
      },
      "source": [
        "멋지네요! :)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CXDgTcrS9i50"
      },
      "source": [
        "새로운 코랄을 생성하기 위해서는 이전의 화음이 주어졌을 때 다음 화음을 예측할 수 있는 모델을 훈련해야 합니다. 한 번에 4개의 음표를 예측하는 식으로 다음 화음을 예측한다면 잘 어울리지 않는 음표를 얻게 됩니다(믿으세요. 제가 해 보았습니다). 한 번에 하나의 음표를 예측하는 것이 간단하고 더 낫습니다. 따라서 모든 코랄을 전처리하여 각 화음을 아르페지오로 바꾸어야 합니다(즉, 동시에 연주되는 음표가 아니라 음표의 시퀀스). 그다음 이전의 모든 음표가 주어졌을 때 다음 음표를 예측하는 모델을 훈련할 수 있습니다. 시퀀스-투-시퀀스 방식을 사용하겠습니다. 신경망에 한 윈도를 주입하고 한 타임 스텝 미래로 이동한 윈도를 예측합니다.\n",
        "\n",
        "또한 0에서 46까지 범위를 갖도록 값을 이동시키겠습니다. 여기에서 0은 무음을 나타내고 1에서 46까지는 36(C1)에서 81(A5)까지를 나타냅니다.\n",
        "\n",
        "128 음표(즉, 32개 화음)의 윈도에서 모델을 훈련하겠습니다.\n",
        "\n",
        "이 데이터셋은 메모리에 올라갈 수 있기 때문에 파이썬 코드를 사용해 RAM에서 코랄을 전처리할 수 있지만 여기에서는 tf.data를 사용해 전처리하는 방법을 시연하겠습니다(다음 장에서 tf.data를 사용해 윈도를 만드는 과정을 자세히 설명하겠습니다)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vE0RH04-9i50"
      },
      "outputs": [],
      "source": [
        "def create_target(batch):\n",
        "    X = batch[:, :-1]\n",
        "    Y = batch[:, 1:] # 각 스텝에서 아르페지오에 있는 다음 음표를 예측합니다\n",
        "    return X, Y\n",
        "\n",
        "def preprocess(window):\n",
        "    window = tf.where(window == 0, window, window - min_note + 1) # 값 이동\n",
        "    return tf.reshape(window, [-1]) # 아르페지오로 변환\n",
        "\n",
        "def bach_dataset(chorales, batch_size=32, shuffle_buffer_size=None,\n",
        "                 window_size=32, window_shift=16, cache=True):\n",
        "    def batch_window(window):\n",
        "        return window.batch(window_size + 1)\n",
        "\n",
        "    def to_windows(chorale):\n",
        "        dataset = tf.data.Dataset.from_tensor_slices(chorale)\n",
        "        dataset = dataset.window(window_size + 1, window_shift, drop_remainder=True)\n",
        "        return dataset.flat_map(batch_window)\n",
        "\n",
        "    chorales = tf.ragged.constant(chorales, ragged_rank=1)\n",
        "    dataset = tf.data.Dataset.from_tensor_slices(chorales)\n",
        "    dataset = dataset.flat_map(to_windows).map(preprocess)\n",
        "    if cache:\n",
        "        dataset = dataset.cache()\n",
        "    if shuffle_buffer_size:\n",
        "        dataset = dataset.shuffle(shuffle_buffer_size)\n",
        "    dataset = dataset.batch(batch_size)\n",
        "    dataset = dataset.map(create_target)\n",
        "    return dataset.prefetch(1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g9cjpoUh9i50"
      },
      "source": [
        "훈련 세트, 검증 세트, 테스트 세트를 만듭니다:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PAAvflCd9i52"
      },
      "outputs": [],
      "source": [
        "train_set = bach_dataset(train_chorales, shuffle_buffer_size=1000)\n",
        "valid_set = bach_dataset(valid_chorales)\n",
        "test_set = bach_dataset(test_chorales)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LNRg6SZc9i53"
      },
      "source": [
        "이제 모델을 만듭니다:\n",
        "\n",
        "* 음표를 실수 값으로 모델에 직접 주입할 수 있지만 좋은 결과를 얻지 못할 것입니다. 음표 간의 관계는 단순하지 않습니다. 예를 들어 C3을 C4로 바꾼다면 두 음표 사이에 반음이 12개(즉 한 옥타브) 떨어져 있음에도 멜로디는 여전히 괜찮게 들립니다. 반대로 C3을 C\\#3으로 바꾼다면 바로 다음 음표임에도 화음이 매우 좋지 않습니다. 따라서 `Embedding` 층을 사용해 음표를 작은 벡터 표현으로 바꾸겠습니다(임베딩에 대해서는 16장을 참고하세요). 5-차원 임베딩을 사용하므로 첫 번째 층의 출력은 `[batch_size, window_size, 5]` 크기가 됩니다.\n",
        "* 그 다음 이 데이터를 4개의 `Conv1D` 층을 쌓고 dilation 비율을 두 배씩 늘린 작은 WeveNet 신경망에 주입합니다. 빠른 수렴을 위해 이 층 다음에 `BatchNormalization` 층을 배치합니다.\n",
        "* 그다음 하나의 `LSTM` 층이 장기 패턴을 감지합니다.\n",
        "* 마지막으로 `Dense` 층이 최종 음표 확률을 생성합니다. 타임 스텝과 (무음을 포함해) 가능한 음표 마다 배치에 있는 각 코랄에 대해 하나의 확률을 예측합니다. 따라서 출력 크기는 `[batch_size, window_size, 47]`가 됩니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wk4GI9rJ9i53",
        "outputId": "439e2ef7-157f-459d-b46d-a577f93df181"
      },
      "outputs": [],
      "source": [
        "n_embedding_dims = 5\n",
        "\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(input_dim=n_notes, output_dim=n_embedding_dims,\n",
        "                           input_shape=[None]),\n",
        "    tf.keras.layers.Conv1D(32, kernel_size=2, padding=\"causal\", activation=\"relu\"),\n",
        "    tf.keras.layers.BatchNormalization(),\n",
        "    tf.keras.layers.Conv1D(48, kernel_size=2, padding=\"causal\", activation=\"relu\", dilation_rate=2),\n",
        "    tf.keras.layers.BatchNormalization(),\n",
        "    tf.keras.layers.Conv1D(64, kernel_size=2, padding=\"causal\", activation=\"relu\", dilation_rate=4),\n",
        "    tf.keras.layers.BatchNormalization(),\n",
        "    tf.keras.layers.Conv1D(96, kernel_size=2, padding=\"causal\", activation=\"relu\", dilation_rate=8),\n",
        "    tf.keras.layers.BatchNormalization(),\n",
        "    tf.keras.layers.LSTM(256, return_sequences=True),\n",
        "    tf.keras.layers.Dense(n_notes, activation=\"softmax\")\n",
        "])\n",
        "\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1wjHad8l9i53"
      },
      "source": [
        "이제 모델을 컴파일하고 훈련할 준비가 되었습니다!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HWM6_7xp9i53",
        "outputId": "624d5f36-19d7-4c11-b19f-14e6dbd6380e"
      },
      "outputs": [],
      "source": [
        "optimizer = tf.keras.optimizers.Nadam(learning_rate=1e-3)\n",
        "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=optimizer,\n",
        "              metrics=[\"accuracy\"])\n",
        "model.fit(train_set, epochs=20, validation_data=valid_set)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BYNU7mPZ9i54"
      },
      "source": [
        "여기서는 하이퍼파라미터 탐색을 많이 수행하지 않았습니다. 자유롭게 이 모델을 사용해 하이퍼파라미터를 탐색하고 최적화해 보세요. 예를 들어 `LSTM` 층을 제거하고 `Conv1D` 층으로 바꿀 수 있습니다. 층의 개수, 학습률, 옵티마이저 등을 실험해 볼 수 있습니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YK8Fux7f9i54"
      },
      "source": [
        "검증 세트에 대한 모델의 성능이 만족스럽다면 모델을 저장하고 테스트 세트에서 마지막으로 평가합니다:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1r1qW9yx9i54",
        "outputId": "120b81ef-8378-4291-e1e3-d4db31b8bba2"
      },
      "outputs": [],
      "source": [
        "# model.save(\"my_bach_model\", save_format=\"tf\")\n",
        "# model.evaluate(test_set)\n",
        "\n",
        "model.save(\"my_bach_model.h5\")\n",
        "model.evaluate(test_set)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I10BcUUx9i55"
      },
      "source": [
        "**노트:** 이 예제에서는 테스트 세트가 진짜로 필요하지 않습니다. 모델이 생성한 음악을 듣는 것이 최종 평가가 되기 때문입니다. 따라서 필요하다면 테스트 세트를 훈련 세트에 넣고 모델을 다시 훈련하여 조금 더 나은 모델을 얻을 수 있습니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CvRloRch9i57"
      },
      "source": [
        "이제 새로운 코랄을 생성하는 함수를 만들어 보죠. 몇 개의 시드 화음을 주고 이를 (모델이 기대하는 포맷인) 아르페지오로 변환합니다. 그다음 모델을 사용해 다음 음표을 예측합니다. 마지막에 4개씩 음표를 모아서 다시 화음을 만들고 최종 코랄을 반환합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tnnviWeS9i57"
      },
      "outputs": [],
      "source": [
        "def generate_chorale(model, seed_chords, length):\n",
        "    arpegio = preprocess(tf.constant(seed_chords, dtype=tf.int64))\n",
        "    arpegio = tf.reshape(arpegio, [1, -1])\n",
        "    for chord in range(length):\n",
        "        for note in range(4):\n",
        "            next_note = model.predict(arpegio, verbose=0).argmax(axis=-1)[:1, -1:]\n",
        "            arpegio = tf.concat([arpegio, next_note], axis=1)\n",
        "    arpegio = tf.where(arpegio == 0, arpegio, arpegio + min_note - 1)\n",
        "    return tf.reshape(arpegio, shape=[-1, 4])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iu7-3RjC9i57"
      },
      "source": [
        "이 함수를 테스트하려면 시드 화음이 필요합니다. 테스트 코랄 중 하나에 있는 처음 8개의 화음을 사용해 보죠(실제로 이는 4번 반복되는 2개의 화음입니다):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 76
        },
        "id": "fK8Ubnk99i58",
        "outputId": "f8c6178a-c476-455b-bd8d-a031ebcd5a66"
      },
      "outputs": [],
      "source": [
        "seed_chords = test_chorales[2][:8]\n",
        "play_chords(seed_chords, amplitude=0.2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yy_gsjwA9i58"
      },
      "source": [
        "첫 번째 코랄을 생성할 준비를 마쳤습니다! 56개의 화음을 생성하여 총 64개의 화음, 즉 4 소절(소절마다 4개의 화음인 4/4박으로 가정합니다)을 만들어 보겠습니다:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 76
        },
        "id": "tGpQLWmI9i58",
        "outputId": "cd1a789b-a1bf-4cf5-c3b6-40f81aa3ff03"
      },
      "outputs": [],
      "source": [
        "new_chorale = generate_chorale(model, seed_chords, 56)\n",
        "play_chords(new_chorale)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jAT5YTAx9i58"
      },
      "source": [
        "이 방식에는 한가지 단점이 있습니다: 너무 보수적인 경우가 많습니다. 실제로 이 모델은 모험을 하지 않아 항상 가장 높은 확률의 음표를 선택합니다. 이전 음표를 반복하면 충분히 듣기 좋고 가장 덜 위험하기 때문에 이 알고리즘은 마지막 음표를 오래 지속시키는 경향이 있습니다. 상당히 지루합니다. 또한 이 모델을 여러 번 실행하면 항상 같은 멜로디를 생성할 것입니다.\n",
        "\n",
        "조금 더 신나게 만들어 보죠! 항상 가장 높은 점수의 음표를 선택하는 대신, 예측된 확률을 기반으로 랜덤하게 다음 음표를 선택하겠습니다. 예를 들어, 모델이 75% 확률로 C3를 예측하고 25% 확률로 G3를 예측했다면 이 확률대로 랜덤하게 두 음표 중 하나를 선택하겠습니다. 또한 `temperature` 매개변수를 추가하여 시스템의 온도(즉 대담성)를 제어하겠습니다. 높은 온도는 예측 확률을 비슷하게 만들어 가능성이 높은 음표의 확률을 줄이고 가능성이 낮은 음표의 확률을 높입니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FTBYAC-b9i58"
      },
      "outputs": [],
      "source": [
        "def generate_chorale_v2(model, seed_chords, length, temperature=1):\n",
        "    arpegio = preprocess(tf.constant(seed_chords, dtype=tf.int64))\n",
        "    arpegio = tf.reshape(arpegio, [1, -1])\n",
        "    for chord in range(length):\n",
        "        for note in range(4):\n",
        "            next_note_probas = model.predict(arpegio)[0, -1:]\n",
        "            rescaled_logits = tf.math.log(next_note_probas) / temperature\n",
        "            next_note = tf.random.categorical(rescaled_logits, num_samples=1)\n",
        "            arpegio = tf.concat([arpegio, next_note], axis=1)\n",
        "    arpegio = tf.where(arpegio == 0, arpegio, arpegio + min_note - 1)\n",
        "    return tf.reshape(arpegio, shape=[-1, 4])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J94Qvfbd9i58"
      },
      "source": [
        "이 함수로 3개의 코랄을 생성해 보겠습니다. 하나는 차갑게, 하나는 중간으로, 하나는 뜨겁게 만듭니다(시드, 길이, 온도를 사용해 자유롭게 실험해 보세요). 다음 코드는 각 코랄을 별개의 파일에 저장합니다. 마음에 드는 음악을 만날 때까지 이 셀을 반복해서 실행할 수 있습니다!\n",
        "\n",
        "**가장 아름다운 코랄을 트위터 @aureliengeron로 공유해 주시면 정말 감사하겠습니다! :))**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "cKpx_tU89i58",
        "outputId": "4a87beb8-0189-4f55-ae9b-1073583b0ed5",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "new_chorale_v2_cold = generate_chorale_v2(model, seed_chords, 56, temperature=0.8)\n",
        "play_chords(new_chorale_v2_cold, filepath=\"bach_cold.wav\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "uTodHp-q9i59",
        "outputId": "8a122b35-95ef-4c49-92b3-bfcbd7f931d4"
      },
      "outputs": [],
      "source": [
        "new_chorale_v2_medium = generate_chorale_v2(model, seed_chords, 56, temperature=1.0)\n",
        "play_chords(new_chorale_v2_medium, filepath=\"bach_medium.wav\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "4ZX42e1i9i59",
        "outputId": "ff1de122-d09f-4a46-cde2-bd753dff90d5"
      },
      "outputs": [],
      "source": [
        "new_chorale_v2_hot = generate_chorale_v2(model, seed_chords, 56, temperature=1.5)\n",
        "play_chords(new_chorale_v2_hot, filepath=\"bach_hot.wav\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QeX35etZ9i59"
      },
      "source": [
        "마지막으로 재미있는 실험을 해 볼 수 있습니다: 친구에게 마음에 드는 코랄 몇 개와 진짜 코랄을 보내고 어떤 것이 진짜인지 물어 보세요!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 76
        },
        "id": "vqOYgu-69i59",
        "outputId": "ec2e3638-aafb-4b71-c80e-03a47b758b68"
      },
      "outputs": [],
      "source": [
        "play_chords(test_chorales[2][:64], filepath=\"bach_test_4.wav\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "handson",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.15"
    },
    "nav_menu": {},
    "toc": {
      "navigate_menu": true,
      "number_sections": true,
      "sideBar": true,
      "threshold": 6,
      "toc_cell": false,
      "toc_section_display": "block",
      "toc_window_display": false
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
