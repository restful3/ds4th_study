{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "076e6c6c-2fbf-49c4-baa7-6ee9e3f47df8",
   "metadata": {},
   "source": [
    "# 대규모 데이터셋 수집하기\n",
    "## 대규모 말뭉치 구축의 어려움"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b60ef60b-18a7-46db-ba32-65d555853331",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc6376df433e4de3a9ef336868350a8e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/656 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "123d80b75d5e4a57abdba492b6f0bd73",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/479M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of OpenAIGPTLMHeadModel were not initialized from the model checkpoint at openai-gpt and are newly initialized: ['position_ids']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8cb50e13f074f95ac427ceee111069c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/74.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "561357f2848b47ff8a800e8a749a617a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20f6ef4315454cc1a3d17849a89a977e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/816k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64ad5ea58f5249ba8d30fa969688ec5c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/458k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5b6c490b4d64e09945b0f7620f9c32a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.27M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING[XFORMERS]: xFormers can't load C++/CUDA extensions. xFormers was built for:\n",
      "    PyTorch 2.2.1+cu121 with CUDA 1201 (you have 2.2.1)\n",
      "    Python  3.9.18 (you have 3.9.18)\n",
      "  Please reinstall xformers (see https://github.com/facebookresearch/xformers#installing-xformers)\n",
      "  Memory-efficient attention, SwiGLU, sparse and more won't be available.\n",
      "  Set XFORMERS_MORE_DETAILS=1 for more details\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4d085e4f84e4607b431af3fabfb6049",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import pipeline, set_seed\n",
    "\n",
    "generation_gpt = pipeline('text-generation', model='openai-gpt')\n",
    "generation_gpt2 = pipeline('text-generation', model='gpt2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "07e1de8d-9a81-4ed3-804e-69c49885b2bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT  크기: 116.5M parameters\n",
      "GPT2 크기: 124.4M parameters\n"
     ]
    }
   ],
   "source": [
    "def model_size(model):\n",
    "    return sum(t.numel() for t in model.parameters())\n",
    "\n",
    "print(f\"GPT  크기: {model_size(generation_gpt.model)/1000**2:.1f}M parameters\")\n",
    "print(f\"GPT2 크기: {model_size(generation_gpt2.model)/1000**2:.1f}M parameters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f772faf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/restful3/anaconda3/envs/trading/lib/python3.9/site-packages/transformers/generation/utils.py:1353: UserWarning: Using `max_length`'s default (50) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT 자동 완성:\n",
      "1.\n",
      "When they came back. how was it that only when i had her back did he feel the same way? how could we possibly work so well together? \n",
      " \" that 'll take some adjustment, \" he said. he took her hand, pulled her\n",
      "2.\n",
      "When they came back to the table, the other men weren't going to make a move. \n",
      " the waitress walked over and took their orders. the men laughed at everything he said and some laughed at the waitress. he wasn't going to get laid\n",
      "3.\n",
      "When they came back and found she had returned for their evening run. \" i have a bad taste in my mouth, \" she murmured to herself. \n",
      " she made another trip to the kitchen to make this sandwich. when she returned, she heard his\n"
     ]
    }
   ],
   "source": [
    "def enum_pipeline_ouputs(pipe, prompt, num_return_sequences):\n",
    "    out = pipe(prompt, num_return_sequences=num_return_sequences,\n",
    "               clean_up_tokenization_spaces=True)\n",
    "    return \"\\n\".join(f\"{i+1}.\" + s[\"generated_text\"] for i, s in enumerate(out))\n",
    "\n",
    "prompt = \"\\nWhen they came back\"\n",
    "print(\"GPT 자동 완성:\\n\" + enum_pipeline_ouputs(generation_gpt, prompt, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "942a0ae2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT-2 자동 완성:\n",
      "1.\n",
      "When they came back to the room, the two young people looked around at the various faces, all who came from different backgrounds.\n",
      "\n",
      "\"Are you sure you don't think he should face this?\"\n",
      "\n",
      "\"Because you're so high\n",
      "2.\n",
      "When they came back to us, they were told there were no problems. They thought their food was safe, they did not think it was kosher. It was my choice. But they continued to tell me I should bring their food and food that\n",
      "3.\n",
      "When they came back, there was no word on whether they were safe.\n",
      "\n",
      "At 1,000 kilometres from the center of the city of Zabul, an army vehicle was parked along the eastern border. The vehicle's driver, the commander\n"
     ]
    }
   ],
   "source": [
    "print(\"GPT-2 자동 완성:\\n\" + enum_pipeline_ouputs(generation_gpt2, prompt, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faa32791",
   "metadata": {},
   "source": [
    "## 사용자 정의 코드 데이터셋 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "16cdeef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !git clone https://huggingface.co/datasets/transformersbook/codeparrot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea804725",
   "metadata": {},
   "source": [
    "## 대용량 데이터셋 다루기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "465c3947",
   "metadata": {},
   "source": [
    "### 메모리 매핑"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b298f6ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f68f66d7977c4d7c8f1696a0515705cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/184 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1006bb4853a412997cea2206cf125e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset, DownloadConfig\n",
    "\n",
    "download_config = DownloadConfig(delete_extracted=True)\n",
    "dataset = load_dataset('/home/restful3/datasets_local/codeparrot', split='train', download_config=download_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7b3f6a11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of python files code in dataset : 18695559\n",
      "Dataset size (cache file) : 183.59 GB\n",
      "RAM used: 3272 MB\n"
     ]
    }
   ],
   "source": [
    "import psutil, os\n",
    "\n",
    "print(f\"Number of python files code in dataset : {len(dataset)}\")\n",
    "ds_size = sum(os.stat(f[\"filename\"]).st_size for f in dataset.cache_files)\n",
    "# os.stat.st_size는 바이트 단위이므로 GB로 바꿉니다\n",
    "print(f\"Dataset size (cache file) : {ds_size / 2**30:.2f} GB\")\n",
    "# Process.memory_info는 바이트 단위이므로 MB로 바꿉니다\n",
    "print(f\"RAM used: {psutil.Process(os.getpid()).memory_info().rss >> 20} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0726cb3b",
   "metadata": {},
   "source": [
    "### 스트리밍"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ce76a7dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09682165781f48e8a71e6665bcf9c1ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/184 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "streamed_dataset = load_dataset('/home/restful3/datasets_local/codeparrot', split='train', streaming=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "472e5eff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "iterator = iter(streamed_dataset)\n",
    "\n",
    "print(dataset[0] == next(iterator))\n",
    "print(dataset[1] == next(iterator))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5b6af45",
   "metadata": {},
   "source": [
    "## 허깅페이스 허브에 데이터셋 추가하기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5534d1be",
   "metadata": {},
   "source": [
    "# 토크나이저 구축하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5973cd9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/restful3/anaconda3/envs/trading/lib/python3.9/site-packages/transformers/models/t5/tokenization_t5_fast.py:155: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
      "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
      "- Be aware that you SHOULD NOT rely on t5-base automatically truncating your input to 512 when padding/encoding.\n",
      "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
      "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "def tok_list(tokenizer, string):\n",
    "    input_ids = tokenizer(string, add_special_tokens=False)[\"input_ids\"]\n",
    "    return [tokenizer.decode(tok) for tok in input_ids]\n",
    "\n",
    "tokenizer_T5 = AutoTokenizer.from_pretrained(\"t5-base\")\n",
    "tokenizer_camembert = AutoTokenizer.from_pretrained(\"camembert-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "766a48ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"sex\"에 대한 T5 토큰: ['', 's', 'ex']\n",
      "\"being\"에 대한 CamemBERT 토큰: ['be', 'ing']\n"
     ]
    }
   ],
   "source": [
    "print(f'\"sex\"에 대한 T5 토큰: {tok_list(tokenizer_T5,\"sex\")}')\n",
    "print(f'\"being\"에 대한 CamemBERT 토큰: {tok_list(tokenizer_camembert,\"being\")}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6756f288",
   "metadata": {},
   "source": [
    "## 파이썬 코드를 위한 토크나이저"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "71511473",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['def', 'Ġsay', '_', 'hello', '():', 'Ċ', 'Ġ', 'Ġ', 'Ġ', 'Ġprint', '(\"', 'hello', 'Ġworld', '\")', 'Ċ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ċ', '#', 'Ġprint', 'Ġit', 'Ċ', 'say', '_', 'hello', '()', 'Ċ']\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "python_code = r\"\"\"def say_hello():\n",
    "    print(\"hello world\")\n",
    "    \n",
    "# print it\n",
    "say_hello()\n",
    "\"\"\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('gpt2')\n",
    "print(tokenizer(python_code).tokens())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2c80274e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.backend_tokenizer.normalizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1f7d1a16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('def', (0, 3)), ('Ġsay', (3, 7)), ('_', (7, 8)), ('hello', (8, 13)), ('():', (13, 16)), ('ĊĠĠĠ', (16, 20)), ('Ġprint', (20, 26)), ('(\"', (26, 28)), ('hello', (28, 33)), ('Ġworld', (33, 39)), ('\")', (39, 41)), ('ĊĠĠĠĠ', (41, 46)), ('Ċ', (46, 47)), ('#', (47, 48)), ('Ġprint', (48, 54)), ('Ġit', (54, 57)), ('Ċ', (57, 58)), ('say', (58, 61)), ('_', (61, 62)), ('hello', (62, 67)), ('()', (67, 69)), ('Ċ', (69, 70))]\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str(python_code))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1a80b958",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "`a`는 단일 바이트 `b'a'`로 인코딩됩니다: 97\n",
      "`€`는 세 바이트 `b'\\xe2\\x82\\xac'`로 인코딩됩니다: [226, 130, 172]\n"
     ]
    }
   ],
   "source": [
    "a, e = u\"a\", u\"€\"\n",
    "byte = ord(a.encode(\"utf-8\"))\n",
    "print(f'`{a}`는 단일 바이트 `{a.encode(\"utf-8\")}`로 인코딩됩니다: {byte}')\n",
    "byte = [ord(chr(i)) for i in e.encode(\"utf-8\")]\n",
    "print(f'`{e}`는 세 바이트 `{e.encode(\"utf-8\")}`로 인코딩됩니다: {byte}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ef33262d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "기본 어휘 사전 크기: 256\n",
      "첫 번째 원소: `!`, last element: `Ń`\n"
     ]
    }
   ],
   "source": [
    "from transformers.models.gpt2.tokenization_gpt2 import bytes_to_unicode\n",
    "\n",
    "byte_to_unicode_map = bytes_to_unicode()\n",
    "unicode_to_byte_map = dict((v, k) for k, v in byte_to_unicode_map.items())\n",
    "base_vocab = list(unicode_to_byte_map.keys())\n",
    "\n",
    "print(f'기본 어휘 사전 크기: {len(base_vocab)}')\n",
    "print(f'첫 번째 원소: `{base_vocab[0]}`, last element: `{base_vocab[-1]}`')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3709fb4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Description</th>\n",
       "      <th>Character</th>\n",
       "      <th>Bytes</th>\n",
       "      <th>Mapped bytes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Regular characters</td>\n",
       "      <td>`a` and `?`</td>\n",
       "      <td>97 and 63</td>\n",
       "      <td>`a` and `?`</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Nonprintable control character (carriage return)</td>\n",
       "      <td>`U+000D`</td>\n",
       "      <td>13</td>\n",
       "      <td>`č`</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A space</td>\n",
       "      <td>` `</td>\n",
       "      <td>32</td>\n",
       "      <td>`Ġ`</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A nonbreakable space</td>\n",
       "      <td>`\\xa0`</td>\n",
       "      <td>160</td>\n",
       "      <td>`ł`</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A newline character</td>\n",
       "      <td>`\\n`</td>\n",
       "      <td>10</td>\n",
       "      <td>`Ċ`</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        Description    Character      Bytes  \\\n",
       "0                                Regular characters  `a` and `?`  97 and 63   \n",
       "1  Nonprintable control character (carriage return)     `U+000D`         13   \n",
       "2                                           A space          ` `         32   \n",
       "3                              A nonbreakable space       `\\xa0`        160   \n",
       "4                               A newline character         `\\n`         10   \n",
       "\n",
       "  Mapped bytes  \n",
       "0  `a` and `?`  \n",
       "1          `č`  \n",
       "2          `Ġ`  \n",
       "3          `ł`  \n",
       "4          `Ċ`  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# BPE 문자 매핑의 예\n",
    "import pandas as pd\n",
    "from transformers.models.gpt2.tokenization_gpt2 import bytes_to_unicode\n",
    "\n",
    "byte_to_unicode_map = bytes_to_unicode()\n",
    "unicode_to_byte_map = dict((v, k) for k, v in byte_to_unicode_map.items())\n",
    "base_vocab = list(unicode_to_byte_map.keys())\n",
    "\n",
    "examples = [\n",
    "    ['Regular characters', '`a` and `?`', f'{ord(\"a\")} and {ord(\"?\")}' , f'`{byte_to_unicode_map[ord(\"a\")]}` and `{byte_to_unicode_map[ord(\"?\")]}`'],\n",
    "    ['Nonprintable control character (carriage return)', '`U+000D`', f'13', f'`{byte_to_unicode_map[13]}`'],\n",
    "    ['A space', '` `', f'{ord(\" \")}', f'`{byte_to_unicode_map[ord(\" \")]}`'],\n",
    "    ['A nonbreakable space', '`\\\\xa0`', '160', f'`{byte_to_unicode_map[ord(chr(160))]}`'],\n",
    "    ['A newline character', '`\\\\n`', '10', f'`{byte_to_unicode_map[ord(chr(10))]}`'],\n",
    "]\n",
    "\n",
    "pd.DataFrame(examples, columns = ['Description', 'Character', 'Bytes', 'Mapped bytes'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "91a47e57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('def', (0, 3)), ('Ġsay', (3, 7)), ('_', (7, 8)), ('hello', (8, 13)), ('():', (13, 16)), ('ĊĠĠĠ', (16, 20)), ('Ġprint', (20, 26)), ('(\"', (26, 28)), ('hello', (28, 33)), ('Ġworld', (33, 39)), ('\")', (39, 41)), ('ĊĠĠĠĠ', (41, 46)), ('Ċ', (46, 47)), ('#', (47, 48)), ('Ġprint', (48, 54)), ('Ġit', (54, 57)), ('Ċ', (57, 58)), ('say', (58, 61)), ('_', (61, 62)), ('hello', (62, 67)), ('()', (67, 69)), ('Ċ', (69, 70))]\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str(python_code))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "99d5d542",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "어휘 사전의 크기: 50257\n"
     ]
    }
   ],
   "source": [
    "print(f\"어휘 사전의 크기: {len(tokenizer)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d92bdcdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['def', 'Ġsay', '_', 'hello', '():', 'Ċ', 'Ġ', 'Ġ', 'Ġ', 'Ġprint', '(\"', 'hello', 'Ġworld', '\")', 'Ċ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ċ', '#', 'Ġprint', 'Ġit', 'Ċ', 'say', '_', 'hello', '()', 'Ċ']\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer(python_code).tokens())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d90dd2c9",
   "metadata": {},
   "source": [
    "## 토크나이저 훈련하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b8a26a73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂ', ' =================================================================', ' ----------------------------------------------------------------', '................................................................', '________________________________________________________________', '================================================================', '----------------------------------------------------------------', 'ÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂ']\n"
     ]
    }
   ],
   "source": [
    "tokens = sorted(tokenizer.vocab.items(), key=lambda x: len(x[0]), reverse=True)\n",
    "# print([f'{tokenizer.convert_tokens_to_string(t)}' for t, _ in tokens[:8]]);\n",
    "print([tokenizer.convert_tokens_to_string([t]) for t, _ in tokens[:8]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d5937fa0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<|endoftext|>', ' gazed', ' informants', ' Collider', ' regress', 'ominated', ' amplification', 'Compar', '….\"', ' (/', 'Commission', ' Hitman']\n"
     ]
    }
   ],
   "source": [
    "tokens = sorted(tokenizer.vocab.items(), key=lambda x: x[1], reverse=True)\n",
    "print([f'{tokenizer.convert_tokens_to_string([t])}' for t, _ in tokens[:12]]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dcf13f12",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/restful3/anaconda3/envs/trading/lib/python3.9/site-packages/huggingface_hub/repocard.py:105: UserWarning: Repo card metadata block was not found. Setting CardData to empty.\n",
      "  warnings.warn(\"Repo card metadata block was not found. Setting CardData to empty.\")\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2787b4e68ffb43f783922eebf849ceb6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/183 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3246651b67cc4512b68d44ea8a54daee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset, DownloadConfig\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "length = 100000\n",
    "dataset_name = 'transformersbook/codeparrot-train'\n",
    "dataset = load_dataset(dataset_name, split=\"train\", streaming=True)\n",
    "iter_dataset = iter(dataset)\n",
    "\n",
    "def batch_iterator(batch_size=10):\n",
    "    for _ in tqdm(range(0, length, batch_size)):\n",
    "        yield [next(iter_dataset)['content'] for _ in range(batch_size)]\n",
    "\n",
    "new_tokenizer = tokenizer.train_new_from_iterator(batch_iterator(), \n",
    "                                                  vocab_size=12500,\n",
    "                                                  initial_alphabet=base_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e31104cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['  ', '    ', '   ', '        ', 'se', 'in', '       ', 're', 'on', 'te', '\\n       ', '\\n        ', 'or', 'st', 'de', '\\n   ', 'th', 'le', ' =', 'lf', 'self', 'me', 'al']\n"
     ]
    }
   ],
   "source": [
    "tokens = sorted(new_tokenizer.vocab.items(), key=lambda x: x[1], reverse=False)\n",
    "print([f'{tokenizer.convert_tokens_to_string([t])}' for t, _ in tokens[257:280]]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "94125587",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' capt', ' embedded', ' regarding', 'Bundle', '355', ' recv', ' dmp', ' vault', ' Mongo', ' possibly', 'implementation', 'Matches']\n"
     ]
    }
   ],
   "source": [
    "print([f'{new_tokenizer.convert_tokens_to_string([t])}' for t,_ in tokens[-12:]]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cdd811e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['def', 'Ġs', 'ay', '_', 'hello', '():', 'ĊĠĠĠ', 'Ġprint', '(\"', 'hello', 'Ġworld', '\")', 'ĊĠĠĠĠ', 'Ċ', '#', 'Ġprint', 'Ġit', 'Ċ', 's', 'ay', '_', 'hello', '()', 'Ċ']\n"
     ]
    }
   ],
   "source": [
    "print(new_tokenizer(python_code).tokens())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5adada3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "파이썬 전체 예약어 개수: 36\n",
      "예약어 `__peg_parser__`는 어휘 사전에 없습니다.\n",
      "예약어 `await`는 어휘 사전에 없습니다.\n",
      "예약어 `finally`는 어휘 사전에 없습니다.\n",
      "예약어 `nonlocal`는 어휘 사전에 없습니다.\n"
     ]
    }
   ],
   "source": [
    "import keyword\n",
    "\n",
    "print(f'파이썬 전체 예약어 개수: {len(keyword.kwlist)}')\n",
    "for keyw in keyword.kwlist:\n",
    "    if keyw not in new_tokenizer.vocab:\n",
    "        print(f'예약어 `{keyw}`는 어휘 사전에 없습니다.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a858862d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "600e10aa035b462d97fe53f3e3c2e459",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "length = 200000\n",
    "new_tokenizer_larger = tokenizer.train_new_from_iterator(batch_iterator(),\n",
    "    vocab_size=32768, initial_alphabet=base_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5e3e314f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\" '<?\", 'Functional', ' Images', 'encoders', ' bibrec', ' OPTIONAL', ' rdclass', 'SocketAddressTag', '资金', 'DEPLOYMENT', '经纪公司代码', \")'],\"]\n"
     ]
    }
   ],
   "source": [
    "tokens = sorted(new_tokenizer_larger.vocab.items(), key=lambda x: x[1],\n",
    "                reverse=False)\n",
    "print([f'{tokenizer.convert_tokens_to_string([t])}' for t, _ in tokens[-12:]]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d959ddad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['def', 'Ġsay', '_', 'hello', '():', 'ĊĠĠĠ', 'Ġprint', '(\"', 'hello', 'Ġworld', '\")', 'ĊĠĠĠĠ', 'Ċ', '#', 'Ġprint', 'Ġit', 'Ċ', 'say', '_', 'hello', '()', 'Ċ']\n"
     ]
    }
   ],
   "source": [
    "print(new_tokenizer_larger(python_code).tokens())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fee98b4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "예약어 `__peg_parser__`는 어휘 사전에 없습니다.\n",
      "예약어 `nonlocal`는 어휘 사전에 없습니다.\n"
     ]
    }
   ],
   "source": [
    "for keyw in keyword.kwlist:\n",
    "    if keyw not in new_tokenizer_larger.vocab:\n",
    "        print(f'예약어 `{keyw}`는 어휘 사전에 없습니다.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98830bda",
   "metadata": {},
   "source": [
    "## 허브에 사용자 정의 토크나이저 저장하기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea609873",
   "metadata": {},
   "source": [
    "# 밑바닥부터 모델 훈련하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "667a1b75",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
