{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 허깅페이스 트랜스포머란"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\miniconda\\envs\\lang310\\lib\\site-packages\\huggingface_hub\\file_download.py:795: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "d:\\miniconda\\envs\\lang310\\lib\\site-packages\\huggingface_hub\\file_download.py:795: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "text = \"What is Huggingface Transformers?\"\n",
    "# BERT 모델 활용\n",
    "bert_model = AutoModel.from_pretrained(\"bert-base-uncased\")\n",
    "bert_tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "encoded_input = bert_tokenizer(text, return_tensors='pt')\n",
    "bert_output = bert_model(**encoded_input)\n",
    "# GPT-2 모델 활용\n",
    "gpt_model = AutoModel.from_pretrained('gpt2')\n",
    "gpt_tokenizer = AutoTokenizer.from_pretrained('gpt2')\n",
    "encoded_input = gpt_tokenizer(text, return_tensors='pt')\n",
    "gpt_output = gpt_model(**encoded_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 허깅페이스 라이브러리 사용법 익히기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 모델 활용하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 예제 3.2. 모델 아이디로 모델 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at klue/roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModel\n",
    "model_id = 'klue/roberta-base'\n",
    "model = AutoModel.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RobertaModel(\n",
       "  (embeddings): RobertaEmbeddings(\n",
       "    (word_embeddings): Embedding(32000, 768, padding_idx=1)\n",
       "    (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "    (token_type_embeddings): Embedding(1, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): RobertaEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0-11): 12 x RobertaLayer(\n",
       "        (attention): RobertaAttention(\n",
       "          (self): RobertaSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): RobertaSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): RobertaIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): RobertaOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): RobertaPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 예제 3.4. 분류 헤드가 포함된 모델 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "model_id = 'SamLowe/roberta-base-go_emotions'\n",
    "classification_model = AutoModelForSequenceClassification.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# 토크나이저 불러오기\n",
    "tokenizer = AutoTokenizer.from_pretrained('SamLowe/roberta-base-go_emotions')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "입력 텍스트: I am feeling very happy today!\n",
      "예측된 감정: joy\n",
      "확률: 0.972\n"
     ]
    }
   ],
   "source": [
    "# 테스트할 텍스트\n",
    "text = \"I am feeling very happy today!\"\n",
    "\n",
    "# 텍스트 인코딩\n",
    "inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "\n",
    "# 모델 추론\n",
    "outputs = classification_model(**inputs)\n",
    "\n",
    "# 확률값으로 변환\n",
    "probs = F.softmax(outputs.logits, dim=-1)\n",
    "\n",
    "# 가장 높은 확률의 감정 클래스 찾기\n",
    "predicted_class = torch.argmax(probs).item()\n",
    "\n",
    "# 감정 레이블 가져오기\n",
    "emotion_labels = classification_model.config.id2label\n",
    "predicted_emotion = emotion_labels[predicted_class]\n",
    "\n",
    "print(f\"입력 텍스트: {text}\")\n",
    "print(f\"예측된 감정: {predicted_emotion}\")\n",
    "print(f\"확률: {probs[0][predicted_class].item():.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 예제 3.6. 분류 헤드가 랜덤으로 초기화된 모델 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at klue/roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "model_id = 'klue/roberta-base'\n",
    "classification_model = AutoModelForSequenceClassification.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 토크나이저 활용하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 예제 3.8. 토크나이저 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "model_id = 'klue/roberta-base'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'do_lower_case': False, 'unk_token': AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True), 'sep_token': AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True), 'pad_token': AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True), 'cls_token': AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True), 'mask_token': AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True), 'tokenize_chinese_chars': True, 'strip_accents': None, 'do_basic_tokenize': True, 'never_split': None, 'bos_token': AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True), 'eos_token': AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True), 'model_max_length': 512, 'name_or_path': 'klue/roberta-base'}\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.init_kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 예제 3.9. 토크나이저 사용하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [0, 9157, 7461, 2190, 2259, 8509, 2138, 1793, 2855, 5385, 2200, 20950, 2], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "tokenized = tokenizer(\"토크나이저는 텍스트를 토큰 단위로 나눈다\")\n",
    "print(tokenized)\n",
    "# {'input_ids': [0, 9157, 7461, 2190, 2259, 8509, 2138, 1793, 2855, 5385, 2200, 20950, 2],\n",
    "#  'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "#  'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', '토크', '##나이', '##저', '##는', '텍스트', '##를', '토', '##큰', '단위', '##로', '나눈다', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.convert_ids_to_tokens(tokenized['input_ids']))\n",
    "# ['[CLS]', '토크', '##나이', '##저', '##는', '텍스트', '##를', '토', '##큰', '단위', '##로', '나눈다', '[SEP]']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] 토크나이저는 텍스트를 토큰 단위로 나눈다 [SEP]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(tokenizer.decode(tokenized['input_ids']))\n",
    "# [CLS] 토크나이저는 텍스트를 토큰 단위로 나눈다 [SEP]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "토크나이저는 텍스트를 토큰 단위로 나눈다\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(tokenized['input_ids'], skip_special_tokens=True))\n",
    "# 토크나이저는 텍스트를 토큰 단위로 나눈다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 예제 3.10. 토크나이저에 여러 문장 넣기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[0, 1656, 1141, 3135, 6265, 2], [0, 864, 1141, 3135, 6265, 2]], 'token_type_ids': [[0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1]]}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(['첫 번째 문장', '두 번째 문장'])\n",
    "\n",
    "# {'input_ids': [[0, 1656, 1141, 3135, 6265, 2], [0, 864, 1141, 3135, 6265, 2]],\n",
    "# 'token_type_ids': [[0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0]],\n",
    "# 'attention_mask': [[1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1]]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 예제 3.11. 하나의 데이터에 여러 문장이 들어가는 경우"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[0, 1656, 1141, 3135, 6265, 2, 864, 1141, 3135, 6265, 2]], 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer([['첫 번째 문장', '두 번째 문장']])\n",
    "\n",
    "# {'input_ids': [[0, 1656, 1141, 3135, 6265, 2, 864, 1141, 3135, 6265, 2]],\n",
    "# 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n",
    "# 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 예제 3.12. 토큰 아이디를 문자열로 복원"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS] 첫 번째 문장 [SEP]', '[CLS] 두 번째 문장 [SEP]']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_tokenized_result = tokenizer(['첫 번째 문장', '두 번째 문장'])['input_ids']\n",
    "tokenizer.batch_decode(first_tokenized_result)\n",
    "# ['[CLS] 첫 번째 문장 [SEP]', '[CLS] 두 번째 문장 [SEP]']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS] 첫 번째 문장 [SEP] 두 번째 문장 [SEP]']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "second_tokenized_result = tokenizer([['첫 번째 문장', '두 번째 문장']])['input_ids']\n",
    "tokenizer.batch_decode(second_tokenized_result)\n",
    "# ['[CLS] 첫 번째 문장 [SEP] 두 번째 문장 [SEP]']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 예제 3.13. BERT 토크나이저와 RoBERTa 토크나이저"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\miniconda\\envs\\lang310\\lib\\site-packages\\huggingface_hub\\file_download.py:795: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[2, 1656, 1141, 3135, 6265, 3, 864, 1141, 3135, 6265, 3]], 'token_type_ids': [[0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_tokenizer = AutoTokenizer.from_pretrained('klue/bert-base')\n",
    "bert_tokenizer([['첫 번째 문장', '두 번째 문장']])\n",
    "# {'input_ids': [[2, 1656, 1141, 3135, 6265, 3, 864, 1141, 3135, 6265, 3]],\n",
    "# 'token_type_ids': [[0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1]],\n",
    "# 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[0, 1656, 1141, 3135, 6265, 2, 864, 1141, 3135, 6265, 2]], 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roberta_tokenizer = AutoTokenizer.from_pretrained('klue/roberta-base')\n",
    "roberta_tokenizer([['첫 번째 문장', '두 번째 문장']])\n",
    "# {'input_ids': [[0, 1656, 1141, 3135, 6265, 2, 864, 1141, 3135, 6265, 2]],\n",
    "# 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n",
    "# 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[0, 9502, 3645, 2, 2, 10815, 3645, 2]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1]]}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en_roberta_tokenizer = AutoTokenizer.from_pretrained('roberta-base')\n",
    "en_roberta_tokenizer([['first sentence', 'second sentence']])\n",
    "# {'input_ids': [[0, 9502, 3645, 2, 2, 10815, 3645, 2]],\n",
    "# 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1]]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 예제 3.14. attention_mask 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[0, 1656, 1141, 3135, 6265, 2073, 1599, 2062, 18, 2, 1, 1, 1, 1, 1, 1], [0, 864, 1141, 3135, 6265, 2073, 1656, 1141, 3135, 6265, 3632, 831, 647, 2062, 18, 2]], 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(['첫 번째 문장은 짧다.', '두 번째 문장은 첫 번째 문장 보다 더 길다.'], padding='longest')\n",
    "\n",
    "# {'input_ids': [[0, 1656, 1141, 3135, 6265, 2073, 1599, 2062, 18, 2, 1, 1, 1, 1, 1, 1],\n",
    "# [0, 864, 1141, 3135, 6265, 2073, 1656, 1141, 3135, 6265, 3632, 831, 647, 2062, 18, 2]],\n",
    "# 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0],\n",
    "# [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 데이터셋 활용하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 예제 3.15. KLUE MRC 데이터셋 다운로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in d:\\miniconda\\envs\\lang310\\lib\\site-packages (3.2.0)\n",
      "Requirement already satisfied: filelock in d:\\miniconda\\envs\\lang310\\lib\\site-packages (from datasets) (3.14.0)\n",
      "Requirement already satisfied: numpy>=1.17 in d:\\miniconda\\envs\\lang310\\lib\\site-packages (from datasets) (1.26.4)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in d:\\miniconda\\envs\\lang310\\lib\\site-packages (from datasets) (16.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in d:\\miniconda\\envs\\lang310\\lib\\site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in d:\\miniconda\\envs\\lang310\\lib\\site-packages (from datasets) (2.2.2)\n",
      "Requirement already satisfied: requests>=2.32.2 in d:\\miniconda\\envs\\lang310\\lib\\site-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in d:\\miniconda\\envs\\lang310\\lib\\site-packages (from datasets) (4.67.1)\n",
      "Requirement already satisfied: xxhash in d:\\miniconda\\envs\\lang310\\lib\\site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in d:\\miniconda\\envs\\lang310\\lib\\site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in d:\\miniconda\\envs\\lang310\\lib\\site-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets) (2024.3.1)\n",
      "Requirement already satisfied: aiohttp in d:\\miniconda\\envs\\lang310\\lib\\site-packages (from datasets) (3.9.5)\n",
      "Requirement already satisfied: huggingface-hub>=0.23.0 in d:\\miniconda\\envs\\lang310\\lib\\site-packages (from datasets) (0.27.1)\n",
      "Requirement already satisfied: packaging in d:\\miniconda\\envs\\lang310\\lib\\site-packages (from datasets) (24.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in d:\\miniconda\\envs\\lang310\\lib\\site-packages (from datasets) (6.0.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in d:\\miniconda\\envs\\lang310\\lib\\site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in d:\\miniconda\\envs\\lang310\\lib\\site-packages (from aiohttp->datasets) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in d:\\miniconda\\envs\\lang310\\lib\\site-packages (from aiohttp->datasets) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in d:\\miniconda\\envs\\lang310\\lib\\site-packages (from aiohttp->datasets) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in d:\\miniconda\\envs\\lang310\\lib\\site-packages (from aiohttp->datasets) (1.9.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in d:\\miniconda\\envs\\lang310\\lib\\site-packages (from aiohttp->datasets) (4.0.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in d:\\miniconda\\envs\\lang310\\lib\\site-packages (from huggingface-hub>=0.23.0->datasets) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in d:\\miniconda\\envs\\lang310\\lib\\site-packages (from requests>=2.32.2->datasets) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\miniconda\\envs\\lang310\\lib\\site-packages (from requests>=2.32.2->datasets) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\miniconda\\envs\\lang310\\lib\\site-packages (from requests>=2.32.2->datasets) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\miniconda\\envs\\lang310\\lib\\site-packages (from requests>=2.32.2->datasets) (2024.2.2)\n",
      "Requirement already satisfied: colorama in d:\\miniconda\\envs\\lang310\\lib\\site-packages (from tqdm>=4.66.3->datasets) (0.4.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in d:\\miniconda\\envs\\lang310\\lib\\site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in d:\\miniconda\\envs\\lang310\\lib\\site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in d:\\miniconda\\envs\\lang310\\lib\\site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: six>=1.5 in d:\\miniconda\\envs\\lang310\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Error parsing dependencies of celery: Expected matching RIGHT_PARENTHESIS for LEFT_PARENTHESIS, after version specifier\n",
      "    pytz (>dev)\n",
      "         ~^\n"
     ]
    }
   ],
   "source": [
    "%pip install --upgrade datasets\n",
    "# 커널재시작"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "klue_mrc_dataset = load_dataset('klue', 'mrc')\n",
    "# klue_mrc_dataset_only_train = load_dataset('klue', 'mrc', split='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['title', 'context', 'news_category', 'source', 'guid', 'is_impossible', 'question_type', 'question', 'answers'],\n",
       "        num_rows: 17554\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['title', 'context', 'news_category', 'source', 'guid', 'is_impossible', 'question_type', 'question', 'answers'],\n",
       "        num_rows: 5841\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "klue_mrc_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'title': '제주도 장마 시작 … 중부는 이달 말부터',\n",
       " 'context': '올여름 장마가 17일 제주도에서 시작됐다. 서울 등 중부지방은 예년보다 사나흘 정도 늦은 이달 말께 장마가 시작될 전망이다.17일 기상청에 따르면 제주도 남쪽 먼바다에 있는 장마전선의 영향으로 이날 제주도 산간 및 내륙지역에 호우주의보가 내려지면서 곳곳에 100㎜에 육박하는 많은 비가 내렸다. 제주의 장마는 평년보다 2~3일, 지난해보다는 하루 일찍 시작됐다. 장마는 고온다습한 북태평양 기단과 한랭 습윤한 오호츠크해 기단이 만나 형성되는 장마전선에서 내리는 비를 뜻한다.장마전선은 18일 제주도 먼 남쪽 해상으로 내려갔다가 20일께 다시 북상해 전남 남해안까지 영향을 줄 것으로 보인다. 이에 따라 20~21일 남부지방에도 예년보다 사흘 정도 장마가 일찍 찾아올 전망이다. 그러나 장마전선을 밀어올리는 북태평양 고기압 세력이 약해 서울 등 중부지방은 평년보다 사나흘가량 늦은 이달 말부터 장마가 시작될 것이라는 게 기상청의 설명이다. 장마전선은 이후 한 달가량 한반도 중남부를 오르내리며 곳곳에 비를 뿌릴 전망이다. 최근 30년간 평균치에 따르면 중부지방의 장마 시작일은 6월24~25일이었으며 장마기간은 32일, 강수일수는 17.2일이었다.기상청은 올해 장마기간의 평균 강수량이 350~400㎜로 평년과 비슷하거나 적을 것으로 내다봤다. 브라질 월드컵 한국과 러시아의 경기가 열리는 18일 오전 서울은 대체로 구름이 많이 끼지만 비는 오지 않을 것으로 예상돼 거리 응원에는 지장이 없을 전망이다.',\n",
       " 'news_category': '종합',\n",
       " 'source': 'hankyung',\n",
       " 'guid': 'klue-mrc-v1_train_12759',\n",
       " 'is_impossible': False,\n",
       " 'question_type': 1,\n",
       " 'question': '북태평양 기단과 오호츠크해 기단이 만나 국내에 머무르는 기간은?',\n",
       " 'answers': {'answer_start': [478, 478], 'text': ['한 달가량', '한 달']}}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "klue_mrc_dataset['train'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 모델 학습시키기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 데이터 준비\n",
    "\n",
    "- 예제 3.17. 모델 학습에 사용할 연합뉴스 데이터셋 다운로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['guid', 'title', 'label', 'url', 'date'],\n",
       "    num_rows: 45678\n",
       "})"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "klue_tc_train = load_dataset('klue', 'ynat', split='train')\n",
    "klue_tc_eval = load_dataset('klue', 'ynat', split='validation')\n",
    "klue_tc_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'guid': 'ynat-v1_train_00000',\n",
       " 'title': '유튜브 내달 2일까지 크리에이터 지원 공간 운영',\n",
       " 'label': 3,\n",
       " 'url': 'https://news.naver.com/main/read.nhn?mode=LS2D&mid=shm&sid1=105&sid2=227&oid=001&aid=0008508947',\n",
       " 'date': '2016.06.30. 오전 10:36'}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "klue_tc_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['IT과학', '경제', '사회', '생활문화', '세계', '스포츠', '정치']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "klue_tc_train.features['label'].names\n",
    "# ['IT과학', '경제', '사회', '생활문화', '세계', '스포츠', '정치']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 예제 3.18. 실습에 사용하지 않는 불필요한 컬럼 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['title', 'label'],\n",
       "    num_rows: 45678\n",
       "})"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "klue_tc_train = klue_tc_train.remove_columns(['guid', 'url', 'date'])\n",
    "klue_tc_eval = klue_tc_eval.remove_columns(['guid', 'url', 'date'])\n",
    "klue_tc_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'title': '유튜브 내달 2일까지 크리에이터 지원 공간 운영', 'label': 3}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "klue_tc_train[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 예제 3.19. 카테고리를 문자로 표기한 label_str 컬럼 추가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ClassLabel(names=['IT과학', '경제', '사회', '생활문화', '세계', '스포츠', '정치'], id=None)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "klue_tc_train.features['label']\n",
    "# ClassLabel(names=['IT과학', '경제', '사회', '생활문화', '세계', '스포츠', '정치'], id=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'경제'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "klue_tc_train.features['label'].int2str(1)\n",
    "# '경제'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "klue_tc_label = klue_tc_train.features['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'title': '유튜브 내달 2일까지 크리에이터 지원 공간 운영', 'label': 3, 'label_str': '생활문화'}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def make_str_label(batch):\n",
    "  batch['label_str'] = klue_tc_label.int2str(batch['label'])\n",
    "  return batch\n",
    "\n",
    "klue_tc_train = klue_tc_train.map(make_str_label, batched=True, batch_size=1000)\n",
    "\n",
    "klue_tc_train[0]\n",
    "# {'title': '유튜브 내달 2일까지 크리에이터 지원 공간 운영', 'label': 3, 'label_str': '생활문화'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 예제 3.20. 학습/검증/테스트 데이터셋 분할"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['title', 'label', 'label_str'],\n",
       "    num_rows: 45678\n",
       "})"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "klue_tc_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = klue_tc_train.train_test_split(test_size=10000, shuffle=True, seed=42)['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['title', 'label', 'label_str'],\n",
       "    num_rows: 10000\n",
       "})"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'title': '신동엽 50주기 맞아 산문 전집·앤솔로지 출간', 'label': 3, 'label_str': '생활문화'}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = klue_tc_eval.train_test_split(test_size=1000, shuffle=True, seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['title', 'label'],\n",
       "        num_rows: 8107\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['title', 'label'],\n",
       "        num_rows: 1000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = dataset['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['title', 'label'],\n",
       "    num_rows: 1000\n",
       "})"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'title': 'UNIST 신현석 교수팀 초저유전율 절연체 비정질 질화붕소 개발', 'label': 0}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_dataset = dataset['train'].train_test_split(test_size=1000, shuffle=True, seed=42)['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['title', 'label'],\n",
       "    num_rows: 1000\n",
       "})"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'title': '고1 딸 자신이 근무한 학교로 전학시킨 부도덕한 교사', 'label': 2}"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 트레이너 API를 사용해 학습하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 예제 3.21. Trainer를 사용한 학습: (1) 준비"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 필요한 라이브러리 임포트\n",
    "import torch\n",
    "import numpy as np\n",
    "from transformers import (\n",
    "    Trainer,\n",
    "    TrainingArguments, \n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoTokenizer\n",
    ")\n",
    "\n",
    "# 토큰화 함수 정의\n",
    "def tokenize_function(examples):\n",
    "    # 입력 텍스트를 토큰화하고 패딩과 자르기 적용\n",
    "    return tokenizer(examples[\"title\"], padding=\"max_length\", truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\miniconda\\envs\\lang310\\lib\\site-packages\\huggingface_hub\\file_download.py:795: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at klue/roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# 사전학습된 모델과 토크나이저 로드\n",
    "model_id = \"klue/roberta-base\"\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_id, num_labels=len(train_dataset.features['label'].names))\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54b4893636784d07b59f1d9f933380a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# 데이터셋에 토큰화 적용\n",
    "train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
    "valid_dataset = valid_dataset.map(tokenize_function, batched=True) \n",
    "test_dataset = test_dataset.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 예제 3.22. Trainer를 사용한 학습: (2) 학습 인자와 평가 함수 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'d:\\\\drive\\\\SelfStudy\\\\ds4th_study\\\\source\\\\LLM을_활용한_실전_AI_애플리케이션_개발'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " D ����̺��� ����: HDD_Sub\n",
      " ���� �Ϸ� ��ȣ: BAFA-80F5\n",
      "\n",
      " d:\\drive\\SelfStudy\\ds4th_study\\source\\LLM��_Ȱ����_����_AI_���ø����̼�_���� ���͸�\n",
      "\n",
      "2025-01-17  ���� 10:05    <DIR>          .\n",
      "2025-01-11  ���� 12:34    <DIR>          ..\n",
      "2025-01-07  ���� 06:52         1,707,556 CH01_LLM_����_song.pdf\n",
      "2025-01-07  ���� 06:52         3,404,350 CH01_LLM_����_song.pptx\n",
      "2025-01-07  ���� 06:52            23,831 CH02_LLM��_����_Ʈ��������_��Ű��ó_���캸��_song.ipynb\n",
      "2025-01-17  ���� 10:48           127,604 CH03_Ʈ��������_����_�ٷ������_������̽�_Ʈ��������_���̺귯��_song.ipynb\n",
      "2025-01-07  ���� 06:52    <DIR>          llm-main\n",
      "2025-01-17  ���� 10:22    <DIR>          models\n",
      "               4�� ����           5,263,341 ����Ʈ\n",
      "               4�� ���͸�  421,030,371,328 ����Ʈ ����\n"
     ]
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d:\\drive\\SelfStudy\\ds4th_study\\source\\LLM을_활용한_실전_AI_애플리케이션_개발\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 예제 3.23. Trainer를 사용한 학습 - (3) 학습 진행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "\n",
    "# 결과 저장 경로 설정\n",
    "output_dir = \"./models\"\n",
    "\n",
    "# 디렉토리 생성 (기존에 같은 이름의 파일이 있을 경우 삭제 후 생성)\n",
    "if os.path.exists(output_dir):\n",
    "    if os.path.isfile(output_dir):\n",
    "        os.remove(output_dir)  # 파일 삭제\n",
    "        print(f\"{output_dir} 파일이 삭제되었습니다.\")\n",
    "    else:\n",
    "        print(f\"{output_dir} 디렉토리가 이미 존재합니다.\")\n",
    "else:\n",
    "    os.makedirs(output_dir)  # 디렉토리 생성\n",
    "    print(f\"{output_dir} 디렉토리가 생성되었습니다.\")\n",
    "\n",
    "# 학습 인자 설정\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir,          # 결과 저장 경로\n",
    "    num_train_epochs=1,              # 학습 에포크 수\n",
    "    per_device_train_batch_size=8,   # 학습 배치 크기\n",
    "    per_device_eval_batch_size=8,    # 평가 배치 크기 \n",
    "    evaluation_strategy=\"epoch\",      # 에포크마다 평가 수행\n",
    "    learning_rate=5e-5,              # 학습률\n",
    "    push_to_hub=False                # 허브에 모델 업로드 안 함\n",
    ")\n",
    "\n",
    "# 평가 메트릭 계산 함수 정의\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred                      # 예측값과 실제값 추출\n",
    "    predictions = np.argmax(logits, axis=-1)        # 예측 클래스 선택\n",
    "    return {\"accuracy\": (predictions == labels).mean()}  # 정확도 계산 및 반환\n",
    "\n",
    "# Trainer 객체 생성\n",
    "trainer = Trainer(\n",
    "    model=model,                     # 학습할 모델\n",
    "    args=training_args,              # 학습 인자\n",
    "    train_dataset=train_dataset,     # 학습 데이터셋\n",
    "    eval_dataset=valid_dataset,      # 검증 데이터셋\n",
    "    tokenizer=tokenizer,             # 토크나이저\n",
    "    compute_metrics=compute_metrics, # 평가 메트릭 계산 함수\n",
    ")\n",
    "\n",
    "# 모델 학습 수행\n",
    "trainer.train()\n",
    "\n",
    "# 테스트 데이터셋으로 모델 평가\n",
    "trainer.evaluate(test_dataset) # 정확도 0.84"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 트레이너 API를 사용하지 않고 학습하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 예제 3.24. Trainer를 사용하지 않는 학습: (1) 학습을 위한 모델과 토크나이저 준비"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\miniconda\\envs\\lang310\\lib\\site-packages\\huggingface_hub\\file_download.py:795: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at klue/roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RobertaForSequenceClassification(\n",
       "  (roberta): RobertaModel(\n",
       "    (embeddings): RobertaEmbeddings(\n",
       "      (word_embeddings): Embedding(32000, 768, padding_idx=1)\n",
       "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): RobertaEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (classifier): RobertaClassificationHead(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (out_proj): Linear(in_features=768, out_features=7, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# PyTorch 관련 라이브러리 임포트\n",
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AdamW\n",
    "\n",
    "# 토큰화 함수 정의\n",
    "def tokenize_function(examples): \n",
    "    # 제목(title) 컬럼을 토큰화하고 패딩과 자르기 적용\n",
    "    return tokenizer(examples[\"title\"], padding=\"max_length\", truncation=True)\n",
    "\n",
    "# GPU 사용 가능 여부 확인 및 디바이스 설정\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# 사전학습된 모델 ID 설정\n",
    "model_id = \"klue/roberta-base\"\n",
    "\n",
    "# 모델 초기화 - 레이블 수에 맞게 분류 헤드 설정\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_id, num_labels=len(train_dataset.features['label'].names))\n",
    "\n",
    "# 토크나이저 초기화\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "# 모델을 지정된 디바이스로 이동\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 예제 3.25 Trainer를 사용하지 않는 학습: (2) 학습을 위한 데이터 준비"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c7a7f3e181649ed9c024ca152bd577e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19fbb2b02d0145cf96a1d2b2771408fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 데이터로더를 생성하는 함수 정의\n",
    "def make_dataloader(dataset, batch_size, shuffle=True):\n",
    "    # 데이터셋의 텍스트를 토큰화하고 PyTorch 텐서 형식으로 변환\n",
    "    dataset = dataset.map(tokenize_function, batched=True).with_format(\"torch\") \n",
    "    \n",
    "    # 'label' 컬럼명을 모델이 인식할 수 있도록 'labels'로 변경\n",
    "    dataset = dataset.rename_column(\"label\", \"labels\") \n",
    "    \n",
    "    # 토큰화된 데이터만 필요하므로 원본 'title' 컬럼 제거\n",
    "    dataset = dataset.remove_columns(column_names=['title']) \n",
    "    \n",
    "    # DataLoader 객체 생성하여 반환\n",
    "    return DataLoader(dataset, batch_size=batch_size, shuffle=shuffle)\n",
    "\n",
    "# 학습용 데이터로더 생성 (배치 크기=8, 셔플=True)\n",
    "train_dataloader = make_dataloader(train_dataset, batch_size=8, shuffle=True)\n",
    "\n",
    "# 검증용 데이터로더 생성 (배치 크기=8, 셔플=False) \n",
    "valid_dataloader = make_dataloader(valid_dataset, batch_size=8, shuffle=False)\n",
    "\n",
    "# 테스트용 데이터로더 생성 (배치 크기=8, 셔플=False)\n",
    "test_dataloader = make_dataloader(test_dataset, batch_size=8, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 예제 3.26. Trainer를 사용하지 않는 학습: (3) 학습을 위한 함수 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, data_loader, optimizer):\n",
    "    \"\"\"\n",
    "    한 에포크 동안 모델을 학습시키는 함수\n",
    "    \n",
    "    Args:\n",
    "        model: 학습할 모델\n",
    "        data_loader: 학습 데이터를 배치 단위로 제공하는 DataLoader 객체\n",
    "        optimizer: 모델 파라미터를 업데이트할 옵티마이저\n",
    "        \n",
    "    Returns:\n",
    "        avg_loss: 에포크의 평균 손실값\n",
    "    \"\"\"\n",
    "    model.train()  # 모델을 학습 모드로 설정\n",
    "    total_loss = 0  # 전체 손실값을 누적할 변수 초기화\n",
    "    \n",
    "    # 배치 단위로 데이터 처리\n",
    "    for batch in tqdm(data_loader):\n",
    "        optimizer.zero_grad()  # 이전 배치의 그래디언트 초기화\n",
    "        \n",
    "        # 배치 데이터를 GPU로 이동\n",
    "        input_ids = batch['input_ids'].to(device)  # 입력 텍스트의 토큰 ID\n",
    "        attention_mask = batch['attention_mask'].to(device)  # 패딩 토큰을 구분하기 위한 마스크\n",
    "        labels = batch['labels'].to(device)  # 정답 레이블\n",
    "        \n",
    "        # 순전파 수행\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss  # 손실값 계산\n",
    "        \n",
    "        # 역전파 및 가중치 업데이트\n",
    "        loss.backward()  # 그래디언트 계산\n",
    "        optimizer.step()  # 모델 파라미터 업데이트\n",
    "        \n",
    "        total_loss += loss.item()  # 배치의 손실값 누적\n",
    "    \n",
    "    # 평균 손실값 계산\n",
    "    avg_loss = total_loss / len(data_loader)\n",
    "    return avg_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 예제 3.27. Trainer를 사용하지 않는 학습: (4) 평가를 위한 함수 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, data_loader):\n",
    "    # 모델을 평가 모드로 설정\n",
    "    model.eval()\n",
    "    \n",
    "    # 손실과 예측값을 저장할 변수 초기화\n",
    "    total_loss = 0\n",
    "    predictions = []\n",
    "    true_labels = []\n",
    "    \n",
    "    # 그래디언트 계산 비활성화\n",
    "    with torch.no_grad():\n",
    "        # 데이터로더에서 배치 단위로 데이터 로드\n",
    "        for batch in tqdm(data_loader):\n",
    "            # 입력 데이터를 GPU로 이동\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            \n",
    "            # 모델 예측 수행\n",
    "            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            logits = outputs.logits\n",
    "            loss = outputs.loss\n",
    "            \n",
    "            # 배치의 손실 값을 누적\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            # 예측값 계산 (가장 높은 확률을 가진 클래스)\n",
    "            preds = torch.argmax(logits, dim=-1)\n",
    "            \n",
    "            # 예측값과 실제 레이블을 CPU로 이동하여 리스트에 추가\n",
    "            predictions.extend(preds.cpu().numpy())\n",
    "            true_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    # 평균 손실 계산\n",
    "    avg_loss = total_loss / len(data_loader)\n",
    "    \n",
    "    # 정확도 계산\n",
    "    accuracy = np.mean(np.array(predictions) == np.array(true_labels))\n",
    "    \n",
    "    return avg_loss, accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 예제 3.28 Trainer를 사용하지 않는 학습: (5) 학습 수행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\miniconda\\envs\\lang310\\lib\\site-packages\\transformers\\optimization.py:521: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8adb7f829224a6fbd45021b8910c31b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 0.6708768357306719\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4618c97efe2644a9b8201d81850291a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/125 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 0.6471084080338478\n",
      "Validation accuracy: 0.794\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea1c617df59c468a9c747bf56f4d66a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/125 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 0.815\n"
     ]
    }
   ],
   "source": [
    "# 학습할 에포크 수 설정\n",
    "num_epochs = 1\n",
    "# AdamW 옵티마이저 초기화 - 학습률 5e-5 설정\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "# 학습 루프 시작\n",
    "for epoch in range(num_epochs):\n",
    "    # 현재 에포크 출력\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "    # train_epoch 함수를 호출하여 한 에포크 학습 수행하고 학습 손실 값 반환\n",
    "    train_loss = train_epoch(model, train_dataloader, optimizer)\n",
    "    print(f\"Training loss: {train_loss}\")\n",
    "    # evaluate 함수를 호출하여 검증 데이터에 대한 손실과 정확도 계산\n",
    "    valid_loss, valid_accuracy = evaluate(model, valid_dataloader)\n",
    "    print(f\"Validation loss: {valid_loss}\")\n",
    "    print(f\"Validation accuracy: {valid_accuracy}\")\n",
    "\n",
    "# 테스트 데이터에 대한 최종 평가\n",
    "# evaluate 함수를 호출하여 테스트 정확도 계산 (손실은 사용하지 않음)\n",
    "_, test_accuracy = evaluate(model, test_dataloader)\n",
    "print(f\"Test accuracy: {test_accuracy}\") # 정확도 0.82"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 학습한 모델 업로드하기\n",
    "\n",
    "- 예제 3.29. 허깅페이스 허브에 모델 업로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델의 예측 아이디와 문자열 레이블을 연결할 데이터를 모델 config에 저장\n",
    "id2label = {i: label for i, label in enumerate(train_dataset.features['label'].names)}\n",
    "label2id = {label: i for i, label in id2label.items()}\n",
    "model.config.id2label = id2label\n",
    "model.config.label2id = label2id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "# 토큰 입력 받기\n",
    "token = input(\"허깅페이스 토큰을 입력하세요: \")\n",
    "login(token=token)\n",
    "\n",
    "# 사용자 아이디 입력 받기\n",
    "user_id = input(\"허깅페이스 사용자 아이디를 입력하세요: \")\n",
    "repo_id = f\"{user_id}/roberta-base-klue-ynat-classification\"\n",
    "\n",
    "# Trainer를 사용한 경우\n",
    "trainer.push_to_hub(repo_id)\n",
    "# 직접 학습한 경우\n",
    "# model.push_to_hub(repo_id)\n",
    "tokenizer.push_to_hub(repo_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 모델 추론하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 파이프라인을 활용한 추론\n",
    "\n",
    "- 예제 3.30. 학습한 모델을 불러와 pipeline을 활용해 추론하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "#실습을 새롭게 시작하는 경우 데이터셋 다시 불러오기 실행\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"klue\", \"ynat\", split=\"validation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tf-keras\n",
      "  Downloading tf_keras-2.18.0-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting tensorflow<2.19,>=2.18 (from tf-keras)\n",
      "  Downloading tensorflow-2.18.0-cp310-cp310-win_amd64.whl.metadata (3.3 kB)\n",
      "Collecting tensorflow-intel==2.18.0 (from tensorflow<2.19,>=2.18->tf-keras)\n",
      "  Downloading tensorflow_intel-2.18.0-cp310-cp310-win_amd64.whl.metadata (4.9 kB)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in d:\\miniconda\\envs\\lang310\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (2.1.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in d:\\miniconda\\envs\\lang310\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in d:\\miniconda\\envs\\lang310\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (24.3.25)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in d:\\miniconda\\envs\\lang310\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (0.4.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in d:\\miniconda\\envs\\lang310\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (0.2.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in d:\\miniconda\\envs\\lang310\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (18.1.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in d:\\miniconda\\envs\\lang310\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (3.3.0)\n",
      "Requirement already satisfied: packaging in d:\\miniconda\\envs\\lang310\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (24.0)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in d:\\miniconda\\envs\\lang310\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (4.23.4)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in d:\\miniconda\\envs\\lang310\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (2.32.3)\n",
      "Requirement already satisfied: setuptools in d:\\miniconda\\envs\\lang310\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (69.5.1)\n",
      "Requirement already satisfied: six>=1.12.0 in d:\\miniconda\\envs\\lang310\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in d:\\miniconda\\envs\\lang310\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (2.4.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in d:\\miniconda\\envs\\lang310\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (4.12.2)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in d:\\miniconda\\envs\\lang310\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (1.12.1)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in d:\\miniconda\\envs\\lang310\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (1.63.0)\n",
      "Collecting tensorboard<2.19,>=2.18 (from tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras)\n",
      "  Downloading tensorboard-2.18.0-py3-none-any.whl.metadata (1.6 kB)\n",
      "Requirement already satisfied: keras>=3.5.0 in d:\\miniconda\\envs\\lang310\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (3.6.0)\n",
      "Requirement already satisfied: numpy<2.1.0,>=1.26.0 in d:\\miniconda\\envs\\lang310\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (1.26.4)\n",
      "Collecting h5py>=3.11.0 (from tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras)\n",
      "  Downloading h5py-3.12.1-cp310-cp310-win_amd64.whl.metadata (2.5 kB)\n",
      "Collecting ml-dtypes<0.5.0,>=0.4.0 (from tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras)\n",
      "  Downloading ml_dtypes-0.4.1-cp310-cp310-win_amd64.whl.metadata (20 kB)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in d:\\miniconda\\envs\\lang310\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (0.31.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in d:\\miniconda\\envs\\lang310\\lib\\site-packages (from astunparse>=1.6.0->tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (0.43.0)\n",
      "Requirement already satisfied: rich in d:\\miniconda\\envs\\lang310\\lib\\site-packages (from keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (13.7.1)\n",
      "Requirement already satisfied: namex in d:\\miniconda\\envs\\lang310\\lib\\site-packages (from keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (0.0.8)\n",
      "Requirement already satisfied: optree in d:\\miniconda\\envs\\lang310\\lib\\site-packages (from keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (0.13.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in d:\\miniconda\\envs\\lang310\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\miniconda\\envs\\lang310\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\miniconda\\envs\\lang310\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\miniconda\\envs\\lang310\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (2024.2.2)\n",
      "Requirement already satisfied: markdown>=2.6.8 in d:\\miniconda\\envs\\lang310\\lib\\site-packages (from tensorboard<2.19,>=2.18->tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (3.6)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in d:\\miniconda\\envs\\lang310\\lib\\site-packages (from tensorboard<2.19,>=2.18->tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in d:\\miniconda\\envs\\lang310\\lib\\site-packages (from tensorboard<2.19,>=2.18->tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (3.0.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in d:\\miniconda\\envs\\lang310\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (2.1.5)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in d:\\miniconda\\envs\\lang310\\lib\\site-packages (from rich->keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in d:\\miniconda\\envs\\lang310\\lib\\site-packages (from rich->keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (2.17.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in d:\\miniconda\\envs\\lang310\\lib\\site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (0.1.2)\n",
      "Downloading tf_keras-2.18.0-py3-none-any.whl (1.7 MB)\n",
      "   ---------------------------------------- 0.0/1.7 MB ? eta -:--:--\n",
      "   ---------------------------------------- 1.7/1.7 MB 18.8 MB/s eta 0:00:00\n",
      "Downloading tensorflow-2.18.0-cp310-cp310-win_amd64.whl (7.5 kB)\n",
      "Downloading tensorflow_intel-2.18.0-cp310-cp310-win_amd64.whl (390.0 MB)\n",
      "   ---------------------------------------- 0.0/390.0 MB ? eta -:--:--\n",
      "   ---------------------------------------- 3.9/390.0 MB 19.6 MB/s eta 0:00:20\n",
      "    --------------------------------------- 7.9/390.0 MB 18.7 MB/s eta 0:00:21\n",
      "   - -------------------------------------- 12.1/390.0 MB 19.4 MB/s eta 0:00:20\n",
      "   - -------------------------------------- 16.0/390.0 MB 19.0 MB/s eta 0:00:20\n",
      "   -- ------------------------------------- 19.9/390.0 MB 19.1 MB/s eta 0:00:20\n",
      "   -- ------------------------------------- 23.9/390.0 MB 19.1 MB/s eta 0:00:20\n",
      "   -- ------------------------------------- 27.8/390.0 MB 19.2 MB/s eta 0:00:19\n",
      "   --- ------------------------------------ 31.7/390.0 MB 19.2 MB/s eta 0:00:19\n",
      "   --- ------------------------------------ 35.9/390.0 MB 19.2 MB/s eta 0:00:19\n",
      "   ---- ----------------------------------- 39.6/390.0 MB 19.1 MB/s eta 0:00:19\n",
      "   ---- ----------------------------------- 43.5/390.0 MB 19.1 MB/s eta 0:00:19\n",
      "   ---- ----------------------------------- 47.4/390.0 MB 19.1 MB/s eta 0:00:18\n",
      "   ----- ---------------------------------- 51.6/390.0 MB 19.1 MB/s eta 0:00:18\n",
      "   ----- ---------------------------------- 55.1/390.0 MB 19.0 MB/s eta 0:00:18\n",
      "   ------ --------------------------------- 59.0/390.0 MB 19.0 MB/s eta 0:00:18\n",
      "   ------ --------------------------------- 62.9/390.0 MB 19.0 MB/s eta 0:00:18\n",
      "   ------ --------------------------------- 66.8/390.0 MB 19.0 MB/s eta 0:00:17\n",
      "   ------- -------------------------------- 71.0/390.0 MB 19.0 MB/s eta 0:00:17\n",
      "   ------- -------------------------------- 75.0/390.0 MB 19.0 MB/s eta 0:00:17\n",
      "   -------- ------------------------------- 78.9/390.0 MB 19.1 MB/s eta 0:00:17\n",
      "   -------- ------------------------------- 81.8/390.0 MB 18.8 MB/s eta 0:00:17\n",
      "   -------- ------------------------------- 85.7/390.0 MB 18.9 MB/s eta 0:00:17\n",
      "   --------- ------------------------------ 89.4/390.0 MB 18.8 MB/s eta 0:00:17\n",
      "   --------- ------------------------------ 90.4/390.0 MB 18.6 MB/s eta 0:00:17\n",
      "   --------- ------------------------------ 93.3/390.0 MB 18.0 MB/s eta 0:00:17\n",
      "   --------- ------------------------------ 97.3/390.0 MB 18.1 MB/s eta 0:00:17\n",
      "   ---------- ---------------------------- 101.2/390.0 MB 18.1 MB/s eta 0:00:16\n",
      "   ---------- ---------------------------- 105.1/390.0 MB 18.1 MB/s eta 0:00:16\n",
      "   ---------- ---------------------------- 109.1/390.0 MB 18.2 MB/s eta 0:00:16\n",
      "   ----------- --------------------------- 113.0/390.0 MB 18.2 MB/s eta 0:00:16\n",
      "   ----------- --------------------------- 116.9/390.0 MB 18.3 MB/s eta 0:00:15\n",
      "   ------------ -------------------------- 120.8/390.0 MB 18.3 MB/s eta 0:00:15\n",
      "   ------------ -------------------------- 124.8/390.0 MB 18.3 MB/s eta 0:00:15\n",
      "   ------------ -------------------------- 128.7/390.0 MB 18.4 MB/s eta 0:00:15\n",
      "   ------------- ------------------------- 132.9/390.0 MB 18.4 MB/s eta 0:00:14\n",
      "   ------------- ------------------------- 136.8/390.0 MB 18.4 MB/s eta 0:00:14\n",
      "   -------------- ------------------------ 140.8/390.0 MB 18.4 MB/s eta 0:00:14\n",
      "   -------------- ------------------------ 144.7/390.0 MB 18.4 MB/s eta 0:00:14\n",
      "   -------------- ------------------------ 148.6/390.0 MB 18.4 MB/s eta 0:00:14\n",
      "   --------------- ----------------------- 152.6/390.0 MB 18.5 MB/s eta 0:00:13\n",
      "   --------------- ----------------------- 156.2/390.0 MB 18.5 MB/s eta 0:00:13\n",
      "   ---------------- ---------------------- 160.4/390.0 MB 18.5 MB/s eta 0:00:13\n",
      "   ---------------- ---------------------- 164.4/390.0 MB 18.5 MB/s eta 0:00:13\n",
      "   ---------------- ---------------------- 168.3/390.0 MB 18.5 MB/s eta 0:00:12\n",
      "   ----------------- --------------------- 172.2/390.0 MB 18.5 MB/s eta 0:00:12\n",
      "   ----------------- --------------------- 176.2/390.0 MB 18.5 MB/s eta 0:00:12\n",
      "   ------------------ -------------------- 180.1/390.0 MB 18.6 MB/s eta 0:00:12\n",
      "   ------------------ -------------------- 184.0/390.0 MB 18.6 MB/s eta 0:00:12\n",
      "   ------------------ -------------------- 188.2/390.0 MB 18.6 MB/s eta 0:00:11\n",
      "   ------------------- ------------------- 191.9/390.0 MB 18.6 MB/s eta 0:00:11\n",
      "   ------------------- ------------------- 195.6/390.0 MB 18.5 MB/s eta 0:00:11\n",
      "   ------------------- ------------------- 199.0/390.0 MB 18.5 MB/s eta 0:00:11\n",
      "   -------------------- ------------------ 201.3/390.0 MB 18.4 MB/s eta 0:00:11\n",
      "   -------------------- ------------------ 205.3/390.0 MB 18.4 MB/s eta 0:00:11\n",
      "   -------------------- ------------------ 209.2/390.0 MB 18.4 MB/s eta 0:00:10\n",
      "   --------------------- ----------------- 213.1/390.0 MB 18.4 MB/s eta 0:00:10\n",
      "   --------------------- ----------------- 217.1/390.0 MB 18.4 MB/s eta 0:00:10\n",
      "   ---------------------- ---------------- 221.0/390.0 MB 18.4 MB/s eta 0:00:10\n",
      "   ---------------------- ---------------- 224.9/390.0 MB 18.5 MB/s eta 0:00:09\n",
      "   ---------------------- ---------------- 228.9/390.0 MB 18.4 MB/s eta 0:00:09\n",
      "   ----------------------- --------------- 232.8/390.0 MB 18.5 MB/s eta 0:00:09\n",
      "   ----------------------- --------------- 236.7/390.0 MB 18.5 MB/s eta 0:00:09\n",
      "   ------------------------ -------------- 240.6/390.0 MB 18.5 MB/s eta 0:00:09\n",
      "   ------------------------ -------------- 244.8/390.0 MB 18.5 MB/s eta 0:00:08\n",
      "   ------------------------ -------------- 248.5/390.0 MB 18.5 MB/s eta 0:00:08\n",
      "   ------------------------- ------------- 252.7/390.0 MB 18.5 MB/s eta 0:00:08\n",
      "   ------------------------- ------------- 256.4/390.0 MB 18.5 MB/s eta 0:00:08\n",
      "   -------------------------- ------------ 260.6/390.0 MB 18.5 MB/s eta 0:00:07\n",
      "   -------------------------- ------------ 264.5/390.0 MB 18.5 MB/s eta 0:00:07\n",
      "   -------------------------- ------------ 267.4/390.0 MB 18.5 MB/s eta 0:00:07\n",
      "   --------------------------- ----------- 271.3/390.0 MB 18.5 MB/s eta 0:00:07\n",
      "   --------------------------- ----------- 275.3/390.0 MB 18.5 MB/s eta 0:00:07\n",
      "   --------------------------- ----------- 279.2/390.0 MB 18.5 MB/s eta 0:00:06\n",
      "   ---------------------------- ---------- 283.1/390.0 MB 18.5 MB/s eta 0:00:06\n",
      "   ---------------------------- ---------- 287.0/390.0 MB 18.5 MB/s eta 0:00:06\n",
      "   ----------------------------- --------- 291.0/390.0 MB 18.5 MB/s eta 0:00:06\n",
      "   ----------------------------- --------- 294.9/390.0 MB 18.5 MB/s eta 0:00:06\n",
      "   ----------------------------- --------- 299.1/390.0 MB 18.5 MB/s eta 0:00:05\n",
      "   ------------------------------ -------- 303.0/390.0 MB 18.5 MB/s eta 0:00:05\n",
      "   ------------------------------ -------- 307.0/390.0 MB 18.5 MB/s eta 0:00:05\n",
      "   ------------------------------- ------- 310.4/390.0 MB 18.4 MB/s eta 0:00:05\n",
      "   ------------------------------- ------- 312.5/390.0 MB 18.3 MB/s eta 0:00:05\n",
      "   ------------------------------- ------- 316.7/390.0 MB 18.4 MB/s eta 0:00:04\n",
      "   -------------------------------- ------ 320.6/390.0 MB 18.4 MB/s eta 0:00:04\n",
      "   -------------------------------- ------ 324.5/390.0 MB 18.4 MB/s eta 0:00:04\n",
      "   -------------------------------- ------ 328.5/390.0 MB 18.3 MB/s eta 0:00:04\n",
      "   --------------------------------- ----- 332.4/390.0 MB 18.3 MB/s eta 0:00:04\n",
      "   --------------------------------- ----- 336.3/390.0 MB 18.3 MB/s eta 0:00:03\n",
      "   ---------------------------------- ---- 340.3/390.0 MB 18.3 MB/s eta 0:00:03\n",
      "   ---------------------------------- ---- 343.7/390.0 MB 18.4 MB/s eta 0:00:03\n",
      "   ---------------------------------- ---- 346.6/390.0 MB 18.3 MB/s eta 0:00:03\n",
      "   ----------------------------------- --- 350.5/390.0 MB 18.3 MB/s eta 0:00:03\n",
      "   ----------------------------------- --- 354.4/390.0 MB 18.6 MB/s eta 0:00:02\n",
      "   ----------------------------------- --- 358.1/390.0 MB 18.6 MB/s eta 0:00:02\n",
      "   ------------------------------------ -- 362.0/390.0 MB 18.6 MB/s eta 0:00:02\n",
      "   ------------------------------------ -- 366.0/390.0 MB 18.6 MB/s eta 0:00:02\n",
      "   ------------------------------------- - 370.1/390.0 MB 18.6 MB/s eta 0:00:02\n",
      "   ------------------------------------- - 374.1/390.0 MB 18.6 MB/s eta 0:00:01\n",
      "   ------------------------------------- - 378.0/390.0 MB 18.6 MB/s eta 0:00:01\n",
      "   --------------------------------------  381.9/390.0 MB 18.6 MB/s eta 0:00:01\n",
      "   --------------------------------------  385.9/390.0 MB 18.6 MB/s eta 0:00:01\n",
      "   --------------------------------------  389.8/390.0 MB 18.6 MB/s eta 0:00:01\n",
      "   --------------------------------------  389.8/390.0 MB 18.6 MB/s eta 0:00:01\n",
      "   --------------------------------------  389.8/390.0 MB 18.6 MB/s eta 0:00:01\n",
      "   --------------------------------------  389.8/390.0 MB 18.6 MB/s eta 0:00:01\n",
      "   --------------------------------------- 390.0/390.0 MB 17.6 MB/s eta 0:00:00\n",
      "Downloading h5py-3.12.1-cp310-cp310-win_amd64.whl (3.0 MB)\n",
      "   ---------------------------------------- 0.0/3.0 MB ? eta -:--:--\n",
      "   ---------------------------------------- 3.0/3.0 MB 17.5 MB/s eta 0:00:00\n",
      "Downloading ml_dtypes-0.4.1-cp310-cp310-win_amd64.whl (126 kB)\n",
      "Downloading tensorboard-2.18.0-py3-none-any.whl (5.5 MB)\n",
      "   ---------------------------------------- 0.0/5.5 MB ? eta -:--:--\n",
      "   ------------------------------ --------- 4.2/5.5 MB 19.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 5.5/5.5 MB 18.6 MB/s eta 0:00:00\n",
      "Installing collected packages: ml-dtypes, h5py, tensorboard, tensorflow-intel, tensorflow, tf-keras\n",
      "  Attempting uninstall: ml-dtypes\n",
      "    Found existing installation: ml-dtypes 0.3.2\n",
      "    Uninstalling ml-dtypes-0.3.2:\n",
      "      Successfully uninstalled ml-dtypes-0.3.2\n",
      "  Attempting uninstall: h5py\n",
      "    Found existing installation: h5py 3.10.0\n",
      "    Uninstalling h5py-3.10.0:\n",
      "      Successfully uninstalled h5py-3.10.0\n",
      "  Attempting uninstall: tensorboard\n",
      "    Found existing installation: tensorboard 2.17.1\n",
      "    Uninstalling tensorboard-2.17.1:\n",
      "      Successfully uninstalled tensorboard-2.17.1\n",
      "  Attempting uninstall: tensorflow-intel\n",
      "    Found existing installation: tensorflow-intel 2.17.0\n",
      "    Uninstalling tensorflow-intel-2.17.0:\n",
      "      Successfully uninstalled tensorflow-intel-2.17.0\n",
      "  Attempting uninstall: tensorflow\n",
      "    Found existing installation: tensorflow 2.13.1\n",
      "    Uninstalling tensorflow-2.13.1:\n",
      "      Successfully uninstalled tensorflow-2.13.1\n",
      "Successfully installed h5py-3.12.1 ml-dtypes-0.4.1 tensorboard-2.18.0 tensorflow-2.18.0 tensorflow-intel-2.18.0 tf-keras-2.18.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Error parsing dependencies of celery: Expected matching RIGHT_PARENTHESIS for LEFT_PARENTHESIS, after version specifier\n",
      "    pytz (>dev)\n",
      "         ~^\n",
      "  WARNING: Failed to remove contents in a temporary directory 'D:\\miniconda\\envs\\lang310\\Lib\\site-packages\\~l_dtypes'.\n",
      "  You can safely remove it manually.\n",
      "  WARNING: Failed to remove contents in a temporary directory 'D:\\miniconda\\envs\\lang310\\Lib\\site-packages\\~5py'.\n",
      "  You can safely remove it manually.\n",
      "  WARNING: Failed to remove contents in a temporary directory 'D:\\miniconda\\envs\\lang310\\Lib\\site-packages\\~~nsorflow'.\n",
      "  You can safely remove it manually.\n",
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "autotrain-advanced 0.7.77 requires datasets[vision]~=2.19.0, but you have datasets 3.2.0 which is incompatible.\n",
      "autotrain-advanced 0.7.77 requires huggingface-hub==0.22.2, but you have huggingface-hub 0.27.1 which is incompatible.\n",
      "autotrain-advanced 0.7.77 requires pydantic==2.7.1, but you have pydantic 2.9.2 which is incompatible.\n",
      "autotrain-advanced 0.7.77 requires requests==2.31.0, but you have requests 2.32.3 which is incompatible.\n",
      "autotrain-advanced 0.7.77 requires tensorboard==2.16.2, but you have tensorboard 2.18.0 which is incompatible.\n",
      "autotrain-advanced 0.7.77 requires tiktoken==0.6.0, but you have tiktoken 0.7.0 which is incompatible.\n",
      "autotrain-advanced 0.7.77 requires tqdm==4.66.2, but you have tqdm 4.67.1 which is incompatible.\n",
      "pykospacing 0.5 requires h5py==3.10.0, but you have h5py 3.12.1 which is incompatible.\n",
      "pykospacing 0.5 requires tensorflow==2.15.1, but you have tensorflow 2.18.0 which is incompatible.\n"
     ]
    }
   ],
   "source": [
    "%pip install tf-keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': '정치', 'score': 0.9410326480865479},\n",
       " {'label': '생활문화', 'score': 0.946600079536438},\n",
       " {'label': '사회', 'score': 0.7741270661354065},\n",
       " {'label': '경제', 'score': 0.7144867181777954},\n",
       " {'label': '사회', 'score': 0.5743294358253479}]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# user_id = input(\"허깅페이스 사용자 아이디를 입력하세요: \")\n",
    "model_id = f\"{user_id}/roberta-base-klue-ynat-classification\"\n",
    "\n",
    "model_pipeline = pipeline(\"text-classification\", model=model_id)\n",
    "\n",
    "model_pipeline(dataset[\"title\"][5:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 직접 추론하기\n",
    "\n",
    "- 예제 3.31. 커스텀 파이프라인 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': '경제', 'score': 0.939355194568634},\n",
       " {'label': 'IT과학', 'score': 0.35274696350097656},\n",
       " {'label': 'IT과학', 'score': 0.9206966161727905},\n",
       " {'label': '경제', 'score': 0.9453174471855164},\n",
       " {'label': '사회', 'score': 0.8021110892295837}]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 필요한 라이브러리 임포트\n",
    "import torch\n",
    "from torch.nn.functional import softmax\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "\n",
    "# 커스텀 파이프라인 클래스 정의\n",
    "class CustomPipeline:\n",
    "    def __init__(self, model_id):\n",
    "        # 모델과 토크나이저 초기화\n",
    "        self.model = AutoModelForSequenceClassification.from_pretrained(model_id)\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "        # 모델을 평가 모드로 설정\n",
    "        self.model.eval()\n",
    "\n",
    "    def __call__(self, texts):\n",
    "        # 입력 텍스트를 토큰화\n",
    "        tokenized = self.tokenizer(texts, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "\n",
    "        # 그래디언트 계산 없이 추론 수행\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(**tokenized)\n",
    "            logits = outputs.logits\n",
    "\n",
    "        # softmax를 사용해 확률값으로 변환\n",
    "        probabilities = softmax(logits, dim=-1)\n",
    "        # 가장 높은 확률값과 해당 레이블 인덱스 추출\n",
    "        scores, labels = torch.max(probabilities, dim=-1)\n",
    "        # 레이블 인덱스를 문자열로 변환\n",
    "        labels_str = [self.model.config.id2label[label_idx] for label_idx in labels.tolist()]\n",
    "\n",
    "        # 결과를 딕셔너리 리스트로 반환\n",
    "        return [{\"label\": label, \"score\": score.item()} for label, score in zip(labels_str, scores)]\n",
    "\n",
    "# 커스텀 파이프라인 인스턴스 생성\n",
    "custom_pipeline = CustomPipeline(model_id)\n",
    "# 데이터셋의 처음 5개 타이틀에 대해 추론 수행\n",
    "custom_pipeline(dataset['title'][:5])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lang310",
   "language": "python",
   "name": "lang310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
